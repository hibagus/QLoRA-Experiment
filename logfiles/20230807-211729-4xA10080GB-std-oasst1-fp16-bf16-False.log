+ deepspeed --num_gpus=4 ../../finetune.py --deepspeed ../../deepspeed/distributed.json --method std --profile False --model_name_or_path huggyllama/llama-7b --output_dir ../../output/experiments/8xA10080GB/llama-7b/20230807-211729-std-oasst1-fp16-bf16-False --run_name 20230807-211729-std-oasst1-fp16-bf16-False --logging_steps 16 --save_strategy steps --data_seed 42 --save_steps 0 --save_total_limit 0 --evaluation_strategy steps --eval_dataset_size 1024 --max_eval_samples 1024 --per_device_eval_batch_size 1 --max_new_tokens 32 --dataloader_num_workers 3 --group_by_length --logging_strategy steps --remove_unused_columns False --do_train --do_eval --do_mmlu_eval --mmlu_dataset mmlu-fs --mmlu_path ../../data/mmlu/ --lora_r 64 --lora_alpha 16 --lora_modules all --double_quant False --quant_type fp16 --bf16 --bits 16 --warmup_ratio 0.03 --lr_scheduler_type cosine --gradient_checkpointing --dataset oasst1 --source_max_len 16 --target_max_len 512 --per_device_train_batch_size 1 --gradient_accumulation_steps 16 --max_steps 2048 --eval_steps 256 --learning_rate 0.0002 --adam_beta2 0.999 --max_grad_norm 0.3 --lora_dropout 0.1 --weight_decay 0.0 --seed 0
+ set +x
07 21:17:30,682] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-07 21:17:33,505] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-08-07 21:17:33,505] [INFO] [runner.py:555:main] cmd = /home/bagus/anaconda3/envs/qlora_1/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None ../../finetune.py --deepspeed ../../deepspeed/distributed.json --method std --profile False --model_name_or_path huggyllama/llama-7b --output_dir ../../output/experiments/8xA10080GB/llama-7b/20230807-211729-std-oasst1-fp16-bf16-False --run_name 20230807-211729-std-oasst1-fp16-bf16-False --logging_steps 16 --save_strategy steps --data_seed 42 --save_steps 0 --save_total_limit 0 --evaluation_strategy steps --eval_dataset_size 1024 --max_eval_samples 1024 --per_device_eval_batch_size 1 --max_new_tokens 32 --dataloader_num_workers 3 --group_by_length --logging_strategy steps --remove_unused_columns False --do_train --do_eval --do_mmlu_eval --mmlu_dataset mmlu-fs --mmlu_path ../../data/mmlu/ --lora_r 64 --lora_alpha 16 --lora_modules all --double_quant False --quant_type fp16 --bf16 --bits 16 --warmup_ratio 0.03 --lr_scheduler_type cosine --gradient_checkpointing --dataset oasst1 --source_max_len 16 --target_max_len 512 --per_device_train_batch_size 1 --gradient_accumulation_steps 16 --max_steps 2048 --eval_steps 256 --learning_rate 0.0002 --adam_beta2 0.999 --max_grad_norm 0.3 --lora_dropout 0.1 --weight_decay 0.0 --seed 0
[2023-08-07 21:17:34,993] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-07 21:17:36,441] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2023-08-07 21:17:36,441] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2023-08-07 21:17:36,441] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2023-08-07 21:17:36,441] [INFO] [launch.py:163:main] dist_world_size=4
[2023-08-07 21:17:36,441] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2023-08-07 21:17:39,699] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-07 21:17:39,862] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-07 21:17:39,942] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-07 21:17:39,961] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-07 21:17:40,304] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-08-07 21:17:40,304] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-08-07 21:17:40,477] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-08-07 21:17:40,477] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-08-07 21:17:40,571] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-08-07 21:17:40,571] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-08-07 21:17:40,571] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-08-07 21:17:40,600] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-08-07 21:17:40,600] [INFO] [comm.py:616:init_distributed] cdb=None
loading base model huggyllama/llama-7b...
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]loading base model huggyllama/llama-7b...
loading base model huggyllama/llama-7b...
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.
  warnings.warn(
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.
  warnings.warn(
loading base model huggyllama/llama-7b...
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.
  warnings.warn(
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.74s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.29s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.66s/it]
Base Model Configuration
trainable params: 6738415616 || all params: 6738415616 || trainable: 100.0
torch.bfloat16 6738415616 1.0
Adapter-attached Model Configuration
trainable params: 6738415616 || all params: 6738415616 || trainable: 100.0
After Adapter-attached Data Type
torch.bfloat16 6738415616 1.0
loaded model
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.
  warnings.warn(
You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
Adding special tokens.
Found cached dataset json (/home/bagus/.cache/huggingface/datasets/timdettmers___json/timdettmers--openassistant-guanaco-6126c710748182cf/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 890.98it/s]
Loading cached processed dataset at /home/bagus/.cache/huggingface/datasets/timdettmers___json/timdettmers--openassistant-guanaco-6126c710748182cf/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-c542fff15d9a8e3b.arrow
Loading cached processed dataset at /home/bagus/.cache/huggingface/datasets/timdettmers___json/timdettmers--openassistant-guanaco-6126c710748182cf/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-02d2d738ac61222a.arrow
Splitting train dataset in train and validation according to `eval_dataset_size`
Loading cached split indices for dataset at /home/bagus/.cache/huggingface/datasets/timdettmers___json/timdettmers--openassistant-guanaco-6126c710748182cf/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1864dd694f9ce606.arrow and /home/bagus/.cache/huggingface/datasets/timdettmers___json/timdettmers--openassistant-guanaco-6126c710748182cf/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-6d495243a6f226aa.arrow
Loading cached processed dataset at /home/bagus/.cache/huggingface/datasets/timdettmers___json/timdettmers--openassistant-guanaco-6126c710748182cf/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34432d081bd4bd9a.arrow
Loading cached processed dataset at /home/bagus/.cache/huggingface/datasets/timdettmers___json/timdettmers--openassistant-guanaco-6126c710748182cf/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e41c85337da3c47b.arrow
Found cached dataset json (/home/bagus/.cache/huggingface/datasets/json/default-a1fa006eb0eb7089/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 886.28it/s]
Before Training Model Configuration
trainable params: 6738423808 || all params: 6738423808 || trainable: 100.0
Before Training Data Type
torch.bfloat16 6738423808 1.0
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.26s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.22s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.53s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.22s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.52s/it]
Base Model Configuration
trainable params: 6738415616 || all params: 6738415616 || trainable: 100.0
torch.bfloat16 6738415616 1.0
Adapter-attached Model Configuration
trainable params: 6738415616 || all params: 6738415616 || trainable: 100.0
After Adapter-attached Data Type
torch.bfloat16 6738415616 1.0
loaded model
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.
  warnings.warn(
You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.21s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.51s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.26s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.60s/it]
Base Model Configuration
trainable params: 6738415616 || all params: 6738415616 || trainable: 100.0
torch.bfloat16 6738415616 1.0
Adapter-attached Model Configuration
trainable params: 6738415616 || all params: 6738415616 || trainable: 100.0
After Adapter-attached Data Type
torch.bfloat16 6738415616 1.0
loaded model
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.
  warnings.warn(
You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
Base Model Configuration
trainable params: 6738415616 || all params: 6738415616 || trainable: 100.0
torch.bfloat16 6738415616 1.0
Adapter-attached Model Configuration
trainable params: 6738415616 || all params: 6738415616 || trainable: 100.0
After Adapter-attached Data Type
torch.bfloat16 6738415616 1.0
loaded model
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.
  warnings.warn(
You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
Adding special tokens.
Adding special tokens.
Adding special tokens.
Found cached dataset json (/home/bagus/.cache/huggingface/datasets/timdettmers___json/timdettmers--openassistant-guanaco-6126c710748182cf/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 1040.00it/s]
Loading cached processed dataset at /home/bagus/.cache/huggingface/datasets/timdettmers___json/timdettmers--openassistant-guanaco-6126c710748182cf/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-c542fff15d9a8e3b.arrow
Loading cached processed dataset at /home/bagus/.cache/huggingface/datasets/timdettmers___json/timdettmers--openassistant-guanaco-6126c710748182cf/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-02d2d738ac61222a.arrow
Splitting train dataset in train and validation according to `eval_dataset_size`
Loading cached split indices for dataset at /home/bagus/.cache/huggingface/datasets/timdettmers___json/timdettmers--openassistant-guanaco-6126c710748182cf/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1864dd694f9ce606.arrow and /home/bagus/.cache/huggingface/datasets/timdettmers___json/timdettmers--openassistant-guanaco-6126c710748182cf/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-6d495243a6f226aa.arrow
Loading cached processed dataset at /home/bagus/.cache/huggingface/datasets/timdettmers___json/timdettmers--openassistant-guanaco-6126c710748182cf/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34432d081bd4bd9a.arrow
Loading cached processed dataset at /home/bagus/.cache/huggingface/datasets/timdettmers___json/timdettmers--openassistant-guanaco-6126c710748182cf/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e41c85337da3c47b.arrow
Found cached dataset json (/home/bagus/.cache/huggingface/datasets/timdettmers___json/timdettmers--openassistant-guanaco-6126c710748182cf/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 1127.20it/s]
Loading cached processed dataset at /home/bagus/.cache/huggingface/datasets/timdettmers___json/timdettmers--openassistant-guanaco-6126c710748182cf/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-c542fff15d9a8e3b.arrow
Loading cached processed dataset at /home/bagus/.cache/huggingface/datasets/timdettmers___json/timdettmers--openassistant-guanaco-6126c710748182cf/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-02d2d738ac61222a.arrow
Splitting train dataset in train and validation according to `eval_dataset_size`
Loading cached split indices for dataset at /home/bagus/.cache/huggingface/datasets/timdettmers___json/timdettmers--openassistant-guanaco-6126c710748182cf/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1864dd694f9ce606.arrow and /home/bagus/.cache/huggingface/datasets/timdettmers___json/timdettmers--openassistant-guanaco-6126c710748182cf/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-6d495243a6f226aa.arrow
Loading cached processed dataset at /home/bagus/.cache/huggingface/datasets/timdettmers___json/timdettmers--openassistant-guanaco-6126c710748182cf/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34432d081bd4bd9a.arrow
Loading cached processed dataset at /home/bagus/.cache/huggingface/datasets/timdettmers___json/timdettmers--openassistant-guanaco-6126c710748182cf/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e41c85337da3c47b.arrow
Found cached dataset json (/home/bagus/.cache/huggingface/datasets/timdettmers___json/timdettmers--openassistant-guanaco-6126c710748182cf/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 1166.06it/s]
Loading cached processed dataset at /home/bagus/.cache/huggingface/datasets/timdettmers___json/timdettmers--openassistant-guanaco-6126c710748182cf/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-c542fff15d9a8e3b.arrow
Loading cached processed dataset at /home/bagus/.cache/huggingface/datasets/timdettmers___json/timdettmers--openassistant-guanaco-6126c710748182cf/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-02d2d738ac61222a.arrow
Splitting train dataset in train and validation according to `eval_dataset_size`
Loading cached split indices for dataset at /home/bagus/.cache/huggingface/datasets/timdettmers___json/timdettmers--openassistant-guanaco-6126c710748182cf/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1864dd694f9ce606.arrow and /home/bagus/.cache/huggingface/datasets/timdettmers___json/timdettmers--openassistant-guanaco-6126c710748182cf/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-6d495243a6f226aa.arrow
Loading cached processed dataset at /home/bagus/.cache/huggingface/datasets/timdettmers___json/timdettmers--openassistant-guanaco-6126c710748182cf/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34432d081bd4bd9a.arrow
Loading cached processed dataset at /home/bagus/.cache/huggingface/datasets/timdettmers___json/timdettmers--openassistant-guanaco-6126c710748182cf/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e41c85337da3c47b.arrow
Found cached dataset json (/home/bagus/.cache/huggingface/datasets/json/default-a1fa006eb0eb7089/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 1405.60it/s]
Found cached dataset json (/home/bagus/.cache/huggingface/datasets/json/default-a1fa006eb0eb7089/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 1489.45it/s]
Found cached dataset json (/home/bagus/.cache/huggingface/datasets/json/default-a1fa006eb0eb7089/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 1504.41it/s]
Before Training Model Configuration
trainable params: 6738423808 || all params: 6738423808 || trainable: 100.0
Before Training Data Type
torch.bfloat16 6738423808 1.0
Before Training Model Configuration
trainable params: 6738423808 || all params: 6738423808 || trainable: 100.0
Before Training Data Type
torch.bfloat16 6738423808 1.0
Before Training Model Configuration
trainable params: 6738423808 || all params: 6738423808 || trainable: 100.0
Before Training Data Type
torch.bfloat16 6738423808 1.0
[2023-08-07 21:18:05,005] [WARNING] [engine.py:1140:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****
Rank: 1 partition count [4] and sizes[(1684605952, False)] 
Rank: 3 partition count [4] and sizes[(1684605952, False)] 
Rank: 0 partition count [4] and sizes[(1684605952, False)] 
Rank: 2 partition count [4] and sizes[(1684605952, False)] 
  0%|          | 0/2048 [00:00<?, ?it/s]/home/bagus/pytorch/torch/utils/checkpoint.py:426: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/bagus/pytorch/torch/utils/checkpoint.py:426: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/bagus/pytorch/torch/utils/checkpoint.py:426: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/bagus/pytorch/torch/utils/checkpoint.py:426: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1830: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /home/bagus/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1830: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /home/bagus/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1830: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /home/bagus/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1830: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /home/bagus/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
  0%|          | 1/2048 [00:05<3:18:44,  5.83s/it]  0%|          | 2/2048 [00:10<3:05:29,  5.44s/it]  0%|          | 3/2048 [00:16<3:01:04,  5.31s/it]  0%|          | 4/2048 [00:21<2:58:32,  5.24s/it]  0%|          | 5/2048 [00:26<2:55:50,  5.16s/it]  0%|          | 6/2048 [00:31<2:50:45,  5.02s/it]  0%|          | 7/2048 [00:35<2:45:29,  4.87s/it]  0%|          | 8/2048 [00:40<2:41:37,  4.75s/it]  0%|          | 9/2048 [00:44<2:33:00,  4.50s/it]  0%|          | 10/2048 [00:48<2:27:50,  4.35s/it]  1%|          | 11/2048 [00:51<2:21:50,  4.18s/it]  1%|          | 12/2048 [00:55<2:13:19,  3.93s/it]  1%|          | 13/2048 [00:59<2:14:41,  3.97s/it]  1%|          | 14/2048 [01:04<2:26:54,  4.33s/it]  1%|          | 15/2048 [01:09<2:35:19,  4.58s/it]  1%|          | 16/2048 [01:14<2:40:35,  4.74s/it]                                                   {'loss': 1.4254, 'learning_rate': 5.161290322580645e-05, 'epoch': 0.12}
  1%|          | 16/2048 [01:14<2:40:35,  4.74s/it]  1%|          | 17/2048 [01:19<2:42:58,  4.81s/it]  1%|          | 18/2048 [01:24<2:42:56,  4.82s/it]  1%|          | 19/2048 [01:28<2:39:02,  4.70s/it]  1%|          | 20/2048 [01:33<2:32:59,  4.53s/it]  1%|          | 21/2048 [01:37<2:27:29,  4.37s/it]  1%|          | 22/2048 [01:40<2:21:41,  4.20s/it]  1%|          | 23/2048 [01:45<2:21:45,  4.20s/it]  1%|          | 24/2048 [01:48<2:15:19,  4.01s/it]  1%|          | 25/2048 [01:51<2:06:06,  3.74s/it]  1%|▏         | 26/2048 [01:56<2:20:29,  4.17s/it]  1%|▏         | 27/2048 [02:02<2:30:32,  4.47s/it]  1%|▏         | 28/2048 [02:07<2:37:19,  4.67s/it]  1%|▏         | 29/2048 [02:12<2:41:44,  4.81s/it]  1%|▏         | 30/2048 [02:17<2:43:04,  4.85s/it]  2%|▏         | 31/2048 [02:22<2:41:14,  4.80s/it]  2%|▏         | 32/2048 [02:26<2:36:52,  4.67s/it]                                                   {'loss': 2.382, 'learning_rate': 0.0001032258064516129, 'epoch': 0.23}
  2%|▏         | 32/2048 [02:26<2:36:52,  4.67s/it]  2%|▏         | 33/2048 [02:30<2:29:26,  4.45s/it]  2%|▏         | 34/2048 [02:34<2:23:46,  4.28s/it]  2%|▏         | 35/2048 [02:38<2:20:14,  4.18s/it]  2%|▏         | 36/2048 [02:41<2:15:04,  4.03s/it]  2%|▏         | 37/2048 [02:45<2:09:33,  3.87s/it]  2%|▏         | 38/2048 [02:49<2:11:51,  3.94s/it]  2%|▏         | 39/2048 [02:54<2:24:12,  4.31s/it]  2%|▏         | 40/2048 [02:59<2:32:45,  4.56s/it]  2%|▏         | 41/2048 [03:04<2:38:30,  4.74s/it]  2%|▏         | 42/2048 [03:09<2:41:50,  4.84s/it]  2%|▏         | 43/2048 [03:14<2:42:10,  4.85s/it]  2%|▏         | 44/2048 [03:19<2:40:26,  4.80s/it]  2%|▏         | 45/2048 [03:23<2:35:30,  4.66s/it]  2%|▏         | 46/2048 [03:28<2:31:20,  4.54s/it]  2%|▏         | 47/2048 [03:32<2:26:21,  4.39s/it]  2%|▏         | 48/2048 [03:35<2:20:02,  4.20s/it]                                                   {'loss': 2.6028, 'learning_rate': 0.00015483870967741937, 'epoch': 0.35}
  2%|▏         | 48/2048 [03:35<2:20:02,  4.20s/it]  2%|▏         | 49/2048 [03:39<2:14:42,  4.04s/it]  2%|▏         | 50/2048 [03:42<2:05:32,  3.77s/it]  2%|▏         | 51/2048 [03:47<2:19:27,  4.19s/it]  3%|▎         | 52/2048 [03:53<2:29:09,  4.48s/it]  3%|▎         | 53/2048 [03:58<2:35:48,  4.69s/it]  3%|▎         | 54/2048 [04:03<2:39:59,  4.81s/it]  3%|▎         | 55/2048 [04:08<2:42:36,  4.90s/it]  3%|▎         | 56/2048 [04:13<2:40:46,  4.84s/it]  3%|▎         | 57/2048 [04:17<2:37:42,  4.75s/it]  3%|▎         | 58/2048 [04:21<2:31:15,  4.56s/it]  3%|▎         | 59/2048 [04:25<2:23:54,  4.34s/it]  3%|▎         | 60/2048 [04:29<2:19:09,  4.20s/it]  3%|▎         | 61/2048 [04:33<2:18:00,  4.17s/it]  3%|▎         | 62/2048 [04:37<2:10:51,  3.95s/it]  3%|▎         | 63/2048 [04:41<2:12:36,  4.01s/it]  3%|▎         | 64/2048 [04:46<2:24:03,  4.36s/it]                                                   {'loss': 1.902, 'learning_rate': 0.0001999994995382497, 'epoch': 0.46}
  3%|▎         | 64/2048 [04:46<2:24:03,  4.36s/it]  3%|▎         | 65/2048 [04:51<2:31:53,  4.60s/it]  3%|▎         | 66/2048 [04:56<2:36:53,  4.75s/it]  3%|▎         | 67/2048 [05:01<2:38:43,  4.81s/it]  3%|▎         | 68/2048 [05:06<2:37:07,  4.76s/it]  3%|▎         | 69/2048 [05:10<2:32:31,  4.62s/it]  3%|▎         | 70/2048 [05:14<2:28:33,  4.51s/it]  3%|▎         | 71/2048 [05:18<2:24:14,  4.38s/it]  4%|▎         | 72/2048 [05:22<2:22:07,  4.32s/it]  4%|▎         | 73/2048 [05:26<2:18:41,  4.21s/it]  4%|▎         | 74/2048 [05:30<2:12:23,  4.02s/it]  4%|▎         | 75/2048 [05:33<2:04:15,  3.78s/it]  4%|▎         | 76/2048 [05:38<2:17:55,  4.20s/it]  4%|▍         | 77/2048 [05:44<2:27:26,  4.49s/it]  4%|▍         | 78/2048 [05:49<2:33:55,  4.69s/it]  4%|▍         | 79/2048 [05:54<2:37:58,  4.81s/it]  4%|▍         | 80/2048 [05:59<2:37:51,  4.81s/it]                                                   {'loss': 2.9111, 'learning_rate': 0.00019995946530314385, 'epoch': 0.58}
  4%|▍         | 80/2048 [05:59<2:37:51,  4.81s/it]  4%|▍         | 81/2048 [06:03<2:36:55,  4.79s/it]  4%|▍         | 82/2048 [06:08<2:32:01,  4.64s/it]  4%|▍         | 83/2048 [06:12<2:29:43,  4.57s/it]  4%|▍         | 84/2048 [06:16<2:22:11,  4.34s/it]  4%|▍         | 85/2048 [06:20<2:14:59,  4.13s/it]  4%|▍         | 86/2048 [06:24<2:14:55,  4.13s/it]  4%|▍         | 87/2048 [06:27<2:08:16,  3.92s/it]  4%|▍         | 88/2048 [06:31<2:10:07,  3.98s/it]  4%|▍         | 89/2048 [06:36<2:21:40,  4.34s/it]  4%|▍         | 90/2048 [06:42<2:29:37,  4.58s/it]  4%|▍         | 91/2048 [06:47<2:34:55,  4.75s/it]  4%|▍         | 92/2048 [06:52<2:36:10,  4.79s/it]  5%|▍         | 93/2048 [06:56<2:34:54,  4.75s/it]  5%|▍         | 94/2048 [07:01<2:32:28,  4.68s/it]  5%|▍         | 95/2048 [07:05<2:27:01,  4.52s/it]  5%|▍         | 96/2048 [07:09<2:22:09,  4.37s/it]                                                   {'loss': 1.929, 'learning_rate': 0.00019985540129493763, 'epoch': 0.7}
  5%|▍         | 96/2048 [07:09<2:22:09,  4.37s/it]  5%|▍         | 97/2048 [07:13<2:16:59,  4.21s/it]  5%|▍         | 98/2048 [07:17<2:13:42,  4.11s/it]  5%|▍         | 99/2048 [07:21<2:12:39,  4.08s/it]  5%|▍         | 100/2048 [07:24<2:03:44,  3.81s/it]  5%|▍         | 101/2048 [07:29<2:16:53,  4.22s/it]  5%|▍         | 102/2048 [07:34<2:26:03,  4.50s/it]  5%|▌         | 103/2048 [07:39<2:32:13,  4.70s/it]  5%|▌         | 104/2048 [07:44<2:36:07,  4.82s/it]  5%|▌         | 105/2048 [07:49<2:37:03,  4.85s/it]  5%|▌         | 106/2048 [07:54<2:34:53,  4.79s/it]  5%|▌         | 107/2048 [07:58<2:29:48,  4.63s/it]  5%|▌         | 108/2048 [08:02<2:23:25,  4.44s/it]  5%|▌         | 109/2048 [08:06<2:18:52,  4.30s/it]  5%|▌         | 110/2048 [08:10<2:14:56,  4.18s/it]  5%|▌         | 111/2048 [08:14<2:12:22,  4.10s/it]  5%|▌         | 112/2048 [08:18<2:09:28,  4.01s/it]                                                    {'loss': 1.7367, 'learning_rate': 0.00019968737417259944, 'epoch': 0.81}
  5%|▌         | 112/2048 [08:18<2:09:28,  4.01s/it]  6%|▌         | 113/2048 [08:22<2:09:59,  4.03s/it]  6%|▌         | 114/2048 [08:27<2:20:57,  4.37s/it]  6%|▌         | 115/2048 [08:32<2:28:26,  4.61s/it]  6%|▌         | 116/2048 [08:37<2:33:20,  4.76s/it]  6%|▌         | 117/2048 [08:42<2:34:33,  4.80s/it]  6%|▌         | 118/2048 [08:47<2:33:35,  4.77s/it]  6%|▌         | 119/2048 [08:51<2:30:24,  4.68s/it]  6%|▌         | 120/2048 [08:56<2:25:22,  4.52s/it]  6%|▌         | 121/2048 [09:00<2:21:00,  4.39s/it]  6%|▌         | 122/2048 [09:04<2:15:39,  4.23s/it]  6%|▌         | 123/2048 [09:07<2:13:16,  4.15s/it]  6%|▌         | 124/2048 [09:11<2:08:31,  4.01s/it]  6%|▌         | 125/2048 [09:14<2:00:14,  3.75s/it]  6%|▌         | 126/2048 [09:19<2:13:47,  4.18s/it]  6%|▌         | 127/2048 [09:25<2:23:15,  4.47s/it]  6%|▋         | 128/2048 [09:30<2:29:42,  4.68s/it]                                                    {'loss': 1.5905, 'learning_rate': 0.00019945549156714234, 'epoch': 0.93}
  6%|▋         | 128/2048 [09:30<2:29:42,  4.68s/it]
-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 128:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

world size:                                                   4       
data parallel size:                                           4       
model parallel size:                                          1       
batch size per GPU:                                           1       
params per gpu:                                               6738.42 M
params of model = params per GPU * mp_size:                   6738.42 M
fwd MACs per GPU:                                             3458.42 GMACs
fwd flops per GPU:                                            6917.29 G
fwd flops of model = fwd flops per GPU * mp_size:             6917.29 G
fwd latency:                                                  82.93 ms
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:          83.41 TFLOPS
bwd latency:                                                  238.13 ms
bwd FLOPS per GPU = 2.0 * fwd flops per GPU / bwd latency:    58.1 TFLOPS
fwd+bwd FLOPS per GPU = 3.0 * fwd flops per GPU / (fwd+bwd latency):   64.63 TFLOPS
step latency:                                                 39.94 us
iter latency:                                                 321.11 ms
FLOPS per GPU = 3.0 * fwd flops per GPU / iter latency:       64.63 TFLOPS
samples/second:                                               12.46   

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'LlamaForCausalLM': '6738.42 M'}
    MACs        - {'LlamaForCausalLM': '3458.42 GMACs'}
    fwd latency - {'LlamaForCausalLM': '82.84 ms'}
depth 1:
    params      - {'LlamaModel': '6607.35 M'}
    MACs        - {'LlamaModel': '3391.18 GMACs'}
    fwd latency - {'LlamaModel': '80.13 ms'}
depth 2:
    params      - {'ModuleList': '6476.27 M'}
    MACs        - {'ModuleList': '3391.18 GMACs'}
    fwd latency - {'ModuleList': '74.99 ms'}
depth 3:
    params      - {'LlamaDecoderLayer': '6476.27 M'}
    MACs        - {'LlamaDecoderLayer': '3391.18 GMACs'}
    fwd latency - {'LlamaDecoderLayer': '74.99 ms'}
depth 4:
    params      - {'LlamaMLP': '4328.52 M'}
    MACs        - {'LlamaMLP': '2220.53 GMACs'}
    fwd latency - {'LlamaAttention': '38.83 ms'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

LlamaForCausalLM(
  6738.42 M, 100.00% Params, 3458.42 GMACs, 100.00% MACs, 82.84 ms, 100.00% latency, 83.5 TFLOPS, 
  (model): LlamaModel(
    6607.35 M, 98.05% Params, 3391.18 GMACs, 98.06% MACs, 80.13 ms, 96.73% latency, 84.64 TFLOPS, 
    (embed_tokens): Embedding(131.08 M, 1.95% Params, 0 MACs, 0.00% MACs, 111.1 us, 0.13% latency, 0.0 FLOPS, 32001, 4096)
    (layers): ModuleList(
      (0): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.58 ms, 3.11% latency, 82.26 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.31 ms, 1.58% latency, 56.02 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 169.99 us, 0.21% latency, 101.26 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.18 us, 0.16% latency, 127.33 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 131.37 us, 0.16% latency, 131.03 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 139.95 us, 0.17% latency, 123.0 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 39.82 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 862.12 us, 1.04% latency, 160.99 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 239.85 us, 0.29% latency, 192.88 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 235.08 us, 0.28% latency, 196.79 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 260.11 us, 0.31% latency, 177.85 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.29 us, 0.04% latency, 160.04 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 197.17 us, 0.24% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 97.27 us, 0.12% latency, 0.0 FLOPS, )
      )
      (1): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.37 ms, 2.86% latency, 89.56 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.22 ms, 1.47% latency, 59.98 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 140.43 us, 0.17% latency, 122.58 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.18 us, 0.16% latency, 132.23 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.7 us, 0.16% latency, 132.72 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.38 us, 0.16% latency, 126.22 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.81 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 857.35 us, 1.03% latency, 161.88 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 238.18 us, 0.29% latency, 194.23 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 240.33 us, 0.29% latency, 192.49 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 258.21 us, 0.31% latency, 179.16 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 169.18 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 98.94 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.6 us, 0.11% latency, 0.0 FLOPS, )
      )
      (2): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.37 ms, 2.86% latency, 89.51 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.23 ms, 1.48% latency, 59.5 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 143.53 us, 0.17% latency, 119.93 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 131.37 us, 0.16% latency, 131.03 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.75 us, 0.16% latency, 133.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.09 us, 0.17% latency, 125.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 36.0 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 851.15 us, 1.03% latency, 163.06 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 238.66 us, 0.29% latency, 193.84 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 234.37 us, 0.28% latency, 197.39 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.97 us, 0.31% latency, 179.33 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 169.18 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 99.66 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.12 us, 0.11% latency, 0.0 FLOPS, )
      )
      (3): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.34 ms, 2.82% latency, 90.77 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.46% latency, 60.37 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 138.28 us, 0.17% latency, 124.48 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.18 us, 0.16% latency, 132.23 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.51 us, 0.16% latency, 133.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.9 us, 0.16% latency, 126.66 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 845.19 us, 1.02% latency, 164.21 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.7 us, 0.29% latency, 194.62 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.93 us, 0.28% latency, 198.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.25 us, 0.31% latency, 179.83 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.42 us, 0.04% latency, 174.16 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.46 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.45 us, 0.11% latency, 0.0 FLOPS, )
      )
      (4): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.81% latency, 91.06 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.45% latency, 60.87 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.85 us, 0.17% latency, 125.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.46 us, 0.16% latency, 132.96 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.22 us, 0.16% latency, 133.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.66 us, 0.16% latency, 126.89 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 849.25 us, 1.03% latency, 163.43 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.51 us, 0.29% latency, 195.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.89 us, 0.28% latency, 197.79 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 258.68 us, 0.31% latency, 178.83 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.55 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.36 us, 0.11% latency, 0.0 FLOPS, )
      )
      (5): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.35 ms, 2.84% latency, 90.16 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.22 ms, 1.47% latency, 60.21 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 140.19 us, 0.17% latency, 122.79 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.65 us, 0.16% latency, 131.75 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 127.55 us, 0.15% latency, 134.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.9 us, 0.16% latency, 126.66 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.33 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 853.78 us, 1.03% latency, 162.56 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.7 us, 0.29% latency, 194.62 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 239.85 us, 0.29% latency, 192.88 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.97 us, 0.31% latency, 179.33 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.42 us, 0.04% latency, 174.16 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 95.61 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.69 us, 0.11% latency, 0.0 FLOPS, )
      )
      (6): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.34 ms, 2.83% latency, 90.4 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.22 ms, 1.47% latency, 59.91 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.09 us, 0.17% latency, 125.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 138.52 us, 0.17% latency, 124.27 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.51 us, 0.16% latency, 133.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.42 us, 0.16% latency, 127.11 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.29 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 847.58 us, 1.02% latency, 163.75 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.23 us, 0.29% latency, 195.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.89 us, 0.28% latency, 197.79 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 258.45 us, 0.31% latency, 179.0 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 170.4 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.98 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.45 us, 0.11% latency, 0.0 FLOPS, )
      )
      (7): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.34 ms, 2.83% latency, 90.5 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.22 ms, 1.47% latency, 60.2 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 143.77 us, 0.17% latency, 119.73 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.94 us, 0.16% latency, 132.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.03 us, 0.15% latency, 134.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.61 us, 0.16% latency, 126.0 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.05 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 847.82 us, 1.02% latency, 163.7 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 238.42 us, 0.29% latency, 194.03 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.65 us, 0.28% latency, 197.99 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.02 us, 0.31% latency, 179.99 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 169.18 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.84 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.03 us, 0.11% latency, 0.0 FLOPS, )
      )
      (8): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.35 ms, 2.83% latency, 90.27 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.46% latency, 60.34 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 139.24 us, 0.17% latency, 123.63 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.18 us, 0.16% latency, 132.23 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.51 us, 0.16% latency, 133.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.99 us, 0.16% latency, 128.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.33 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 847.1 us, 1.02% latency, 163.84 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.46 us, 0.29% latency, 194.81 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.89 us, 0.28% latency, 197.79 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.82 us, 0.31% latency, 180.83 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 170.4 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 94.65 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.93 us, 0.11% latency, 0.0 FLOPS, )
      )
      (9): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.81% latency, 91.2 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.45% latency, 60.99 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.38 us, 0.16% latency, 126.22 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.65 us, 0.16% latency, 131.75 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.03 us, 0.15% latency, 134.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.94 us, 0.16% latency, 127.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 849.49 us, 1.03% latency, 163.38 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.23 us, 0.29% latency, 195.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.7 us, 0.29% latency, 194.62 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.3 us, 0.31% latency, 180.5 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.62 us, 0.04% latency, 167.98 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.79 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.78 us, 0.10% latency, 0.0 FLOPS, )
      )
      (10): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.34 ms, 2.83% latency, 90.57 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.46% latency, 60.37 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 139.71 us, 0.17% latency, 123.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.18 us, 0.16% latency, 132.23 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.51 us, 0.16% latency, 133.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.42 us, 0.16% latency, 127.11 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.81 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 847.58 us, 1.02% latency, 163.75 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.94 us, 0.29% latency, 194.42 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.17 us, 0.28% latency, 198.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.78 us, 0.31% latency, 180.16 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 95.13 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.45 us, 0.11% latency, 0.0 FLOPS, )
      )
      (11): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.82% latency, 90.84 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.46% latency, 60.43 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.81 us, 0.17% latency, 124.91 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.18 us, 0.16% latency, 132.23 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.75 us, 0.16% latency, 133.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.71 us, 0.16% latency, 127.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.33 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 847.34 us, 1.02% latency, 163.79 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.46 us, 0.29% latency, 194.81 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.65 us, 0.28% latency, 197.99 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.25 us, 0.31% latency, 179.83 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.62 us, 0.04% latency, 167.98 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.46 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.98 us, 0.11% latency, 0.0 FLOPS, )
      )
      (12): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.81% latency, 90.9 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.46% latency, 60.56 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 142.1 us, 0.17% latency, 121.14 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.41 us, 0.16% latency, 131.99 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.15% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.99 us, 0.16% latency, 128.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 852.58 us, 1.03% latency, 162.79 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 242.23 us, 0.29% latency, 190.98 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.41 us, 0.28% latency, 198.2 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.02 us, 0.31% latency, 179.99 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.62 us, 0.04% latency, 167.98 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.6 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.31 us, 0.10% latency, 0.0 FLOPS, )
      )
      (13): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.35 ms, 2.83% latency, 90.31 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.46% latency, 60.37 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 138.28 us, 0.17% latency, 124.48 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.41 us, 0.16% latency, 131.99 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.75 us, 0.16% latency, 133.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.71 us, 0.16% latency, 127.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.05 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 851.87 us, 1.03% latency, 162.92 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.23 us, 0.29% latency, 195.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 238.9 us, 0.29% latency, 193.65 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 258.45 us, 0.31% latency, 179.0 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 96.56 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.17 us, 0.11% latency, 0.0 FLOPS, )
      )
      (14): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.81% latency, 91.01 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.45% latency, 60.86 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.33 us, 0.17% latency, 125.34 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.22 us, 0.16% latency, 133.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.15% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.71 us, 0.16% latency, 127.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 846.62 us, 1.02% latency, 163.93 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.51 us, 0.29% latency, 195.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.17 us, 0.28% latency, 198.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 258.45 us, 0.31% latency, 179.0 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.04% latency, 166.8 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.46 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.6 us, 0.11% latency, 0.0 FLOPS, )
      )
      (15): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.34 ms, 2.82% latency, 90.74 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.46% latency, 60.4 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 139.24 us, 0.17% latency, 123.63 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.65 us, 0.16% latency, 131.75 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.51 us, 0.16% latency, 133.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.71 us, 0.16% latency, 127.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.62 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 847.1 us, 1.02% latency, 163.84 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 239.13 us, 0.29% latency, 193.45 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.89 us, 0.28% latency, 197.79 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.06 us, 0.31% latency, 180.66 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.42 us, 0.04% latency, 174.16 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.46 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.93 us, 0.11% latency, 0.0 FLOPS, )
      )
      (16): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.80% latency, 91.47 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.45% latency, 60.98 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 138.04 us, 0.17% latency, 124.69 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.7 us, 0.16% latency, 132.72 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 127.79 us, 0.15% latency, 134.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.99 us, 0.16% latency, 128.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.66 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 842.33 us, 1.02% latency, 164.77 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.46 us, 0.29% latency, 194.81 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.93 us, 0.28% latency, 198.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.06 us, 0.31% latency, 180.66 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.66 us, 0.04% latency, 172.89 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.51 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.26 us, 0.11% latency, 0.0 FLOPS, )
      )
      (17): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.82% latency, 90.87 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.46% latency, 60.45 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 143.05 us, 0.17% latency, 120.33 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.46 us, 0.16% latency, 132.96 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.51 us, 0.16% latency, 133.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.71 us, 0.16% latency, 127.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 845.91 us, 1.02% latency, 164.07 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 238.18 us, 0.29% latency, 194.23 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.93 us, 0.28% latency, 198.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.49 us, 0.31% latency, 179.66 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.66 us, 0.04% latency, 172.89 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.31 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.69 us, 0.11% latency, 0.0 FLOPS, )
      )
      (18): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.34 ms, 2.82% latency, 90.74 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.22 ms, 1.47% latency, 60.06 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 138.28 us, 0.17% latency, 124.48 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.41 us, 0.16% latency, 131.99 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.75 us, 0.16% latency, 133.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.66 us, 0.16% latency, 126.89 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 37.19 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 843.52 us, 1.02% latency, 164.53 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.51 us, 0.29% latency, 195.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.93 us, 0.28% latency, 198.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.58 us, 0.31% latency, 181.0 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 170.4 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.74 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.31 us, 0.10% latency, 0.0 FLOPS, )
      )
      (19): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.80% latency, 91.48 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.45% latency, 60.71 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.85 us, 0.17% latency, 125.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.41 us, 0.16% latency, 131.99 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.51 us, 0.16% latency, 133.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.71 us, 0.16% latency, 127.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.29 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 841.62 us, 1.02% latency, 164.91 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.03 us, 0.28% latency, 195.99 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.41 us, 0.28% latency, 198.2 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 254.87 us, 0.31% latency, 181.51 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.66 us, 0.04% latency, 172.89 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.12 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.26 us, 0.11% latency, 0.0 FLOPS, )
      )
      (20): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.81% latency, 91.1 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.45% latency, 60.96 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.33 us, 0.17% latency, 125.34 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.75 us, 0.16% latency, 133.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.75 us, 0.16% latency, 133.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.42 us, 0.16% latency, 127.11 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 849.72 us, 1.03% latency, 163.33 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.27 us, 0.29% latency, 195.8 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 240.33 us, 0.29% latency, 192.49 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.78 us, 0.31% latency, 180.16 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 169.18 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.36 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.31 us, 0.10% latency, 0.0 FLOPS, )
      )
      (21): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.35 ms, 2.84% latency, 90.05 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.22 ms, 1.47% latency, 59.96 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 140.43 us, 0.17% latency, 122.58 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.41 us, 0.16% latency, 131.99 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.51 us, 0.16% latency, 133.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.85 us, 0.17% latency, 125.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.76 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 847.58 us, 1.02% latency, 163.75 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 238.18 us, 0.29% latency, 194.23 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.93 us, 0.28% latency, 198.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.49 us, 0.31% latency, 179.66 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 170.4 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 96.32 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.21 us, 0.11% latency, 0.0 FLOPS, )
      )
      (22): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.34 ms, 2.82% latency, 90.72 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.46% latency, 60.31 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 140.43 us, 0.17% latency, 122.58 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.65 us, 0.16% latency, 131.75 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.15% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.42 us, 0.16% latency, 127.11 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.62 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 849.01 us, 1.02% latency, 163.47 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.46 us, 0.29% latency, 194.81 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.41 us, 0.28% latency, 198.2 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 258.21 us, 0.31% latency, 179.16 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 170.4 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.51 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.02 us, 0.11% latency, 0.0 FLOPS, )
      )
      (23): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.80% latency, 91.33 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.45% latency, 60.85 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.38 us, 0.16% latency, 126.22 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.7 us, 0.16% latency, 132.72 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.51 us, 0.16% latency, 133.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.94 us, 0.16% latency, 127.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.42 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 844.0 us, 1.02% latency, 164.44 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.99 us, 0.29% latency, 195.2 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.93 us, 0.28% latency, 198.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.3 us, 0.31% latency, 180.5 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.08 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.69 us, 0.11% latency, 0.0 FLOPS, )
      )
      (24): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.35 ms, 2.83% latency, 90.27 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.22 ms, 1.47% latency, 59.96 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 139.95 us, 0.17% latency, 123.0 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.65 us, 0.16% latency, 131.75 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.22 us, 0.16% latency, 133.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.42 us, 0.16% latency, 127.11 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.52 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 847.82 us, 1.02% latency, 163.7 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 238.18 us, 0.29% latency, 194.23 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 231.98 us, 0.28% latency, 199.42 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.97 us, 0.31% latency, 179.33 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.04% latency, 165.63 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 95.61 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.98 us, 0.11% latency, 0.0 FLOPS, )
      )
      (25): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.81% latency, 91.17 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.45% latency, 60.73 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.81 us, 0.17% latency, 124.91 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.65 us, 0.16% latency, 131.75 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.98 us, 0.16% latency, 133.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.47 us, 0.16% latency, 128.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.62 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 844.24 us, 1.02% latency, 164.4 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.46 us, 0.29% latency, 194.81 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.17 us, 0.28% latency, 198.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.3 us, 0.31% latency, 180.5 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.62 us, 0.04% latency, 167.98 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.03 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.17 us, 0.11% latency, 0.0 FLOPS, )
      )
      (26): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.82% latency, 90.83 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.46% latency, 60.44 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.14 us, 0.16% latency, 126.44 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.41 us, 0.16% latency, 131.99 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.51 us, 0.16% latency, 133.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.57 us, 0.17% latency, 125.13 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.76 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 844.72 us, 1.02% latency, 164.3 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 238.18 us, 0.29% latency, 194.23 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.7 us, 0.28% latency, 198.8 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.3 us, 0.31% latency, 180.5 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.51 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.88 us, 0.11% latency, 0.0 FLOPS, )
      )
      (27): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.34 ms, 2.82% latency, 90.65 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.22 ms, 1.47% latency, 60.16 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 143.05 us, 0.17% latency, 120.33 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.94 us, 0.16% latency, 132.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.75 us, 0.16% latency, 133.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.71 us, 0.16% latency, 127.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 844.0 us, 1.02% latency, 164.44 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.7 us, 0.29% latency, 194.62 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.41 us, 0.28% latency, 198.2 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.11 us, 0.31% latency, 181.34 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.62 us, 0.04% latency, 167.98 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.51 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.41 us, 0.11% latency, 0.0 FLOPS, )
      )
      (28): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.81% latency, 91.09 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.45% latency, 60.86 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.14 us, 0.16% latency, 126.44 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.18 us, 0.16% latency, 132.23 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.15% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.18 us, 0.16% latency, 127.33 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 850.92 us, 1.03% latency, 163.11 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.75 us, 0.29% latency, 195.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.17 us, 0.28% latency, 198.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 263.45 us, 0.32% latency, 175.6 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.31 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.74 us, 0.11% latency, 0.0 FLOPS, )
      )
      (29): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.81% latency, 90.98 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.46% latency, 60.5 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.81 us, 0.17% latency, 124.91 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.7 us, 0.16% latency, 132.72 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.98 us, 0.16% latency, 133.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.85 us, 0.17% latency, 125.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 844.96 us, 1.02% latency, 164.26 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.46 us, 0.29% latency, 194.81 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.93 us, 0.28% latency, 198.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.25 us, 0.31% latency, 179.83 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.84 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.21 us, 0.11% latency, 0.0 FLOPS, )
      )
      (30): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.81% latency, 91.03 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.45% latency, 60.77 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.85 us, 0.17% latency, 125.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.94 us, 0.16% latency, 132.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.15% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.47 us, 0.16% latency, 128.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 845.91 us, 1.02% latency, 164.07 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.75 us, 0.29% latency, 195.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.7 us, 0.28% latency, 198.8 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.73 us, 0.31% latency, 179.49 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 170.4 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.7 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.5 us, 0.11% latency, 0.0 FLOPS, )
      )
      (31): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.80% latency, 91.34 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.45% latency, 60.88 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.38 us, 0.16% latency, 126.22 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.65 us, 0.16% latency, 131.75 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.15% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.47 us, 0.16% latency, 128.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 843.76 us, 1.02% latency, 164.49 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.03 us, 0.28% latency, 195.99 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.7 us, 0.28% latency, 198.8 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.49 us, 0.31% latency, 179.66 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.84 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.74 us, 0.11% latency, 0.0 FLOPS, )
      )
    )
    (norm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 100.14 us, 0.12% latency, 0.0 FLOPS, )
  )
  (lm_head): Linear(131.08 M, 1.95% Params, 67.24 GMACs, 1.94% MACs, 2.37 ms, 2.86% latency, 56.74 TFLOPS, in_features=4096, out_features=32001, bias=False)
)
------------------------------------------------------------------------------

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 128:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

world size:                                                   4       
data parallel size:                                           4       
model parallel size:                                          1       
batch size per GPU:                                           1       
params per gpu:                                               6738.42 M
params of model = params per GPU * mp_size:                   6738.42 M
fwd MACs per GPU:                                             3458.42 GMACs
fwd flops per GPU:                                            6917.29 G
fwd flops of model = fwd flops per GPU * mp_size:             6917.29 G
fwd latency:                                                  82.48 ms
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:          83.86 TFLOPS
bwd latency:                                                  244.73 ms
bwd FLOPS per GPU = 2.0 * fwd flops per GPU / bwd latency:    56.53 TFLOPS
fwd+bwd FLOPS per GPU = 3.0 * fwd flops per GPU / (fwd+bwd latency):   63.42 TFLOPS
step latency:                                                 30.72 us
iter latency:                                                 327.25 ms
FLOPS per GPU = 3.0 * fwd flops per GPU / iter latency:       63.41 TFLOPS
samples/second:                                               12.22   

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'LlamaForCausalLM': '6738.42 M'}
    MACs        - {'LlamaForCausalLM': '3458.42 GMACs'}
    fwd latency - {'LlamaForCausalLM': '82.39 ms'}
depth 1:
    params      - {'LlamaModel': '6607.35 M'}
    MACs        - {'LlamaModel': '3391.18 GMACs'}
    fwd latency - {'LlamaModel': '79.71 ms'}
depth 2:
    params      - {'ModuleList': '6476.27 M'}
    MACs        - {'ModuleList': '3391.18 GMACs'}
    fwd latency - {'ModuleList': '74.65 ms'}
depth 3:
    params      - {'LlamaDecoderLayer': '6476.27 M'}
    MACs        - {'LlamaDecoderLayer': '3391.18 GMACs'}
    fwd latency - {'LlamaDecoderLayer': '74.65 ms'}
depth 4:
    params      - {'LlamaMLP': '4328.52 M'}
    MACs        - {'LlamaMLP': '2220.53 GMACs'}
    fwd latency - {'LlamaAttention': '38.66 ms'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

LlamaForCausalLM(
  6738.42 M, 100.00% Params, 3458.42 GMACs, 100.00% MACs, 82.39 ms, 100.00% latency, 83.95 TFLOPS, 
  (model): LlamaModel(
    6607.35 M, 98.05% Params, 3391.18 GMACs, 98.06% MACs, 79.71 ms, 96.74% latency, 85.1 TFLOPS, 
    (embed_tokens): Embedding(131.08 M, 1.95% Params, 0 MACs, 0.00% MACs, 75.34 us, 0.09% latency, 0.0 FLOPS, 32001, 4096)
    (layers): ModuleList(
      (0): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.49 ms, 3.03% latency, 85.03 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.27 ms, 1.55% latency, 57.47 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 167.61 us, 0.20% latency, 102.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.75 us, 0.16% latency, 128.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.18 us, 0.16% latency, 132.23 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 138.76 us, 0.17% latency, 124.05 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.76 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 865.46 us, 1.05% latency, 160.36 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 240.09 us, 0.29% latency, 192.68 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 240.09 us, 0.29% latency, 192.68 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 260.35 us, 0.32% latency, 177.69 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.81 us, 0.04% latency, 162.23 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 149.49 us, 0.18% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 96.56 us, 0.12% latency, 0.0 FLOPS, )
      )
      (1): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.35 ms, 2.85% latency, 90.14 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.47% latency, 60.31 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 139.0 us, 0.17% latency, 123.84 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.65 us, 0.16% latency, 131.75 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.51 us, 0.16% latency, 133.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.66 us, 0.16% latency, 126.89 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.33 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 849.49 us, 1.03% latency, 163.38 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 238.18 us, 0.29% latency, 194.23 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.65 us, 0.28% latency, 197.99 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.02 us, 0.31% latency, 179.99 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.33 us, 0.04% latency, 164.48 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 100.14 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.31 us, 0.11% latency, 0.0 FLOPS, )
      )
      (2): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.82% latency, 91.08 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.8 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.81 us, 0.17% latency, 124.91 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.94 us, 0.16% latency, 132.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.98 us, 0.16% latency, 133.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.66 us, 0.16% latency, 126.89 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.62 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 847.34 us, 1.03% latency, 163.79 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 238.18 us, 0.29% latency, 194.23 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.93 us, 0.28% latency, 198.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.73 us, 0.31% latency, 179.49 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.04% latency, 165.63 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 94.18 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.26 us, 0.11% latency, 0.0 FLOPS, )
      )
      (3): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.36 ms, 2.86% latency, 89.98 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.22 ms, 1.48% latency, 59.85 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 140.91 us, 0.17% latency, 122.16 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.89 us, 0.16% latency, 131.51 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.75 us, 0.16% latency, 133.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.85 us, 0.17% latency, 125.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.05 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 848.53 us, 1.03% latency, 163.56 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 238.9 us, 0.29% latency, 193.65 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.17 us, 0.28% latency, 198.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.49 us, 0.31% latency, 179.66 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.62 us, 0.04% latency, 167.98 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 97.75 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.17 us, 0.11% latency, 0.0 FLOPS, )
      )
      (4): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.34 ms, 2.84% latency, 90.57 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.47% latency, 60.37 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 138.76 us, 0.17% latency, 124.05 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.89 us, 0.16% latency, 131.51 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.98 us, 0.16% latency, 133.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 139.71 us, 0.17% latency, 123.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 846.86 us, 1.03% latency, 163.89 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 238.66 us, 0.29% latency, 193.84 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.17 us, 0.28% latency, 198.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.02 us, 0.31% latency, 179.99 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 170.4 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 100.85 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.74 us, 0.11% latency, 0.0 FLOPS, )
      )
      (5): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.82% latency, 91.17 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.8 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.38 us, 0.17% latency, 126.22 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.7 us, 0.16% latency, 132.72 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.03 us, 0.16% latency, 134.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.47 us, 0.16% latency, 128.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.81 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 847.82 us, 1.03% latency, 163.7 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 238.18 us, 0.29% latency, 194.23 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 234.13 us, 0.28% latency, 197.59 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.78 us, 0.31% latency, 180.16 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 170.4 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.08 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.21 us, 0.11% latency, 0.0 FLOPS, )
      )
      (6): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.83% latency, 90.99 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.47% latency, 60.59 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 139.0 us, 0.17% latency, 123.84 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 131.13 us, 0.16% latency, 131.27 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.75 us, 0.16% latency, 133.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.18 us, 0.16% latency, 127.33 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.33 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 846.62 us, 1.03% latency, 163.93 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.7 us, 0.29% latency, 194.62 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.93 us, 0.28% latency, 198.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.97 us, 0.31% latency, 179.33 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 94.41 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.31 us, 0.10% latency, 0.0 FLOPS, )
      )
      (7): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.83% latency, 90.85 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.47% latency, 60.29 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.85 us, 0.17% latency, 125.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.65 us, 0.16% latency, 131.75 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.46 us, 0.16% latency, 132.96 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.38 us, 0.17% latency, 126.22 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 844.96 us, 1.03% latency, 164.26 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.23 us, 0.29% latency, 195.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.89 us, 0.28% latency, 197.79 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.02 us, 0.31% latency, 179.99 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 170.4 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.74 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.02 us, 0.11% latency, 0.0 FLOPS, )
      )
      (8): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.83% latency, 90.87 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.46% latency, 60.7 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.81 us, 0.17% latency, 124.91 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.65 us, 0.16% latency, 131.75 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.98 us, 0.16% latency, 133.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.85 us, 0.17% latency, 125.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 850.2 us, 1.03% latency, 163.24 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.94 us, 0.29% latency, 194.42 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 234.13 us, 0.28% latency, 197.59 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 258.92 us, 0.31% latency, 178.67 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.04% latency, 165.63 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.98 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.93 us, 0.11% latency, 0.0 FLOPS, )
      )
      (9): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.34 ms, 2.84% latency, 90.63 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.47% latency, 60.25 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 138.76 us, 0.17% latency, 124.05 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.89 us, 0.16% latency, 131.51 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.16% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.71 us, 0.16% latency, 127.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.05 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 848.29 us, 1.03% latency, 163.61 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.7 us, 0.29% latency, 194.62 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.17 us, 0.28% latency, 198.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.3 us, 0.31% latency, 180.5 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.7 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.26 us, 0.11% latency, 0.0 FLOPS, )
      )
      (10): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.82% latency, 91.29 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.84 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.61 us, 0.17% latency, 126.0 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.94 us, 0.16% latency, 132.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.16% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.18 us, 0.16% latency, 127.33 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 845.19 us, 1.03% latency, 164.21 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.75 us, 0.29% latency, 195.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.89 us, 0.28% latency, 197.79 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.06 us, 0.31% latency, 180.66 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.42 us, 0.04% latency, 174.16 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.27 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.02 us, 0.11% latency, 0.0 FLOPS, )
      )
      (11): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.81% latency, 91.43 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.96 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.61 us, 0.17% latency, 126.0 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.89 us, 0.16% latency, 131.51 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.75 us, 0.16% latency, 133.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.99 us, 0.16% latency, 128.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 845.67 us, 1.03% latency, 164.12 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.46 us, 0.29% latency, 194.81 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.41 us, 0.28% latency, 198.2 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.3 us, 0.31% latency, 180.5 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.55 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.07 us, 0.10% latency, 0.0 FLOPS, )
      )
      (12): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.34 ms, 2.85% latency, 90.41 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.22 ms, 1.48% latency, 59.97 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 140.19 us, 0.17% latency, 122.79 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 131.37 us, 0.16% latency, 131.03 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.22 us, 0.16% latency, 133.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.9 us, 0.16% latency, 126.66 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.05 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 844.48 us, 1.02% latency, 164.35 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.46 us, 0.29% latency, 194.81 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.41 us, 0.28% latency, 198.2 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.06 us, 0.31% latency, 180.66 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 170.4 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 96.32 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.45 us, 0.11% latency, 0.0 FLOPS, )
      )
      (13): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.83% latency, 90.88 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.47% latency, 60.52 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 138.28 us, 0.17% latency, 124.48 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.89 us, 0.16% latency, 131.51 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.03 us, 0.16% latency, 134.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.28 us, 0.16% latency, 129.16 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.81 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 844.72 us, 1.03% latency, 164.3 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.23 us, 0.29% latency, 195.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.17 us, 0.28% latency, 198.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.06 us, 0.31% latency, 180.66 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.66 us, 0.04% latency, 172.89 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.98 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.98 us, 0.11% latency, 0.0 FLOPS, )
      )
      (14): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.82% latency, 91.22 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.73 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.33 us, 0.17% latency, 125.34 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.46 us, 0.16% latency, 132.96 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.51 us, 0.16% latency, 133.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.14 us, 0.17% latency, 126.44 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.33 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 844.72 us, 1.03% latency, 164.3 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.46 us, 0.29% latency, 194.81 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.41 us, 0.28% latency, 198.2 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.3 us, 0.31% latency, 180.5 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.66 us, 0.04% latency, 172.89 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.79 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.69 us, 0.11% latency, 0.0 FLOPS, )
      )
      (15): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.82% latency, 91.23 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.46% latency, 60.67 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.85 us, 0.17% latency, 125.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.18 us, 0.16% latency, 132.23 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.16% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.94 us, 0.16% latency, 127.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.29 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 842.57 us, 1.02% latency, 164.72 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.99 us, 0.29% latency, 195.2 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.46 us, 0.28% latency, 199.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.58 us, 0.31% latency, 181.0 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.27 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.26 us, 0.11% latency, 0.0 FLOPS, )
      )
      (16): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.82% latency, 91.33 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.8 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.61 us, 0.17% latency, 126.0 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.94 us, 0.16% latency, 132.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.03 us, 0.16% latency, 134.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.71 us, 0.16% latency, 127.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.29 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 843.52 us, 1.02% latency, 164.53 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.27 us, 0.29% latency, 195.8 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.89 us, 0.28% latency, 197.79 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.49 us, 0.31% latency, 179.66 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.19 us, 0.04% latency, 175.45 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.94 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.07 us, 0.10% latency, 0.0 FLOPS, )
      )
      (17): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.82% latency, 91.07 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.47% latency, 60.59 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.57 us, 0.17% latency, 125.13 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.41 us, 0.16% latency, 131.99 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.98 us, 0.16% latency, 133.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.71 us, 0.16% latency, 127.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.33 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 843.52 us, 1.02% latency, 164.53 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.99 us, 0.29% latency, 195.2 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.17 us, 0.28% latency, 198.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.3 us, 0.31% latency, 180.5 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 95.61 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.78 us, 0.11% latency, 0.0 FLOPS, )
      )
      (18): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.34 ms, 2.84% latency, 90.47 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.22 ms, 1.48% latency, 60.04 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 139.71 us, 0.17% latency, 123.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.41 us, 0.16% latency, 131.99 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.51 us, 0.16% latency, 133.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.85 us, 0.17% latency, 125.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 847.34 us, 1.03% latency, 163.79 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 238.18 us, 0.29% latency, 194.23 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.41 us, 0.28% latency, 198.2 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.02 us, 0.31% latency, 179.99 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 169.18 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.98 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.5 us, 0.11% latency, 0.0 FLOPS, )
      )
      (19): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.82% latency, 91.38 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.88 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.61 us, 0.17% latency, 126.0 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.7 us, 0.16% latency, 132.72 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.75 us, 0.16% latency, 133.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.94 us, 0.16% latency, 127.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 845.19 us, 1.03% latency, 164.21 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.23 us, 0.29% latency, 195.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.17 us, 0.28% latency, 198.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.78 us, 0.31% latency, 180.16 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.51 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.07 us, 0.10% latency, 0.0 FLOPS, )
      )
      (20): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.31 ms, 2.80% latency, 91.78 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.45% latency, 61.04 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.61 us, 0.17% latency, 126.0 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.7 us, 0.16% latency, 132.72 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.16% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.71 us, 0.16% latency, 127.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 840.19 us, 1.02% latency, 165.19 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.03 us, 0.29% latency, 195.99 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.22 us, 0.28% latency, 199.21 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.82 us, 0.31% latency, 180.83 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.66 us, 0.04% latency, 172.89 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.08 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.78 us, 0.11% latency, 0.0 FLOPS, )
      )
      (21): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.31 ms, 2.80% latency, 91.77 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.19 ms, 1.45% latency, 61.28 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.42 us, 0.16% latency, 127.11 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.22 us, 0.16% latency, 133.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 127.55 us, 0.15% latency, 134.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.47 us, 0.16% latency, 128.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 846.62 us, 1.03% latency, 163.93 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.51 us, 0.29% latency, 195.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.46 us, 0.28% latency, 199.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 260.59 us, 0.32% latency, 177.52 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 170.4 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.12 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.02 us, 0.11% latency, 0.0 FLOPS, )
      )
      (22): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.34 ms, 2.84% latency, 90.63 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.22 ms, 1.48% latency, 60.2 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 140.43 us, 0.17% latency, 122.58 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.94 us, 0.16% latency, 132.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 127.79 us, 0.16% latency, 134.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.66 us, 0.16% latency, 126.89 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 842.81 us, 1.02% latency, 164.67 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.51 us, 0.29% latency, 195.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.41 us, 0.28% latency, 198.2 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.82 us, 0.31% latency, 180.83 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 169.18 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 95.37 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.65 us, 0.11% latency, 0.0 FLOPS, )
      )
      (23): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.82% latency, 91.23 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.97 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.9 us, 0.16% latency, 126.66 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.98 us, 0.16% latency, 133.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 127.55 us, 0.15% latency, 134.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.75 us, 0.16% latency, 128.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 842.57 us, 1.02% latency, 164.72 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.23 us, 0.29% latency, 195.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.22 us, 0.28% latency, 199.21 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.06 us, 0.31% latency, 180.66 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 170.4 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.55 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 94.65 us, 0.11% latency, 0.0 FLOPS, )
      )
      (24): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.81% latency, 91.55 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 61.03 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.61 us, 0.17% latency, 126.0 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.18 us, 0.16% latency, 132.23 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.03 us, 0.16% latency, 134.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.23 us, 0.16% latency, 128.24 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 838.04 us, 1.02% latency, 165.61 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.27 us, 0.29% latency, 195.8 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 231.74 us, 0.28% latency, 199.62 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.82 us, 0.31% latency, 180.83 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.46 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.31 us, 0.10% latency, 0.0 FLOPS, )
      )
      (25): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.82% latency, 91.12 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.8 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.66 us, 0.16% latency, 126.89 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.46 us, 0.16% latency, 132.96 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.16% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.47 us, 0.16% latency, 128.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 844.72 us, 1.03% latency, 164.3 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.23 us, 0.29% latency, 195.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.93 us, 0.28% latency, 198.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.78 us, 0.31% latency, 180.16 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 170.4 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.79 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.36 us, 0.11% latency, 0.0 FLOPS, )
      )
      (26): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.31 ms, 2.81% latency, 91.61 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.45% latency, 61.09 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.38 us, 0.17% latency, 126.22 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.98 us, 0.16% latency, 133.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.51 us, 0.16% latency, 133.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.23 us, 0.16% latency, 128.24 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 841.14 us, 1.02% latency, 165.0 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.03 us, 0.29% latency, 195.99 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.22 us, 0.28% latency, 199.21 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.11 us, 0.31% latency, 181.34 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.42 us, 0.04% latency, 174.16 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.46 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.26 us, 0.11% latency, 0.0 FLOPS, )
      )
      (27): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.31 ms, 2.80% latency, 91.8 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.19 ms, 1.45% latency, 61.31 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.9 us, 0.16% latency, 126.66 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.7 us, 0.16% latency, 132.72 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.03 us, 0.16% latency, 134.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.51 us, 0.16% latency, 128.93 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.62 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 843.29 us, 1.02% latency, 164.58 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.03 us, 0.29% latency, 195.99 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 231.74 us, 0.28% latency, 199.62 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.11 us, 0.31% latency, 181.34 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 169.18 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.84 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.26 us, 0.11% latency, 0.0 FLOPS, )
      )
      (28): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.81% latency, 91.52 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.19 ms, 1.45% latency, 61.24 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.42 us, 0.16% latency, 127.11 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.46 us, 0.16% latency, 132.96 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.03 us, 0.16% latency, 134.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.75 us, 0.16% latency, 128.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 844.48 us, 1.02% latency, 164.35 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.51 us, 0.29% latency, 195.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.46 us, 0.28% latency, 199.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.3 us, 0.31% latency, 180.5 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.62 us, 0.04% latency, 167.98 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.74 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.5 us, 0.11% latency, 0.0 FLOPS, )
      )
      (29): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.34 ms, 2.84% latency, 90.61 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.47% latency, 60.27 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.57 us, 0.17% latency, 125.13 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 131.37 us, 0.16% latency, 131.03 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.16% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.94 us, 0.16% latency, 127.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.81 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 841.14 us, 1.02% latency, 165.0 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.27 us, 0.29% latency, 195.8 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.22 us, 0.28% latency, 199.21 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 254.63 us, 0.31% latency, 181.68 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 169.18 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 98.47 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.36 us, 0.11% latency, 0.0 FLOPS, )
      )
      (30): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.83% latency, 91.06 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.91 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.33 us, 0.17% latency, 125.34 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.94 us, 0.16% latency, 132.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 127.32 us, 0.15% latency, 135.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.99 us, 0.16% latency, 128.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 841.38 us, 1.02% latency, 164.95 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 235.8 us, 0.29% latency, 196.19 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.93 us, 0.28% latency, 198.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.58 us, 0.31% latency, 181.0 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.46 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.74 us, 0.11% latency, 0.0 FLOPS, )
      )
      (31): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.82% latency, 91.26 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.79 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.09 us, 0.17% latency, 125.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.94 us, 0.16% latency, 132.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.03 us, 0.16% latency, 134.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.42 us, 0.16% latency, 127.11 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.33 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 842.57 us, 1.02% latency, 164.72 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.75 us, 0.29% latency, 195.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.22 us, 0.28% latency, 199.21 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.58 us, 0.31% latency, 181.0 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.6 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.17 us, 0.11% latency, 0.0 FLOPS, )
      )
    )
    (norm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 99.18 us, 0.12% latency, 0.0 FLOPS, )
  )
  (lm_head): Linear(131.08 M, 1.95% Params, 67.24 GMACs, 1.94% MACs, 2.36 ms, 2.86% latency, 57.09 TFLOPS, in_features=4096, out_features=32001, bias=False)
)
------------------------------------------------------------------------------

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 128:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

world size:                                                   4       
data parallel size:                                           4       
model parallel size:                                          1       
batch size per GPU:                                           1       
params per gpu:                                               6738.42 M
params of model = params per GPU * mp_size:                   6738.42 M
fwd MACs per GPU:                                             3458.42 GMACs
fwd flops per GPU:                                            6917.29 G
fwd flops of model = fwd flops per GPU * mp_size:             6917.29 G
fwd latency:                                                  82.59 ms
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:          83.76 TFLOPS
bwd latency:                                                  244.45 ms
bwd FLOPS per GPU = 2.0 * fwd flops per GPU / bwd latency:    56.6 TFLOPS
fwd+bwd FLOPS per GPU = 3.0 * fwd flops per GPU / (fwd+bwd latency):   63.45 TFLOPS
step latency:                                                 29.7 us 
iter latency:                                                 327.06 ms
FLOPS per GPU = 3.0 * fwd flops per GPU / iter latency:       63.45 TFLOPS
samples/second:                                               12.23   

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'LlamaForCausalLM': '6738.42 M'}
    MACs        - {'LlamaForCausalLM': '3458.42 GMACs'}
    fwd latency - {'LlamaForCausalLM': '82.5 ms'}
depth 1:
    params      - {'LlamaModel': '6607.35 M'}
    MACs        - {'LlamaModel': '3391.18 GMACs'}
    fwd latency - {'LlamaModel': '79.8 ms'}
depth 2:
    params      - {'ModuleList': '6476.27 M'}
    MACs        - {'ModuleList': '3391.18 GMACs'}
    fwd latency - {'ModuleList': '74.76 ms'}
depth 3:
    params      - {'LlamaDecoderLayer': '6476.27 M'}
    MACs        - {'LlamaDecoderLayer': '3391.18 GMACs'}
    fwd latency - {'LlamaDecoderLayer': '74.76 ms'}
depth 4:
    params      - {'LlamaMLP': '4328.52 M'}
    MACs        - {'LlamaMLP': '2220.53 GMACs'}
    fwd latency - {'LlamaAttention': '38.7 ms'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

LlamaForCausalLM(
  6738.42 M, 100.00% Params, 3458.42 GMACs, 100.00% MACs, 82.5 ms, 100.00% latency, 83.85 TFLOPS, 
  (model): LlamaModel(
    6607.35 M, 98.05% Params, 3391.18 GMACs, 98.06% MACs, 79.8 ms, 96.73% latency, 84.99 TFLOPS, 
    (embed_tokens): Embedding(131.08 M, 1.95% Params, 0 MACs, 0.00% MACs, 72.48 us, 0.09% latency, 0.0 FLOPS, 32001, 4096)
    (layers): ModuleList(
      (0): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.52 ms, 3.06% latency, 84.05 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.27 ms, 1.54% latency, 57.41 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 171.42 us, 0.21% latency, 100.41 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.18 us, 0.16% latency, 127.33 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.75 us, 0.16% latency, 133.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 138.28 us, 0.17% latency, 124.48 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 37.43 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 860.45 us, 1.04% latency, 161.3 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 240.8 us, 0.29% latency, 192.11 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 235.08 us, 0.28% latency, 196.79 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 259.64 us, 0.31% latency, 178.18 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.29 us, 0.04% latency, 160.04 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 183.34 us, 0.22% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 97.51 us, 0.12% latency, 0.0 FLOPS, )
      )
      (1): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.36 ms, 2.86% latency, 89.77 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.22 ms, 1.48% latency, 60.12 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 138.28 us, 0.17% latency, 124.48 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.65 us, 0.16% latency, 131.75 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.98 us, 0.16% latency, 133.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 139.71 us, 0.17% latency, 123.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.81 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 854.73 us, 1.04% latency, 162.38 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 240.56 us, 0.29% latency, 192.3 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 234.84 us, 0.28% latency, 196.99 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 258.92 us, 0.31% latency, 178.67 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.04% latency, 166.8 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 97.51 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 94.41 us, 0.11% latency, 0.0 FLOPS, )
      )
      (2): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.34 ms, 2.84% latency, 90.55 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.47% latency, 60.3 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 138.28 us, 0.17% latency, 124.48 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.7 us, 0.16% latency, 132.72 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.99 us, 0.16% latency, 128.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.66 us, 0.16% latency, 126.89 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.05 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 847.1 us, 1.03% latency, 163.84 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 238.18 us, 0.29% latency, 194.23 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.65 us, 0.28% latency, 197.99 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.02 us, 0.31% latency, 179.99 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 170.4 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 96.32 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.69 us, 0.11% latency, 0.0 FLOPS, )
      )
      (3): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.82% latency, 91.06 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.46% latency, 60.67 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.81 us, 0.17% latency, 124.91 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.18 us, 0.16% latency, 132.23 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.75 us, 0.16% latency, 133.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.18 us, 0.16% latency, 127.33 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 845.91 us, 1.03% latency, 164.07 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.7 us, 0.29% latency, 194.62 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.17 us, 0.28% latency, 198.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.25 us, 0.31% latency, 179.83 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 169.18 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 94.89 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.26 us, 0.11% latency, 0.0 FLOPS, )
      )
      (4): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.83% latency, 90.89 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.86 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.85 us, 0.17% latency, 125.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.94 us, 0.16% latency, 132.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.51 us, 0.16% latency, 133.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.18 us, 0.16% latency, 127.33 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 849.25 us, 1.03% latency, 163.43 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 238.9 us, 0.29% latency, 193.65 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 234.6 us, 0.28% latency, 197.19 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.02 us, 0.31% latency, 179.99 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.04% latency, 166.8 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.03 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.74 us, 0.11% latency, 0.0 FLOPS, )
      )
      (5): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.34 ms, 2.83% latency, 90.63 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.22 ms, 1.47% latency, 60.21 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 138.28 us, 0.17% latency, 124.48 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.18 us, 0.16% latency, 132.23 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.51 us, 0.16% latency, 133.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.18 us, 0.16% latency, 127.33 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 36.24 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 845.19 us, 1.02% latency, 164.21 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.23 us, 0.29% latency, 195.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.17 us, 0.28% latency, 198.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.54 us, 0.31% latency, 180.33 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 170.4 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 95.13 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.45 us, 0.11% latency, 0.0 FLOPS, )
      )
      (6): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.81% latency, 91.37 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.79 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.14 us, 0.17% latency, 126.44 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.94 us, 0.16% latency, 132.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.51 us, 0.16% latency, 133.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.71 us, 0.16% latency, 127.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.62 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 843.29 us, 1.02% latency, 164.58 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.23 us, 0.29% latency, 195.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.65 us, 0.28% latency, 197.99 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.06 us, 0.31% latency, 180.66 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 170.4 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.79 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.5 us, 0.11% latency, 0.0 FLOPS, )
      )
      (7): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.82% latency, 91.14 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.73 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.85 us, 0.17% latency, 125.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.22 us, 0.16% latency, 133.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.51 us, 0.16% latency, 133.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.18 us, 0.16% latency, 127.33 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.62 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 847.82 us, 1.03% latency, 163.7 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 238.18 us, 0.29% latency, 194.23 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 234.13 us, 0.28% latency, 197.59 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.02 us, 0.31% latency, 179.99 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 170.4 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.12 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.45 us, 0.11% latency, 0.0 FLOPS, )
      )
      (8): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.83% latency, 90.87 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.46% latency, 60.55 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.81 us, 0.17% latency, 124.91 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.18 us, 0.16% latency, 132.23 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.75 us, 0.16% latency, 133.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.18 us, 0.16% latency, 127.33 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 846.86 us, 1.03% latency, 163.89 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.99 us, 0.29% latency, 195.2 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.89 us, 0.28% latency, 197.79 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.49 us, 0.31% latency, 179.66 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 170.4 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.98 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.08 us, 0.11% latency, 0.0 FLOPS, )
      )
      (9): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.82% latency, 91.19 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.74 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.81 us, 0.17% latency, 124.91 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.7 us, 0.16% latency, 132.72 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.51 us, 0.16% latency, 133.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.42 us, 0.16% latency, 127.11 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.62 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 843.76 us, 1.02% latency, 164.49 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.46 us, 0.29% latency, 194.81 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.7 us, 0.28% latency, 198.8 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.11 us, 0.31% latency, 181.34 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.27 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.65 us, 0.11% latency, 0.0 FLOPS, )
      )
      (10): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.83% latency, 90.9 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.46% latency, 60.65 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.66 us, 0.16% latency, 126.89 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.18 us, 0.16% latency, 132.23 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.46 us, 0.16% latency, 132.96 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.14 us, 0.17% latency, 126.44 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 847.34 us, 1.03% latency, 163.79 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.7 us, 0.29% latency, 194.62 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.41 us, 0.28% latency, 198.2 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.78 us, 0.31% latency, 180.16 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.62 us, 0.04% latency, 167.98 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.79 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.41 us, 0.11% latency, 0.0 FLOPS, )
      )
      (11): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.82% latency, 91.04 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.46% latency, 60.62 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.85 us, 0.17% latency, 125.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.65 us, 0.16% latency, 131.75 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.03 us, 0.16% latency, 134.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.61 us, 0.17% latency, 126.0 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.81 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 847.1 us, 1.03% latency, 163.84 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.23 us, 0.29% latency, 195.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.89 us, 0.28% latency, 197.79 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.02 us, 0.31% latency, 179.99 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.46 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.78 us, 0.11% latency, 0.0 FLOPS, )
      )
      (12): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.82% latency, 91.21 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.84 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.33 us, 0.17% latency, 125.34 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.41 us, 0.16% latency, 131.99 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.16% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.94 us, 0.16% latency, 127.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.33 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 848.29 us, 1.03% latency, 163.61 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.03 us, 0.29% latency, 195.99 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.93 us, 0.28% latency, 198.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.82 us, 0.31% latency, 180.83 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.66 us, 0.04% latency, 172.89 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.27 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.31 us, 0.10% latency, 0.0 FLOPS, )
      )
      (13): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.81% latency, 91.28 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.46% latency, 60.55 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 138.04 us, 0.17% latency, 124.69 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.23 us, 0.16% latency, 128.24 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.16% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.23 us, 0.16% latency, 128.24 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.81 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 841.14 us, 1.02% latency, 165.0 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.46 us, 0.29% latency, 194.81 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.93 us, 0.28% latency, 198.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.11 us, 0.31% latency, 181.34 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 170.4 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.79 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.98 us, 0.11% latency, 0.0 FLOPS, )
      )
      (14): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.83% latency, 90.78 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.47% latency, 60.44 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 139.71 us, 0.17% latency, 123.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 131.13 us, 0.16% latency, 131.27 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.03 us, 0.16% latency, 134.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.9 us, 0.16% latency, 126.66 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 846.15 us, 1.03% latency, 164.02 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 238.18 us, 0.29% latency, 194.23 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.65 us, 0.28% latency, 197.99 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.78 us, 0.31% latency, 180.16 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 170.4 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 94.41 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.69 us, 0.11% latency, 0.0 FLOPS, )
      )
      (15): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.82% latency, 91.21 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.46% latency, 60.67 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.85 us, 0.17% latency, 125.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.94 us, 0.16% latency, 132.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.16% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.71 us, 0.16% latency, 127.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 842.81 us, 1.02% latency, 164.67 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.75 us, 0.29% latency, 195.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.93 us, 0.28% latency, 198.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.54 us, 0.31% latency, 180.33 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 170.4 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.55 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.5 us, 0.11% latency, 0.0 FLOPS, )
      )
      (16): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.81% latency, 91.52 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.45% latency, 61.03 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.33 us, 0.17% latency, 125.34 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.18 us, 0.16% latency, 132.23 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.51 us, 0.16% latency, 133.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.94 us, 0.16% latency, 127.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 844.48 us, 1.02% latency, 164.35 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.46 us, 0.29% latency, 194.81 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.17 us, 0.28% latency, 198.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.78 us, 0.31% latency, 180.16 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.19 us, 0.04% latency, 175.45 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.7 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.31 us, 0.10% latency, 0.0 FLOPS, )
      )
      (17): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.81% latency, 91.44 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.45% latency, 61.03 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.38 us, 0.17% latency, 126.22 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.94 us, 0.16% latency, 132.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.75 us, 0.16% latency, 133.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.23 us, 0.16% latency, 128.24 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 846.86 us, 1.03% latency, 163.89 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.7 us, 0.29% latency, 194.62 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.17 us, 0.28% latency, 198.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.54 us, 0.31% latency, 180.33 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 169.18 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.36 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.69 us, 0.11% latency, 0.0 FLOPS, )
      )
      (18): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.35 ms, 2.85% latency, 90.25 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.22 ms, 1.47% latency, 60.18 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 142.1 us, 0.17% latency, 121.14 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 131.13 us, 0.16% latency, 131.27 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.98 us, 0.16% latency, 133.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.66 us, 0.16% latency, 126.89 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 847.82 us, 1.03% latency, 163.7 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 238.9 us, 0.29% latency, 193.65 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.93 us, 0.28% latency, 198.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.25 us, 0.31% latency, 179.83 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.04% latency, 166.8 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 98.23 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.17 us, 0.11% latency, 0.0 FLOPS, )
      )
      (19): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.82% latency, 91.08 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.46% latency, 60.68 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 138.28 us, 0.17% latency, 124.48 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.18 us, 0.16% latency, 132.23 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.51 us, 0.16% latency, 133.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.94 us, 0.16% latency, 127.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 846.62 us, 1.03% latency, 163.93 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.23 us, 0.29% latency, 195.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.65 us, 0.28% latency, 197.99 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.3 us, 0.31% latency, 180.5 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 170.4 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.51 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.45 us, 0.11% latency, 0.0 FLOPS, )
      )
      (20): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.34 ms, 2.83% latency, 90.7 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.47% latency, 60.49 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.61 us, 0.17% latency, 126.0 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.41 us, 0.16% latency, 131.99 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.51 us, 0.16% latency, 133.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.47 us, 0.16% latency, 128.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 851.87 us, 1.03% latency, 162.92 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.94 us, 0.29% latency, 194.42 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.93 us, 0.28% latency, 198.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 262.98 us, 0.32% latency, 175.91 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 170.4 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.03 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.5 us, 0.11% latency, 0.0 FLOPS, )
      )
      (21): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.34 ms, 2.84% latency, 90.41 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.22 ms, 1.48% latency, 59.96 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 138.76 us, 0.17% latency, 124.05 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.89 us, 0.16% latency, 131.51 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.51 us, 0.16% latency, 133.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.94 us, 0.16% latency, 127.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 36.72 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 846.86 us, 1.03% latency, 163.89 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 238.42 us, 0.29% latency, 194.03 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.17 us, 0.28% latency, 198.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.82 us, 0.31% latency, 180.83 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.46 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.69 us, 0.11% latency, 0.0 FLOPS, )
      )
      (22): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.82% latency, 91.22 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.46% latency, 60.65 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.9 us, 0.16% latency, 126.66 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.41 us, 0.16% latency, 131.99 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.03 us, 0.16% latency, 134.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.18 us, 0.16% latency, 127.33 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.81 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 843.76 us, 1.02% latency, 164.49 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.23 us, 0.29% latency, 195.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.17 us, 0.28% latency, 198.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.78 us, 0.31% latency, 180.16 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.66 us, 0.04% latency, 172.89 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.31 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.02 us, 0.11% latency, 0.0 FLOPS, )
      )
      (23): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.82% latency, 91.07 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.81 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.14 us, 0.17% latency, 126.44 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.22 us, 0.16% latency, 133.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 127.79 us, 0.15% latency, 134.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.85 us, 0.17% latency, 125.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.81 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 847.58 us, 1.03% latency, 163.75 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.7 us, 0.29% latency, 194.62 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.65 us, 0.28% latency, 197.99 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.49 us, 0.31% latency, 179.66 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.36 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.36 us, 0.11% latency, 0.0 FLOPS, )
      )
      (24): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.83% latency, 90.92 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.47% latency, 60.4 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.33 us, 0.17% latency, 125.34 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.65 us, 0.16% latency, 131.75 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.98 us, 0.16% latency, 133.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.71 us, 0.16% latency, 127.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.52 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 844.72 us, 1.02% latency, 164.3 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.99 us, 0.29% latency, 195.2 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.89 us, 0.28% latency, 197.79 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.78 us, 0.31% latency, 180.16 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 170.4 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 94.18 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.98 us, 0.11% latency, 0.0 FLOPS, )
      )
      (25): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.35 ms, 2.84% latency, 90.39 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.47% latency, 60.49 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.09 us, 0.17% latency, 125.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.65 us, 0.16% latency, 131.75 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.51 us, 0.16% latency, 133.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.61 us, 0.17% latency, 126.0 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.33 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 854.97 us, 1.04% latency, 162.33 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 244.38 us, 0.30% latency, 189.3 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.89 us, 0.28% latency, 197.79 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.49 us, 0.31% latency, 179.66 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.33 us, 0.04% latency, 164.48 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.55 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.03 us, 0.11% latency, 0.0 FLOPS, )
      )
      (26): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.82% latency, 91.15 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.46% latency, 60.71 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.57 us, 0.17% latency, 125.13 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.18 us, 0.16% latency, 132.23 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.22 us, 0.16% latency, 133.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.99 us, 0.16% latency, 128.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.29 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 844.24 us, 1.02% latency, 164.4 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.75 us, 0.29% latency, 195.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.17 us, 0.28% latency, 198.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.78 us, 0.31% latency, 180.16 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.42 us, 0.04% latency, 174.16 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.46 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.74 us, 0.11% latency, 0.0 FLOPS, )
      )
      (27): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.31 ms, 2.81% latency, 91.59 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.87 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.61 us, 0.17% latency, 126.0 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.41 us, 0.16% latency, 131.99 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.98 us, 0.16% latency, 133.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.47 us, 0.16% latency, 128.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 842.57 us, 1.02% latency, 164.72 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.75 us, 0.29% latency, 195.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.7 us, 0.28% latency, 198.8 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.49 us, 0.31% latency, 179.66 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.95 us, 0.04% latency, 176.76 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.36 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 85.83 us, 0.10% latency, 0.0 FLOPS, )
      )
      (28): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.82% latency, 91.13 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.87 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.09 us, 0.17% latency, 125.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.7 us, 0.16% latency, 132.72 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.16% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.18 us, 0.16% latency, 127.33 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.62 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 850.2 us, 1.03% latency, 163.24 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.23 us, 0.29% latency, 195.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.89 us, 0.28% latency, 197.79 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 259.88 us, 0.32% latency, 178.01 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 170.4 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.84 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.98 us, 0.11% latency, 0.0 FLOPS, )
      )
      (29): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.83% latency, 90.86 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.47% latency, 60.38 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.09 us, 0.17% latency, 125.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.18 us, 0.16% latency, 132.23 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.51 us, 0.16% latency, 133.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.47 us, 0.16% latency, 128.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 36.0 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 843.76 us, 1.02% latency, 164.49 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.75 us, 0.29% latency, 195.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.41 us, 0.28% latency, 198.2 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.35 us, 0.31% latency, 181.17 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 169.18 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 94.89 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.21 us, 0.11% latency, 0.0 FLOPS, )
      )
      (30): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.82% latency, 91.05 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.46% latency, 60.69 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.33 us, 0.17% latency, 125.34 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.94 us, 0.16% latency, 132.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.51 us, 0.16% latency, 133.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.99 us, 0.16% latency, 128.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.33 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 845.43 us, 1.02% latency, 164.16 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.75 us, 0.29% latency, 195.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.17 us, 0.28% latency, 198.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.3 us, 0.31% latency, 180.5 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 169.18 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.79 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.55 us, 0.11% latency, 0.0 FLOPS, )
      )
      (31): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.82% latency, 91.19 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.88 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.14 us, 0.17% latency, 126.44 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.41 us, 0.16% latency, 131.99 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.16% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.85 us, 0.17% latency, 125.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 846.62 us, 1.03% latency, 163.93 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.94 us, 0.29% latency, 194.42 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.65 us, 0.28% latency, 197.99 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.78 us, 0.31% latency, 180.16 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.62 us, 0.04% latency, 167.98 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.84 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.41 us, 0.11% latency, 0.0 FLOPS, )
      )
    )
    (norm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 97.75 us, 0.12% latency, 0.0 FLOPS, )
  )
  (lm_head): Linear(131.08 M, 1.95% Params, 67.24 GMACs, 1.94% MACs, 2.36 ms, 2.87% latency, 56.88 TFLOPS, in_features=4096, out_features=32001, bias=False)
)
------------------------------------------------------------------------------

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 128:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

world size:                                                   4       
data parallel size:                                           4       
model parallel size:                                          1       
batch size per GPU:                                           1       
params per gpu:                                               6738.42 M
params of model = params per GPU * mp_size:                   6738.42 M
fwd MACs per GPU:                                             3183.8 GMACs
fwd flops per GPU:                                            6367.99 G
fwd flops of model = fwd flops per GPU * mp_size:             6367.99 G
fwd latency:                                                  77.46 ms
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:          82.21 TFLOPS
bwd latency:                                                  227.08 ms
bwd FLOPS per GPU = 2.0 * fwd flops per GPU / bwd latency:    56.09 TFLOPS
fwd+bwd FLOPS per GPU = 3.0 * fwd flops per GPU / (fwd+bwd latency):   62.73 TFLOPS
step latency:                                                 29.7 us 
iter latency:                                                 304.57 ms
FLOPS per GPU = 3.0 * fwd flops per GPU / iter latency:       62.72 TFLOPS
samples/second:                                               13.13   

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'LlamaForCausalLM': '6738.42 M'}
    MACs        - {'LlamaForCausalLM': '3183.8 GMACs'}
    fwd latency - {'LlamaForCausalLM': '77.37 ms'}
depth 1:
    params      - {'LlamaModel': '6607.35 M'}
    MACs        - {'LlamaModel': '3121.8 GMACs'}
    fwd latency - {'LlamaModel': '75.12 ms'}
depth 2:
    params      - {'ModuleList': '6476.27 M'}
    MACs        - {'ModuleList': '3121.8 GMACs'}
    fwd latency - {'ModuleList': '70.08 ms'}
depth 3:
    params      - {'LlamaDecoderLayer': '6476.27 M'}
    MACs        - {'LlamaDecoderLayer': '3121.8 GMACs'}
    fwd latency - {'LlamaDecoderLayer': '70.08 ms'}
depth 4:
    params      - {'LlamaMLP': '4328.52 M'}
    MACs        - {'LlamaMLP': '2047.39 GMACs'}
    fwd latency - {'LlamaAttention': '35.14 ms'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

LlamaForCausalLM(
  6738.42 M, 100.00% Params, 3183.8 GMACs, 100.00% MACs, 77.37 ms, 100.00% latency, 82.31 TFLOPS, 
  (model): LlamaModel(
    6607.35 M, 98.05% Params, 3121.8 GMACs, 98.05% MACs, 75.12 ms, 97.10% latency, 83.12 TFLOPS, 
    (embed_tokens): Embedding(131.08 M, 1.95% Params, 0 MACs, 0.00% MACs, 68.9 us, 0.09% latency, 0.0 FLOPS, 32001, 4096)
    (layers): ModuleList(
      (0): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.56 GMACs, 3.06% MACs, 2.48 ms, 3.21% latency, 78.69 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.58 GMACs, 1.05% MACs, 1.17 ms, 1.51% latency, 57.5 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 156.64 us, 0.20% latency, 101.32 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 120.88 us, 0.16% latency, 131.3 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 116.11 us, 0.15% latency, 136.69 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 124.69 us, 0.16% latency, 127.28 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 38.39 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.98 GMACs, 2.01% MACs, 818.49 us, 1.06% latency, 156.35 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 239.13 us, 0.31% latency, 178.37 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 233.17 us, 0.30% latency, 182.93 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 224.59 us, 0.29% latency, 189.92 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.04% latency, 152.72 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 290.16 us, 0.38% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 96.08 us, 0.12% latency, 0.0 FLOPS, )
      )
      (1): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.56 GMACs, 3.06% MACs, 2.21 ms, 2.86% latency, 88.29 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.58 GMACs, 1.05% MACs, 1.11 ms, 1.44% latency, 60.45 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 128.75 us, 0.17% latency, 123.28 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 116.35 us, 0.15% latency, 136.41 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 114.92 us, 0.15% latency, 138.11 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 122.79 us, 0.16% latency, 129.26 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 36.72 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.98 GMACs, 2.01% MACs, 812.05 us, 1.05% latency, 157.58 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 237.7 us, 0.31% latency, 179.44 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 231.98 us, 0.30% latency, 183.87 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 224.11 us, 0.29% latency, 190.32 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 155.99 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 99.42 us, 0.13% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.31 us, 0.12% latency, 0.0 FLOPS, )
      )
      (2): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.56 GMACs, 3.06% MACs, 2.19 ms, 2.83% latency, 89.2 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.58 GMACs, 1.05% MACs, 1.1 ms, 1.42% latency, 61.29 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 124.69 us, 0.16% latency, 127.28 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 114.92 us, 0.15% latency, 138.11 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 114.68 us, 0.15% latency, 138.4 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 122.55 us, 0.16% latency, 129.51 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.05 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.98 GMACs, 2.01% MACs, 808.72 us, 1.05% latency, 158.23 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 236.51 us, 0.31% latency, 180.35 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 231.74 us, 0.30% latency, 184.06 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 224.59 us, 0.29% latency, 189.92 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.42 us, 0.04% latency, 160.58 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.03 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.74 us, 0.11% latency, 0.0 FLOPS, )
      )
      (3): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.56 GMACs, 3.06% MACs, 2.19 ms, 2.83% latency, 89.2 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.58 GMACs, 1.05% MACs, 1.1 ms, 1.42% latency, 61.27 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 123.74 us, 0.16% latency, 128.26 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 116.35 us, 0.15% latency, 136.41 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 113.73 us, 0.15% latency, 139.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 124.45 us, 0.16% latency, 127.53 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.98 GMACs, 2.01% MACs, 812.05 us, 1.05% latency, 157.58 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 236.75 us, 0.31% latency, 180.16 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 231.74 us, 0.30% latency, 184.06 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 224.35 us, 0.29% latency, 190.12 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.66 us, 0.04% latency, 159.41 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.12 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.79 us, 0.12% latency, 0.0 FLOPS, )
      )
      (4): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.56 GMACs, 3.06% MACs, 2.19 ms, 2.83% latency, 89.23 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.58 GMACs, 1.05% MACs, 1.1 ms, 1.42% latency, 61.18 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 125.17 us, 0.16% latency, 126.8 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 116.83 us, 0.15% latency, 135.85 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 113.96 us, 0.15% latency, 139.27 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 121.59 us, 0.16% latency, 130.53 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.81 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.98 GMACs, 2.01% MACs, 809.91 us, 1.05% latency, 158.0 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 236.75 us, 0.31% latency, 180.16 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 231.74 us, 0.30% latency, 184.06 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 225.31 us, 0.29% latency, 189.32 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.19 us, 0.04% latency, 161.77 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.7 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.02 us, 0.11% latency, 0.0 FLOPS, )
      )
      (5): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.56 GMACs, 3.06% MACs, 2.18 ms, 2.82% latency, 89.35 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.58 GMACs, 1.05% MACs, 1.09 ms, 1.41% latency, 61.39 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 122.55 us, 0.16% latency, 129.51 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 116.11 us, 0.15% latency, 136.69 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 112.53 us, 0.15% latency, 141.04 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 122.31 us, 0.16% latency, 129.76 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.33 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.98 GMACs, 2.01% MACs, 813.01 us, 1.05% latency, 157.4 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 236.99 us, 0.31% latency, 179.98 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 231.5 us, 0.30% latency, 184.25 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 224.35 us, 0.29% latency, 190.12 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.19 us, 0.04% latency, 161.77 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.79 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.74 us, 0.11% latency, 0.0 FLOPS, )
      )
      (6): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.56 GMACs, 3.06% MACs, 2.18 ms, 2.82% latency, 89.57 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.58 GMACs, 1.05% MACs, 1.09 ms, 1.41% latency, 61.6 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 122.31 us, 0.16% latency, 129.76 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 114.2 us, 0.15% latency, 138.97 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 113.49 us, 0.15% latency, 139.85 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 122.55 us, 0.16% latency, 129.51 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.98 GMACs, 2.01% MACs, 812.77 us, 1.05% latency, 157.45 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 236.75 us, 0.31% latency, 180.16 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 232.7 us, 0.30% latency, 183.3 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 226.02 us, 0.29% latency, 188.72 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.19 us, 0.04% latency, 161.77 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.65 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.45 us, 0.11% latency, 0.0 FLOPS, )
      )
      (7): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.56 GMACs, 3.06% MACs, 2.17 ms, 2.81% latency, 89.72 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.58 GMACs, 1.05% MACs, 1.09 ms, 1.41% latency, 61.37 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 123.26 us, 0.16% latency, 128.76 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 114.68 us, 0.15% latency, 138.4 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 113.73 us, 0.15% latency, 139.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 121.36 us, 0.16% latency, 130.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.29 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.98 GMACs, 2.01% MACs, 807.76 us, 1.04% latency, 158.42 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 237.23 us, 0.31% latency, 179.8 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 232.7 us, 0.30% latency, 183.3 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 224.35 us, 0.29% latency, 190.12 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.23 us, 0.04% latency, 166.71 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.55 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.07 us, 0.11% latency, 0.0 FLOPS, )
      )
      (8): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.56 GMACs, 3.06% MACs, 2.18 ms, 2.82% latency, 89.39 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.58 GMACs, 1.05% MACs, 1.1 ms, 1.42% latency, 60.98 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 123.02 us, 0.16% latency, 129.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 116.59 us, 0.15% latency, 136.13 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 115.87 us, 0.15% latency, 136.97 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 120.64 us, 0.16% latency, 131.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.05 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.98 GMACs, 2.01% MACs, 809.19 us, 1.05% latency, 158.14 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 236.27 us, 0.31% latency, 180.53 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 231.03 us, 0.30% latency, 184.63 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 223.4 us, 0.29% latency, 190.93 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.95 us, 0.04% latency, 162.98 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.08 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 85.59 us, 0.11% latency, 0.0 FLOPS, )
      )
      (9): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.56 GMACs, 3.06% MACs, 2.17 ms, 2.81% latency, 89.81 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.58 GMACs, 1.05% MACs, 1.09 ms, 1.41% latency, 61.54 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 121.12 us, 0.16% latency, 131.04 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 115.63 us, 0.15% latency, 137.26 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 114.92 us, 0.15% latency, 138.11 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 123.02 us, 0.16% latency, 129.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.62 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.98 GMACs, 2.01% MACs, 808.48 us, 1.04% latency, 158.28 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 236.51 us, 0.31% latency, 180.35 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 232.46 us, 0.30% latency, 183.49 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 222.92 us, 0.29% latency, 191.34 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.19 us, 0.04% latency, 161.77 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.93 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.69 us, 0.11% latency, 0.0 FLOPS, )
      )
      (10): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.56 GMACs, 3.06% MACs, 2.18 ms, 2.82% latency, 89.4 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.58 GMACs, 1.05% MACs, 1.1 ms, 1.42% latency, 61.08 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 124.22 us, 0.16% latency, 127.77 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 116.11 us, 0.15% latency, 136.69 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 114.68 us, 0.15% latency, 138.4 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 121.83 us, 0.16% latency, 130.27 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.76 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.98 GMACs, 2.01% MACs, 811.1 us, 1.05% latency, 157.77 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 237.7 us, 0.31% latency, 179.44 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 232.22 us, 0.30% latency, 183.68 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 224.59 us, 0.29% latency, 189.92 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.19 us, 0.04% latency, 161.77 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.84 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 85.12 us, 0.11% latency, 0.0 FLOPS, )
      )
      (11): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.56 GMACs, 3.06% MACs, 2.19 ms, 2.83% latency, 89.12 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.58 GMACs, 1.05% MACs, 1.1 ms, 1.42% latency, 61.19 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 123.26 us, 0.16% latency, 128.76 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 114.2 us, 0.15% latency, 138.97 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 114.2 us, 0.15% latency, 138.97 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 121.36 us, 0.16% latency, 130.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.98 GMACs, 2.01% MACs, 809.91 us, 1.05% latency, 158.0 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 237.7 us, 0.31% latency, 179.44 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 232.22 us, 0.30% latency, 183.68 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 224.11 us, 0.29% latency, 190.32 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.66 us, 0.04% latency, 159.41 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.79 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 85.59 us, 0.11% latency, 0.0 FLOPS, )
      )
      (12): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.56 GMACs, 3.06% MACs, 2.18 ms, 2.82% latency, 89.55 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.58 GMACs, 1.05% MACs, 1.1 ms, 1.42% latency, 61.01 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 122.79 us, 0.16% latency, 129.26 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 116.11 us, 0.15% latency, 136.69 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 113.73 us, 0.15% latency, 139.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 121.59 us, 0.16% latency, 130.53 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 36.24 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.98 GMACs, 2.01% MACs, 805.62 us, 1.04% latency, 158.84 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 235.08 us, 0.30% latency, 181.44 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 230.79 us, 0.30% latency, 184.82 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 223.64 us, 0.29% latency, 190.73 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.47 us, 0.04% latency, 165.45 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.55 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.31 us, 0.11% latency, 0.0 FLOPS, )
      )
      (13): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.56 GMACs, 3.06% MACs, 2.17 ms, 2.81% latency, 89.85 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.58 GMACs, 1.05% MACs, 1.1 ms, 1.42% latency, 61.26 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 121.83 us, 0.16% latency, 130.27 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 116.35 us, 0.15% latency, 136.41 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 114.44 us, 0.15% latency, 138.69 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 121.12 us, 0.16% latency, 131.04 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.98 GMACs, 2.01% MACs, 803.71 us, 1.04% latency, 159.22 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 235.32 us, 0.30% latency, 181.26 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 231.03 us, 0.30% latency, 184.63 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 223.64 us, 0.29% latency, 190.73 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.47 us, 0.04% latency, 165.45 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.17 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.78 us, 0.11% latency, 0.0 FLOPS, )
      )
      (14): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.56 GMACs, 3.06% MACs, 2.16 ms, 2.80% latency, 90.19 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.58 GMACs, 1.05% MACs, 1.09 ms, 1.41% latency, 61.73 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 122.55 us, 0.16% latency, 129.51 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 115.63 us, 0.15% latency, 137.26 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 113.25 us, 0.15% latency, 140.14 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 121.36 us, 0.16% latency, 130.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.98 GMACs, 2.01% MACs, 806.81 us, 1.04% latency, 158.61 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 236.27 us, 0.31% latency, 180.53 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 231.03 us, 0.30% latency, 184.63 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 224.35 us, 0.29% latency, 190.12 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.71 us, 0.04% latency, 164.2 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.17 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 85.83 us, 0.11% latency, 0.0 FLOPS, )
      )
      (15): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.56 GMACs, 3.06% MACs, 2.2 ms, 2.84% latency, 88.71 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.58 GMACs, 1.05% MACs, 1.1 ms, 1.43% latency, 60.82 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 129.22 us, 0.17% latency, 122.82 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 115.63 us, 0.15% latency, 137.26 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 113.49 us, 0.15% latency, 139.85 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 122.31 us, 0.16% latency, 129.76 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.29 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.98 GMACs, 2.01% MACs, 808.0 us, 1.04% latency, 158.38 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 236.99 us, 0.31% latency, 179.98 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 231.98 us, 0.30% latency, 183.87 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 223.16 us, 0.29% latency, 191.14 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.95 us, 0.04% latency, 162.98 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 94.18 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.45 us, 0.11% latency, 0.0 FLOPS, )
      )
      (16): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.56 GMACs, 3.06% MACs, 2.17 ms, 2.81% latency, 89.72 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.58 GMACs, 1.05% MACs, 1.09 ms, 1.42% latency, 61.34 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 121.83 us, 0.16% latency, 130.27 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 114.68 us, 0.15% latency, 138.4 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 116.11 us, 0.15% latency, 136.69 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 120.88 us, 0.16% latency, 131.3 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.98 GMACs, 2.01% MACs, 808.95 us, 1.05% latency, 158.19 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 235.32 us, 0.30% latency, 181.26 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 230.55 us, 0.30% latency, 185.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 225.31 us, 0.29% latency, 189.32 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.95 us, 0.04% latency, 162.98 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.36 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 85.59 us, 0.11% latency, 0.0 FLOPS, )
      )
      (17): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.56 GMACs, 3.06% MACs, 2.18 ms, 2.82% latency, 89.33 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.58 GMACs, 1.05% MACs, 1.09 ms, 1.41% latency, 61.48 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 123.26 us, 0.16% latency, 128.76 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 115.87 us, 0.15% latency, 136.97 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 113.73 us, 0.15% latency, 139.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 121.12 us, 0.16% latency, 131.04 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.29 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.98 GMACs, 2.01% MACs, 813.01 us, 1.05% latency, 157.4 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 236.99 us, 0.31% latency, 179.98 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 231.74 us, 0.30% latency, 184.06 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 225.07 us, 0.29% latency, 189.52 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.95 us, 0.04% latency, 162.98 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.03 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.45 us, 0.11% latency, 0.0 FLOPS, )
      )
      (18): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.56 GMACs, 3.06% MACs, 2.18 ms, 2.81% latency, 89.62 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.58 GMACs, 1.05% MACs, 1.1 ms, 1.42% latency, 61.31 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 124.69 us, 0.16% latency, 127.28 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 114.92 us, 0.15% latency, 138.11 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 114.2 us, 0.15% latency, 138.97 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 120.88 us, 0.16% latency, 131.3 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.29 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.98 GMACs, 2.01% MACs, 807.05 us, 1.04% latency, 158.56 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 235.08 us, 0.30% latency, 181.44 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 230.55 us, 0.30% latency, 185.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 223.16 us, 0.29% latency, 191.14 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.19 us, 0.04% latency, 161.77 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.08 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.74 us, 0.11% latency, 0.0 FLOPS, )
      )
      (19): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.56 GMACs, 3.06% MACs, 2.16 ms, 2.79% latency, 90.33 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.58 GMACs, 1.05% MACs, 1.09 ms, 1.40% latency, 61.87 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 122.79 us, 0.16% latency, 129.26 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 114.92 us, 0.15% latency, 138.11 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 113.01 us, 0.15% latency, 140.44 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 120.4 us, 0.16% latency, 131.82 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.98 GMACs, 2.01% MACs, 805.85 us, 1.04% latency, 158.8 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 235.32 us, 0.30% latency, 181.26 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 231.03 us, 0.30% latency, 184.63 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 224.83 us, 0.29% latency, 189.72 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.71 us, 0.04% latency, 164.2 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.36 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 84.4 us, 0.11% latency, 0.0 FLOPS, )
      )
      (20): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.56 GMACs, 3.06% MACs, 2.2 ms, 2.84% latency, 88.67 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.58 GMACs, 1.05% MACs, 1.12 ms, 1.45% latency, 60.05 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 130.41 us, 0.17% latency, 121.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 116.11 us, 0.15% latency, 136.69 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 113.96 us, 0.15% latency, 139.27 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 123.02 us, 0.16% latency, 129.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 42.68 us, 0.06% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.98 GMACs, 2.01% MACs, 805.38 us, 1.04% latency, 158.89 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 236.75 us, 0.31% latency, 180.16 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 231.27 us, 0.30% latency, 184.44 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 223.16 us, 0.29% latency, 191.14 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.19 us, 0.04% latency, 161.77 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.7 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.74 us, 0.11% latency, 0.0 FLOPS, )
      )
      (21): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.56 GMACs, 3.06% MACs, 2.16 ms, 2.80% latency, 90.21 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.58 GMACs, 1.05% MACs, 1.09 ms, 1.40% latency, 61.89 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 122.55 us, 0.16% latency, 129.51 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 114.68 us, 0.15% latency, 138.4 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 113.01 us, 0.15% latency, 140.44 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 120.64 us, 0.16% latency, 131.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.98 GMACs, 2.01% MACs, 802.52 us, 1.04% latency, 159.46 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 234.37 us, 0.30% latency, 182.0 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 231.5 us, 0.30% latency, 184.25 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 224.11 us, 0.29% latency, 190.32 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.95 us, 0.04% latency, 162.98 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 94.89 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.31 us, 0.11% latency, 0.0 FLOPS, )
      )
      (22): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.56 GMACs, 3.06% MACs, 2.17 ms, 2.80% latency, 89.95 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.58 GMACs, 1.05% MACs, 1.09 ms, 1.41% latency, 61.46 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 123.74 us, 0.16% latency, 128.26 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 116.11 us, 0.15% latency, 136.69 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 113.49 us, 0.15% latency, 139.85 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 120.4 us, 0.16% latency, 131.82 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 36.0 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.98 GMACs, 2.01% MACs, 802.99 us, 1.04% latency, 159.36 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 235.08 us, 0.30% latency, 181.44 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 229.84 us, 0.30% latency, 185.58 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 223.16 us, 0.29% latency, 191.14 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.95 us, 0.04% latency, 162.98 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 94.18 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 85.59 us, 0.11% latency, 0.0 FLOPS, )
      )
      (23): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.56 GMACs, 3.06% MACs, 2.23 ms, 2.88% latency, 87.64 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.58 GMACs, 1.05% MACs, 1.1 ms, 1.43% latency, 60.9 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 126.36 us, 0.16% latency, 125.6 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 114.68 us, 0.15% latency, 138.4 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 114.2 us, 0.15% latency, 138.97 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 122.31 us, 0.16% latency, 129.76 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.29 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.98 GMACs, 2.01% MACs, 848.29 us, 1.10% latency, 150.85 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 236.75 us, 0.31% latency, 180.16 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 231.74 us, 0.30% latency, 184.06 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 224.35 us, 0.29% latency, 190.12 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 72.24 us, 0.09% latency, 72.08 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.94 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.5 us, 0.11% latency, 0.0 FLOPS, )
      )
      (24): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.56 GMACs, 3.06% MACs, 2.18 ms, 2.82% latency, 89.42 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.58 GMACs, 1.05% MACs, 1.09 ms, 1.41% latency, 61.35 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 122.79 us, 0.16% latency, 129.26 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 115.87 us, 0.15% latency, 136.97 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 114.2 us, 0.15% latency, 138.97 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 122.55 us, 0.16% latency, 129.51 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.29 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.98 GMACs, 2.01% MACs, 810.62 us, 1.05% latency, 157.86 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 236.99 us, 0.31% latency, 179.98 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 231.74 us, 0.30% latency, 184.06 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 224.83 us, 0.29% latency, 189.72 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.19 us, 0.04% latency, 161.77 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.51 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.41 us, 0.12% latency, 0.0 FLOPS, )
      )
      (25): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.56 GMACs, 3.06% MACs, 2.18 ms, 2.82% latency, 89.54 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.58 GMACs, 1.05% MACs, 1.09 ms, 1.41% latency, 61.35 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 122.55 us, 0.16% latency, 129.51 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 116.11 us, 0.15% latency, 136.69 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 114.2 us, 0.15% latency, 138.97 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 120.88 us, 0.16% latency, 131.3 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.98 GMACs, 2.01% MACs, 807.05 us, 1.04% latency, 158.56 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 236.03 us, 0.31% latency, 180.71 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 231.98 us, 0.30% latency, 183.87 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 223.88 us, 0.29% latency, 190.53 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.71 us, 0.04% latency, 164.2 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.22 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.26 us, 0.11% latency, 0.0 FLOPS, )
      )
      (26): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.56 GMACs, 3.06% MACs, 2.17 ms, 2.81% latency, 89.9 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.58 GMACs, 1.05% MACs, 1.09 ms, 1.41% latency, 61.48 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 122.07 us, 0.16% latency, 130.02 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 116.35 us, 0.15% latency, 136.41 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 114.68 us, 0.15% latency, 138.4 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 121.59 us, 0.16% latency, 130.53 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.76 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.98 GMACs, 2.01% MACs, 807.29 us, 1.04% latency, 158.52 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 236.03 us, 0.31% latency, 180.71 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 231.27 us, 0.30% latency, 184.44 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 224.11 us, 0.29% latency, 190.32 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.71 us, 0.04% latency, 164.2 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.36 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.07 us, 0.11% latency, 0.0 FLOPS, )
      )
      (27): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.56 GMACs, 3.06% MACs, 2.18 ms, 2.82% latency, 89.31 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.58 GMACs, 1.05% MACs, 1.1 ms, 1.42% latency, 61.06 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 123.5 us, 0.16% latency, 128.51 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 116.11 us, 0.15% latency, 136.69 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 116.35 us, 0.15% latency, 136.41 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 122.31 us, 0.16% latency, 129.76 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.05 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.98 GMACs, 2.01% MACs, 809.19 us, 1.05% latency, 158.14 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 236.51 us, 0.31% latency, 180.35 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 232.46 us, 0.30% latency, 183.49 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 224.11 us, 0.29% latency, 190.32 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.71 us, 0.04% latency, 164.2 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.65 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.21 us, 0.11% latency, 0.0 FLOPS, )
      )
      (28): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.56 GMACs, 3.06% MACs, 2.18 ms, 2.82% latency, 89.48 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.58 GMACs, 1.05% MACs, 1.09 ms, 1.41% latency, 61.37 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 124.93 us, 0.16% latency, 127.04 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 115.87 us, 0.15% latency, 136.97 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 115.16 us, 0.15% latency, 137.82 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 121.12 us, 0.16% latency, 131.04 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.33 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.98 GMACs, 2.01% MACs, 807.05 us, 1.04% latency, 158.56 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 235.56 us, 0.30% latency, 181.08 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 231.27 us, 0.30% latency, 184.44 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 224.35 us, 0.29% latency, 190.12 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.19 us, 0.04% latency, 161.77 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.46 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.93 us, 0.11% latency, 0.0 FLOPS, )
      )
      (29): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.56 GMACs, 3.06% MACs, 2.16 ms, 2.79% latency, 90.42 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.58 GMACs, 1.05% MACs, 1.08 ms, 1.40% latency, 61.93 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 123.02 us, 0.16% latency, 129.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 116.11 us, 0.15% latency, 136.69 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 113.73 us, 0.15% latency, 139.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 121.12 us, 0.16% latency, 131.04 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.98 GMACs, 2.01% MACs, 804.19 us, 1.04% latency, 159.13 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 235.8 us, 0.30% latency, 180.89 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 230.55 us, 0.30% latency, 185.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 223.4 us, 0.29% latency, 190.93 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.95 us, 0.04% latency, 162.98 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.88 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.78 us, 0.11% latency, 0.0 FLOPS, )
      )
      (30): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.56 GMACs, 3.06% MACs, 2.16 ms, 2.79% latency, 90.32 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.58 GMACs, 1.05% MACs, 1.09 ms, 1.40% latency, 61.87 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 123.5 us, 0.16% latency, 128.51 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 115.87 us, 0.15% latency, 136.97 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 114.44 us, 0.15% latency, 138.69 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 120.88 us, 0.16% latency, 131.3 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.98 GMACs, 2.01% MACs, 807.52 us, 1.04% latency, 158.47 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 236.51 us, 0.31% latency, 180.35 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 231.5 us, 0.30% latency, 184.25 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 224.11 us, 0.29% latency, 190.32 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.19 us, 0.04% latency, 161.77 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.21 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.31 us, 0.11% latency, 0.0 FLOPS, )
      )
      (31): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.56 GMACs, 3.06% MACs, 2.18 ms, 2.81% latency, 89.69 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.58 GMACs, 1.05% MACs, 1.1 ms, 1.42% latency, 61.18 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 122.79 us, 0.16% latency, 129.26 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 115.39 us, 0.15% latency, 137.54 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 114.2 us, 0.15% latency, 138.97 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.94 GMACs, 0.25% MACs, 120.88 us, 0.16% latency, 131.3 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 36.0 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.98 GMACs, 2.01% MACs, 806.09 us, 1.04% latency, 158.75 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 235.32 us, 0.30% latency, 181.26 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 231.03 us, 0.30% latency, 184.63 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.33 GMACs, 0.67% MACs, 224.35 us, 0.29% latency, 190.12 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.19 us, 0.04% latency, 161.77 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.84 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 85.83 us, 0.11% latency, 0.0 FLOPS, )
      )
    )
    (norm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 95.61 us, 0.12% latency, 0.0 FLOPS, )
  )
  (lm_head): Linear(131.08 M, 1.95% Params, 62.0 GMACs, 1.95% MACs, 1.92 ms, 2.49% latency, 64.46 TFLOPS, in_features=4096, out_features=32001, bias=False)
)
------------------------------------------------------------------------------

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 128:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

world size:                                                   4       
data parallel size:                                           4       
model parallel size:                                          1       
batch size per GPU:                                           1       
params per gpu:                                               6738.42 M
params of model = params per GPU * mp_size:                   6738.42 M
fwd MACs per GPU:                                             3458.42 GMACs
fwd flops per GPU:                                            6917.29 G
fwd flops of model = fwd flops per GPU * mp_size:             6917.29 G
fwd latency:                                                  82.54 ms
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:          83.81 TFLOPS
bwd latency:                                                  244.59 ms
bwd FLOPS per GPU = 2.0 * fwd flops per GPU / bwd latency:    56.56 TFLOPS
fwd+bwd FLOPS per GPU = 3.0 * fwd flops per GPU / (fwd+bwd latency):   63.44 TFLOPS
step latency:                                                 32.77 us
iter latency:                                                 327.16 ms
FLOPS per GPU = 3.0 * fwd flops per GPU / iter latency:       63.43 TFLOPS
samples/second:                                               12.23   

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'LlamaForCausalLM': '6738.42 M'}
    MACs        - {'LlamaForCausalLM': '3458.42 GMACs'}
    fwd latency - {'LlamaForCausalLM': '82.45 ms'}
depth 1:
    params      - {'LlamaModel': '6607.35 M'}
    MACs        - {'LlamaModel': '3391.18 GMACs'}
    fwd latency - {'LlamaModel': '79.76 ms'}
depth 2:
    params      - {'ModuleList': '6476.27 M'}
    MACs        - {'ModuleList': '3391.18 GMACs'}
    fwd latency - {'ModuleList': '74.7 ms'}
depth 3:
    params      - {'LlamaDecoderLayer': '6476.27 M'}
    MACs        - {'LlamaDecoderLayer': '3391.18 GMACs'}
    fwd latency - {'LlamaDecoderLayer': '74.7 ms'}
depth 4:
    params      - {'LlamaMLP': '4328.52 M'}
    MACs        - {'LlamaMLP': '2220.53 GMACs'}
    fwd latency - {'LlamaAttention': '38.68 ms'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

LlamaForCausalLM(
  6738.42 M, 100.00% Params, 3458.42 GMACs, 100.00% MACs, 82.45 ms, 100.00% latency, 83.9 TFLOPS, 
  (model): LlamaModel(
    6607.35 M, 98.05% Params, 3391.18 GMACs, 98.06% MACs, 79.76 ms, 96.73% latency, 85.04 TFLOPS, 
    (embed_tokens): Embedding(131.08 M, 1.95% Params, 0 MACs, 0.00% MACs, 71.53 us, 0.09% latency, 0.0 FLOPS, 32001, 4096)
    (layers): ModuleList(
      (0): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.48 ms, 3.01% latency, 85.49 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.27 ms, 1.54% latency, 57.76 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 167.61 us, 0.20% latency, 102.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.75 us, 0.16% latency, 128.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.22 us, 0.16% latency, 133.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.85 us, 0.17% latency, 125.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 36.0 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 854.97 us, 1.04% latency, 162.33 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 239.61 us, 0.29% latency, 193.07 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.65 us, 0.28% latency, 197.99 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 258.68 us, 0.31% latency, 178.83 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.81 us, 0.04% latency, 162.23 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 154.26 us, 0.19% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 96.8 us, 0.12% latency, 0.0 FLOPS, )
      )
      (1): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.36 ms, 2.86% latency, 89.89 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.22 ms, 1.48% latency, 59.83 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.81 us, 0.17% latency, 124.91 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 131.13 us, 0.16% latency, 131.27 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.98 us, 0.16% latency, 133.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.66 us, 0.16% latency, 126.89 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.29 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 849.72 us, 1.03% latency, 163.33 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 238.66 us, 0.29% latency, 193.84 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.89 us, 0.28% latency, 197.79 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.49 us, 0.31% latency, 179.66 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.04% latency, 166.8 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 97.27 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.79 us, 0.11% latency, 0.0 FLOPS, )
      )
      (2): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.82% latency, 91.25 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.84 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 138.28 us, 0.17% latency, 124.48 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.65 us, 0.16% latency, 131.75 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.51 us, 0.16% latency, 133.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.94 us, 0.16% latency, 127.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 845.91 us, 1.03% latency, 164.07 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.94 us, 0.29% latency, 194.42 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.17 us, 0.28% latency, 198.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.25 us, 0.31% latency, 179.83 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.66 us, 0.04% latency, 172.89 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.7 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.02 us, 0.11% latency, 0.0 FLOPS, )
      )
      (3): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.35 ms, 2.85% latency, 90.1 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.22 ms, 1.48% latency, 59.89 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 141.86 us, 0.17% latency, 121.34 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 131.85 us, 0.16% latency, 130.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.98 us, 0.16% latency, 133.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.14 us, 0.17% latency, 126.44 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.81 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 847.82 us, 1.03% latency, 163.7 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 238.66 us, 0.29% latency, 193.84 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.41 us, 0.28% latency, 198.2 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 258.45 us, 0.31% latency, 179.0 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 170.4 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 97.04 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.17 us, 0.11% latency, 0.0 FLOPS, )
      )
      (4): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.82% latency, 91.11 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.84 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.14 us, 0.17% latency, 126.44 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.89 us, 0.16% latency, 131.51 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.7 us, 0.16% latency, 132.72 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.42 us, 0.16% latency, 127.11 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 846.86 us, 1.03% latency, 163.89 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.7 us, 0.29% latency, 194.62 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.65 us, 0.28% latency, 197.99 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.78 us, 0.31% latency, 180.16 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 170.4 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.94 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.21 us, 0.11% latency, 0.0 FLOPS, )
      )
      (5): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.81% latency, 91.4 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.9 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.61 us, 0.17% latency, 126.0 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.41 us, 0.16% latency, 131.99 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.51 us, 0.16% latency, 133.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.71 us, 0.16% latency, 127.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 846.86 us, 1.03% latency, 163.89 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 238.18 us, 0.29% latency, 194.23 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.41 us, 0.28% latency, 198.2 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 258.21 us, 0.31% latency, 179.16 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.66 us, 0.04% latency, 172.89 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.84 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.74 us, 0.11% latency, 0.0 FLOPS, )
      )
      (6): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.35 ms, 2.85% latency, 90.3 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.47% latency, 60.24 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 140.43 us, 0.17% latency, 122.58 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.18 us, 0.16% latency, 132.23 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.16% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.9 us, 0.16% latency, 126.66 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.52 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 847.58 us, 1.03% latency, 163.75 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 238.66 us, 0.29% latency, 193.84 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.17 us, 0.28% latency, 198.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.49 us, 0.31% latency, 179.66 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 169.18 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 97.51 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.69 us, 0.11% latency, 0.0 FLOPS, )
      )
      (7): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.82% latency, 91.3 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.82 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.61 us, 0.17% latency, 126.0 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.65 us, 0.16% latency, 131.75 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.03 us, 0.16% latency, 134.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.18 us, 0.16% latency, 127.33 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.62 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 846.39 us, 1.03% latency, 163.98 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.7 us, 0.29% latency, 194.62 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.93 us, 0.28% latency, 198.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.49 us, 0.31% latency, 179.66 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.84 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 85.83 us, 0.10% latency, 0.0 FLOPS, )
      )
      (8): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.83% latency, 90.78 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.47% latency, 60.5 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.81 us, 0.17% latency, 124.91 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.41 us, 0.16% latency, 131.99 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.98 us, 0.16% latency, 133.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 138.28 us, 0.17% latency, 124.48 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 848.77 us, 1.03% latency, 163.52 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 238.9 us, 0.29% latency, 193.65 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.89 us, 0.28% latency, 197.79 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.25 us, 0.31% latency, 179.83 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 169.18 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.31 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.17 us, 0.11% latency, 0.0 FLOPS, )
      )
      (9): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.34 ms, 2.84% latency, 90.57 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.47% latency, 60.26 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.33 us, 0.17% latency, 125.34 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.7 us, 0.16% latency, 132.72 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 127.79 us, 0.15% latency, 134.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.94 us, 0.16% latency, 127.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 847.34 us, 1.03% latency, 163.79 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.23 us, 0.29% latency, 195.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.65 us, 0.28% latency, 197.99 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.25 us, 0.31% latency, 179.83 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 169.18 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.7 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.45 us, 0.11% latency, 0.0 FLOPS, )
      )
      (10): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.81% latency, 91.5 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.81 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.61 us, 0.17% latency, 126.0 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.94 us, 0.16% latency, 132.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.51 us, 0.16% latency, 133.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.71 us, 0.16% latency, 127.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 842.09 us, 1.02% latency, 164.81 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.99 us, 0.29% latency, 195.2 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.7 us, 0.28% latency, 198.8 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.06 us, 0.31% latency, 180.66 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.19 us, 0.04% latency, 175.45 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.08 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.55 us, 0.10% latency, 0.0 FLOPS, )
      )
      (11): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.82% latency, 91.14 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.46% latency, 60.67 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 138.28 us, 0.17% latency, 124.48 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.51 us, 0.16% latency, 133.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 127.55 us, 0.15% latency, 134.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.42 us, 0.16% latency, 127.11 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 845.91 us, 1.03% latency, 164.07 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 238.66 us, 0.29% latency, 193.84 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.17 us, 0.28% latency, 198.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.25 us, 0.31% latency, 179.83 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 169.18 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.55 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.26 us, 0.11% latency, 0.0 FLOPS, )
      )
      (12): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.83% latency, 90.82 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.47% latency, 60.52 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.14 us, 0.17% latency, 126.44 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.65 us, 0.16% latency, 131.75 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.16% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.99 us, 0.16% latency, 128.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.05 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 849.72 us, 1.03% latency, 163.33 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 238.18 us, 0.29% latency, 194.23 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.17 us, 0.28% latency, 198.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.25 us, 0.31% latency, 179.83 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 169.18 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.03 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.78 us, 0.11% latency, 0.0 FLOPS, )
      )
      (13): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.35 ms, 2.85% latency, 90.18 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.22 ms, 1.48% latency, 60.0 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 140.91 us, 0.17% latency, 122.16 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 131.61 us, 0.16% latency, 130.79 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.22 us, 0.16% latency, 133.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.42 us, 0.16% latency, 127.11 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.05 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 847.58 us, 1.03% latency, 163.75 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 238.18 us, 0.29% latency, 194.23 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.65 us, 0.28% latency, 197.99 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.73 us, 0.31% latency, 179.49 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 169.18 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 97.04 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.17 us, 0.11% latency, 0.0 FLOPS, )
      )
      (14): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.83% latency, 90.96 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.46% latency, 60.64 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.33 us, 0.17% latency, 125.34 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.94 us, 0.16% latency, 132.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.22 us, 0.16% latency, 133.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.99 us, 0.16% latency, 128.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.29 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 849.25 us, 1.03% latency, 163.43 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.7 us, 0.29% latency, 194.62 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.93 us, 0.28% latency, 198.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.49 us, 0.31% latency, 179.66 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.66 us, 0.04% latency, 172.89 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.22 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.26 us, 0.11% latency, 0.0 FLOPS, )
      )
      (15): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.31 ms, 2.81% latency, 91.58 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.45% latency, 61.09 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.66 us, 0.16% latency, 126.89 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.22 us, 0.16% latency, 133.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.03 us, 0.16% latency, 134.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.18 us, 0.16% latency, 127.33 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 846.39 us, 1.03% latency, 163.98 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.99 us, 0.29% latency, 195.2 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.17 us, 0.28% latency, 198.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.02 us, 0.31% latency, 179.99 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.6 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.78 us, 0.11% latency, 0.0 FLOPS, )
      )
      (16): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.82% latency, 91.21 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.97 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.9 us, 0.16% latency, 126.66 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.94 us, 0.16% latency, 132.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.03 us, 0.16% latency, 134.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.94 us, 0.16% latency, 127.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 844.72 us, 1.02% latency, 164.3 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.7 us, 0.29% latency, 194.62 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.7 us, 0.28% latency, 198.8 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.06 us, 0.31% latency, 180.66 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.66 us, 0.04% latency, 172.89 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.51 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.36 us, 0.11% latency, 0.0 FLOPS, )
      )
      (17): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.82% latency, 91.09 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.84 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.14 us, 0.17% latency, 126.44 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 131.13 us, 0.16% latency, 131.27 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.16% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.71 us, 0.16% latency, 127.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 846.15 us, 1.03% latency, 164.02 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.7 us, 0.29% latency, 194.62 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.17 us, 0.28% latency, 198.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.54 us, 0.31% latency, 180.33 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.7 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.84 us, 0.11% latency, 0.0 FLOPS, )
      )
      (18): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.31 ms, 2.81% latency, 91.57 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.97 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.38 us, 0.17% latency, 126.22 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.94 us, 0.16% latency, 132.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.16% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.71 us, 0.16% latency, 127.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.62 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 844.0 us, 1.02% latency, 164.44 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.99 us, 0.29% latency, 195.2 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.7 us, 0.28% latency, 198.8 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.11 us, 0.31% latency, 181.34 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 169.18 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.31 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.55 us, 0.10% latency, 0.0 FLOPS, )
      )
      (19): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.83% latency, 90.89 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.82 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 138.28 us, 0.17% latency, 124.48 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.94 us, 0.16% latency, 132.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 127.55 us, 0.15% latency, 134.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.14 us, 0.17% latency, 126.44 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 846.15 us, 1.03% latency, 164.02 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 238.18 us, 0.29% latency, 194.23 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.41 us, 0.28% latency, 198.2 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.54 us, 0.31% latency, 180.33 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.36 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.21 us, 0.11% latency, 0.0 FLOPS, )
      )
      (20): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.83% latency, 90.84 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.47% latency, 60.55 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.81 us, 0.17% latency, 124.91 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.65 us, 0.16% latency, 131.75 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.51 us, 0.16% latency, 133.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.23 us, 0.16% latency, 128.24 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 850.68 us, 1.03% latency, 163.15 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.94 us, 0.29% latency, 194.42 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.89 us, 0.28% latency, 197.79 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.73 us, 0.31% latency, 179.49 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.04% latency, 165.63 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.74 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.5 us, 0.11% latency, 0.0 FLOPS, )
      )
      (21): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.82% latency, 91.29 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.85 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.33 us, 0.17% latency, 125.34 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.65 us, 0.16% latency, 131.75 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.16% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.9 us, 0.16% latency, 126.66 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.33 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 845.67 us, 1.03% latency, 164.12 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.99 us, 0.29% latency, 195.2 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.41 us, 0.28% latency, 198.2 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.49 us, 0.31% latency, 179.66 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.42 us, 0.04% latency, 174.16 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.27 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.98 us, 0.11% latency, 0.0 FLOPS, )
      )
      (22): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.34 ms, 2.83% latency, 90.7 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.47% latency, 60.38 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 138.52 us, 0.17% latency, 124.27 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.7 us, 0.16% latency, 132.72 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.22 us, 0.16% latency, 133.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.38 us, 0.17% latency, 126.22 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.29 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 847.58 us, 1.03% latency, 163.75 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 238.18 us, 0.29% latency, 194.23 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.89 us, 0.28% latency, 197.79 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.73 us, 0.31% latency, 179.49 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.66 us, 0.04% latency, 172.89 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.22 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.93 us, 0.11% latency, 0.0 FLOPS, )
      )
      (23): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.82% latency, 91.16 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.81 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.09 us, 0.17% latency, 125.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.7 us, 0.16% latency, 132.72 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.75 us, 0.16% latency, 133.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.23 us, 0.16% latency, 128.24 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 846.15 us, 1.03% latency, 164.02 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.94 us, 0.29% latency, 194.42 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.17 us, 0.28% latency, 198.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.78 us, 0.31% latency, 180.16 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 170.4 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 94.41 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.31 us, 0.10% latency, 0.0 FLOPS, )
      )
      (24): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.31 ms, 2.81% latency, 91.59 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.97 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.09 us, 0.17% latency, 125.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.46 us, 0.16% latency, 132.96 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.16% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.75 us, 0.16% latency, 128.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 844.0 us, 1.02% latency, 164.44 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.51 us, 0.29% latency, 195.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.7 us, 0.28% latency, 198.8 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.54 us, 0.31% latency, 180.33 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.79 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 85.59 us, 0.10% latency, 0.0 FLOPS, )
      )
      (25): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.34 ms, 2.84% latency, 90.47 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.47% latency, 60.56 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 139.47 us, 0.17% latency, 123.42 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.18 us, 0.16% latency, 132.23 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.51 us, 0.16% latency, 133.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.18 us, 0.16% latency, 127.33 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 847.1 us, 1.03% latency, 163.84 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.94 us, 0.29% latency, 194.42 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.65 us, 0.28% latency, 197.99 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.02 us, 0.31% latency, 179.99 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.62 us, 0.04% latency, 167.98 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 94.41 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.65 us, 0.11% latency, 0.0 FLOPS, )
      )
      (26): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.82% latency, 91.29 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.86 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.61 us, 0.17% latency, 126.0 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.22 us, 0.16% latency, 133.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.03 us, 0.16% latency, 134.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.71 us, 0.16% latency, 127.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 846.15 us, 1.03% latency, 164.02 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.99 us, 0.29% latency, 195.2 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.7 us, 0.28% latency, 198.8 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.82 us, 0.31% latency, 180.83 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.31 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.21 us, 0.11% latency, 0.0 FLOPS, )
      )
      (27): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.81% latency, 91.37 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.46% latency, 60.69 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.14 us, 0.17% latency, 126.44 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.18 us, 0.16% latency, 132.23 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.71 us, 0.16% latency, 127.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.47 us, 0.16% latency, 128.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 843.05 us, 1.02% latency, 164.63 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.99 us, 0.29% latency, 195.2 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.93 us, 0.28% latency, 198.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.54 us, 0.31% latency, 180.33 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 170.4 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.84 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 85.59 us, 0.10% latency, 0.0 FLOPS, )
      )
      (28): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.34 ms, 2.84% latency, 90.63 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.47% latency, 60.37 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 139.71 us, 0.17% latency, 123.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.65 us, 0.16% latency, 131.75 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.16% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.71 us, 0.16% latency, 127.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 850.2 us, 1.03% latency, 163.24 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 238.18 us, 0.29% latency, 194.23 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.41 us, 0.28% latency, 198.2 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 258.45 us, 0.31% latency, 179.0 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 170.4 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 94.41 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.78 us, 0.11% latency, 0.0 FLOPS, )
      )
      (29): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.82% latency, 91.3 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.77 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 138.28 us, 0.17% latency, 124.48 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.41 us, 0.16% latency, 131.99 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.16% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.66 us, 0.16% latency, 126.89 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 846.15 us, 1.03% latency, 164.02 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.7 us, 0.29% latency, 194.62 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.65 us, 0.28% latency, 197.99 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.02 us, 0.31% latency, 179.99 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.66 us, 0.04% latency, 172.89 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.55 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.26 us, 0.11% latency, 0.0 FLOPS, )
      )
      (30): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.82% latency, 91.1 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.76 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.14 us, 0.17% latency, 126.44 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.18 us, 0.16% latency, 132.23 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.16% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.38 us, 0.17% latency, 126.22 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 847.58 us, 1.03% latency, 163.75 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.94 us, 0.29% latency, 194.42 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 234.6 us, 0.28% latency, 197.19 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 258.21 us, 0.31% latency, 179.16 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.66 us, 0.04% latency, 172.89 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.88 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.41 us, 0.11% latency, 0.0 FLOPS, )
      )
      (31): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.82% latency, 91.2 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.47% latency, 60.54 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.9 us, 0.16% latency, 126.66 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.18 us, 0.16% latency, 132.23 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.16% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.99 us, 0.16% latency, 128.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 843.05 us, 1.02% latency, 164.63 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.27 us, 0.29% latency, 195.8 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.7 us, 0.28% latency, 198.8 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.3 us, 0.31% latency, 180.5 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 170.4 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.55 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.07 us, 0.10% latency, 0.0 FLOPS, )
      )
    )
    (norm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 96.08 us, 0.12% latency, 0.0 FLOPS, )
  )
  (lm_head): Linear(131.08 M, 1.95% Params, 67.24 GMACs, 1.94% MACs, 2.36 ms, 2.86% latency, 57.04 TFLOPS, in_features=4096, out_features=32001, bias=False)
)
------------------------------------------------------------------------------

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 128:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

world size:                                                   4       
data parallel size:                                           4       
model parallel size:                                          1       
batch size per GPU:                                           1       
params per gpu:                                               6738.42 M
params of model = params per GPU * mp_size:                   6738.42 M
fwd MACs per GPU:                                             3458.42 GMACs
fwd flops per GPU:                                            6917.29 G
fwd flops of model = fwd flops per GPU * mp_size:             6917.29 G
fwd latency:                                                  82.26 ms
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:          84.09 TFLOPS
bwd latency:                                                  242.99 ms
bwd FLOPS per GPU = 2.0 * fwd flops per GPU / bwd latency:    56.94 TFLOPS
fwd+bwd FLOPS per GPU = 3.0 * fwd flops per GPU / (fwd+bwd latency):   63.8 TFLOPS
step latency:                                                 31.74 us
iter latency:                                                 325.28 ms
FLOPS per GPU = 3.0 * fwd flops per GPU / iter latency:       63.8 TFLOPS
samples/second:                                               12.30   

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'LlamaForCausalLM': '6738.42 M'}
    MACs        - {'LlamaForCausalLM': '3458.42 GMACs'}
    fwd latency - {'LlamaForCausalLM': '82.17 ms'}
depth 1:
    params      - {'LlamaModel': '6607.35 M'}
    MACs        - {'LlamaModel': '3391.18 GMACs'}
    fwd latency - {'LlamaModel': '79.48 ms'}
depth 2:
    params      - {'ModuleList': '6476.27 M'}
    MACs        - {'ModuleList': '3391.18 GMACs'}
    fwd latency - {'ModuleList': '74.49 ms'}
depth 3:
    params      - {'LlamaDecoderLayer': '6476.27 M'}
    MACs        - {'LlamaDecoderLayer': '3391.18 GMACs'}
    fwd latency - {'LlamaDecoderLayer': '74.49 ms'}
depth 4:
    params      - {'LlamaMLP': '4328.52 M'}
    MACs        - {'LlamaMLP': '2220.53 GMACs'}
    fwd latency - {'LlamaAttention': '38.56 ms'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

LlamaForCausalLM(
  6738.42 M, 100.00% Params, 3458.42 GMACs, 100.00% MACs, 82.17 ms, 100.00% latency, 84.18 TFLOPS, 
  (model): LlamaModel(
    6607.35 M, 98.05% Params, 3391.18 GMACs, 98.06% MACs, 79.48 ms, 96.72% latency, 85.34 TFLOPS, 
    (embed_tokens): Embedding(131.08 M, 1.95% Params, 0 MACs, 0.00% MACs, 72.96 us, 0.09% latency, 0.0 FLOPS, 32001, 4096)
    (layers): ModuleList(
      (0): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.56 ms, 3.11% latency, 82.96 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.29 ms, 1.57% latency, 56.87 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 172.62 us, 0.21% latency, 99.72 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.23 us, 0.16% latency, 128.24 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.98 us, 0.16% latency, 133.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.85 us, 0.17% latency, 125.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 38.62 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 857.59 us, 1.04% latency, 161.84 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 240.09 us, 0.29% latency, 192.68 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.41 us, 0.28% latency, 198.2 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.49 us, 0.31% latency, 179.66 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.04% latency, 163.35 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 203.61 us, 0.25% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 98.94 us, 0.12% latency, 0.0 FLOPS, )
      )
      (1): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.35 ms, 2.86% latency, 90.06 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.47% latency, 60.42 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 138.28 us, 0.17% latency, 124.48 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.18 us, 0.16% latency, 132.23 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 127.79 us, 0.16% latency, 134.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.94 us, 0.16% latency, 127.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.29 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 848.05 us, 1.03% latency, 163.66 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.7 us, 0.29% latency, 194.62 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.17 us, 0.28% latency, 198.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.25 us, 0.31% latency, 179.83 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.62 us, 0.04% latency, 167.98 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 99.18 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.98 us, 0.11% latency, 0.0 FLOPS, )
      )
      (2): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.83% latency, 91.05 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.47% latency, 60.77 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.33 us, 0.17% latency, 125.34 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.18 us, 0.16% latency, 132.23 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.16% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.94 us, 0.16% latency, 127.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.33 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 845.67 us, 1.03% latency, 164.12 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.46 us, 0.29% latency, 194.81 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.93 us, 0.28% latency, 198.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.54 us, 0.31% latency, 180.33 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 94.18 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.21 us, 0.11% latency, 0.0 FLOPS, )
      )
      (3): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.82% latency, 91.42 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 61.05 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.38 us, 0.17% latency, 126.22 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.94 us, 0.16% latency, 132.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.16% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.47 us, 0.16% latency, 128.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 846.62 us, 1.03% latency, 163.93 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.7 us, 0.29% latency, 194.62 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.93 us, 0.28% latency, 198.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.02 us, 0.31% latency, 179.99 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 169.18 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.03 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.45 us, 0.11% latency, 0.0 FLOPS, )
      )
      (4): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.34 ms, 2.85% latency, 90.57 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.47% latency, 60.61 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 138.28 us, 0.17% latency, 124.48 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.94 us, 0.16% latency, 132.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 127.55 us, 0.16% latency, 134.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.71 us, 0.16% latency, 127.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 850.68 us, 1.04% latency, 163.15 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 238.66 us, 0.29% latency, 193.84 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.89 us, 0.28% latency, 197.79 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.3 us, 0.31% latency, 180.5 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.66 us, 0.04% latency, 172.89 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 95.84 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.93 us, 0.11% latency, 0.0 FLOPS, )
      )
      (5): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.31 ms, 2.82% latency, 91.62 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 61.2 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.9 us, 0.17% latency, 126.66 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.03 us, 0.16% latency, 134.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 127.32 us, 0.15% latency, 135.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.99 us, 0.16% latency, 128.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.66 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 841.14 us, 1.02% latency, 165.0 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.23 us, 0.29% latency, 195.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 231.74 us, 0.28% latency, 199.62 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.58 us, 0.31% latency, 181.0 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.74 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.21 us, 0.11% latency, 0.0 FLOPS, )
      )
      (6): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.31 ms, 2.82% latency, 91.62 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 61.07 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.38 us, 0.17% latency, 126.22 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.98 us, 0.16% latency, 133.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 127.79 us, 0.16% latency, 134.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.47 us, 0.16% latency, 128.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 841.14 us, 1.02% latency, 165.0 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.51 us, 0.29% latency, 195.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.93 us, 0.28% latency, 198.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.06 us, 0.31% latency, 180.66 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.55 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.02 us, 0.11% latency, 0.0 FLOPS, )
      )
      (7): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.34 ms, 2.85% latency, 90.52 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.48% latency, 60.24 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 139.95 us, 0.17% latency, 123.0 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.41 us, 0.16% latency, 131.99 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.16% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.14 us, 0.17% latency, 126.44 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.52 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 846.62 us, 1.03% latency, 163.93 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 238.42 us, 0.29% latency, 194.03 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.17 us, 0.28% latency, 198.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.78 us, 0.31% latency, 180.16 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.42 us, 0.04% latency, 174.16 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.7 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.27 us, 0.11% latency, 0.0 FLOPS, )
      )
      (8): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.83% latency, 91.11 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.81 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.81 us, 0.17% latency, 124.91 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.65 us, 0.16% latency, 131.75 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.22 us, 0.16% latency, 133.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 132.8 us, 0.16% latency, 129.62 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 842.09 us, 1.02% latency, 164.81 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.27 us, 0.29% latency, 195.8 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.93 us, 0.28% latency, 198.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.82 us, 0.31% latency, 180.83 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.42 us, 0.04% latency, 174.16 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.27 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.5 us, 0.11% latency, 0.0 FLOPS, )
      )
      (9): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.31 ms, 2.81% latency, 91.69 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.19 ms, 1.45% latency, 61.24 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.38 us, 0.17% latency, 126.22 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.22 us, 0.16% latency, 133.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.16% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.99 us, 0.16% latency, 128.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 844.96 us, 1.03% latency, 164.26 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.23 us, 0.29% latency, 195.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 231.98 us, 0.28% latency, 199.42 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 254.63 us, 0.31% latency, 181.68 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.6 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.74 us, 0.11% latency, 0.0 FLOPS, )
      )
      (10): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.82% latency, 91.46 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 61.0 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.09 us, 0.17% latency, 125.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.18 us, 0.16% latency, 132.23 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.16% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.47 us, 0.16% latency, 128.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 844.0 us, 1.03% latency, 164.44 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.46 us, 0.29% latency, 194.81 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.7 us, 0.28% latency, 198.8 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.54 us, 0.31% latency, 180.33 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.74 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.74 us, 0.11% latency, 0.0 FLOPS, )
      )
      (11): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.3 ms, 2.80% latency, 92.02 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.19 ms, 1.45% latency, 61.25 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.61 us, 0.17% latency, 126.0 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.16% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 126.12 us, 0.15% latency, 136.48 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.51 us, 0.16% latency, 128.93 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 840.19 us, 1.02% latency, 165.19 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.27 us, 0.29% latency, 195.8 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 231.5 us, 0.28% latency, 199.83 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.35 us, 0.31% latency, 181.17 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.19 us, 0.04% latency, 175.45 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.88 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.78 us, 0.11% latency, 0.0 FLOPS, )
      )
      (12): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.31 ms, 2.81% latency, 91.66 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.96 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.61 us, 0.17% latency, 126.0 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.46 us, 0.16% latency, 132.96 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.03 us, 0.16% latency, 134.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.99 us, 0.16% latency, 128.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.05 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 842.33 us, 1.03% latency, 164.77 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.03 us, 0.29% latency, 195.99 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 231.74 us, 0.28% latency, 199.62 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.58 us, 0.31% latency, 181.0 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.08 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 85.83 us, 0.10% latency, 0.0 FLOPS, )
      )
      (13): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.82% latency, 91.47 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.45% latency, 61.22 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.14 us, 0.17% latency, 126.44 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.98 us, 0.16% latency, 133.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 127.32 us, 0.15% latency, 135.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.47 us, 0.16% latency, 128.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 843.05 us, 1.03% latency, 164.63 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.23 us, 0.29% latency, 195.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.7 us, 0.28% latency, 198.8 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.11 us, 0.31% latency, 181.34 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 169.18 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 94.18 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.41 us, 0.11% latency, 0.0 FLOPS, )
      )
      (14): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.83% latency, 91.1 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.47% latency, 60.76 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.38 us, 0.17% latency, 126.22 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.94 us, 0.16% latency, 132.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.51 us, 0.16% latency, 133.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.51 us, 0.16% latency, 128.93 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.76 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 840.19 us, 1.02% latency, 165.19 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.03 us, 0.29% latency, 195.99 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 231.98 us, 0.28% latency, 199.42 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.35 us, 0.31% latency, 181.17 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.08 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 95.61 us, 0.12% latency, 0.0 FLOPS, )
      )
      (15): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.82% latency, 91.41 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.94 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.9 us, 0.17% latency, 126.66 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.94 us, 0.16% latency, 132.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.16% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.71 us, 0.16% latency, 127.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 843.52 us, 1.03% latency, 164.53 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.99 us, 0.29% latency, 195.2 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.93 us, 0.28% latency, 198.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.11 us, 0.31% latency, 181.34 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.42 us, 0.04% latency, 174.16 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.27 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.02 us, 0.11% latency, 0.0 FLOPS, )
      )
      (16): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.84% latency, 90.88 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.47% latency, 60.44 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.85 us, 0.17% latency, 125.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.46 us, 0.16% latency, 132.96 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.16% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.47 us, 0.16% latency, 128.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 36.72 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 840.19 us, 1.02% latency, 165.19 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.27 us, 0.29% latency, 195.8 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.46 us, 0.28% latency, 199.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.35 us, 0.31% latency, 181.17 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.66 us, 0.04% latency, 172.89 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 96.56 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.65 us, 0.11% latency, 0.0 FLOPS, )
      )
      (17): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.31 ms, 2.81% latency, 91.67 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.97 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.94 us, 0.16% latency, 127.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.46 us, 0.16% latency, 132.96 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.03 us, 0.16% latency, 134.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.51 us, 0.16% latency, 128.93 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.33 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 840.66 us, 1.02% latency, 165.09 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.27 us, 0.29% latency, 195.8 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.22 us, 0.28% latency, 199.21 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 254.39 us, 0.31% latency, 181.85 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.84 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.69 us, 0.11% latency, 0.0 FLOPS, )
      )
      (18): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.31 ms, 2.81% latency, 91.79 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 61.04 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.38 us, 0.17% latency, 126.22 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.46 us, 0.16% latency, 132.96 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.03 us, 0.16% latency, 134.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.51 us, 0.16% latency, 128.93 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 841.14 us, 1.02% latency, 165.0 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.75 us, 0.29% latency, 195.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.46 us, 0.28% latency, 199.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.58 us, 0.31% latency, 181.0 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 169.18 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.31 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 85.83 us, 0.10% latency, 0.0 FLOPS, )
      )
      (19): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.83% latency, 91.06 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.22 ms, 1.48% latency, 60.19 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 138.28 us, 0.17% latency, 124.48 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.22 us, 0.16% latency, 133.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.03 us, 0.16% latency, 134.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 140.43 us, 0.17% latency, 122.58 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 36.48 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 838.76 us, 1.02% latency, 165.47 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 235.56 us, 0.29% latency, 196.39 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 231.5 us, 0.28% latency, 199.83 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 254.63 us, 0.31% latency, 181.68 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.66 us, 0.04% latency, 172.89 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.79 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.31 us, 0.11% latency, 0.0 FLOPS, )
      )
      (20): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.82% latency, 91.55 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.97 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.61 us, 0.17% latency, 126.0 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.94 us, 0.16% latency, 132.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 127.79 us, 0.16% latency, 134.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.75 us, 0.16% latency, 128.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 842.57 us, 1.03% latency, 164.72 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.03 us, 0.29% latency, 195.99 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.46 us, 0.28% latency, 199.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 254.39 us, 0.31% latency, 181.85 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.08 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.31 us, 0.11% latency, 0.0 FLOPS, )
      )
      (21): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.83% latency, 91.04 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.47% latency, 60.48 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.38 us, 0.17% latency, 126.22 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.18 us, 0.16% latency, 132.23 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.75 us, 0.16% latency, 133.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.71 us, 0.16% latency, 127.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.62 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 841.38 us, 1.02% latency, 164.95 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.75 us, 0.29% latency, 195.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.46 us, 0.28% latency, 199.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 254.39 us, 0.31% latency, 181.85 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.03 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.17 us, 0.11% latency, 0.0 FLOPS, )
      )
      (22): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.82% latency, 91.42 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.47% latency, 60.65 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.14 us, 0.17% latency, 126.44 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.22 us, 0.16% latency, 133.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.75 us, 0.16% latency, 133.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.71 us, 0.16% latency, 127.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.81 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 839.71 us, 1.02% latency, 165.28 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.27 us, 0.29% latency, 195.8 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 231.74 us, 0.28% latency, 199.62 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.58 us, 0.31% latency, 181.0 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 170.4 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.79 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.02 us, 0.11% latency, 0.0 FLOPS, )
      )
      (23): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.31 ms, 2.81% latency, 91.77 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 61.02 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.14 us, 0.17% latency, 126.44 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.7 us, 0.16% latency, 132.72 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 127.55 us, 0.16% latency, 134.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.23 us, 0.16% latency, 128.24 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.33 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 838.76 us, 1.02% latency, 165.47 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.51 us, 0.29% latency, 195.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 231.74 us, 0.28% latency, 199.62 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 254.39 us, 0.31% latency, 181.85 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.42 us, 0.04% latency, 174.16 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.6 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.31 us, 0.11% latency, 0.0 FLOPS, )
      )
      (24): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.31 ms, 2.81% latency, 91.68 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 61.08 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.42 us, 0.16% latency, 127.11 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.98 us, 0.16% latency, 133.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 127.08 us, 0.15% latency, 135.46 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.99 us, 0.16% latency, 128.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.19 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 842.33 us, 1.03% latency, 164.77 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.03 us, 0.29% latency, 195.99 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.46 us, 0.28% latency, 199.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.3 us, 0.31% latency, 180.5 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.95 us, 0.04% latency, 176.76 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.31 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.55 us, 0.11% latency, 0.0 FLOPS, )
      )
      (25): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.83% latency, 91.23 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.8 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.66 us, 0.17% latency, 126.89 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.22 us, 0.16% latency, 133.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.16% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.75 us, 0.16% latency, 128.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.33 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 841.86 us, 1.02% latency, 164.86 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.03 us, 0.29% latency, 195.99 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 231.74 us, 0.28% latency, 199.62 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.82 us, 0.31% latency, 180.83 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 170.4 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.7 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.93 us, 0.11% latency, 0.0 FLOPS, )
      )
      (26): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.83% latency, 91.3 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.93 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.33 us, 0.17% latency, 125.34 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.7 us, 0.16% latency, 132.72 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 127.32 us, 0.15% latency, 135.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.75 us, 0.16% latency, 128.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 842.33 us, 1.03% latency, 164.77 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.51 us, 0.29% latency, 195.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 231.03 us, 0.28% latency, 200.24 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 254.63 us, 0.31% latency, 181.68 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 98.47 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.07 us, 0.10% latency, 0.0 FLOPS, )
      )
      (27): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.3 ms, 2.80% latency, 92.1 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.19 ms, 1.45% latency, 61.35 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.42 us, 0.16% latency, 127.11 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.98 us, 0.16% latency, 133.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.03 us, 0.16% latency, 134.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.75 us, 0.16% latency, 128.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 838.52 us, 1.02% latency, 165.52 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.03 us, 0.29% latency, 195.99 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 231.5 us, 0.28% latency, 199.83 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 254.87 us, 0.31% latency, 181.51 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.66 us, 0.04% latency, 172.89 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.88 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.55 us, 0.11% latency, 0.0 FLOPS, )
      )
      (28): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.84% latency, 90.97 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.47% latency, 60.67 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 138.76 us, 0.17% latency, 124.05 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.22 us, 0.16% latency, 133.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.16% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.99 us, 0.16% latency, 128.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 843.29 us, 1.03% latency, 164.58 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.99 us, 0.29% latency, 195.2 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 231.98 us, 0.28% latency, 199.42 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.06 us, 0.31% latency, 180.66 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 170.4 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 96.32 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.45 us, 0.11% latency, 0.0 FLOPS, )
      )
      (29): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.82% latency, 91.53 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.9 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.14 us, 0.17% latency, 126.44 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.7 us, 0.16% latency, 132.72 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 127.55 us, 0.16% latency, 134.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.66 us, 0.17% latency, 126.89 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 839.71 us, 1.02% latency, 165.28 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.27 us, 0.29% latency, 195.8 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.46 us, 0.28% latency, 199.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 254.87 us, 0.31% latency, 181.51 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.55 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.12 us, 0.11% latency, 0.0 FLOPS, )
      )
      (30): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.83% latency, 91.19 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.47% latency, 60.73 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.38 us, 0.17% latency, 126.22 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.94 us, 0.16% latency, 132.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 127.79 us, 0.16% latency, 134.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.99 us, 0.16% latency, 128.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 844.24 us, 1.03% latency, 164.4 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.75 us, 0.29% latency, 195.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.22 us, 0.28% latency, 199.21 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.54 us, 0.31% latency, 180.33 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.51 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.98 us, 0.11% latency, 0.0 FLOPS, )
      )
      (31): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.31 ms, 2.82% latency, 91.59 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.81 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.33 us, 0.17% latency, 125.34 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.7 us, 0.16% latency, 132.72 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 127.79 us, 0.16% latency, 134.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.47 us, 0.16% latency, 128.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.05 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 839.47 us, 1.02% latency, 165.33 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.27 us, 0.29% latency, 195.8 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 231.5 us, 0.28% latency, 199.83 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.82 us, 0.31% latency, 180.83 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.19 us, 0.04% latency, 175.45 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.6 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.31 us, 0.11% latency, 0.0 FLOPS, )
      )
    )
    (norm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 98.23 us, 0.12% latency, 0.0 FLOPS, )
  )
  (lm_head): Linear(131.08 M, 1.95% Params, 67.24 GMACs, 1.94% MACs, 2.36 ms, 2.87% latency, 56.94 TFLOPS, in_features=4096, out_features=32001, bias=False)
)
------------------------------------------------------------------------------

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 128:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

world size:                                                   4       
data parallel size:                                           4       
model parallel size:                                          1       
batch size per GPU:                                           1       
params per gpu:                                               6738.42 M
params of model = params per GPU * mp_size:                   6738.42 M
fwd MACs per GPU:                                             3197.51 GMACs
fwd flops per GPU:                                            6395.42 G
fwd flops of model = fwd flops per GPU * mp_size:             6395.42 G
fwd latency:                                                  77.26 ms
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:          82.78 TFLOPS
bwd latency:                                                  227.0 ms
bwd FLOPS per GPU = 2.0 * fwd flops per GPU / bwd latency:    56.35 TFLOPS
fwd+bwd FLOPS per GPU = 3.0 * fwd flops per GPU / (fwd+bwd latency):   63.06 TFLOPS
step latency:                                                 29.7 us 
iter latency:                                                 304.29 ms
FLOPS per GPU = 3.0 * fwd flops per GPU / iter latency:       63.05 TFLOPS
samples/second:                                               13.15   

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'LlamaForCausalLM': '6738.42 M'}
    MACs        - {'LlamaForCausalLM': '3197.51 GMACs'}
    fwd latency - {'LlamaForCausalLM': '77.17 ms'}
depth 1:
    params      - {'LlamaModel': '6607.35 M'}
    MACs        - {'LlamaModel': '3135.25 GMACs'}
    fwd latency - {'LlamaModel': '74.93 ms'}
depth 2:
    params      - {'ModuleList': '6476.27 M'}
    MACs        - {'ModuleList': '3135.25 GMACs'}
    fwd latency - {'ModuleList': '69.93 ms'}
depth 3:
    params      - {'LlamaDecoderLayer': '6476.27 M'}
    MACs        - {'LlamaDecoderLayer': '3135.25 GMACs'}
    fwd latency - {'LlamaDecoderLayer': '69.93 ms'}
depth 4:
    params      - {'LlamaMLP': '4328.52 M'}
    MACs        - {'LlamaMLP': '2056.05 GMACs'}
    fwd latency - {'LlamaAttention': '35.12 ms'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

LlamaForCausalLM(
  6738.42 M, 100.00% Params, 3197.51 GMACs, 100.00% MACs, 77.17 ms, 100.00% latency, 82.87 TFLOPS, 
  (model): LlamaModel(
    6607.35 M, 98.05% Params, 3135.25 GMACs, 98.05% MACs, 74.93 ms, 97.09% latency, 83.69 TFLOPS, 
    (embed_tokens): Embedding(131.08 M, 1.95% Params, 0 MACs, 0.00% MACs, 68.19 us, 0.09% latency, 0.0 FLOPS, 32001, 4096)
    (layers): ModuleList(
      (0): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.98 GMACs, 3.06% MACs, 2.42 ms, 3.13% latency, 81.1 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.73 GMACs, 1.05% MACs, 1.18 ms, 1.53% latency, 57.18 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 160.46 us, 0.21% latency, 99.33 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 120.4 us, 0.16% latency, 132.38 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 119.21 us, 0.15% latency, 133.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 126.6 us, 0.16% latency, 125.9 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 37.19 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 64.25 GMACs, 2.01% MACs, 824.45 us, 1.07% latency, 155.87 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 239.13 us, 0.31% latency, 179.12 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 234.6 us, 0.30% latency, 182.58 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 225.54 us, 0.29% latency, 189.92 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 156.65 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 207.66 us, 0.27% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 96.56 us, 0.13% latency, 0.0 FLOPS, )
      )
      (1): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.98 GMACs, 3.06% MACs, 2.2 ms, 2.85% latency, 89.14 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.73 GMACs, 1.05% MACs, 1.1 ms, 1.42% latency, 61.44 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 124.45 us, 0.16% latency, 128.07 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 115.39 us, 0.15% latency, 138.12 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 113.25 us, 0.15% latency, 140.74 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 122.31 us, 0.16% latency, 130.31 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.52 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 64.25 GMACs, 2.01% MACs, 813.72 us, 1.05% latency, 157.93 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 236.99 us, 0.31% latency, 180.74 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 232.22 us, 0.30% latency, 184.46 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 225.31 us, 0.29% latency, 190.12 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.19 us, 0.04% latency, 162.45 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 98.94 us, 0.13% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.08 us, 0.12% latency, 0.0 FLOPS, )
      )
      (2): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.98 GMACs, 3.06% MACs, 2.17 ms, 2.82% latency, 90.17 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.73 GMACs, 1.05% MACs, 1.09 ms, 1.42% latency, 61.74 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 123.5 us, 0.16% latency, 129.05 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 115.63 us, 0.15% latency, 137.84 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 114.2 us, 0.15% latency, 139.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 121.12 us, 0.16% latency, 131.6 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 64.25 GMACs, 2.01% MACs, 805.85 us, 1.04% latency, 159.47 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 235.32 us, 0.30% latency, 182.03 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 231.74 us, 0.30% latency, 184.84 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 223.88 us, 0.29% latency, 191.33 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.19 us, 0.04% latency, 162.45 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.79 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.45 us, 0.11% latency, 0.0 FLOPS, )
      )
      (3): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.98 GMACs, 3.06% MACs, 2.19 ms, 2.84% latency, 89.35 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.73 GMACs, 1.05% MACs, 1.1 ms, 1.43% latency, 61.07 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 130.65 us, 0.17% latency, 121.99 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 114.92 us, 0.15% latency, 138.69 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 113.73 us, 0.15% latency, 140.15 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 122.55 us, 0.16% latency, 130.06 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.52 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 64.25 GMACs, 2.01% MACs, 809.91 us, 1.05% latency, 158.67 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 236.99 us, 0.31% latency, 180.74 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 232.46 us, 0.30% latency, 184.27 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 224.35 us, 0.29% latency, 190.92 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.42 us, 0.04% latency, 161.26 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 94.65 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.98 us, 0.11% latency, 0.0 FLOPS, )
      )
      (4): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.98 GMACs, 3.06% MACs, 2.17 ms, 2.82% latency, 90.11 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.73 GMACs, 1.05% MACs, 1.09 ms, 1.42% latency, 61.68 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 124.45 us, 0.16% latency, 128.07 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 115.63 us, 0.15% latency, 137.84 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 113.25 us, 0.15% latency, 140.74 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 120.4 us, 0.16% latency, 132.38 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.29 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 64.25 GMACs, 2.01% MACs, 807.29 us, 1.05% latency, 159.19 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 235.8 us, 0.31% latency, 181.66 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 231.5 us, 0.30% latency, 185.03 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 224.59 us, 0.29% latency, 190.72 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.19 us, 0.04% latency, 162.45 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.94 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.78 us, 0.11% latency, 0.0 FLOPS, )
      )
      (5): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.98 GMACs, 3.06% MACs, 2.17 ms, 2.81% latency, 90.32 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.73 GMACs, 1.05% MACs, 1.09 ms, 1.41% latency, 61.86 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 123.26 us, 0.16% latency, 129.3 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 115.87 us, 0.15% latency, 137.55 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 114.68 us, 0.15% latency, 138.98 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 120.64 us, 0.16% latency, 132.12 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.62 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 64.25 GMACs, 2.01% MACs, 806.09 us, 1.04% latency, 159.42 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 234.84 us, 0.30% latency, 182.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 231.98 us, 0.30% latency, 184.65 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 224.59 us, 0.29% latency, 190.72 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.19 us, 0.04% latency, 162.45 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.55 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.31 us, 0.11% latency, 0.0 FLOPS, )
      )
      (6): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.98 GMACs, 3.06% MACs, 2.19 ms, 2.84% latency, 89.44 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.73 GMACs, 1.05% MACs, 1.1 ms, 1.43% latency, 61.19 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 126.84 us, 0.16% latency, 125.66 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 115.87 us, 0.15% latency, 137.55 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 113.01 us, 0.15% latency, 141.03 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 121.59 us, 0.16% latency, 131.08 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 64.25 GMACs, 2.01% MACs, 809.67 us, 1.05% latency, 158.72 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 235.8 us, 0.31% latency, 181.66 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 231.98 us, 0.30% latency, 184.65 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 224.35 us, 0.29% latency, 190.92 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.19 us, 0.04% latency, 162.45 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 94.89 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.02 us, 0.11% latency, 0.0 FLOPS, )
      )
      (7): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.98 GMACs, 3.06% MACs, 2.17 ms, 2.81% latency, 90.24 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.73 GMACs, 1.05% MACs, 1.09 ms, 1.42% latency, 61.74 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 122.31 us, 0.16% latency, 130.31 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 115.63 us, 0.15% latency, 137.84 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 115.39 us, 0.15% latency, 138.12 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 121.12 us, 0.16% latency, 131.6 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 64.25 GMACs, 2.01% MACs, 808.72 us, 1.05% latency, 158.9 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 236.51 us, 0.31% latency, 181.11 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 231.98 us, 0.30% latency, 184.65 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 224.59 us, 0.29% latency, 190.72 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.71 us, 0.04% latency, 164.9 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.65 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.31 us, 0.11% latency, 0.0 FLOPS, )
      )
      (8): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.98 GMACs, 3.06% MACs, 2.18 ms, 2.83% latency, 89.81 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.73 GMACs, 1.05% MACs, 1.1 ms, 1.42% latency, 61.47 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 122.79 us, 0.16% latency, 129.81 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 116.59 us, 0.15% latency, 136.71 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 114.44 us, 0.15% latency, 139.27 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 124.22 us, 0.16% latency, 128.31 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.05 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 64.25 GMACs, 2.01% MACs, 811.1 us, 1.05% latency, 158.44 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 236.99 us, 0.31% latency, 180.74 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 231.98 us, 0.30% latency, 184.65 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 224.35 us, 0.29% latency, 190.92 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.42 us, 0.04% latency, 161.26 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.65 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.98 us, 0.11% latency, 0.0 FLOPS, )
      )
      (9): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.98 GMACs, 3.06% MACs, 2.19 ms, 2.83% latency, 89.66 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.73 GMACs, 1.05% MACs, 1.11 ms, 1.43% latency, 61.04 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 123.26 us, 0.16% latency, 129.3 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 117.06 us, 0.15% latency, 136.15 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 114.44 us, 0.15% latency, 139.27 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 120.88 us, 0.16% latency, 131.85 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.33 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 64.25 GMACs, 2.01% MACs, 807.52 us, 1.05% latency, 159.14 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 236.27 us, 0.31% latency, 181.29 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 231.74 us, 0.30% latency, 184.84 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 223.4 us, 0.29% latency, 191.74 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.19 us, 0.04% latency, 162.45 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.55 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.31 us, 0.11% latency, 0.0 FLOPS, )
      )
      (10): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.98 GMACs, 3.06% MACs, 2.17 ms, 2.81% latency, 90.42 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.73 GMACs, 1.05% MACs, 1.09 ms, 1.41% latency, 62.07 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 122.07 us, 0.16% latency, 130.57 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 114.68 us, 0.15% latency, 138.98 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 112.53 us, 0.15% latency, 141.63 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 120.64 us, 0.16% latency, 132.12 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 64.25 GMACs, 2.01% MACs, 810.38 us, 1.05% latency, 158.58 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 236.27 us, 0.31% latency, 181.29 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 232.46 us, 0.30% latency, 184.27 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 223.88 us, 0.29% latency, 191.33 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.19 us, 0.04% latency, 162.45 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.65 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 85.83 us, 0.11% latency, 0.0 FLOPS, )
      )
      (11): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.98 GMACs, 3.06% MACs, 2.18 ms, 2.82% latency, 90.06 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.73 GMACs, 1.05% MACs, 1.09 ms, 1.41% latency, 61.82 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 122.79 us, 0.16% latency, 129.81 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 115.87 us, 0.15% latency, 137.55 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 113.25 us, 0.15% latency, 140.74 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 122.79 us, 0.16% latency, 129.81 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 64.25 GMACs, 2.01% MACs, 814.68 us, 1.06% latency, 157.74 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 236.03 us, 0.31% latency, 181.47 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 231.74 us, 0.30% latency, 184.84 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 224.11 us, 0.29% latency, 191.13 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.19 us, 0.04% latency, 162.45 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.41 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.02 us, 0.11% latency, 0.0 FLOPS, )
      )
      (12): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.98 GMACs, 3.06% MACs, 2.19 ms, 2.83% latency, 89.62 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.73 GMACs, 1.05% MACs, 1.1 ms, 1.42% latency, 61.53 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 122.55 us, 0.16% latency, 130.06 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 115.87 us, 0.15% latency, 137.55 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 113.25 us, 0.15% latency, 140.74 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 123.26 us, 0.16% latency, 129.3 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.52 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 64.25 GMACs, 2.01% MACs, 815.15 us, 1.06% latency, 157.65 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 235.8 us, 0.31% latency, 181.66 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 231.74 us, 0.30% latency, 184.84 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 224.35 us, 0.29% latency, 190.92 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.19 us, 0.04% latency, 162.45 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.03 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.55 us, 0.11% latency, 0.0 FLOPS, )
      )
      (13): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.98 GMACs, 3.06% MACs, 2.18 ms, 2.83% latency, 89.8 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.73 GMACs, 1.05% MACs, 1.09 ms, 1.42% latency, 61.62 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 123.26 us, 0.16% latency, 129.3 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 115.87 us, 0.15% latency, 137.55 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 116.35 us, 0.15% latency, 136.99 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 121.83 us, 0.16% latency, 130.82 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.33 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 64.25 GMACs, 2.01% MACs, 811.34 us, 1.05% latency, 158.39 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 236.51 us, 0.31% latency, 181.11 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 231.98 us, 0.30% latency, 184.65 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 224.35 us, 0.29% latency, 190.92 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.19 us, 0.04% latency, 162.45 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.65 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.41 us, 0.12% latency, 0.0 FLOPS, )
      )
      (14): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.98 GMACs, 3.06% MACs, 2.17 ms, 2.81% latency, 90.3 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.73 GMACs, 1.05% MACs, 1.09 ms, 1.42% latency, 61.63 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 122.79 us, 0.16% latency, 129.81 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 115.87 us, 0.15% latency, 137.55 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 114.92 us, 0.15% latency, 138.69 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 120.4 us, 0.16% latency, 132.38 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 36.0 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 64.25 GMACs, 2.01% MACs, 805.62 us, 1.04% latency, 159.52 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 234.84 us, 0.30% latency, 182.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 231.5 us, 0.30% latency, 185.03 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 223.64 us, 0.29% latency, 191.54 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.42 us, 0.04% latency, 161.26 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.79 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.31 us, 0.11% latency, 0.0 FLOPS, )
      )
      (15): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.98 GMACs, 3.06% MACs, 2.17 ms, 2.81% latency, 90.51 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.73 GMACs, 1.05% MACs, 1.09 ms, 1.41% latency, 61.83 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 122.07 us, 0.16% latency, 130.57 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 116.11 us, 0.15% latency, 137.27 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 113.73 us, 0.15% latency, 140.15 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 120.64 us, 0.16% latency, 132.12 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 64.25 GMACs, 2.01% MACs, 805.14 us, 1.04% latency, 159.61 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 235.08 us, 0.30% latency, 182.21 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 232.22 us, 0.30% latency, 184.46 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 223.88 us, 0.29% latency, 191.33 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.19 us, 0.04% latency, 162.45 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.12 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 85.59 us, 0.11% latency, 0.0 FLOPS, )
      )
      (16): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.98 GMACs, 3.06% MACs, 2.19 ms, 2.83% latency, 89.62 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.73 GMACs, 1.05% MACs, 1.1 ms, 1.43% latency, 61.19 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 126.84 us, 0.16% latency, 125.66 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 115.87 us, 0.15% latency, 137.55 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 113.96 us, 0.15% latency, 139.85 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 120.4 us, 0.16% latency, 132.38 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.81 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 64.25 GMACs, 2.01% MACs, 806.57 us, 1.05% latency, 159.33 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 236.03 us, 0.31% latency, 181.47 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 232.46 us, 0.30% latency, 184.27 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 224.11 us, 0.29% latency, 191.13 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.71 us, 0.04% latency, 164.9 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 94.41 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.5 us, 0.11% latency, 0.0 FLOPS, )
      )
      (17): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.98 GMACs, 3.06% MACs, 2.18 ms, 2.82% latency, 90.07 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.73 GMACs, 1.05% MACs, 1.09 ms, 1.41% latency, 61.87 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 122.79 us, 0.16% latency, 129.81 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 113.73 us, 0.15% latency, 140.15 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 113.49 us, 0.15% latency, 140.44 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 121.36 us, 0.16% latency, 131.34 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.62 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 64.25 GMACs, 2.01% MACs, 807.05 us, 1.05% latency, 159.23 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 235.08 us, 0.30% latency, 182.21 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 230.07 us, 0.30% latency, 186.18 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 223.88 us, 0.29% latency, 191.33 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.19 us, 0.04% latency, 162.45 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.36 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.45 us, 0.11% latency, 0.0 FLOPS, )
      )
      (18): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.98 GMACs, 3.06% MACs, 2.21 ms, 2.87% latency, 88.59 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.73 GMACs, 1.05% MACs, 1.12 ms, 1.45% latency, 60.43 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 129.94 us, 0.17% latency, 122.66 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 117.54 us, 0.15% latency, 135.6 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 115.16 us, 0.15% latency, 138.41 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 121.59 us, 0.16% latency, 131.08 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 37.91 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 64.25 GMACs, 2.01% MACs, 812.29 us, 1.05% latency, 158.2 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 237.46 us, 0.31% latency, 180.38 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 231.74 us, 0.30% latency, 184.84 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 224.59 us, 0.29% latency, 190.72 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.19 us, 0.04% latency, 162.45 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 96.8 us, 0.13% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.12 us, 0.12% latency, 0.0 FLOPS, )
      )
      (19): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.98 GMACs, 3.06% MACs, 2.17 ms, 2.82% latency, 90.11 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.73 GMACs, 1.05% MACs, 1.09 ms, 1.42% latency, 61.66 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 122.79 us, 0.16% latency, 129.81 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 115.63 us, 0.15% latency, 137.84 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 113.01 us, 0.15% latency, 141.03 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 121.12 us, 0.16% latency, 131.6 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 64.25 GMACs, 2.01% MACs, 810.86 us, 1.05% latency, 158.48 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 236.03 us, 0.31% latency, 181.47 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 231.74 us, 0.30% latency, 184.84 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 223.88 us, 0.29% latency, 191.33 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.66 us, 0.04% latency, 160.08 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.12 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.78 us, 0.11% latency, 0.0 FLOPS, )
      )
      (20): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.98 GMACs, 3.06% MACs, 2.17 ms, 2.82% latency, 90.18 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.73 GMACs, 1.05% MACs, 1.09 ms, 1.41% latency, 61.95 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 123.5 us, 0.16% latency, 129.05 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 115.16 us, 0.15% latency, 138.41 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 113.25 us, 0.15% latency, 140.74 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 121.12 us, 0.16% latency, 131.6 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 64.25 GMACs, 2.01% MACs, 807.52 us, 1.05% latency, 159.14 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 236.27 us, 0.31% latency, 181.29 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 231.5 us, 0.30% latency, 185.03 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 224.35 us, 0.29% latency, 190.92 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.42 us, 0.04% latency, 161.26 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.79 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.17 us, 0.12% latency, 0.0 FLOPS, )
      )
      (21): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.98 GMACs, 3.06% MACs, 2.19 ms, 2.83% latency, 89.68 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.73 GMACs, 1.05% MACs, 1.1 ms, 1.42% latency, 61.36 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 123.02 us, 0.16% latency, 129.55 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 117.06 us, 0.15% latency, 136.15 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 113.25 us, 0.15% latency, 140.74 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 121.36 us, 0.16% latency, 131.34 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 64.25 GMACs, 2.01% MACs, 808.95 us, 1.05% latency, 158.86 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 236.75 us, 0.31% latency, 180.93 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 231.74 us, 0.30% latency, 184.84 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 223.4 us, 0.29% latency, 191.74 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 158.92 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.7 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.17 us, 0.12% latency, 0.0 FLOPS, )
      )
      (22): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.98 GMACs, 3.06% MACs, 2.17 ms, 2.81% latency, 90.33 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.73 GMACs, 1.05% MACs, 1.09 ms, 1.42% latency, 61.66 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 122.55 us, 0.16% latency, 130.06 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 115.16 us, 0.15% latency, 138.41 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 114.2 us, 0.15% latency, 139.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 121.12 us, 0.16% latency, 131.6 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 64.25 GMACs, 2.01% MACs, 806.33 us, 1.04% latency, 159.37 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 235.56 us, 0.31% latency, 181.84 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 231.27 us, 0.30% latency, 185.22 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 224.35 us, 0.29% latency, 190.92 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.95 us, 0.04% latency, 163.67 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.84 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.55 us, 0.11% latency, 0.0 FLOPS, )
      )
      (23): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.98 GMACs, 3.06% MACs, 2.17 ms, 2.82% latency, 90.18 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.73 GMACs, 1.05% MACs, 1.09 ms, 1.42% latency, 61.67 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 121.83 us, 0.16% latency, 130.82 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 115.16 us, 0.15% latency, 138.41 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 113.49 us, 0.15% latency, 140.44 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 123.5 us, 0.16% latency, 129.05 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 64.25 GMACs, 2.01% MACs, 805.14 us, 1.04% latency, 159.61 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 235.8 us, 0.31% latency, 181.66 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 230.79 us, 0.30% latency, 185.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 223.4 us, 0.29% latency, 191.74 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.71 us, 0.04% latency, 164.9 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.17 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.5 us, 0.11% latency, 0.0 FLOPS, )
      )
      (24): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.98 GMACs, 3.06% MACs, 2.18 ms, 2.82% latency, 90.06 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.73 GMACs, 1.05% MACs, 1.09 ms, 1.41% latency, 61.83 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 123.5 us, 0.16% latency, 129.05 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 115.39 us, 0.15% latency, 138.12 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 113.01 us, 0.15% latency, 141.03 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 120.16 us, 0.16% latency, 132.64 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 64.25 GMACs, 2.01% MACs, 815.15 us, 1.06% latency, 157.65 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 235.8 us, 0.31% latency, 181.66 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 238.66 us, 0.31% latency, 179.48 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 224.35 us, 0.29% latency, 190.92 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.95 us, 0.04% latency, 163.67 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.36 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 85.35 us, 0.11% latency, 0.0 FLOPS, )
      )
      (25): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.98 GMACs, 3.06% MACs, 2.17 ms, 2.81% latency, 90.29 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.73 GMACs, 1.05% MACs, 1.09 ms, 1.42% latency, 61.71 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 122.07 us, 0.16% latency, 130.57 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 115.87 us, 0.15% latency, 137.55 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 113.49 us, 0.15% latency, 140.44 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 120.16 us, 0.16% latency, 132.64 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.29 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 64.25 GMACs, 2.01% MACs, 808.72 us, 1.05% latency, 158.9 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 235.8 us, 0.31% latency, 181.66 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 231.74 us, 0.30% latency, 184.84 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 224.83 us, 0.29% latency, 190.52 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.19 us, 0.04% latency, 162.45 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.41 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 85.83 us, 0.11% latency, 0.0 FLOPS, )
      )
      (26): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.98 GMACs, 3.06% MACs, 2.19 ms, 2.84% latency, 89.48 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.73 GMACs, 1.05% MACs, 1.1 ms, 1.42% latency, 61.35 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 124.93 us, 0.16% latency, 127.58 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 115.87 us, 0.15% latency, 137.55 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 114.2 us, 0.15% latency, 139.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 121.36 us, 0.16% latency, 131.34 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.52 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 64.25 GMACs, 2.01% MACs, 805.85 us, 1.04% latency, 159.47 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 235.56 us, 0.31% latency, 181.84 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 231.03 us, 0.30% latency, 185.41 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 223.16 us, 0.29% latency, 191.94 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.42 us, 0.04% latency, 161.26 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.51 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.46 us, 0.12% latency, 0.0 FLOPS, )
      )
      (27): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.98 GMACs, 3.06% MACs, 2.17 ms, 2.81% latency, 90.37 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.73 GMACs, 1.05% MACs, 1.09 ms, 1.42% latency, 61.74 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 123.98 us, 0.16% latency, 128.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 115.39 us, 0.15% latency, 138.12 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 113.49 us, 0.15% latency, 140.44 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 120.64 us, 0.16% latency, 132.12 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 64.25 GMACs, 2.01% MACs, 807.52 us, 1.05% latency, 159.14 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 236.99 us, 0.31% latency, 180.74 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 231.03 us, 0.30% latency, 185.41 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 223.16 us, 0.29% latency, 191.94 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.95 us, 0.04% latency, 163.67 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.21 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 85.83 us, 0.11% latency, 0.0 FLOPS, )
      )
      (28): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.98 GMACs, 3.06% MACs, 2.16 ms, 2.80% latency, 90.56 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.73 GMACs, 1.05% MACs, 1.09 ms, 1.41% latency, 62.02 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 122.55 us, 0.16% latency, 130.06 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 116.35 us, 0.15% latency, 136.99 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 114.2 us, 0.15% latency, 139.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 121.36 us, 0.16% latency, 131.34 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.33 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 64.25 GMACs, 2.01% MACs, 804.9 us, 1.04% latency, 159.66 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 236.03 us, 0.31% latency, 181.47 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 231.03 us, 0.30% latency, 185.41 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 223.64 us, 0.29% latency, 191.54 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.95 us, 0.04% latency, 163.67 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.55 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 85.59 us, 0.11% latency, 0.0 FLOPS, )
      )
      (29): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.98 GMACs, 3.06% MACs, 2.18 ms, 2.82% latency, 90.06 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.73 GMACs, 1.05% MACs, 1.09 ms, 1.42% latency, 61.75 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 124.93 us, 0.16% latency, 127.58 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 114.92 us, 0.15% latency, 138.69 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 113.96 us, 0.15% latency, 139.85 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 121.83 us, 0.16% latency, 130.82 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 64.25 GMACs, 2.01% MACs, 808.48 us, 1.05% latency, 158.95 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 236.75 us, 0.31% latency, 180.93 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 232.22 us, 0.30% latency, 184.46 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 224.83 us, 0.29% latency, 190.52 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.95 us, 0.04% latency, 163.67 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.08 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.98 us, 0.11% latency, 0.0 FLOPS, )
      )
      (30): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.98 GMACs, 3.06% MACs, 2.17 ms, 2.82% latency, 90.12 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.73 GMACs, 1.05% MACs, 1.09 ms, 1.42% latency, 61.71 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 124.22 us, 0.16% latency, 128.31 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 114.68 us, 0.15% latency, 138.98 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 113.73 us, 0.15% latency, 140.15 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 120.4 us, 0.16% latency, 132.38 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.05 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 64.25 GMACs, 2.01% MACs, 804.42 us, 1.04% latency, 159.75 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 234.6 us, 0.30% latency, 182.58 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 231.74 us, 0.30% latency, 184.84 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 223.4 us, 0.29% latency, 191.74 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.66 us, 0.04% latency, 160.08 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 96.08 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 85.83 us, 0.11% latency, 0.0 FLOPS, )
      )
      (31): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 97.98 GMACs, 3.06% MACs, 2.16 ms, 2.80% latency, 90.78 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.73 GMACs, 1.05% MACs, 1.08 ms, 1.40% latency, 62.32 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 122.55 us, 0.16% latency, 130.06 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 115.39 us, 0.15% latency, 138.12 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 113.96 us, 0.15% latency, 139.85 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.97 GMACs, 0.25% MACs, 120.4 us, 0.16% latency, 132.38 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 64.25 GMACs, 2.01% MACs, 807.52 us, 1.05% latency, 159.14 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 235.8 us, 0.31% latency, 181.66 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 230.31 us, 0.30% latency, 185.98 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.42 GMACs, 0.67% MACs, 224.83 us, 0.29% latency, 190.52 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.71 us, 0.04% latency, 164.9 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.41 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 84.64 us, 0.11% latency, 0.0 FLOPS, )
      )
    )
    (norm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 96.56 us, 0.13% latency, 0.0 FLOPS, )
  )
  (lm_head): Linear(131.08 M, 1.95% Params, 62.26 GMACs, 1.95% MACs, 1.93 ms, 2.50% latency, 64.61 TFLOPS, in_features=4096, out_features=32001, bias=False)
)
------------------------------------------------------------------------------

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 128:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

world size:                                                   4       
data parallel size:                                           4       
model parallel size:                                          1       
batch size per GPU:                                           1       
params per gpu:                                               6738.42 M
params of model = params per GPU * mp_size:                   6738.42 M
fwd MACs per GPU:                                             3142.68 GMACs
fwd flops per GPU:                                            6285.74 G
fwd flops of model = fwd flops per GPU * mp_size:             6285.74 G
fwd latency:                                                  76.95 ms
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:          81.69 TFLOPS
bwd latency:                                                  228.79 ms
bwd FLOPS per GPU = 2.0 * fwd flops per GPU / bwd latency:    54.95 TFLOPS
fwd+bwd FLOPS per GPU = 3.0 * fwd flops per GPU / (fwd+bwd latency):   61.68 TFLOPS
step latency:                                                 31.74 us
iter latency:                                                 305.77 ms
FLOPS per GPU = 3.0 * fwd flops per GPU / iter latency:       61.67 TFLOPS
samples/second:                                               13.08   

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'LlamaForCausalLM': '6738.42 M'}
    MACs        - {'LlamaForCausalLM': '3142.68 GMACs'}
    fwd latency - {'LlamaForCausalLM': '76.86 ms'}
depth 1:
    params      - {'LlamaModel': '6607.35 M'}
    MACs        - {'LlamaModel': '3081.47 GMACs'}
    fwd latency - {'LlamaModel': '74.63 ms'}
depth 2:
    params      - {'ModuleList': '6476.27 M'}
    MACs        - {'ModuleList': '3081.47 GMACs'}
    fwd latency - {'ModuleList': '69.53 ms'}
depth 3:
    params      - {'LlamaDecoderLayer': '6476.27 M'}
    MACs        - {'LlamaDecoderLayer': '3081.47 GMACs'}
    fwd latency - {'LlamaDecoderLayer': '69.53 ms'}
depth 4:
    params      - {'LlamaMLP': '4328.52 M'}
    MACs        - {'LlamaMLP': '2021.42 GMACs'}
    fwd latency - {'LlamaAttention': '34.79 ms'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

LlamaForCausalLM(
  6738.42 M, 100.00% Params, 3142.68 GMACs, 100.00% MACs, 76.86 ms, 100.00% latency, 81.79 TFLOPS, 
  (model): LlamaModel(
    6607.35 M, 98.05% Params, 3081.47 GMACs, 98.05% MACs, 74.63 ms, 97.10% latency, 82.59 TFLOPS, 
    (embed_tokens): Embedding(131.08 M, 1.95% Params, 0 MACs, 0.00% MACs, 94.41 us, 0.12% latency, 0.0 FLOPS, 32001, 4096)
    (layers): ModuleList(
      (0): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 96.3 GMACs, 3.06% MACs, 2.34 ms, 3.05% latency, 82.23 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.13 GMACs, 1.05% MACs, 1.16 ms, 1.51% latency, 57.16 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 156.4 us, 0.20% latency, 100.19 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 120.4 us, 0.16% latency, 130.15 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 116.83 us, 0.15% latency, 134.13 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 123.74 us, 0.16% latency, 126.64 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 36.95 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.17 GMACs, 2.01% MACs, 824.45 us, 1.07% latency, 153.25 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 238.9 us, 0.31% latency, 176.28 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 233.89 us, 0.30% latency, 180.06 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 226.02 us, 0.29% latency, 186.32 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 36.48 us, 0.05% latency, 140.93 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 155.69 us, 0.20% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 97.51 us, 0.13% latency, 0.0 FLOPS, )
      )
      (1): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 96.3 GMACs, 3.06% MACs, 2.19 ms, 2.85% latency, 87.81 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.13 GMACs, 1.05% MACs, 1.09 ms, 1.42% latency, 60.57 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 126.6 us, 0.16% latency, 123.77 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 115.63 us, 0.15% latency, 135.51 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 114.68 us, 0.15% latency, 136.64 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 121.12 us, 0.16% latency, 129.38 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.81 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.17 GMACs, 2.01% MACs, 812.77 us, 1.06% latency, 155.45 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 237.7 us, 0.31% latency, 177.17 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 232.22 us, 0.30% latency, 181.35 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 225.31 us, 0.29% latency, 186.91 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 155.12 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 98.94 us, 0.13% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.65 us, 0.12% latency, 0.0 FLOPS, )
      )
      (2): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 96.3 GMACs, 3.06% MACs, 2.17 ms, 2.82% latency, 88.88 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.13 GMACs, 1.05% MACs, 1.08 ms, 1.41% latency, 61.28 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 121.83 us, 0.16% latency, 128.62 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 115.39 us, 0.15% latency, 135.79 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 114.44 us, 0.15% latency, 136.93 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 123.26 us, 0.16% latency, 127.13 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.17 GMACs, 2.01% MACs, 810.38 us, 1.05% latency, 155.91 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 236.99 us, 0.31% latency, 177.7 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 233.17 us, 0.30% latency, 180.61 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 224.59 us, 0.29% latency, 187.51 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.19 us, 0.04% latency, 159.72 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.27 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.26 us, 0.11% latency, 0.0 FLOPS, )
      )
      (3): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 96.3 GMACs, 3.06% MACs, 2.17 ms, 2.83% latency, 88.71 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.13 GMACs, 1.05% MACs, 1.08 ms, 1.41% latency, 61.27 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 123.26 us, 0.16% latency, 127.13 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 114.92 us, 0.15% latency, 136.36 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 114.68 us, 0.15% latency, 136.64 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 120.64 us, 0.16% latency, 129.89 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.17 GMACs, 2.01% MACs, 813.01 us, 1.06% latency, 155.4 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 237.23 us, 0.31% latency, 177.52 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 233.41 us, 0.30% latency, 180.42 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 225.78 us, 0.29% latency, 186.52 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.66 us, 0.04% latency, 157.39 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.6 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.65 us, 0.12% latency, 0.0 FLOPS, )
      )
      (4): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 96.3 GMACs, 3.06% MACs, 2.18 ms, 2.84% latency, 88.27 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.13 GMACs, 1.05% MACs, 1.09 ms, 1.42% latency, 60.88 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 123.5 us, 0.16% latency, 126.88 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 117.3 us, 0.15% latency, 133.59 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 115.16 us, 0.15% latency, 136.08 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 120.64 us, 0.16% latency, 129.89 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.17 GMACs, 2.01% MACs, 817.78 us, 1.06% latency, 154.5 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 237.7 us, 0.31% latency, 177.17 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 232.7 us, 0.30% latency, 180.98 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 225.07 us, 0.29% latency, 187.11 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 37.67 us, 0.05% latency, 136.47 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 94.41 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.02 us, 0.11% latency, 0.0 FLOPS, )
      )
      (5): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 96.3 GMACs, 3.06% MACs, 2.16 ms, 2.81% latency, 89.07 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.13 GMACs, 1.05% MACs, 1.08 ms, 1.40% latency, 61.4 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 122.55 us, 0.16% latency, 127.87 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 115.87 us, 0.15% latency, 135.24 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 114.92 us, 0.15% latency, 136.36 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 120.64 us, 0.16% latency, 129.89 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.17 GMACs, 2.01% MACs, 809.43 us, 1.05% latency, 156.09 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 236.27 us, 0.31% latency, 178.24 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 231.74 us, 0.30% latency, 181.72 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 224.83 us, 0.29% latency, 187.31 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.71 us, 0.04% latency, 162.12 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.79 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.02 us, 0.11% latency, 0.0 FLOPS, )
      )
      (6): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 96.3 GMACs, 3.06% MACs, 2.16 ms, 2.81% latency, 89.08 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.13 GMACs, 1.05% MACs, 1.09 ms, 1.41% latency, 61.07 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 124.93 us, 0.16% latency, 125.43 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 116.35 us, 0.15% latency, 134.68 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 113.73 us, 0.15% latency, 137.79 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 120.4 us, 0.16% latency, 130.15 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.17 GMACs, 2.01% MACs, 807.29 us, 1.05% latency, 156.5 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 236.51 us, 0.31% latency, 178.06 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 232.7 us, 0.30% latency, 180.98 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 223.88 us, 0.29% latency, 188.11 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.95 us, 0.04% latency, 160.91 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.31 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.78 us, 0.11% latency, 0.0 FLOPS, )
      )
      (7): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 96.3 GMACs, 3.06% MACs, 2.16 ms, 2.81% latency, 89.12 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.13 GMACs, 1.05% MACs, 1.08 ms, 1.41% latency, 61.17 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 123.5 us, 0.16% latency, 126.88 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 115.87 us, 0.15% latency, 135.24 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 114.44 us, 0.15% latency, 136.93 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 121.36 us, 0.16% latency, 129.12 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.17 GMACs, 2.01% MACs, 808.0 us, 1.05% latency, 156.37 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 236.51 us, 0.31% latency, 178.06 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 232.93 us, 0.30% latency, 180.79 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 223.4 us, 0.29% latency, 188.51 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.71 us, 0.04% latency, 162.12 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.08 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 84.88 us, 0.11% latency, 0.0 FLOPS, )
      )
      (8): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 96.3 GMACs, 3.06% MACs, 2.16 ms, 2.81% latency, 89.05 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.13 GMACs, 1.05% MACs, 1.08 ms, 1.41% latency, 61.21 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 122.31 us, 0.16% latency, 128.12 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 116.35 us, 0.15% latency, 134.68 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 114.2 us, 0.15% latency, 137.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 122.07 us, 0.16% latency, 128.37 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.81 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.17 GMACs, 2.01% MACs, 809.67 us, 1.05% latency, 156.04 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 236.99 us, 0.31% latency, 177.7 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 233.17 us, 0.30% latency, 180.61 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 225.54 us, 0.29% latency, 186.72 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.23 us, 0.04% latency, 164.59 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.08 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 85.83 us, 0.11% latency, 0.0 FLOPS, )
      )
      (9): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 96.3 GMACs, 3.06% MACs, 2.16 ms, 2.81% latency, 89.15 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.13 GMACs, 1.05% MACs, 1.08 ms, 1.41% latency, 61.27 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 122.31 us, 0.16% latency, 128.12 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 116.11 us, 0.15% latency, 134.96 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 114.2 us, 0.15% latency, 137.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 124.93 us, 0.16% latency, 125.43 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.17 GMACs, 2.01% MACs, 809.91 us, 1.05% latency, 156.0 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 236.99 us, 0.31% latency, 177.7 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 233.17 us, 0.30% latency, 180.61 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 225.54 us, 0.29% latency, 186.72 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.95 us, 0.04% latency, 160.91 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.41 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.78 us, 0.11% latency, 0.0 FLOPS, )
      )
      (10): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 96.3 GMACs, 3.06% MACs, 2.17 ms, 2.82% latency, 88.74 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.13 GMACs, 1.05% MACs, 1.09 ms, 1.41% latency, 60.93 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 124.22 us, 0.16% latency, 126.15 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 115.39 us, 0.15% latency, 135.79 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 114.2 us, 0.15% latency, 137.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 120.88 us, 0.16% latency, 129.63 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.81 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.17 GMACs, 2.01% MACs, 809.67 us, 1.05% latency, 156.04 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 236.27 us, 0.31% latency, 178.24 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 233.41 us, 0.30% latency, 180.42 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 224.83 us, 0.29% latency, 187.31 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.95 us, 0.04% latency, 160.91 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.79 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.78 us, 0.11% latency, 0.0 FLOPS, )
      )
      (11): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 96.3 GMACs, 3.06% MACs, 2.17 ms, 2.82% latency, 88.78 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.13 GMACs, 1.05% MACs, 1.09 ms, 1.42% latency, 60.87 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 123.26 us, 0.16% latency, 127.13 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 123.02 us, 0.16% latency, 127.37 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 113.73 us, 0.15% latency, 137.79 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 120.88 us, 0.16% latency, 129.63 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.17 GMACs, 2.01% MACs, 806.81 us, 1.05% latency, 156.6 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 236.51 us, 0.31% latency, 178.06 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 232.46 us, 0.30% latency, 181.16 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 223.88 us, 0.29% latency, 188.11 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.71 us, 0.04% latency, 162.12 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.6 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.41 us, 0.12% latency, 0.0 FLOPS, )
      )
      (12): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 96.3 GMACs, 3.06% MACs, 2.17 ms, 2.82% latency, 88.96 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.13 GMACs, 1.05% MACs, 1.08 ms, 1.41% latency, 61.3 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 123.5 us, 0.16% latency, 126.88 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 115.63 us, 0.15% latency, 135.51 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 115.63 us, 0.15% latency, 135.51 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 122.31 us, 0.16% latency, 128.12 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.17 GMACs, 2.01% MACs, 809.91 us, 1.05% latency, 156.0 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 237.7 us, 0.31% latency, 177.17 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 233.41 us, 0.30% latency, 180.42 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 224.83 us, 0.29% latency, 187.31 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.23 us, 0.04% latency, 164.59 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.79 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.98 us, 0.11% latency, 0.0 FLOPS, )
      )
      (13): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 96.3 GMACs, 3.06% MACs, 2.17 ms, 2.83% latency, 88.6 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.13 GMACs, 1.05% MACs, 1.09 ms, 1.42% latency, 60.88 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 124.93 us, 0.16% latency, 125.43 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 115.39 us, 0.15% latency, 135.79 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 113.96 us, 0.15% latency, 137.5 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 121.83 us, 0.16% latency, 128.62 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.81 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.17 GMACs, 2.01% MACs, 812.53 us, 1.06% latency, 155.49 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 237.23 us, 0.31% latency, 177.52 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 232.7 us, 0.30% latency, 180.98 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 226.02 us, 0.29% latency, 186.32 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.42 us, 0.04% latency, 158.54 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.08 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.78 us, 0.11% latency, 0.0 FLOPS, )
      )
      (14): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 96.3 GMACs, 3.06% MACs, 2.17 ms, 2.82% latency, 88.92 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.13 GMACs, 1.05% MACs, 1.08 ms, 1.41% latency, 61.24 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 123.02 us, 0.16% latency, 127.37 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 116.59 us, 0.15% latency, 134.41 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 114.92 us, 0.15% latency, 136.36 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 121.12 us, 0.16% latency, 129.38 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.17 GMACs, 2.01% MACs, 812.77 us, 1.06% latency, 155.45 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 237.7 us, 0.31% latency, 177.17 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 232.7 us, 0.30% latency, 180.98 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 225.54 us, 0.29% latency, 186.72 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.42 us, 0.04% latency, 158.54 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.41 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.07 us, 0.11% latency, 0.0 FLOPS, )
      )
      (15): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 96.3 GMACs, 3.06% MACs, 2.16 ms, 2.81% latency, 89.05 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.13 GMACs, 1.05% MACs, 1.08 ms, 1.41% latency, 61.24 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 122.31 us, 0.16% latency, 128.12 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 115.63 us, 0.15% latency, 135.51 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 116.35 us, 0.15% latency, 134.68 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 121.12 us, 0.16% latency, 129.38 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.17 GMACs, 2.01% MACs, 807.76 us, 1.05% latency, 156.41 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 236.27 us, 0.31% latency, 178.24 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 232.22 us, 0.30% latency, 181.35 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 224.59 us, 0.29% latency, 187.51 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.95 us, 0.04% latency, 160.91 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.88 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.26 us, 0.11% latency, 0.0 FLOPS, )
      )
      (16): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 96.3 GMACs, 3.06% MACs, 2.18 ms, 2.83% latency, 88.45 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.13 GMACs, 1.05% MACs, 1.09 ms, 1.42% latency, 60.65 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 123.02 us, 0.16% latency, 127.37 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 115.16 us, 0.15% latency, 136.08 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 116.11 us, 0.15% latency, 134.96 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 121.83 us, 0.16% latency, 128.62 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 36.95 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.17 GMACs, 2.01% MACs, 812.29 us, 1.06% latency, 155.54 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 237.94 us, 0.31% latency, 176.99 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 232.93 us, 0.30% latency, 180.79 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 225.07 us, 0.29% latency, 187.11 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.95 us, 0.04% latency, 160.91 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.79 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.78 us, 0.11% latency, 0.0 FLOPS, )
      )
      (17): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 96.3 GMACs, 3.06% MACs, 2.17 ms, 2.82% latency, 88.81 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.13 GMACs, 1.05% MACs, 1.09 ms, 1.42% latency, 60.84 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 122.79 us, 0.16% latency, 127.62 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 115.87 us, 0.15% latency, 135.24 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 114.44 us, 0.15% latency, 136.93 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 120.88 us, 0.16% latency, 129.63 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 36.24 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.17 GMACs, 2.01% MACs, 805.85 us, 1.05% latency, 156.78 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 236.03 us, 0.31% latency, 178.42 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 231.98 us, 0.30% latency, 181.54 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 223.4 us, 0.29% latency, 188.51 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.19 us, 0.04% latency, 159.72 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.41 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.07 us, 0.11% latency, 0.0 FLOPS, )
      )
      (18): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 96.3 GMACs, 3.06% MACs, 2.17 ms, 2.82% latency, 88.9 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.13 GMACs, 1.05% MACs, 1.08 ms, 1.41% latency, 61.12 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 123.74 us, 0.16% latency, 126.64 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 115.87 us, 0.15% latency, 135.24 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 113.96 us, 0.15% latency, 137.5 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 121.36 us, 0.16% latency, 129.12 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.81 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.17 GMACs, 2.01% MACs, 808.0 us, 1.05% latency, 156.37 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 236.03 us, 0.31% latency, 178.42 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 231.74 us, 0.30% latency, 181.72 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 224.83 us, 0.29% latency, 187.31 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.19 us, 0.04% latency, 159.72 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.93 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.65 us, 0.12% latency, 0.0 FLOPS, )
      )
      (19): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 96.3 GMACs, 3.06% MACs, 2.16 ms, 2.81% latency, 89.19 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.13 GMACs, 1.05% MACs, 1.08 ms, 1.40% latency, 61.36 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 122.31 us, 0.16% latency, 128.12 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 114.92 us, 0.15% latency, 136.36 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 114.68 us, 0.15% latency, 136.64 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 119.92 us, 0.16% latency, 130.66 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.81 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.17 GMACs, 2.01% MACs, 805.85 us, 1.05% latency, 156.78 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 235.8 us, 0.31% latency, 178.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 231.5 us, 0.30% latency, 181.91 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 224.11 us, 0.29% latency, 187.91 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.71 us, 0.04% latency, 162.12 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.74 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 85.83 us, 0.11% latency, 0.0 FLOPS, )
      )
      (20): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 96.3 GMACs, 3.06% MACs, 2.15 ms, 2.80% latency, 89.54 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.13 GMACs, 1.05% MACs, 1.08 ms, 1.40% latency, 61.61 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 121.59 us, 0.16% latency, 128.87 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 114.2 us, 0.15% latency, 137.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 113.96 us, 0.15% latency, 137.5 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 119.92 us, 0.16% latency, 130.66 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.17 GMACs, 2.01% MACs, 804.9 us, 1.05% latency, 156.97 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 235.56 us, 0.31% latency, 178.78 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 231.74 us, 0.30% latency, 181.72 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 224.35 us, 0.29% latency, 187.71 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.71 us, 0.04% latency, 162.12 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.55 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.07 us, 0.11% latency, 0.0 FLOPS, )
      )
      (21): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 96.3 GMACs, 3.06% MACs, 2.19 ms, 2.85% latency, 87.99 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.13 GMACs, 1.05% MACs, 1.09 ms, 1.42% latency, 60.61 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 127.79 us, 0.17% latency, 122.62 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 116.83 us, 0.15% latency, 134.13 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 114.44 us, 0.15% latency, 136.93 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 122.55 us, 0.16% latency, 127.87 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.29 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.17 GMACs, 2.01% MACs, 809.43 us, 1.05% latency, 156.09 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 237.46 us, 0.31% latency, 177.34 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 231.98 us, 0.30% latency, 181.54 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 225.31 us, 0.29% latency, 186.91 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.42 us, 0.04% latency, 158.54 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 96.08 us, 0.13% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.94 us, 0.12% latency, 0.0 FLOPS, )
      )
      (22): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 96.3 GMACs, 3.06% MACs, 2.16 ms, 2.81% latency, 89.29 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.13 GMACs, 1.05% MACs, 1.08 ms, 1.41% latency, 61.31 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 122.79 us, 0.16% latency, 127.62 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 116.11 us, 0.15% latency, 134.96 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 113.96 us, 0.15% latency, 137.5 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 121.12 us, 0.16% latency, 129.38 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.17 GMACs, 2.01% MACs, 807.29 us, 1.05% latency, 156.5 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 235.32 us, 0.31% latency, 178.96 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 231.5 us, 0.30% latency, 181.91 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 224.35 us, 0.29% latency, 187.71 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.19 us, 0.04% latency, 159.72 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.36 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 84.64 us, 0.11% latency, 0.0 FLOPS, )
      )
      (23): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 96.3 GMACs, 3.06% MACs, 2.16 ms, 2.81% latency, 89.06 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.13 GMACs, 1.05% MACs, 1.08 ms, 1.40% latency, 61.36 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 123.02 us, 0.16% latency, 127.37 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 116.35 us, 0.15% latency, 134.68 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 114.44 us, 0.15% latency, 136.93 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 122.31 us, 0.16% latency, 128.12 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.66 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.17 GMACs, 2.01% MACs, 808.0 us, 1.05% latency, 156.37 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 236.03 us, 0.31% latency, 178.42 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 231.74 us, 0.30% latency, 181.72 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 224.83 us, 0.29% latency, 187.31 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.19 us, 0.04% latency, 159.72 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.41 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.93 us, 0.12% latency, 0.0 FLOPS, )
      )
      (24): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 96.3 GMACs, 3.06% MACs, 2.17 ms, 2.82% latency, 88.96 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.13 GMACs, 1.05% MACs, 1.09 ms, 1.41% latency, 60.97 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 123.74 us, 0.16% latency, 126.64 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 114.68 us, 0.15% latency, 136.64 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 113.49 us, 0.15% latency, 138.08 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 120.4 us, 0.16% latency, 130.15 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.17 GMACs, 2.01% MACs, 805.14 us, 1.05% latency, 156.92 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 235.56 us, 0.31% latency, 178.78 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 231.74 us, 0.30% latency, 181.72 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 223.64 us, 0.29% latency, 188.31 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.47 us, 0.04% latency, 163.35 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.98 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 85.83 us, 0.11% latency, 0.0 FLOPS, )
      )
      (25): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 96.3 GMACs, 3.06% MACs, 2.18 ms, 2.83% latency, 88.49 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.13 GMACs, 1.05% MACs, 1.09 ms, 1.42% latency, 60.8 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 123.02 us, 0.16% latency, 127.37 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 115.16 us, 0.15% latency, 136.08 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 119.69 us, 0.16% latency, 130.93 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 120.4 us, 0.16% latency, 130.15 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.33 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.17 GMACs, 2.01% MACs, 805.38 us, 1.05% latency, 156.88 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 236.03 us, 0.31% latency, 178.42 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 231.74 us, 0.30% latency, 181.72 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 223.4 us, 0.29% latency, 188.51 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.47 us, 0.04% latency, 163.35 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.41 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.31 us, 0.11% latency, 0.0 FLOPS, )
      )
      (26): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 96.3 GMACs, 3.06% MACs, 2.16 ms, 2.81% latency, 89.28 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.13 GMACs, 1.05% MACs, 1.08 ms, 1.40% latency, 61.4 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 122.55 us, 0.16% latency, 127.87 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 115.63 us, 0.15% latency, 135.51 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 113.73 us, 0.15% latency, 137.79 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 123.26 us, 0.16% latency, 127.13 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.17 GMACs, 2.01% MACs, 808.0 us, 1.05% latency, 156.37 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 236.03 us, 0.31% latency, 178.42 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 232.46 us, 0.30% latency, 181.16 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 223.88 us, 0.29% latency, 188.11 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.95 us, 0.04% latency, 160.91 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.65 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.31 us, 0.11% latency, 0.0 FLOPS, )
      )
      (27): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 96.3 GMACs, 3.06% MACs, 2.17 ms, 2.83% latency, 88.58 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.13 GMACs, 1.05% MACs, 1.09 ms, 1.42% latency, 60.83 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 123.74 us, 0.16% latency, 126.64 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 115.87 us, 0.15% latency, 135.24 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 114.2 us, 0.15% latency, 137.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 121.36 us, 0.16% latency, 129.12 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.52 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.17 GMACs, 2.01% MACs, 807.52 us, 1.05% latency, 156.46 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 235.56 us, 0.31% latency, 178.78 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 231.98 us, 0.30% latency, 181.54 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 223.64 us, 0.29% latency, 188.31 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.95 us, 0.04% latency, 160.91 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.22 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.07 us, 0.11% latency, 0.0 FLOPS, )
      )
      (28): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 96.3 GMACs, 3.06% MACs, 2.17 ms, 2.82% latency, 88.77 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.13 GMACs, 1.05% MACs, 1.09 ms, 1.42% latency, 60.85 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 121.83 us, 0.16% latency, 128.62 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 114.92 us, 0.15% latency, 136.36 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 113.73 us, 0.15% latency, 137.79 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 120.88 us, 0.16% latency, 129.63 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.81 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.17 GMACs, 2.01% MACs, 806.57 us, 1.05% latency, 156.64 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 236.03 us, 0.31% latency, 178.42 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 231.98 us, 0.30% latency, 181.54 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 223.64 us, 0.29% latency, 188.31 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.95 us, 0.04% latency, 160.91 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.03 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 85.35 us, 0.11% latency, 0.0 FLOPS, )
      )
      (29): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 96.3 GMACs, 3.06% MACs, 2.17 ms, 2.82% latency, 88.76 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.13 GMACs, 1.05% MACs, 1.09 ms, 1.41% latency, 60.93 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 123.5 us, 0.16% latency, 126.88 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 121.83 us, 0.16% latency, 128.62 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 114.2 us, 0.15% latency, 137.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 120.16 us, 0.16% latency, 130.41 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.81 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.17 GMACs, 2.01% MACs, 809.43 us, 1.05% latency, 156.09 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 236.27 us, 0.31% latency, 178.24 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 231.74 us, 0.30% latency, 181.72 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 225.07 us, 0.29% latency, 187.11 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.95 us, 0.04% latency, 160.91 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.36 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.74 us, 0.11% latency, 0.0 FLOPS, )
      )
      (30): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 96.3 GMACs, 3.06% MACs, 2.16 ms, 2.81% latency, 89.04 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.13 GMACs, 1.05% MACs, 1.09 ms, 1.42% latency, 60.85 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 122.79 us, 0.16% latency, 127.62 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 114.92 us, 0.15% latency, 136.36 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 113.49 us, 0.15% latency, 138.08 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 121.59 us, 0.16% latency, 128.87 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.17 GMACs, 2.01% MACs, 803.47 us, 1.05% latency, 157.25 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 235.32 us, 0.31% latency, 178.96 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 232.22 us, 0.30% latency, 181.35 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 223.64 us, 0.29% latency, 188.31 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.95 us, 0.04% latency, 160.91 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.69 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.02 us, 0.11% latency, 0.0 FLOPS, )
      )
      (31): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 96.3 GMACs, 3.06% MACs, 2.15 ms, 2.80% latency, 89.43 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 33.13 GMACs, 1.05% MACs, 1.08 ms, 1.41% latency, 61.28 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 122.55 us, 0.16% latency, 127.87 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 116.35 us, 0.15% latency, 134.68 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 115.63 us, 0.15% latency, 135.51 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.83 GMACs, 0.25% MACs, 120.16 us, 0.16% latency, 130.41 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 63.17 GMACs, 2.01% MACs, 802.28 us, 1.04% latency, 157.48 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 236.03 us, 0.31% latency, 178.42 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 230.31 us, 0.30% latency, 182.85 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 21.06 GMACs, 0.67% MACs, 223.64 us, 0.29% latency, 188.31 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.71 us, 0.04% latency, 162.12 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.65 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 84.16 us, 0.11% latency, 0.0 FLOPS, )
      )
    )
    (norm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 95.37 us, 0.12% latency, 0.0 FLOPS, )
  )
  (lm_head): Linear(131.08 M, 1.95% Params, 61.21 GMACs, 1.95% MACs, 1.91 ms, 2.49% latency, 64.0 TFLOPS, in_features=4096, out_features=32001, bias=False)
)
------------------------------------------------------------------------------

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 128:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

world size:                                                   4       
data parallel size:                                           4       
model parallel size:                                          1       
batch size per GPU:                                           1       
params per gpu:                                               6738.42 M
params of model = params per GPU * mp_size:                   6738.42 M
fwd MACs per GPU:                                             2582.58 GMACs
fwd flops per GPU:                                            5165.45 G
fwd flops of model = fwd flops per GPU * mp_size:             5165.45 G
fwd latency:                                                  73.34 ms
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:          70.43 TFLOPS
bwd latency:                                                  213.45 ms
bwd FLOPS per GPU = 2.0 * fwd flops per GPU / bwd latency:    48.4 TFLOPS
fwd+bwd FLOPS per GPU = 3.0 * fwd flops per GPU / (fwd+bwd latency):   54.03 TFLOPS
step latency:                                                 29.7 us 
iter latency:                                                 286.82 ms
FLOPS per GPU = 3.0 * fwd flops per GPU / iter latency:       54.03 TFLOPS
samples/second:                                               13.95   

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'LlamaForCausalLM': '6738.42 M'}
    MACs        - {'LlamaForCausalLM': '2582.58 GMACs'}
    fwd latency - {'LlamaForCausalLM': '73.25 ms'}
depth 1:
    params      - {'LlamaModel': '6607.35 M'}
    MACs        - {'LlamaModel': '2532.12 GMACs'}
    fwd latency - {'LlamaModel': '71.13 ms'}
depth 2:
    params      - {'ModuleList': '6476.27 M'}
    MACs        - {'ModuleList': '2532.12 GMACs'}
    fwd latency - {'ModuleList': '65.98 ms'}
depth 3:
    params      - {'LlamaDecoderLayer': '6476.27 M'}
    MACs        - {'LlamaDecoderLayer': '2532.12 GMACs'}
    fwd latency - {'LlamaDecoderLayer': '65.98 ms'}
depth 4:
    params      - {'LlamaMLP': '4328.52 M'}
    MACs        - {'LlamaMLP': '1666.48 GMACs'}
    fwd latency - {'LlamaAttention': '31.68 ms'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

LlamaForCausalLM(
  6738.42 M, 100.00% Params, 2582.58 GMACs, 100.00% MACs, 73.25 ms, 100.00% latency, 70.52 TFLOPS, 
  (model): LlamaModel(
    6607.35 M, 98.05% Params, 2532.12 GMACs, 98.05% MACs, 71.13 ms, 97.11% latency, 71.2 TFLOPS, 
    (embed_tokens): Embedding(131.08 M, 1.95% Params, 0 MACs, 0.00% MACs, 77.96 us, 0.11% latency, 0.0 FLOPS, 32001, 4096)
    (layers): ModuleList(
      (0): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 79.13 GMACs, 3.06% MACs, 2.24 ms, 3.06% latency, 70.52 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 27.05 GMACs, 1.05% MACs, 1.07 ms, 1.47% latency, 50.38 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 158.07 us, 0.22% latency, 81.73 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 121.12 us, 0.17% latency, 106.66 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 115.63 us, 0.16% latency, 111.72 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 125.89 us, 0.17% latency, 102.62 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 39.1 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 52.08 GMACs, 2.02% MACs, 811.34 us, 1.11% latency, 128.38 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 236.75 us, 0.32% latency, 146.65 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 231.5 us, 0.32% latency, 149.97 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 224.11 us, 0.31% latency, 154.91 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.71 us, 0.04% latency, 133.65 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 159.03 us, 0.22% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.98 us, 0.13% latency, 0.0 FLOPS, )
      )
      (1): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 79.13 GMACs, 3.06% MACs, 2.09 ms, 2.86% latency, 75.55 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 27.05 GMACs, 1.05% MACs, 1.01 ms, 1.38% latency, 53.52 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 125.89 us, 0.17% latency, 102.62 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 117.06 us, 0.16% latency, 110.35 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 114.2 us, 0.16% latency, 113.12 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 123.74 us, 0.17% latency, 104.4 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 36.0 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 52.08 GMACs, 2.02% MACs, 804.9 us, 1.10% latency, 129.41 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 236.27 us, 0.32% latency, 146.94 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 231.98 us, 0.32% latency, 149.66 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 222.44 us, 0.30% latency, 156.08 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.23 us, 0.04% latency, 135.69 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 95.61 us, 0.13% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.26 us, 0.12% latency, 0.0 FLOPS, )
      )
      (2): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 79.13 GMACs, 3.06% MACs, 2.08 ms, 2.83% latency, 76.26 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 27.05 GMACs, 1.05% MACs, 998.02 us, 1.36% latency, 54.21 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 127.32 us, 0.17% latency, 101.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 115.16 us, 0.16% latency, 112.18 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 113.96 us, 0.16% latency, 113.36 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 123.02 us, 0.17% latency, 105.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.52 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 52.08 GMACs, 2.02% MACs, 804.66 us, 1.10% latency, 129.44 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 236.27 us, 0.32% latency, 146.94 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 232.46 us, 0.32% latency, 149.35 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 224.83 us, 0.31% latency, 154.42 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.52 us, 0.04% latency, 138.87 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.27 us, 0.13% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 84.16 us, 0.11% latency, 0.0 FLOPS, )
      )
      (3): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 79.13 GMACs, 3.06% MACs, 2.05 ms, 2.80% latency, 77.12 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 27.05 GMACs, 1.05% MACs, 985.86 us, 1.35% latency, 54.88 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 123.74 us, 0.17% latency, 104.4 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 116.35 us, 0.16% latency, 111.03 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 113.49 us, 0.15% latency, 113.83 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 121.59 us, 0.17% latency, 106.24 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.05 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 52.08 GMACs, 2.02% MACs, 800.85 us, 1.09% latency, 130.06 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 235.8 us, 0.32% latency, 147.24 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 230.79 us, 0.32% latency, 150.43 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 222.92 us, 0.30% latency, 155.74 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 29.8 us, 0.04% latency, 142.21 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.41 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 82.49 us, 0.11% latency, 0.0 FLOPS, )
      )
      (4): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 79.13 GMACs, 3.06% MACs, 2.04 ms, 2.78% latency, 77.73 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 27.05 GMACs, 1.05% MACs, 973.46 us, 1.33% latency, 55.58 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 122.55 us, 0.17% latency, 105.42 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 113.96 us, 0.16% latency, 113.36 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 113.01 us, 0.15% latency, 114.31 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 120.16 us, 0.16% latency, 107.51 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.33 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 52.08 GMACs, 2.02% MACs, 802.76 us, 1.10% latency, 129.75 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 235.32 us, 0.32% latency, 147.54 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 231.27 us, 0.32% latency, 150.12 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 223.88 us, 0.31% latency, 155.08 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.28 us, 0.04% latency, 139.97 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 85.83 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 80.82 us, 0.11% latency, 0.0 FLOPS, )
      )
      (5): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 79.13 GMACs, 3.06% MACs, 2.06 ms, 2.81% latency, 76.89 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 27.05 GMACs, 1.05% MACs, 988.25 us, 1.35% latency, 54.75 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 124.45 us, 0.17% latency, 103.8 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 114.92 us, 0.16% latency, 112.41 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 113.96 us, 0.16% latency, 113.36 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 123.74 us, 0.17% latency, 104.4 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.05 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 52.08 GMACs, 2.02% MACs, 801.09 us, 1.09% latency, 130.02 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 235.08 us, 0.32% latency, 147.69 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 231.27 us, 0.32% latency, 150.12 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 223.88 us, 0.31% latency, 155.08 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.28 us, 0.04% latency, 139.97 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.26 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.78 us, 0.12% latency, 0.0 FLOPS, )
      )
      (6): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 79.13 GMACs, 3.06% MACs, 2.04 ms, 2.79% latency, 77.43 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 27.05 GMACs, 1.05% MACs, 979.9 us, 1.34% latency, 55.22 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 124.69 us, 0.17% latency, 103.6 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 114.44 us, 0.16% latency, 112.88 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 113.49 us, 0.15% latency, 113.83 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 121.12 us, 0.17% latency, 106.66 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.62 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 52.08 GMACs, 2.02% MACs, 798.7 us, 1.09% latency, 130.41 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 235.08 us, 0.32% latency, 147.69 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 230.55 us, 0.31% latency, 150.59 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 222.92 us, 0.30% latency, 155.74 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.52 us, 0.04% latency, 138.87 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.41 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 81.54 us, 0.11% latency, 0.0 FLOPS, )
      )
      (7): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 79.13 GMACs, 3.06% MACs, 2.04 ms, 2.78% latency, 77.62 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 27.05 GMACs, 1.05% MACs, 978.47 us, 1.34% latency, 55.3 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 124.22 us, 0.17% latency, 104.0 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 114.92 us, 0.16% latency, 112.41 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 113.25 us, 0.15% latency, 114.07 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 120.4 us, 0.16% latency, 107.29 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 52.08 GMACs, 2.02% MACs, 800.61 us, 1.09% latency, 130.1 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 235.56 us, 0.32% latency, 147.39 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 231.5 us, 0.32% latency, 149.97 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 223.64 us, 0.31% latency, 155.24 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 29.8 us, 0.04% latency, 142.21 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.31 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 81.3 us, 0.11% latency, 0.0 FLOPS, )
      )
      (8): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 79.13 GMACs, 3.06% MACs, 2.06 ms, 2.81% latency, 76.93 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 27.05 GMACs, 1.05% MACs, 986.1 us, 1.35% latency, 54.87 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 124.93 us, 0.17% latency, 103.4 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 115.16 us, 0.16% latency, 112.18 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 113.01 us, 0.15% latency, 114.31 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 122.07 us, 0.17% latency, 105.83 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 52.08 GMACs, 2.02% MACs, 802.04 us, 1.09% latency, 129.87 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 236.27 us, 0.32% latency, 146.94 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 231.74 us, 0.32% latency, 149.81 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 222.68 us, 0.30% latency, 155.91 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.04 us, 0.04% latency, 141.08 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.93 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 83.68 us, 0.11% latency, 0.0 FLOPS, )
      )
      (9): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 79.13 GMACs, 3.06% MACs, 2.05 ms, 2.80% latency, 77.26 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 27.05 GMACs, 1.05% MACs, 983.95 us, 1.34% latency, 54.99 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 123.26 us, 0.17% latency, 104.8 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 114.68 us, 0.16% latency, 112.65 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 113.49 us, 0.15% latency, 113.83 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 121.12 us, 0.17% latency, 106.66 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 52.08 GMACs, 2.02% MACs, 800.61 us, 1.09% latency, 130.1 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 234.6 us, 0.32% latency, 147.99 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 231.27 us, 0.32% latency, 150.12 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 223.64 us, 0.31% latency, 155.24 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.04 us, 0.04% latency, 141.08 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.74 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 82.97 us, 0.11% latency, 0.0 FLOPS, )
      )
      (10): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 79.13 GMACs, 3.06% MACs, 2.08 ms, 2.84% latency, 76.16 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 27.05 GMACs, 1.05% MACs, 1.0 ms, 1.37% latency, 53.97 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 125.89 us, 0.17% latency, 102.62 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 115.87 us, 0.16% latency, 111.49 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 113.25 us, 0.15% latency, 114.07 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 122.79 us, 0.17% latency, 105.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.52 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 52.08 GMACs, 2.02% MACs, 805.38 us, 1.10% latency, 129.33 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 236.27 us, 0.32% latency, 146.94 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 231.03 us, 0.32% latency, 150.28 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 224.59 us, 0.31% latency, 154.59 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 29.8 us, 0.04% latency, 142.21 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.17 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 83.45 us, 0.11% latency, 0.0 FLOPS, )
      )
      (11): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 79.13 GMACs, 3.06% MACs, 2.05 ms, 2.79% latency, 77.39 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 27.05 GMACs, 1.05% MACs, 978.47 us, 1.34% latency, 55.3 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 123.98 us, 0.17% latency, 104.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 113.73 us, 0.16% latency, 113.59 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 112.77 us, 0.15% latency, 114.55 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 120.16 us, 0.16% latency, 107.51 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 52.08 GMACs, 2.02% MACs, 800.37 us, 1.09% latency, 130.14 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 234.6 us, 0.32% latency, 147.99 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 230.31 us, 0.31% latency, 150.74 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 223.64 us, 0.31% latency, 155.24 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.04 us, 0.04% latency, 141.08 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.41 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 81.06 us, 0.11% latency, 0.0 FLOPS, )
      )
      (12): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 79.13 GMACs, 3.06% MACs, 2.05 ms, 2.80% latency, 77.3 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 27.05 GMACs, 1.05% MACs, 989.68 us, 1.35% latency, 54.67 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 123.98 us, 0.17% latency, 104.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 114.92 us, 0.16% latency, 112.41 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 114.44 us, 0.16% latency, 112.88 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 119.92 us, 0.16% latency, 107.72 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.05 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 52.08 GMACs, 2.02% MACs, 798.23 us, 1.09% latency, 130.49 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 234.37 us, 0.32% latency, 148.14 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 230.31 us, 0.31% latency, 150.74 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 223.88 us, 0.31% latency, 155.08 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.04 us, 0.04% latency, 141.08 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.02 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 81.3 us, 0.11% latency, 0.0 FLOPS, )
      )
      (13): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 79.13 GMACs, 3.06% MACs, 2.05 ms, 2.79% latency, 77.38 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 27.05 GMACs, 1.05% MACs, 975.85 us, 1.33% latency, 55.45 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 122.79 us, 0.17% latency, 105.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 114.92 us, 0.16% latency, 112.41 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 113.49 us, 0.15% latency, 113.83 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 120.64 us, 0.16% latency, 107.08 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.95 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 52.08 GMACs, 2.02% MACs, 802.52 us, 1.10% latency, 129.79 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 235.08 us, 0.32% latency, 147.69 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 231.27 us, 0.32% latency, 150.12 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 223.4 us, 0.30% latency, 155.41 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.04 us, 0.04% latency, 141.08 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.26 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.31 us, 0.12% latency, 0.0 FLOPS, )
      )
      (14): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 79.13 GMACs, 3.06% MACs, 2.05 ms, 2.80% latency, 77.22 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 27.05 GMACs, 1.05% MACs, 983.48 us, 1.34% latency, 55.02 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 123.26 us, 0.17% latency, 104.8 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 116.11 us, 0.16% latency, 111.26 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 114.2 us, 0.16% latency, 113.12 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 120.88 us, 0.17% latency, 106.87 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 52.08 GMACs, 2.02% MACs, 798.7 us, 1.09% latency, 130.41 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 233.89 us, 0.32% latency, 148.44 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 231.03 us, 0.32% latency, 150.28 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 222.21 us, 0.30% latency, 156.24 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.04 us, 0.04% latency, 141.08 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.84 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 81.3 us, 0.11% latency, 0.0 FLOPS, )
      )
      (15): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 79.13 GMACs, 3.06% MACs, 2.04 ms, 2.79% latency, 77.55 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 27.05 GMACs, 1.05% MACs, 979.66 us, 1.34% latency, 55.23 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 122.79 us, 0.17% latency, 105.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 114.68 us, 0.16% latency, 112.65 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 112.77 us, 0.15% latency, 114.55 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 121.12 us, 0.17% latency, 106.66 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.81 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 52.08 GMACs, 2.02% MACs, 798.7 us, 1.09% latency, 130.41 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 233.89 us, 0.32% latency, 148.44 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 230.79 us, 0.32% latency, 150.43 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 222.44 us, 0.30% latency, 156.08 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 29.8 us, 0.04% latency, 142.21 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.5 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 82.02 us, 0.11% latency, 0.0 FLOPS, )
      )
      (16): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 79.13 GMACs, 3.06% MACs, 2.08 ms, 2.84% latency, 75.95 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 27.05 GMACs, 1.05% MACs, 999.93 us, 1.37% latency, 54.11 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 128.51 us, 0.18% latency, 100.53 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 116.11 us, 0.16% latency, 111.26 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 112.3 us, 0.15% latency, 115.04 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 123.98 us, 0.17% latency, 104.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.76 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 52.08 GMACs, 2.02% MACs, 805.38 us, 1.10% latency, 129.33 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 236.27 us, 0.32% latency, 146.94 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 231.27 us, 0.32% latency, 150.12 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 223.64 us, 0.31% latency, 155.24 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.52 us, 0.04% latency, 138.87 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.7 us, 0.13% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 85.35 us, 0.12% latency, 0.0 FLOPS, )
      )
      (17): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 79.13 GMACs, 3.06% MACs, 2.06 ms, 2.82% latency, 76.71 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 27.05 GMACs, 1.05% MACs, 989.68 us, 1.35% latency, 54.67 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 123.02 us, 0.17% latency, 105.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 116.59 us, 0.16% latency, 110.81 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 114.44 us, 0.16% latency, 112.88 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 120.64 us, 0.16% latency, 107.08 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.05 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 52.08 GMACs, 2.02% MACs, 803.47 us, 1.10% latency, 129.64 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 234.13 us, 0.32% latency, 148.29 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 231.03 us, 0.32% latency, 150.28 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 223.64 us, 0.31% latency, 155.24 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.04 us, 0.04% latency, 141.08 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.93 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 83.68 us, 0.11% latency, 0.0 FLOPS, )
      )
      (18): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 79.13 GMACs, 3.06% MACs, 2.05 ms, 2.80% latency, 77.11 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 27.05 GMACs, 1.05% MACs, 985.38 us, 1.35% latency, 54.91 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 123.98 us, 0.17% latency, 104.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 115.16 us, 0.16% latency, 112.18 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 114.2 us, 0.16% latency, 113.12 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 119.21 us, 0.16% latency, 108.37 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.05 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 52.08 GMACs, 2.02% MACs, 800.13 us, 1.09% latency, 130.18 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 234.13 us, 0.32% latency, 148.29 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 230.31 us, 0.31% latency, 150.74 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 222.92 us, 0.30% latency, 155.74 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.04 us, 0.04% latency, 141.08 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.98 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 81.54 us, 0.11% latency, 0.0 FLOPS, )
      )
      (19): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 79.13 GMACs, 3.06% MACs, 2.09 ms, 2.86% latency, 75.61 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 27.05 GMACs, 1.05% MACs, 1.0 ms, 1.37% latency, 53.85 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 128.27 us, 0.18% latency, 100.71 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 116.11 us, 0.16% latency, 111.26 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 113.01 us, 0.15% latency, 114.31 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 123.26 us, 0.17% latency, 104.8 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.76 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 52.08 GMACs, 2.02% MACs, 810.86 us, 1.11% latency, 128.46 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 236.27 us, 0.32% latency, 146.94 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 231.03 us, 0.32% latency, 150.28 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 223.4 us, 0.30% latency, 155.41 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.28 us, 0.04% latency, 139.97 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 95.84 us, 0.13% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 83.68 us, 0.11% latency, 0.0 FLOPS, )
      )
      (20): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 79.13 GMACs, 3.06% MACs, 2.05 ms, 2.80% latency, 77.3 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 27.05 GMACs, 1.05% MACs, 982.28 us, 1.34% latency, 55.08 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 123.98 us, 0.17% latency, 104.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 113.96 us, 0.16% latency, 113.36 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 113.01 us, 0.15% latency, 114.31 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 121.59 us, 0.17% latency, 106.24 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.29 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 52.08 GMACs, 2.02% MACs, 799.89 us, 1.09% latency, 130.22 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 235.08 us, 0.32% latency, 147.69 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 231.5 us, 0.32% latency, 149.97 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 223.4 us, 0.30% latency, 155.41 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.04 us, 0.04% latency, 141.08 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.74 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 82.49 us, 0.11% latency, 0.0 FLOPS, )
      )
      (21): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 79.13 GMACs, 3.06% MACs, 2.05 ms, 2.80% latency, 77.29 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 27.05 GMACs, 1.05% MACs, 980.85 us, 1.34% latency, 55.16 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 122.07 us, 0.17% latency, 105.83 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 113.96 us, 0.16% latency, 113.36 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 112.77 us, 0.15% latency, 114.55 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 121.83 us, 0.17% latency, 106.04 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.05 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 52.08 GMACs, 2.02% MACs, 802.04 us, 1.09% latency, 129.87 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 235.32 us, 0.32% latency, 147.54 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 231.5 us, 0.32% latency, 149.97 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 222.92 us, 0.30% latency, 155.74 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.76 us, 0.04% latency, 137.8 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 85.35 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 84.4 us, 0.12% latency, 0.0 FLOPS, )
      )
      (22): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 79.13 GMACs, 3.06% MACs, 2.06 ms, 2.81% latency, 76.83 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 27.05 GMACs, 1.05% MACs, 996.59 us, 1.36% latency, 54.29 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 124.69 us, 0.17% latency, 103.6 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 116.35 us, 0.16% latency, 111.03 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 112.53 us, 0.15% latency, 114.8 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 120.64 us, 0.16% latency, 107.08 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 36.0 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 52.08 GMACs, 2.02% MACs, 799.89 us, 1.09% latency, 130.22 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 235.32 us, 0.32% latency, 147.54 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 231.27 us, 0.32% latency, 150.12 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 223.4 us, 0.30% latency, 155.41 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 29.8 us, 0.04% latency, 142.21 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.45 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 81.3 us, 0.11% latency, 0.0 FLOPS, )
      )
      (23): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 79.13 GMACs, 3.06% MACs, 2.04 ms, 2.79% latency, 77.49 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 27.05 GMACs, 1.05% MACs, 981.81 us, 1.34% latency, 55.11 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 123.5 us, 0.17% latency, 104.6 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 115.87 us, 0.16% latency, 111.49 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 113.25 us, 0.15% latency, 114.07 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 121.36 us, 0.17% latency, 106.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.62 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 52.08 GMACs, 2.02% MACs, 799.89 us, 1.09% latency, 130.22 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 234.6 us, 0.32% latency, 147.99 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 230.55 us, 0.31% latency, 150.59 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 222.44 us, 0.30% latency, 156.08 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.28 us, 0.04% latency, 139.97 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.55 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 82.02 us, 0.11% latency, 0.0 FLOPS, )
      )
      (24): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 79.13 GMACs, 3.06% MACs, 2.05 ms, 2.80% latency, 77.19 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 27.05 GMACs, 1.05% MACs, 985.62 us, 1.35% latency, 54.9 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 122.79 us, 0.17% latency, 105.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 115.63 us, 0.16% latency, 111.72 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 112.77 us, 0.15% latency, 114.55 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 123.98 us, 0.17% latency, 104.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 52.08 GMACs, 2.02% MACs, 798.7 us, 1.09% latency, 130.41 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 235.32 us, 0.32% latency, 147.54 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 230.31 us, 0.31% latency, 150.74 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 223.64 us, 0.31% latency, 155.24 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.04 us, 0.04% latency, 141.08 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 84.64 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 84.4 us, 0.12% latency, 0.0 FLOPS, )
      )
      (25): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 79.13 GMACs, 3.06% MACs, 2.07 ms, 2.83% latency, 76.44 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 27.05 GMACs, 1.05% MACs, 1.0 ms, 1.37% latency, 53.85 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 144.0 us, 0.20% latency, 89.71 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 116.35 us, 0.16% latency, 111.03 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 113.96 us, 0.16% latency, 113.36 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 121.36 us, 0.17% latency, 106.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.05 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 52.08 GMACs, 2.02% MACs, 800.13 us, 1.09% latency, 130.18 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 235.08 us, 0.32% latency, 147.69 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 231.27 us, 0.32% latency, 150.12 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 222.44 us, 0.30% latency, 156.08 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.04 us, 0.04% latency, 141.08 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.74 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 84.4 us, 0.12% latency, 0.0 FLOPS, )
      )
      (26): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 79.13 GMACs, 3.06% MACs, 2.04 ms, 2.79% latency, 77.57 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 27.05 GMACs, 1.05% MACs, 975.37 us, 1.33% latency, 55.47 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 122.79 us, 0.17% latency, 105.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 114.44 us, 0.16% latency, 112.88 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 111.82 us, 0.15% latency, 115.53 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 120.16 us, 0.16% latency, 107.51 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 52.08 GMACs, 2.02% MACs, 799.66 us, 1.09% latency, 130.26 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 234.37 us, 0.32% latency, 148.14 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 231.03 us, 0.32% latency, 150.28 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 223.4 us, 0.30% latency, 155.41 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.28 us, 0.04% latency, 139.97 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.93 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 82.25 us, 0.11% latency, 0.0 FLOPS, )
      )
      (27): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 79.13 GMACs, 3.06% MACs, 2.07 ms, 2.83% latency, 76.35 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 27.05 GMACs, 1.05% MACs, 994.21 us, 1.36% latency, 54.42 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 127.55 us, 0.17% latency, 101.28 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 115.39 us, 0.16% latency, 111.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 112.53 us, 0.15% latency, 114.8 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 121.36 us, 0.17% latency, 106.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.29 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 52.08 GMACs, 2.02% MACs, 802.28 us, 1.10% latency, 129.83 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 234.6 us, 0.32% latency, 147.99 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 230.55 us, 0.31% latency, 150.59 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 224.59 us, 0.31% latency, 154.59 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.28 us, 0.04% latency, 139.97 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 95.61 us, 0.13% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 83.21 us, 0.11% latency, 0.0 FLOPS, )
      )
      (28): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 79.13 GMACs, 3.06% MACs, 2.04 ms, 2.79% latency, 77.51 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 27.05 GMACs, 1.05% MACs, 979.66 us, 1.34% latency, 55.23 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 122.79 us, 0.17% latency, 105.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 114.68 us, 0.16% latency, 112.65 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 113.01 us, 0.15% latency, 114.31 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 121.36 us, 0.17% latency, 106.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 52.08 GMACs, 2.02% MACs, 796.56 us, 1.09% latency, 130.76 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 233.89 us, 0.32% latency, 148.44 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 230.55 us, 0.31% latency, 150.59 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 223.4 us, 0.30% latency, 155.41 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 28.85 us, 0.04% latency, 146.91 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.21 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 83.45 us, 0.11% latency, 0.0 FLOPS, )
      )
      (29): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 79.13 GMACs, 3.06% MACs, 2.04 ms, 2.79% latency, 77.46 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 27.05 GMACs, 1.05% MACs, 979.19 us, 1.34% latency, 55.26 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 122.79 us, 0.17% latency, 105.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 114.68 us, 0.16% latency, 112.65 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 112.77 us, 0.15% latency, 114.55 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 120.64 us, 0.16% latency, 107.08 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 52.08 GMACs, 2.02% MACs, 799.89 us, 1.09% latency, 130.22 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 235.08 us, 0.32% latency, 147.69 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 231.74 us, 0.32% latency, 149.81 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 222.68 us, 0.30% latency, 155.91 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 29.56 us, 0.04% latency, 143.35 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 85.59 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 82.49 us, 0.11% latency, 0.0 FLOPS, )
      )
      (30): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 79.13 GMACs, 3.06% MACs, 2.06 ms, 2.82% latency, 76.72 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 27.05 GMACs, 1.05% MACs, 991.11 us, 1.35% latency, 54.59 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 124.93 us, 0.17% latency, 103.4 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 115.87 us, 0.16% latency, 111.49 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 114.44 us, 0.16% latency, 112.88 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 120.4 us, 0.16% latency, 107.29 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 52.08 GMACs, 2.02% MACs, 800.61 us, 1.09% latency, 130.1 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 234.6 us, 0.32% latency, 147.99 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 231.74 us, 0.32% latency, 149.81 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 222.92 us, 0.30% latency, 155.74 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.52 us, 0.04% latency, 138.87 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.78 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.88 us, 0.12% latency, 0.0 FLOPS, )
      )
      (31): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 79.13 GMACs, 3.06% MACs, 2.05 ms, 2.80% latency, 77.19 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 27.05 GMACs, 1.05% MACs, 983.48 us, 1.34% latency, 55.02 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 123.5 us, 0.17% latency, 104.6 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 115.63 us, 0.16% latency, 111.72 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 112.77 us, 0.15% latency, 114.55 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.46 GMACs, 0.25% MACs, 121.36 us, 0.17% latency, 106.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.05 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 52.08 GMACs, 2.02% MACs, 800.61 us, 1.09% latency, 130.1 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 235.08 us, 0.32% latency, 147.69 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 231.5 us, 0.32% latency, 149.97 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.36 GMACs, 0.67% MACs, 222.92 us, 0.30% latency, 155.74 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 29.8 us, 0.04% latency, 142.21 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.93 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 82.97 us, 0.11% latency, 0.0 FLOPS, )
      )
    )
    (norm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.27 us, 0.13% latency, 0.0 FLOPS, )
  )
  (lm_head): Linear(131.08 M, 1.95% Params, 50.46 GMACs, 1.95% MACs, 1.85 ms, 2.52% latency, 54.68 TFLOPS, in_features=4096, out_features=32001, bias=False)
)
------------------------------------------------------------------------------

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 128:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

world size:                                                   4       
data parallel size:                                           4       
model parallel size:                                          1       
batch size per GPU:                                           1       
params per gpu:                                               6738.42 M
params of model = params per GPU * mp_size:                   6738.42 M
fwd MACs per GPU:                                             3458.42 GMACs
fwd flops per GPU:                                            6917.29 G
fwd flops of model = fwd flops per GPU * mp_size:             6917.29 G
fwd latency:                                                  82.45 ms
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:          83.89 TFLOPS
bwd latency:                                                  243.24 ms
bwd FLOPS per GPU = 2.0 * fwd flops per GPU / bwd latency:    56.88 TFLOPS
fwd+bwd FLOPS per GPU = 3.0 * fwd flops per GPU / (fwd+bwd latency):   63.72 TFLOPS
step latency:                                                 30.72 us
iter latency:                                                 325.72 ms
FLOPS per GPU = 3.0 * fwd flops per GPU / iter latency:       63.71 TFLOPS
samples/second:                                               12.28   

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'LlamaForCausalLM': '6738.42 M'}
    MACs        - {'LlamaForCausalLM': '3458.42 GMACs'}
    fwd latency - {'LlamaForCausalLM': '82.36 ms'}
depth 1:
    params      - {'LlamaModel': '6607.35 M'}
    MACs        - {'LlamaModel': '3391.18 GMACs'}
    fwd latency - {'LlamaModel': '79.67 ms'}
depth 2:
    params      - {'ModuleList': '6476.27 M'}
    MACs        - {'ModuleList': '3391.18 GMACs'}
    fwd latency - {'ModuleList': '74.64 ms'}
depth 3:
    params      - {'LlamaDecoderLayer': '6476.27 M'}
    MACs        - {'LlamaDecoderLayer': '3391.18 GMACs'}
    fwd latency - {'LlamaDecoderLayer': '74.64 ms'}
depth 4:
    params      - {'LlamaMLP': '4328.52 M'}
    MACs        - {'LlamaMLP': '2220.53 GMACs'}
    fwd latency - {'LlamaAttention': '38.63 ms'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

LlamaForCausalLM(
  6738.42 M, 100.00% Params, 3458.42 GMACs, 100.00% MACs, 82.36 ms, 100.00% latency, 83.98 TFLOPS, 
  (model): LlamaModel(
    6607.35 M, 98.05% Params, 3391.18 GMACs, 98.06% MACs, 79.67 ms, 96.73% latency, 85.14 TFLOPS, 
    (embed_tokens): Embedding(131.08 M, 1.95% Params, 0 MACs, 0.00% MACs, 73.19 us, 0.09% latency, 0.0 FLOPS, 32001, 4096)
    (layers): ModuleList(
      (0): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.52 ms, 3.06% latency, 84.09 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.27 ms, 1.55% latency, 57.45 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 168.09 us, 0.20% latency, 102.41 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.99 us, 0.16% latency, 128.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.22 us, 0.16% latency, 133.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.61 us, 0.17% latency, 126.0 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 36.24 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 858.07 us, 1.04% latency, 161.75 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 240.09 us, 0.29% latency, 192.68 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 234.13 us, 0.28% latency, 197.59 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 258.45 us, 0.31% latency, 179.0 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.04% latency, 163.35 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 180.96 us, 0.22% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 98.47 us, 0.12% latency, 0.0 FLOPS, )
      )
      (1): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.35 ms, 2.85% latency, 90.25 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.47% latency, 60.42 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 138.28 us, 0.17% latency, 124.48 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.18 us, 0.16% latency, 132.23 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.51 us, 0.16% latency, 133.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.94 us, 0.16% latency, 127.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.29 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 847.58 us, 1.03% latency, 163.75 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 238.66 us, 0.29% latency, 193.84 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 234.13 us, 0.28% latency, 197.59 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.25 us, 0.31% latency, 179.83 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 169.18 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 99.66 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.27 us, 0.11% latency, 0.0 FLOPS, )
      )
      (2): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.35 ms, 2.86% latency, 90.11 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.22 ms, 1.48% latency, 60.01 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 140.43 us, 0.17% latency, 122.58 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.89 us, 0.16% latency, 131.51 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.22 us, 0.16% latency, 133.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.38 us, 0.17% latency, 126.22 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.05 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 847.58 us, 1.03% latency, 163.75 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 239.13 us, 0.29% latency, 193.45 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.41 us, 0.28% latency, 198.2 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.54 us, 0.31% latency, 180.33 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.04% latency, 165.63 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 99.18 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.6 us, 0.11% latency, 0.0 FLOPS, )
      )
      (3): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.82% latency, 91.19 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.84 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.14 us, 0.17% latency, 126.44 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.94 us, 0.16% latency, 132.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 127.79 us, 0.16% latency, 134.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.71 us, 0.16% latency, 127.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.62 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 847.58 us, 1.03% latency, 163.75 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.94 us, 0.29% latency, 194.42 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.93 us, 0.28% latency, 198.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.02 us, 0.31% latency, 179.99 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.62 us, 0.04% latency, 167.98 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.74 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.26 us, 0.11% latency, 0.0 FLOPS, )
      )
      (4): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.82% latency, 91.36 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.88 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.66 us, 0.16% latency, 126.89 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.22 us, 0.16% latency, 133.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 127.79 us, 0.16% latency, 134.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.04 us, 0.16% latency, 129.39 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 841.62 us, 1.02% latency, 164.91 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 235.08 us, 0.29% latency, 196.79 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.46 us, 0.28% latency, 199.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.78 us, 0.31% latency, 180.16 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.66 us, 0.04% latency, 172.89 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 94.18 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.02 us, 0.11% latency, 0.0 FLOPS, )
      )
      (5): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.82% latency, 91.17 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 61.05 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.14 us, 0.17% latency, 126.44 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.22 us, 0.16% latency, 133.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 127.55 us, 0.15% latency, 134.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.71 us, 0.16% latency, 127.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.66 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 848.53 us, 1.03% latency, 163.56 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.7 us, 0.29% latency, 194.62 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.41 us, 0.28% latency, 198.2 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.97 us, 0.31% latency, 179.33 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.04% latency, 166.8 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.55 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.84 us, 0.11% latency, 0.0 FLOPS, )
      )
      (6): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.83% latency, 91.03 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.47% latency, 60.62 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.09 us, 0.17% latency, 125.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.94 us, 0.16% latency, 132.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.03 us, 0.16% latency, 134.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.47 us, 0.16% latency, 128.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 844.24 us, 1.03% latency, 164.4 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.46 us, 0.29% latency, 194.81 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.93 us, 0.28% latency, 198.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.82 us, 0.31% latency, 180.83 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 169.18 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.94 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.78 us, 0.11% latency, 0.0 FLOPS, )
      )
      (7): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.81% latency, 91.49 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.79 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.71 us, 0.16% latency, 127.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.98 us, 0.16% latency, 133.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 127.55 us, 0.15% latency, 134.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.99 us, 0.16% latency, 128.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 41.25 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 841.38 us, 1.02% latency, 164.95 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.99 us, 0.29% latency, 195.2 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.46 us, 0.28% latency, 199.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 254.87 us, 0.31% latency, 181.51 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.42 us, 0.04% latency, 174.16 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.84 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.02 us, 0.11% latency, 0.0 FLOPS, )
      )
      (8): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.34 ms, 2.84% latency, 90.67 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.47% latency, 60.42 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 139.47 us, 0.17% latency, 123.42 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.18 us, 0.16% latency, 132.23 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.03 us, 0.16% latency, 134.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.66 us, 0.16% latency, 126.89 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.81 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 845.19 us, 1.03% latency, 164.21 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 238.18 us, 0.29% latency, 194.23 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.41 us, 0.28% latency, 198.2 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.06 us, 0.31% latency, 180.66 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 170.4 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 96.32 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.93 us, 0.11% latency, 0.0 FLOPS, )
      )
      (9): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.82% latency, 91.24 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.94 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.85 us, 0.17% latency, 125.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.22 us, 0.16% latency, 133.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.16% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.94 us, 0.16% latency, 127.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.62 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 844.24 us, 1.03% latency, 164.4 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.23 us, 0.29% latency, 195.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.22 us, 0.28% latency, 199.21 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.58 us, 0.31% latency, 181.0 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 170.4 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.22 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.17 us, 0.11% latency, 0.0 FLOPS, )
      )
      (10): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.31 ms, 2.80% latency, 91.78 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.45% latency, 61.13 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.09 us, 0.17% latency, 125.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.46 us, 0.16% latency, 132.96 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.98 us, 0.16% latency, 133.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.23 us, 0.16% latency, 128.24 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 839.95 us, 1.02% latency, 165.23 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 235.8 us, 0.29% latency, 196.19 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.93 us, 0.28% latency, 198.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 254.39 us, 0.31% latency, 181.85 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.66 us, 0.04% latency, 172.89 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.31 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.26 us, 0.11% latency, 0.0 FLOPS, )
      )
      (11): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.82% latency, 91.27 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.9 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.61 us, 0.17% latency, 126.0 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.94 us, 0.16% latency, 132.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.03 us, 0.16% latency, 134.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.75 us, 0.16% latency, 128.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 844.0 us, 1.02% latency, 164.44 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.03 us, 0.29% latency, 195.99 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.7 us, 0.28% latency, 198.8 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.06 us, 0.31% latency, 180.66 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 169.18 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.79 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.17 us, 0.11% latency, 0.0 FLOPS, )
      )
      (12): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.83% latency, 90.94 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.47% latency, 60.39 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.38 us, 0.17% latency, 126.22 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.22 us, 0.16% latency, 133.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.51 us, 0.16% latency, 133.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.47 us, 0.16% latency, 128.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.05 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 842.33 us, 1.02% latency, 164.77 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 235.8 us, 0.29% latency, 196.19 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.7 us, 0.28% latency, 198.8 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.06 us, 0.31% latency, 180.66 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.19 us, 0.04% latency, 175.45 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.79 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.21 us, 0.11% latency, 0.0 FLOPS, )
      )
      (13): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.83% latency, 90.84 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.47% latency, 60.59 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.61 us, 0.17% latency, 126.0 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.94 us, 0.16% latency, 132.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.03 us, 0.16% latency, 134.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.75 us, 0.16% latency, 128.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 36.48 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 846.62 us, 1.03% latency, 163.93 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 238.42 us, 0.29% latency, 194.03 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 231.98 us, 0.28% latency, 199.42 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.58 us, 0.31% latency, 181.0 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.66 us, 0.04% latency, 172.89 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 94.65 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.26 us, 0.11% latency, 0.0 FLOPS, )
      )
      (14): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.83% latency, 90.83 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.46% latency, 60.65 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.66 us, 0.16% latency, 126.89 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.7 us, 0.16% latency, 132.72 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.03 us, 0.16% latency, 134.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.09 us, 0.17% latency, 125.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.33 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 843.76 us, 1.02% latency, 164.49 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.94 us, 0.29% latency, 194.42 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.93 us, 0.28% latency, 198.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.35 us, 0.31% latency, 181.17 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.46 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.6 us, 0.11% latency, 0.0 FLOPS, )
      )
      (15): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.82% latency, 91.15 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.76 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.57 us, 0.17% latency, 125.13 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.94 us, 0.16% latency, 132.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.16% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.42 us, 0.16% latency, 127.11 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 843.52 us, 1.02% latency, 164.53 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.51 us, 0.29% latency, 195.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.46 us, 0.28% latency, 199.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.82 us, 0.31% latency, 180.83 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.22 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.45 us, 0.11% latency, 0.0 FLOPS, )
      )
      (16): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.83% latency, 90.82 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.46% latency, 60.67 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.66 us, 0.16% latency, 126.89 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.16% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 127.79 us, 0.16% latency, 134.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.14 us, 0.17% latency, 126.44 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.81 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 845.67 us, 1.03% latency, 164.12 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.46 us, 0.29% latency, 194.81 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.7 us, 0.28% latency, 198.8 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.11 us, 0.31% latency, 181.34 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.66 us, 0.04% latency, 172.89 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.03 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.6 us, 0.11% latency, 0.0 FLOPS, )
      )
      (17): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.35 ms, 2.85% latency, 90.27 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.47% latency, 60.39 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 139.24 us, 0.17% latency, 123.63 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.41 us, 0.16% latency, 131.99 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.16% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.94 us, 0.16% latency, 127.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 36.48 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 846.39 us, 1.03% latency, 163.98 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.7 us, 0.29% latency, 194.62 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.22 us, 0.28% latency, 199.21 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.82 us, 0.31% latency, 180.83 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.62 us, 0.04% latency, 167.98 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 96.08 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.41 us, 0.11% latency, 0.0 FLOPS, )
      )
      (18): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.82% latency, 91.39 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.76 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.33 us, 0.17% latency, 125.34 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.7 us, 0.16% latency, 132.72 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.75 us, 0.16% latency, 133.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.23 us, 0.16% latency, 128.24 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 841.62 us, 1.02% latency, 164.91 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.03 us, 0.29% latency, 195.99 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.46 us, 0.28% latency, 199.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.35 us, 0.31% latency, 181.17 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 169.18 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.27 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.07 us, 0.10% latency, 0.0 FLOPS, )
      )
      (19): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.31 ms, 2.81% latency, 91.7 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.96 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.81 us, 0.17% latency, 124.91 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.7 us, 0.16% latency, 132.72 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.16% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.04 us, 0.16% latency, 129.39 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.62 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 840.19 us, 1.02% latency, 165.19 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 235.8 us, 0.29% latency, 196.19 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 231.74 us, 0.28% latency, 199.62 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.58 us, 0.31% latency, 181.0 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.42 us, 0.04% latency, 174.16 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.03 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.78 us, 0.11% latency, 0.0 FLOPS, )
      )
      (20): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.34 ms, 2.84% latency, 90.53 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.22 ms, 1.48% latency, 59.97 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.66 us, 0.16% latency, 126.89 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.98 us, 0.16% latency, 133.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.03 us, 0.16% latency, 134.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.61 us, 0.17% latency, 126.0 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 843.52 us, 1.02% latency, 164.53 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.7 us, 0.29% latency, 194.62 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.93 us, 0.28% latency, 198.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.58 us, 0.31% latency, 181.0 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.62 us, 0.04% latency, 167.98 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.84 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.36 us, 0.11% latency, 0.0 FLOPS, )
      )
      (21): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.82% latency, 91.24 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.77 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.85 us, 0.17% latency, 125.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.46 us, 0.16% latency, 132.96 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.51 us, 0.16% latency, 133.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.75 us, 0.16% latency, 128.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.05 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 844.96 us, 1.03% latency, 164.26 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.51 us, 0.29% latency, 195.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 231.98 us, 0.28% latency, 199.42 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.06 us, 0.31% latency, 180.66 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.04% latency, 166.8 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.74 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.45 us, 0.11% latency, 0.0 FLOPS, )
      )
      (22): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.81% latency, 91.56 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.87 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.57 us, 0.17% latency, 125.13 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.16% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 127.32 us, 0.15% latency, 135.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.23 us, 0.16% latency, 128.24 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 841.86 us, 1.02% latency, 164.86 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.03 us, 0.29% latency, 195.99 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 231.98 us, 0.28% latency, 199.42 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 254.87 us, 0.31% latency, 181.51 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 170.4 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.79 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.78 us, 0.11% latency, 0.0 FLOPS, )
      )
      (23): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.31 ms, 2.81% latency, 91.67 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 61.05 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.61 us, 0.17% latency, 126.0 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.7 us, 0.16% latency, 132.72 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.16% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.99 us, 0.16% latency, 128.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 840.9 us, 1.02% latency, 165.05 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.27 us, 0.29% latency, 195.8 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.46 us, 0.28% latency, 199.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.58 us, 0.31% latency, 181.0 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.66 us, 0.04% latency, 172.89 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.03 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.26 us, 0.11% latency, 0.0 FLOPS, )
      )
      (24): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.34 ms, 2.84% latency, 90.66 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.47% latency, 60.44 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 139.71 us, 0.17% latency, 123.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.94 us, 0.16% latency, 132.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.03 us, 0.16% latency, 134.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.18 us, 0.16% latency, 127.33 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.76 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 844.0 us, 1.02% latency, 164.44 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.23 us, 0.29% latency, 195.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.22 us, 0.28% latency, 199.21 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.3 us, 0.31% latency, 180.5 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 169.18 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 98.23 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.41 us, 0.11% latency, 0.0 FLOPS, )
      )
      (25): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.82% latency, 91.2 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 61.0 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.14 us, 0.17% latency, 126.44 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.94 us, 0.16% latency, 132.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.51 us, 0.16% latency, 133.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.23 us, 0.16% latency, 128.24 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 849.25 us, 1.03% latency, 163.43 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.99 us, 0.29% latency, 195.2 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 231.5 us, 0.28% latency, 199.83 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 260.11 us, 0.32% latency, 177.85 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 169.18 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.55 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.98 us, 0.11% latency, 0.0 FLOPS, )
      )
      (26): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.34 ms, 2.84% latency, 90.74 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.47% latency, 60.49 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 138.52 us, 0.17% latency, 124.27 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.18 us, 0.16% latency, 132.23 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.16% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.47 us, 0.16% latency, 128.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.05 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 843.76 us, 1.02% latency, 164.49 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.99 us, 0.29% latency, 195.2 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 231.98 us, 0.28% latency, 199.42 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.54 us, 0.31% latency, 180.33 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 95.84 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.45 us, 0.11% latency, 0.0 FLOPS, )
      )
      (27): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.31 ms, 2.81% latency, 91.67 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.45% latency, 61.16 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.38 us, 0.17% latency, 126.22 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.94 us, 0.16% latency, 132.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 127.08 us, 0.15% latency, 135.46 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.23 us, 0.16% latency, 128.24 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 842.09 us, 1.02% latency, 164.81 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.27 us, 0.29% latency, 195.8 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.17 us, 0.28% latency, 198.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.82 us, 0.31% latency, 180.83 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.27 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.26 us, 0.11% latency, 0.0 FLOPS, )
      )
      (28): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.81% latency, 91.53 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.45% latency, 61.09 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.9 us, 0.16% latency, 126.66 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.41 us, 0.16% latency, 131.99 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 127.55 us, 0.15% latency, 134.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.94 us, 0.16% latency, 127.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 843.76 us, 1.02% latency, 164.49 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.99 us, 0.29% latency, 195.2 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 231.98 us, 0.28% latency, 199.42 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.82 us, 0.31% latency, 180.83 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.66 us, 0.04% latency, 172.89 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.79 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.17 us, 0.11% latency, 0.0 FLOPS, )
      )
      (29): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.82% latency, 91.15 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.46% latency, 60.71 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.14 us, 0.17% latency, 126.44 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.22 us, 0.16% latency, 133.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 127.32 us, 0.15% latency, 135.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.66 us, 0.16% latency, 126.89 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 844.96 us, 1.03% latency, 164.26 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.75 us, 0.29% latency, 195.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.46 us, 0.28% latency, 199.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.78 us, 0.31% latency, 180.16 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 170.4 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.51 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.98 us, 0.11% latency, 0.0 FLOPS, )
      )
      (30): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.81% latency, 91.46 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 61.0 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.9 us, 0.16% latency, 126.66 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.46 us, 0.16% latency, 132.96 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 127.32 us, 0.15% latency, 135.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.75 us, 0.16% latency, 128.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 842.33 us, 1.02% latency, 164.77 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.27 us, 0.29% latency, 195.8 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.7 us, 0.28% latency, 198.8 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.35 us, 0.31% latency, 181.17 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.66 us, 0.04% latency, 172.89 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 96.8 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.78 us, 0.11% latency, 0.0 FLOPS, )
      )
      (31): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.82% latency, 91.42 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.93 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.66 us, 0.16% latency, 126.89 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.46 us, 0.16% latency, 132.96 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 127.55 us, 0.15% latency, 134.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.14 us, 0.17% latency, 126.44 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 844.0 us, 1.02% latency, 164.44 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.99 us, 0.29% latency, 195.2 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.22 us, 0.28% latency, 199.21 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.06 us, 0.31% latency, 180.66 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.17 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.17 us, 0.11% latency, 0.0 FLOPS, )
      )
    )
    (norm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 98.47 us, 0.12% latency, 0.0 FLOPS, )
  )
  (lm_head): Linear(131.08 M, 1.95% Params, 67.24 GMACs, 1.94% MACs, 2.36 ms, 2.86% latency, 57.04 TFLOPS, in_features=4096, out_features=32001, bias=False)
)
------------------------------------------------------------------------------

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 128:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

world size:                                                   4       
data parallel size:                                           4       
model parallel size:                                          1       
batch size per GPU:                                           1       
params per gpu:                                               6738.42 M
params of model = params per GPU * mp_size:                   6738.42 M
fwd MACs per GPU:                                             3458.42 GMACs
fwd flops per GPU:                                            6917.29 G
fwd flops of model = fwd flops per GPU * mp_size:             6917.29 G
fwd latency:                                                  82.49 ms
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:          83.85 TFLOPS
bwd latency:                                                  244.33 ms
bwd FLOPS per GPU = 2.0 * fwd flops per GPU / bwd latency:    56.62 TFLOPS
fwd+bwd FLOPS per GPU = 3.0 * fwd flops per GPU / (fwd+bwd latency):   63.5 TFLOPS
step latency:                                                 30.72 us
iter latency:                                                 326.85 ms
FLOPS per GPU = 3.0 * fwd flops per GPU / iter latency:       63.49 TFLOPS
samples/second:                                               12.24   

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'LlamaForCausalLM': '6738.42 M'}
    MACs        - {'LlamaForCausalLM': '3458.42 GMACs'}
    fwd latency - {'LlamaForCausalLM': '82.41 ms'}
depth 1:
    params      - {'LlamaModel': '6607.35 M'}
    MACs        - {'LlamaModel': '3391.18 GMACs'}
    fwd latency - {'LlamaModel': '79.71 ms'}
depth 2:
    params      - {'ModuleList': '6476.27 M'}
    MACs        - {'ModuleList': '3391.18 GMACs'}
    fwd latency - {'ModuleList': '74.71 ms'}
depth 3:
    params      - {'LlamaDecoderLayer': '6476.27 M'}
    MACs        - {'LlamaDecoderLayer': '3391.18 GMACs'}
    fwd latency - {'LlamaDecoderLayer': '74.71 ms'}
depth 4:
    params      - {'LlamaMLP': '4328.52 M'}
    MACs        - {'LlamaMLP': '2220.53 GMACs'}
    fwd latency - {'LlamaAttention': '38.67 ms'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

LlamaForCausalLM(
  6738.42 M, 100.00% Params, 3458.42 GMACs, 100.00% MACs, 82.41 ms, 100.00% latency, 83.94 TFLOPS, 
  (model): LlamaModel(
    6607.35 M, 98.05% Params, 3391.18 GMACs, 98.06% MACs, 79.71 ms, 96.73% latency, 85.09 TFLOPS, 
    (embed_tokens): Embedding(131.08 M, 1.95% Params, 0 MACs, 0.00% MACs, 76.53 us, 0.09% latency, 0.0 FLOPS, 32001, 4096)
    (layers): ModuleList(
      (0): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.53 ms, 3.07% latency, 83.91 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.28 ms, 1.55% latency, 57.38 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 166.89 us, 0.20% latency, 103.14 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.75 us, 0.16% latency, 128.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.75 us, 0.16% latency, 133.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.81 us, 0.17% latency, 124.91 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 38.62 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 859.5 us, 1.04% latency, 161.48 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 240.56 us, 0.29% latency, 192.3 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 235.32 us, 0.29% latency, 196.59 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 259.16 us, 0.31% latency, 178.5 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.81 us, 0.04% latency, 162.23 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 183.82 us, 0.22% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 98.47 us, 0.12% latency, 0.0 FLOPS, )
      )
      (1): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.36 ms, 2.87% latency, 89.77 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.22 ms, 1.48% latency, 59.83 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 139.0 us, 0.17% latency, 123.84 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.65 us, 0.16% latency, 131.75 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.75 us, 0.16% latency, 133.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.33 us, 0.17% latency, 125.34 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 36.48 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 847.82 us, 1.03% latency, 163.7 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 238.42 us, 0.29% latency, 194.03 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.89 us, 0.28% latency, 197.79 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.97 us, 0.31% latency, 179.33 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 170.4 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 98.23 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.22 us, 0.11% latency, 0.0 FLOPS, )
      )
      (2): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.35 ms, 2.85% latency, 90.12 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.22 ms, 1.47% latency, 60.21 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 139.0 us, 0.17% latency, 123.84 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.41 us, 0.16% latency, 131.99 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 127.79 us, 0.16% latency, 134.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.18 us, 0.16% latency, 127.33 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 36.72 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 851.15 us, 1.03% latency, 163.06 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 238.9 us, 0.29% latency, 193.65 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.41 us, 0.28% latency, 198.2 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 258.45 us, 0.31% latency, 179.0 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 169.18 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 96.56 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.93 us, 0.11% latency, 0.0 FLOPS, )
      )
      (3): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.83% latency, 90.95 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.86 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.33 us, 0.17% latency, 125.34 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.65 us, 0.16% latency, 131.75 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.16% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.71 us, 0.16% latency, 127.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.62 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 844.24 us, 1.02% latency, 164.4 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 238.18 us, 0.29% latency, 194.23 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.93 us, 0.28% latency, 198.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.54 us, 0.31% latency, 180.33 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.74 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 95.13 us, 0.12% latency, 0.0 FLOPS, )
      )
      (4): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.81% latency, 91.49 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.97 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.09 us, 0.17% latency, 125.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.18 us, 0.16% latency, 132.23 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.03 us, 0.16% latency, 134.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.99 us, 0.16% latency, 128.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 843.76 us, 1.02% latency, 164.49 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.51 us, 0.29% latency, 195.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.93 us, 0.28% latency, 198.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.78 us, 0.31% latency, 180.16 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.42 us, 0.04% latency, 174.16 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.27 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.5 us, 0.11% latency, 0.0 FLOPS, )
      )
      (5): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.35 ms, 2.85% latency, 90.38 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.22 ms, 1.48% latency, 60.18 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 139.71 us, 0.17% latency, 123.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.41 us, 0.16% latency, 131.99 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.03 us, 0.16% latency, 134.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.85 us, 0.17% latency, 125.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 36.0 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 846.39 us, 1.03% latency, 163.98 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 238.18 us, 0.29% latency, 194.23 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.41 us, 0.28% latency, 198.2 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.3 us, 0.31% latency, 180.5 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 170.4 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 95.37 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.36 us, 0.11% latency, 0.0 FLOPS, )
      )
      (6): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.82% latency, 91.33 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.46% latency, 60.7 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.09 us, 0.17% latency, 125.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.7 us, 0.16% latency, 132.72 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 127.79 us, 0.16% latency, 134.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.18 us, 0.16% latency, 127.33 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.29 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 844.0 us, 1.02% latency, 164.44 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.99 us, 0.29% latency, 195.2 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.89 us, 0.28% latency, 197.79 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.06 us, 0.31% latency, 180.66 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 170.4 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.6 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.02 us, 0.11% latency, 0.0 FLOPS, )
      )
      (7): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.81% latency, 91.46 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.98 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.57 us, 0.17% latency, 125.13 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.46 us, 0.16% latency, 132.96 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.16% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.71 us, 0.16% latency, 127.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 845.91 us, 1.03% latency, 164.07 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.99 us, 0.29% latency, 195.2 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.17 us, 0.28% latency, 198.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.49 us, 0.31% latency, 179.66 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.19 us, 0.04% latency, 175.45 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.79 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.55 us, 0.11% latency, 0.0 FLOPS, )
      )
      (8): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.34 ms, 2.83% latency, 90.75 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.47% latency, 60.31 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.85 us, 0.17% latency, 125.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.46 us, 0.16% latency, 132.96 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.22 us, 0.16% latency, 133.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 138.04 us, 0.17% latency, 124.69 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.62 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 847.82 us, 1.03% latency, 163.7 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.94 us, 0.29% latency, 194.42 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.65 us, 0.28% latency, 197.99 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.02 us, 0.31% latency, 179.99 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 170.4 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.84 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.41 us, 0.11% latency, 0.0 FLOPS, )
      )
      (9): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.81% latency, 91.42 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.45% latency, 61.08 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.85 us, 0.17% latency, 125.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.46 us, 0.16% latency, 132.96 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.75 us, 0.16% latency, 133.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.47 us, 0.16% latency, 128.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 846.86 us, 1.03% latency, 163.89 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.99 us, 0.29% latency, 195.2 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.17 us, 0.28% latency, 198.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.97 us, 0.31% latency, 179.33 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 170.4 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.84 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.45 us, 0.11% latency, 0.0 FLOPS, )
      )
      (10): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.81% latency, 91.51 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.9 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.38 us, 0.17% latency, 126.22 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.41 us, 0.16% latency, 131.99 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.51 us, 0.16% latency, 133.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.94 us, 0.16% latency, 127.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 843.52 us, 1.02% latency, 164.53 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.94 us, 0.29% latency, 194.42 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.17 us, 0.28% latency, 198.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.73 us, 0.31% latency, 179.49 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.66 us, 0.04% latency, 172.89 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.88 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.55 us, 0.11% latency, 0.0 FLOPS, )
      )
      (11): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.82% latency, 91.19 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.88 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.09 us, 0.17% latency, 125.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 131.37 us, 0.16% latency, 131.03 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.51 us, 0.16% latency, 133.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.51 us, 0.16% latency, 128.93 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 849.96 us, 1.03% latency, 163.29 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.94 us, 0.29% latency, 194.42 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 234.13 us, 0.28% latency, 197.59 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 258.21 us, 0.31% latency, 179.16 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.42 us, 0.04% latency, 174.16 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.88 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.55 us, 0.11% latency, 0.0 FLOPS, )
      )
      (12): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.34 ms, 2.84% latency, 90.59 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.47% latency, 60.37 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.9 us, 0.16% latency, 126.66 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.18 us, 0.16% latency, 132.23 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 127.79 us, 0.16% latency, 134.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.47 us, 0.16% latency, 128.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.52 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 845.19 us, 1.03% latency, 164.21 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.94 us, 0.29% latency, 194.42 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.93 us, 0.28% latency, 198.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 254.87 us, 0.31% latency, 181.51 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 97.27 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.74 us, 0.11% latency, 0.0 FLOPS, )
      )
      (13): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.82% latency, 91.2 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.46% latency, 60.64 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.38 us, 0.17% latency, 126.22 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.18 us, 0.16% latency, 132.23 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.51 us, 0.16% latency, 133.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.71 us, 0.16% latency, 127.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.76 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 842.57 us, 1.02% latency, 164.72 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.23 us, 0.29% latency, 195.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.93 us, 0.28% latency, 198.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.35 us, 0.31% latency, 181.17 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.08 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.17 us, 0.11% latency, 0.0 FLOPS, )
      )
      (14): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.35 ms, 2.85% latency, 90.39 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.47% latency, 60.37 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 139.71 us, 0.17% latency, 123.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.41 us, 0.16% latency, 131.99 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.51 us, 0.16% latency, 133.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.61 us, 0.17% latency, 126.0 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.05 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 850.92 us, 1.03% latency, 163.11 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.23 us, 0.29% latency, 195.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 234.13 us, 0.28% latency, 197.59 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 258.21 us, 0.31% latency, 179.16 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.04% latency, 166.8 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 96.56 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.69 us, 0.11% latency, 0.0 FLOPS, )
      )
      (15): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.35 ms, 2.85% latency, 90.35 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.22 ms, 1.48% latency, 60.12 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.81 us, 0.17% latency, 124.91 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.18 us, 0.16% latency, 132.23 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.98 us, 0.16% latency, 133.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.14 us, 0.17% latency, 126.44 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.76 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 849.96 us, 1.03% latency, 163.29 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 238.18 us, 0.29% latency, 194.23 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.65 us, 0.28% latency, 197.99 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.78 us, 0.31% latency, 180.16 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.22 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.88 us, 0.11% latency, 0.0 FLOPS, )
      )
      (16): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.83% latency, 90.82 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.73 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.81 us, 0.17% latency, 124.91 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.41 us, 0.16% latency, 131.99 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.16% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.23 us, 0.16% latency, 128.24 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 844.24 us, 1.02% latency, 164.4 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.94 us, 0.29% latency, 194.42 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.17 us, 0.28% latency, 198.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.3 us, 0.31% latency, 180.5 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.66 us, 0.04% latency, 172.89 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.94 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.7 us, 0.11% latency, 0.0 FLOPS, )
      )
      (17): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.81% latency, 91.49 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.45% latency, 61.07 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.38 us, 0.17% latency, 126.22 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.7 us, 0.16% latency, 132.72 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 127.55 us, 0.15% latency, 134.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.75 us, 0.16% latency, 128.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 846.15 us, 1.03% latency, 164.02 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.7 us, 0.29% latency, 194.62 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.65 us, 0.28% latency, 197.99 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.49 us, 0.31% latency, 179.66 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.66 us, 0.04% latency, 172.89 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.55 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.26 us, 0.11% latency, 0.0 FLOPS, )
      )
      (18): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.31 ms, 2.81% latency, 91.67 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.45% latency, 61.16 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.14 us, 0.17% latency, 126.44 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.7 us, 0.16% latency, 132.72 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.03 us, 0.16% latency, 134.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.47 us, 0.16% latency, 128.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.66 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 846.39 us, 1.03% latency, 163.98 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.46 us, 0.29% latency, 194.81 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.89 us, 0.28% latency, 197.79 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.78 us, 0.31% latency, 180.16 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.12 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.78 us, 0.11% latency, 0.0 FLOPS, )
      )
      (19): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.34 ms, 2.84% latency, 90.63 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.47% latency, 60.3 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 138.76 us, 0.17% latency, 124.05 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.41 us, 0.16% latency, 131.99 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 127.79 us, 0.16% latency, 134.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.94 us, 0.16% latency, 127.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.52 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 844.96 us, 1.03% latency, 164.26 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 238.18 us, 0.29% latency, 194.23 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.41 us, 0.28% latency, 198.2 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.35 us, 0.31% latency, 181.17 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 94.89 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.45 us, 0.11% latency, 0.0 FLOPS, )
      )
      (20): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.31 ms, 2.81% latency, 91.57 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.96 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.85 us, 0.17% latency, 125.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.22 us, 0.16% latency, 133.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.16% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.99 us, 0.16% latency, 128.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 844.72 us, 1.03% latency, 164.3 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.7 us, 0.29% latency, 194.62 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.93 us, 0.28% latency, 198.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.54 us, 0.31% latency, 180.33 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.42 us, 0.04% latency, 174.16 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.84 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.02 us, 0.11% latency, 0.0 FLOPS, )
      )
      (21): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.82% latency, 91.3 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.46% latency, 60.62 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.38 us, 0.17% latency, 126.22 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.94 us, 0.16% latency, 132.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.51 us, 0.16% latency, 133.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 141.62 us, 0.17% latency, 121.55 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 842.57 us, 1.02% latency, 164.72 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.99 us, 0.29% latency, 195.2 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.93 us, 0.28% latency, 198.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.06 us, 0.31% latency, 180.66 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 170.4 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.12 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.98 us, 0.11% latency, 0.0 FLOPS, )
      )
      (22): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.34 ms, 2.85% latency, 90.4 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.22 ms, 1.48% latency, 60.13 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 139.47 us, 0.17% latency, 123.42 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 131.13 us, 0.16% latency, 131.27 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.51 us, 0.16% latency, 133.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.66 us, 0.16% latency, 126.89 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.52 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 847.1 us, 1.03% latency, 163.84 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 238.42 us, 0.29% latency, 194.03 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.89 us, 0.28% latency, 197.79 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.54 us, 0.31% latency, 180.33 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.62 us, 0.04% latency, 167.98 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 94.65 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.17 us, 0.11% latency, 0.0 FLOPS, )
      )
      (23): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.82% latency, 91.34 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.84 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.09 us, 0.17% latency, 125.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.41 us, 0.16% latency, 131.99 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.51 us, 0.16% latency, 133.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.18 us, 0.16% latency, 127.33 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 846.15 us, 1.03% latency, 164.02 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 238.18 us, 0.29% latency, 194.23 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.65 us, 0.28% latency, 197.99 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.35 us, 0.31% latency, 181.17 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.42 us, 0.04% latency, 174.16 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.31 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.55 us, 0.11% latency, 0.0 FLOPS, )
      )
      (24): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.31 ms, 2.81% latency, 91.61 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.45% latency, 61.17 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.9 us, 0.16% latency, 126.66 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.22 us, 0.16% latency, 133.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.03 us, 0.16% latency, 134.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.99 us, 0.16% latency, 128.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 845.91 us, 1.03% latency, 164.07 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 238.42 us, 0.29% latency, 194.03 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.93 us, 0.28% latency, 198.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.25 us, 0.31% latency, 179.83 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.66 us, 0.04% latency, 172.89 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.79 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 85.59 us, 0.10% latency, 0.0 FLOPS, )
      )
      (25): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.82% latency, 91.36 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.92 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.42 us, 0.16% latency, 127.11 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.94 us, 0.16% latency, 132.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.51 us, 0.16% latency, 133.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.23 us, 0.16% latency, 128.24 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 844.96 us, 1.03% latency, 164.26 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.94 us, 0.29% latency, 194.42 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.7 us, 0.28% latency, 198.8 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.54 us, 0.31% latency, 180.33 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.08 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.93 us, 0.11% latency, 0.0 FLOPS, )
      )
      (26): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.83% latency, 91.02 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.73 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.61 us, 0.17% latency, 126.0 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.7 us, 0.16% latency, 132.72 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.16% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.47 us, 0.16% latency, 128.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.33 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 845.43 us, 1.03% latency, 164.16 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.7 us, 0.29% latency, 194.62 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.7 us, 0.28% latency, 198.8 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.78 us, 0.31% latency, 180.16 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 170.4 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.7 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.26 us, 0.11% latency, 0.0 FLOPS, )
      )
      (27): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.31 ms, 2.81% latency, 91.59 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.92 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.14 us, 0.17% latency, 126.44 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.18 us, 0.16% latency, 132.23 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.51 us, 0.16% latency, 133.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.47 us, 0.16% latency, 128.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.62 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 842.57 us, 1.02% latency, 164.72 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.99 us, 0.29% latency, 195.2 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.93 us, 0.28% latency, 198.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.54 us, 0.31% latency, 180.33 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.42 us, 0.04% latency, 174.16 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.31 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.07 us, 0.10% latency, 0.0 FLOPS, )
      )
      (28): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.34 ms, 2.84% latency, 90.72 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.87 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.85 us, 0.17% latency, 125.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.94 us, 0.16% latency, 132.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.16% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.38 us, 0.17% latency, 126.22 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.66 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 851.15 us, 1.03% latency, 163.06 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 239.13 us, 0.29% latency, 193.45 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.89 us, 0.28% latency, 197.79 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.73 us, 0.31% latency, 179.49 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 98.23 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.65 us, 0.11% latency, 0.0 FLOPS, )
      )
      (29): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.82% latency, 91.19 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.46% latency, 60.65 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 139.0 us, 0.17% latency, 123.84 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.18 us, 0.16% latency, 132.23 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.98 us, 0.16% latency, 133.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.99 us, 0.16% latency, 128.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 844.96 us, 1.03% latency, 164.26 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.99 us, 0.29% latency, 195.2 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.65 us, 0.28% latency, 197.99 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.78 us, 0.31% latency, 180.16 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.22 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.02 us, 0.11% latency, 0.0 FLOPS, )
      )
      (30): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.81% latency, 91.53 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.87 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.61 us, 0.17% latency, 126.0 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.65 us, 0.16% latency, 131.75 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.51 us, 0.16% latency, 133.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.71 us, 0.16% latency, 127.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 844.72 us, 1.03% latency, 164.3 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.46 us, 0.29% latency, 194.81 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.41 us, 0.28% latency, 198.2 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.25 us, 0.31% latency, 179.83 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.42 us, 0.04% latency, 174.16 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.84 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 85.35 us, 0.10% latency, 0.0 FLOPS, )
      )
      (31): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.83% latency, 90.97 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.47% latency, 60.27 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.33 us, 0.17% latency, 125.34 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.94 us, 0.16% latency, 132.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.98 us, 0.16% latency, 133.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.94 us, 0.16% latency, 127.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 842.81 us, 1.02% latency, 164.67 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.23 us, 0.29% latency, 195.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.93 us, 0.28% latency, 198.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.06 us, 0.31% latency, 180.66 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.12 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.88 us, 0.11% latency, 0.0 FLOPS, )
      )
    )
    (norm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 97.51 us, 0.12% latency, 0.0 FLOPS, )
  )
  (lm_head): Linear(131.08 M, 1.95% Params, 67.24 GMACs, 1.94% MACs, 2.36 ms, 2.87% latency, 56.95 TFLOPS, in_features=4096, out_features=32001, bias=False)
)
------------------------------------------------------------------------------

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 128:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

world size:                                                   4       
data parallel size:                                           4       
model parallel size:                                          1       
batch size per GPU:                                           1       
params per gpu:                                               6738.42 M
params of model = params per GPU * mp_size:                   6738.42 M
fwd MACs per GPU:                                             3458.42 GMACs
fwd flops per GPU:                                            6917.29 G
fwd flops of model = fwd flops per GPU * mp_size:             6917.29 G
fwd latency:                                                  82.26 ms
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:          84.09 TFLOPS
bwd latency:                                                  243.44 ms
bwd FLOPS per GPU = 2.0 * fwd flops per GPU / bwd latency:    56.83 TFLOPS
fwd+bwd FLOPS per GPU = 3.0 * fwd flops per GPU / (fwd+bwd latency):   63.72 TFLOPS
step latency:                                                 30.72 us
iter latency:                                                 325.73 ms
FLOPS per GPU = 3.0 * fwd flops per GPU / iter latency:       63.71 TFLOPS
samples/second:                                               12.28   

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'LlamaForCausalLM': '6738.42 M'}
    MACs        - {'LlamaForCausalLM': '3458.42 GMACs'}
    fwd latency - {'LlamaForCausalLM': '82.17 ms'}
depth 1:
    params      - {'LlamaModel': '6607.35 M'}
    MACs        - {'LlamaModel': '3391.18 GMACs'}
    fwd latency - {'LlamaModel': '79.48 ms'}
depth 2:
    params      - {'ModuleList': '6476.27 M'}
    MACs        - {'ModuleList': '3391.18 GMACs'}
    fwd latency - {'ModuleList': '74.48 ms'}
depth 3:
    params      - {'LlamaDecoderLayer': '6476.27 M'}
    MACs        - {'LlamaDecoderLayer': '3391.18 GMACs'}
    fwd latency - {'LlamaDecoderLayer': '74.48 ms'}
depth 4:
    params      - {'LlamaMLP': '4328.52 M'}
    MACs        - {'LlamaMLP': '2220.53 GMACs'}
    fwd latency - {'LlamaAttention': '38.58 ms'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

LlamaForCausalLM(
  6738.42 M, 100.00% Params, 3458.42 GMACs, 100.00% MACs, 82.17 ms, 100.00% latency, 84.18 TFLOPS, 
  (model): LlamaModel(
    6607.35 M, 98.05% Params, 3391.18 GMACs, 98.06% MACs, 79.48 ms, 96.73% latency, 85.34 TFLOPS, 
    (embed_tokens): Embedding(131.08 M, 1.95% Params, 0 MACs, 0.00% MACs, 70.81 us, 0.09% latency, 0.0 FLOPS, 32001, 4096)
    (layers): ModuleList(
      (0): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.55 ms, 3.10% latency, 83.1 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.28 ms, 1.56% latency, 56.99 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 171.42 us, 0.21% latency, 100.41 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.47 us, 0.16% latency, 128.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.75 us, 0.16% latency, 133.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 138.04 us, 0.17% latency, 124.69 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 37.43 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 860.21 us, 1.05% latency, 161.34 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 240.33 us, 0.29% latency, 192.49 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 234.37 us, 0.29% latency, 197.39 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 259.88 us, 0.32% latency, 178.01 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.04% latency, 163.35 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 200.27 us, 0.24% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 97.04 us, 0.12% latency, 0.0 FLOPS, )
      )
      (1): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.36 ms, 2.88% latency, 89.64 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.22 ms, 1.49% latency, 59.91 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 138.76 us, 0.17% latency, 124.05 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.7 us, 0.16% latency, 132.72 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 127.08 us, 0.15% latency, 135.46 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.14 us, 0.17% latency, 126.44 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 37.67 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 849.96 us, 1.03% latency, 163.29 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 238.9 us, 0.29% latency, 193.65 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 234.13 us, 0.28% latency, 197.59 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.49 us, 0.31% latency, 179.66 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.04% latency, 165.63 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 101.09 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.22 us, 0.11% latency, 0.0 FLOPS, )
      )
      (2): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.83% latency, 91.02 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.47% latency, 60.76 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.09 us, 0.17% latency, 125.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.7 us, 0.16% latency, 132.72 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.75 us, 0.16% latency, 133.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.47 us, 0.16% latency, 128.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.33 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 845.43 us, 1.03% latency, 164.16 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.46 us, 0.29% latency, 194.81 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.7 us, 0.28% latency, 198.8 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.3 us, 0.31% latency, 180.5 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 170.4 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 95.13 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.98 us, 0.11% latency, 0.0 FLOPS, )
      )
      (3): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.83% latency, 91.17 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.92 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.14 us, 0.17% latency, 126.44 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.03 us, 0.16% latency, 134.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 127.32 us, 0.15% latency, 135.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.47 us, 0.16% latency, 128.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 843.52 us, 1.03% latency, 164.53 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.99 us, 0.29% latency, 195.2 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.7 us, 0.28% latency, 198.8 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.11 us, 0.31% latency, 181.34 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.42 us, 0.04% latency, 174.16 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 99.9 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.55 us, 0.11% latency, 0.0 FLOPS, )
      )
      (4): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.34 ms, 2.85% latency, 90.62 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.48% latency, 60.34 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 140.67 us, 0.17% latency, 122.37 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 131.37 us, 0.16% latency, 131.03 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 127.79 us, 0.16% latency, 134.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.14 us, 0.17% latency, 126.44 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 845.43 us, 1.03% latency, 164.16 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.23 us, 0.29% latency, 195.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.17 us, 0.28% latency, 198.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.54 us, 0.31% latency, 180.33 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 169.18 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 96.8 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.5 us, 0.11% latency, 0.0 FLOPS, )
      )
      (5): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.82% latency, 91.46 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.85 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.85 us, 0.17% latency, 125.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.18 us, 0.16% latency, 132.23 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 127.79 us, 0.16% latency, 134.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.51 us, 0.16% latency, 128.93 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 841.14 us, 1.02% latency, 165.0 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.27 us, 0.29% latency, 195.8 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.7 us, 0.28% latency, 198.8 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.11 us, 0.31% latency, 181.34 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.27 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.5 us, 0.11% latency, 0.0 FLOPS, )
      )
      (6): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.82% latency, 91.39 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.47% latency, 60.62 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.18 us, 0.16% latency, 127.33 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.46 us, 0.16% latency, 132.96 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.51 us, 0.16% latency, 133.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.23 us, 0.16% latency, 128.24 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 841.86 us, 1.02% latency, 164.86 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.99 us, 0.29% latency, 195.2 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.7 us, 0.28% latency, 198.8 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.58 us, 0.31% latency, 181.0 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.27 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.78 us, 0.11% latency, 0.0 FLOPS, )
      )
      (7): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.83% latency, 91.23 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.47% latency, 60.67 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.33 us, 0.17% latency, 125.34 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.98 us, 0.16% latency, 133.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.51 us, 0.16% latency, 133.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.94 us, 0.16% latency, 127.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.05 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 844.96 us, 1.03% latency, 164.26 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.27 us, 0.29% latency, 195.8 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.46 us, 0.28% latency, 199.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.78 us, 0.31% latency, 180.16 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.17 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.78 us, 0.11% latency, 0.0 FLOPS, )
      )
      (8): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.31 ms, 2.81% latency, 91.66 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 61.03 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.61 us, 0.17% latency, 126.0 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.7 us, 0.16% latency, 132.72 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.03 us, 0.16% latency, 134.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.99 us, 0.16% latency, 128.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.33 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 843.05 us, 1.03% latency, 164.63 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.75 us, 0.29% latency, 195.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.22 us, 0.28% latency, 199.21 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.3 us, 0.31% latency, 180.5 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.66 us, 0.04% latency, 172.89 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.88 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.02 us, 0.11% latency, 0.0 FLOPS, )
      )
      (9): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.83% latency, 91.3 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.47% latency, 60.68 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.9 us, 0.17% latency, 126.66 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 132.08 us, 0.16% latency, 130.32 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.16% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.23 us, 0.16% latency, 128.24 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.42 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 844.96 us, 1.03% latency, 164.26 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.46 us, 0.29% latency, 194.81 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.93 us, 0.28% latency, 198.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.06 us, 0.31% latency, 180.66 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.93 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.21 us, 0.11% latency, 0.0 FLOPS, )
      )
      (10): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.34 ms, 2.85% latency, 90.64 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.48% latency, 60.36 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 140.19 us, 0.17% latency, 122.79 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.94 us, 0.16% latency, 132.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.03 us, 0.16% latency, 134.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.94 us, 0.16% latency, 127.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 36.0 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 846.86 us, 1.03% latency, 163.89 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.23 us, 0.29% latency, 195.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.17 us, 0.28% latency, 198.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.3 us, 0.31% latency, 180.5 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.66 us, 0.04% latency, 172.89 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 95.37 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.78 us, 0.11% latency, 0.0 FLOPS, )
      )
      (11): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.31 ms, 2.81% latency, 91.64 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.99 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.9 us, 0.17% latency, 126.66 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.22 us, 0.16% latency, 133.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.03 us, 0.16% latency, 134.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.47 us, 0.16% latency, 128.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 841.62 us, 1.02% latency, 164.91 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.03 us, 0.29% latency, 195.99 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 231.5 us, 0.28% latency, 199.83 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.58 us, 0.31% latency, 181.0 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.84 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 85.83 us, 0.10% latency, 0.0 FLOPS, )
      )
      (12): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.31 ms, 2.81% latency, 91.74 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 61.17 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.42 us, 0.16% latency, 127.11 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.16% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 126.6 us, 0.15% latency, 135.97 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.47 us, 0.16% latency, 128.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.33 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 842.33 us, 1.03% latency, 164.77 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.51 us, 0.29% latency, 195.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.93 us, 0.28% latency, 198.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.06 us, 0.31% latency, 180.66 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 170.4 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.55 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 84.88 us, 0.10% latency, 0.0 FLOPS, )
      )
      (13): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.34 ms, 2.84% latency, 90.77 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.47% latency, 60.55 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.81 us, 0.17% latency, 124.91 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.65 us, 0.16% latency, 131.75 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.75 us, 0.16% latency, 133.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.71 us, 0.16% latency, 127.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 848.05 us, 1.03% latency, 163.66 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.23 us, 0.29% latency, 195.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.93 us, 0.28% latency, 198.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.82 us, 0.31% latency, 180.83 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.04% latency, 166.8 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.22 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.93 us, 0.11% latency, 0.0 FLOPS, )
      )
      (14): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.31 ms, 2.81% latency, 91.77 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 61.16 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.38 us, 0.17% latency, 126.22 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.16% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 127.79 us, 0.16% latency, 134.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.23 us, 0.16% latency, 128.24 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 844.24 us, 1.03% latency, 164.4 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.23 us, 0.29% latency, 195.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.46 us, 0.28% latency, 199.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 257.02 us, 0.31% latency, 179.99 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 170.4 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.12 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.07 us, 0.10% latency, 0.0 FLOPS, )
      )
      (15): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.3 ms, 2.80% latency, 92.2 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.19 ms, 1.45% latency, 61.41 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.66 us, 0.17% latency, 126.89 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.22 us, 0.16% latency, 133.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 127.08 us, 0.15% latency, 135.46 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.51 us, 0.16% latency, 128.93 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 841.62 us, 1.02% latency, 164.91 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.99 us, 0.29% latency, 195.2 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 231.98 us, 0.28% latency, 199.42 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.06 us, 0.31% latency, 180.66 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 170.4 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.17 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 84.88 us, 0.10% latency, 0.0 FLOPS, )
      )
      (16): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.33 ms, 2.84% latency, 90.97 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.47% latency, 60.54 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 138.76 us, 0.17% latency, 124.05 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.89 us, 0.16% latency, 131.51 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.16% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.99 us, 0.16% latency, 128.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 845.67 us, 1.03% latency, 164.12 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.03 us, 0.29% latency, 195.99 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.22 us, 0.28% latency, 199.21 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.3 us, 0.31% latency, 180.5 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.62 us, 0.04% latency, 167.98 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.7 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.5 us, 0.11% latency, 0.0 FLOPS, )
      )
      (17): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.82% latency, 91.4 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 61.09 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.9 us, 0.17% latency, 126.66 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.46 us, 0.16% latency, 132.96 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 127.55 us, 0.16% latency, 134.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.75 us, 0.16% latency, 128.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 849.01 us, 1.03% latency, 163.47 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.75 us, 0.29% latency, 195.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.46 us, 0.28% latency, 199.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.35 us, 0.31% latency, 181.17 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 39.82 us, 0.05% latency, 141.83 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.08 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.98 us, 0.11% latency, 0.0 FLOPS, )
      )
      (18): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.31 ms, 2.81% latency, 91.66 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 61.08 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.66 us, 0.17% latency, 126.89 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.41 us, 0.16% latency, 131.99 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.03 us, 0.16% latency, 134.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.99 us, 0.16% latency, 128.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 843.76 us, 1.03% latency, 164.49 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.75 us, 0.29% latency, 195.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.65 us, 0.28% latency, 197.99 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.82 us, 0.31% latency, 180.83 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 170.4 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.6 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.26 us, 0.11% latency, 0.0 FLOPS, )
      )
      (19): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.83% latency, 91.3 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.47% latency, 60.44 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 139.24 us, 0.17% latency, 123.63 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.98 us, 0.16% latency, 133.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 127.55 us, 0.16% latency, 134.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.47 us, 0.16% latency, 128.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.33 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 839.47 us, 1.02% latency, 165.33 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.27 us, 0.29% latency, 195.8 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.93 us, 0.28% latency, 198.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.35 us, 0.31% latency, 181.17 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.19 us, 0.04% latency, 175.45 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.12 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.78 us, 0.11% latency, 0.0 FLOPS, )
      )
      (20): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.3 ms, 2.80% latency, 91.99 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 61.09 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.9 us, 0.17% latency, 126.66 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.46 us, 0.16% latency, 132.96 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 127.55 us, 0.16% latency, 134.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.04 us, 0.16% latency, 129.39 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.62 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 838.04 us, 1.02% latency, 165.61 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.51 us, 0.29% latency, 195.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 231.98 us, 0.28% latency, 199.42 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 254.87 us, 0.31% latency, 181.51 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.42 us, 0.04% latency, 174.16 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.41 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.31 us, 0.11% latency, 0.0 FLOPS, )
      )
      (21): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.31 ms, 2.81% latency, 91.74 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 61.15 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.09 us, 0.17% latency, 125.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.22 us, 0.16% latency, 133.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 127.55 us, 0.16% latency, 134.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.47 us, 0.16% latency, 128.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.66 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 838.28 us, 1.02% latency, 165.56 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 235.08 us, 0.29% latency, 196.79 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 231.74 us, 0.28% latency, 199.62 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.11 us, 0.31% latency, 181.34 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.66 us, 0.04% latency, 172.89 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 96.08 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.31 us, 0.11% latency, 0.0 FLOPS, )
      )
      (22): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.82% latency, 91.34 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.97 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.14 us, 0.17% latency, 126.44 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.22 us, 0.16% latency, 133.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 126.6 us, 0.15% latency, 135.97 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.71 us, 0.16% latency, 127.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 843.76 us, 1.03% latency, 164.49 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.75 us, 0.29% latency, 195.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.17 us, 0.28% latency, 198.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.11 us, 0.31% latency, 181.34 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.33 us, 0.04% latency, 164.48 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.03 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.69 us, 0.11% latency, 0.0 FLOPS, )
      )
      (23): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.31 ms, 2.81% latency, 91.9 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 61.03 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.61 us, 0.17% latency, 126.0 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.98 us, 0.16% latency, 133.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 127.55 us, 0.16% latency, 134.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.51 us, 0.16% latency, 128.93 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 837.09 us, 1.02% latency, 165.8 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.51 us, 0.29% latency, 195.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 231.74 us, 0.28% latency, 199.62 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 253.92 us, 0.31% latency, 182.19 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 171.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.55 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.55 us, 0.11% latency, 0.0 FLOPS, )
      )
      (24): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.34 ms, 2.84% latency, 90.69 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.21 ms, 1.48% latency, 60.33 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.33 us, 0.17% latency, 125.34 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.75 us, 0.16% latency, 133.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 127.08 us, 0.15% latency, 135.46 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.42 us, 0.16% latency, 127.11 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 42.68 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 846.86 us, 1.03% latency, 163.89 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.75 us, 0.29% latency, 195.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.46 us, 0.28% latency, 199.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.58 us, 0.31% latency, 181.0 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 37.91 us, 0.05% latency, 148.97 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.46 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.26 us, 0.11% latency, 0.0 FLOPS, )
      )
      (25): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.31 ms, 2.81% latency, 91.72 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 61.07 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.9 us, 0.17% latency, 126.66 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.7 us, 0.16% latency, 132.72 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.51 us, 0.16% latency, 133.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.51 us, 0.16% latency, 128.93 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.62 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 841.14 us, 1.02% latency, 165.0 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.51 us, 0.29% latency, 195.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 231.98 us, 0.28% latency, 199.42 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.35 us, 0.31% latency, 181.17 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.19 us, 0.04% latency, 175.45 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.79 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.55 us, 0.11% latency, 0.0 FLOPS, )
      )
      (26): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.83% latency, 91.2 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.47% latency, 60.77 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.38 us, 0.17% latency, 126.22 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 130.65 us, 0.16% latency, 131.75 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.03 us, 0.16% latency, 134.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.75 us, 0.16% latency, 128.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 848.29 us, 1.03% latency, 163.61 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 237.7 us, 0.29% latency, 194.62 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.93 us, 0.28% latency, 198.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.35 us, 0.31% latency, 181.17 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.62 us, 0.04% latency, 167.98 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.08 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.02 us, 0.11% latency, 0.0 FLOPS, )
      )
      (27): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.82% latency, 91.54 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.8 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 137.09 us, 0.17% latency, 125.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.22 us, 0.16% latency, 133.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 127.32 us, 0.15% latency, 135.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.99 us, 0.16% latency, 128.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.52 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 839.47 us, 1.02% latency, 165.33 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.27 us, 0.29% latency, 195.8 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 231.27 us, 0.28% latency, 200.03 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.35 us, 0.31% latency, 181.17 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 169.18 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.36 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.31 us, 0.11% latency, 0.0 FLOPS, )
      )
      (28): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.31 ms, 2.81% latency, 91.73 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 61.16 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.85 us, 0.17% latency, 125.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.27 us, 0.16% latency, 134.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 127.79 us, 0.16% latency, 134.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.75 us, 0.16% latency, 128.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 843.52 us, 1.03% latency, 164.53 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.03 us, 0.29% latency, 195.99 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.17 us, 0.28% latency, 198.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.06 us, 0.31% latency, 180.66 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 170.4 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.6 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.02 us, 0.11% latency, 0.0 FLOPS, )
      )
      (29): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.82% latency, 91.46 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.46% latency, 60.97 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.9 us, 0.17% latency, 126.66 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.98 us, 0.16% latency, 133.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 127.79 us, 0.16% latency, 134.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 135.66 us, 0.17% latency, 126.89 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.66 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 842.33 us, 1.03% latency, 164.77 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.75 us, 0.29% latency, 195.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 233.17 us, 0.28% latency, 198.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.11 us, 0.31% latency, 181.34 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.04% latency, 166.8 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.88 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.17 us, 0.11% latency, 0.0 FLOPS, )
      )
      (30): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.82% latency, 91.56 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.47% latency, 60.77 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.38 us, 0.17% latency, 126.22 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.46 us, 0.16% latency, 132.96 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 127.79 us, 0.16% latency, 134.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 134.47 us, 0.16% latency, 128.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.29 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 840.19 us, 1.02% latency, 165.19 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.51 us, 0.29% latency, 195.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 232.22 us, 0.28% latency, 199.21 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 255.58 us, 0.31% latency, 181.0 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.66 us, 0.04% latency, 172.89 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.31 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.26 us, 0.11% latency, 0.0 FLOPS, )
      )
      (31): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 105.97 GMACs, 3.06% MACs, 2.32 ms, 2.82% latency, 91.5 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 36.58 GMACs, 1.06% MACs, 1.2 ms, 1.47% latency, 60.73 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 136.85 us, 0.17% latency, 125.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 129.22 us, 0.16% latency, 133.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 128.98 us, 0.16% latency, 133.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 8.61 GMACs, 0.25% MACs, 133.51 us, 0.16% latency, 128.93 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 69.39 GMACs, 2.01% MACs, 839.95 us, 1.02% latency, 165.23 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 236.51 us, 0.29% latency, 195.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 231.03 us, 0.28% latency, 200.24 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 23.13 GMACs, 0.67% MACs, 256.54 us, 0.31% latency, 180.33 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.04% latency, 170.4 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.31 us, 0.11% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 85.83 us, 0.10% latency, 0.0 FLOPS, )
      )
    )
    (norm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 97.99 us, 0.12% latency, 0.0 FLOPS, )
  )
  (lm_head): Linear(131.08 M, 1.95% Params, 67.24 GMACs, 1.94% MACs, 2.36 ms, 2.87% latency, 57.09 TFLOPS, in_features=4096, out_features=32001, bias=False)
)
------------------------------------------------------------------------------

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 128:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

world size:                                                   4       
data parallel size:                                           4       
model parallel size:                                          1       
batch size per GPU:                                           1       
params per gpu:                                               6738.42 M
params of model = params per GPU * mp_size:                   6738.42 M
fwd MACs per GPU:                                             2684.78 GMACs
fwd flops per GPU:                                            5369.86 G
fwd flops of model = fwd flops per GPU * mp_size:             5369.86 G
fwd latency:                                                  71.4 ms 
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:          75.21 TFLOPS
bwd latency:                                                  209.59 ms
bwd FLOPS per GPU = 2.0 * fwd flops per GPU / bwd latency:    51.24 TFLOPS
fwd+bwd FLOPS per GPU = 3.0 * fwd flops per GPU / (fwd+bwd latency):   57.33 TFLOPS
step latency:                                                 29.7 us 
iter latency:                                                 281.02 ms
FLOPS per GPU = 3.0 * fwd flops per GPU / iter latency:       57.32 TFLOPS
samples/second:                                               14.23   

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'LlamaForCausalLM': '6738.42 M'}
    MACs        - {'LlamaForCausalLM': '2684.78 GMACs'}
    fwd latency - {'LlamaForCausalLM': '71.31 ms'}
depth 1:
    params      - {'LlamaModel': '6607.35 M'}
    MACs        - {'LlamaModel': '2632.35 GMACs'}
    fwd latency - {'LlamaModel': '69.18 ms'}
depth 2:
    params      - {'ModuleList': '6476.27 M'}
    MACs        - {'ModuleList': '2632.35 GMACs'}
    fwd latency - {'ModuleList': '64.2 ms'}
depth 3:
    params      - {'LlamaDecoderLayer': '6476.27 M'}
    MACs        - {'LlamaDecoderLayer': '2632.35 GMACs'}
    fwd latency - {'LlamaDecoderLayer': '64.2 ms'}
depth 4:
    params      - {'LlamaMLP': '4328.52 M'}
    MACs        - {'LlamaMLP': '1731.41 GMACs'}
    fwd latency - {'LlamaAttention': '29.78 ms'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

LlamaForCausalLM(
  6738.42 M, 100.00% Params, 2684.78 GMACs, 100.00% MACs, 71.31 ms, 100.00% latency, 75.3 TFLOPS, 
  (model): LlamaModel(
    6607.35 M, 98.05% Params, 2632.35 GMACs, 98.05% MACs, 69.18 ms, 97.01% latency, 76.1 TFLOPS, 
    (embed_tokens): Embedding(131.08 M, 1.95% Params, 0 MACs, 0.00% MACs, 66.52 us, 0.09% latency, 0.0 FLOPS, 32001, 4096)
    (layers): ModuleList(
      (0): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 82.26 GMACs, 3.06% MACs, 2.21 ms, 3.10% latency, 74.34 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.15 GMACs, 1.05% MACs, 1.01 ms, 1.42% latency, 55.69 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 158.55 us, 0.22% latency, 84.65 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 120.16 us, 0.17% latency, 111.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 115.39 us, 0.16% latency, 116.31 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 123.02 us, 0.17% latency, 109.1 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 37.19 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 54.11 GMACs, 2.02% MACs, 816.35 us, 1.14% latency, 132.56 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 237.7 us, 0.33% latency, 151.75 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 232.7 us, 0.33% latency, 155.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 224.83 us, 0.32% latency, 160.44 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.05% latency, 132.87 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 183.82 us, 0.26% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 94.65 us, 0.13% latency, 0.0 FLOPS, )
      )
      (1): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 82.26 GMACs, 3.06% MACs, 2.04 ms, 2.86% latency, 80.74 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.15 GMACs, 1.05% MACs, 944.85 us, 1.32% latency, 59.6 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 127.79 us, 0.18% latency, 105.03 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 117.78 us, 0.17% latency, 113.96 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 113.96 us, 0.16% latency, 117.77 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 123.02 us, 0.17% latency, 109.1 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.81 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 54.11 GMACs, 2.02% MACs, 806.33 us, 1.13% latency, 134.21 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 236.03 us, 0.33% latency, 152.82 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 231.98 us, 0.33% latency, 155.49 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 224.59 us, 0.31% latency, 160.61 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.99 us, 0.04% latency, 142.06 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 103.47 us, 0.15% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.07 us, 0.12% latency, 0.0 FLOPS, )
      )
      (2): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 82.26 GMACs, 3.06% MACs, 2.02 ms, 2.84% latency, 81.26 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.15 GMACs, 1.05% MACs, 941.51 us, 1.32% latency, 59.81 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 128.27 us, 0.18% latency, 104.64 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 116.83 us, 0.16% latency, 114.89 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 114.2 us, 0.16% latency, 117.53 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 122.07 us, 0.17% latency, 109.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.52 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 54.11 GMACs, 2.02% MACs, 803.23 us, 1.13% latency, 134.73 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 234.6 us, 0.33% latency, 153.75 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 231.74 us, 0.32% latency, 155.65 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 224.83 us, 0.32% latency, 160.44 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.52 us, 0.04% latency, 144.28 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 96.56 us, 0.14% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 84.88 us, 0.12% latency, 0.0 FLOPS, )
      )
      (3): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 82.26 GMACs, 3.06% MACs, 2.0 ms, 2.80% latency, 82.3 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.15 GMACs, 1.05% MACs, 925.54 us, 1.30% latency, 60.84 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 124.45 us, 0.17% latency, 107.84 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 115.16 us, 0.16% latency, 116.55 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 113.96 us, 0.16% latency, 117.77 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 121.83 us, 0.17% latency, 110.17 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 54.11 GMACs, 2.02% MACs, 806.09 us, 1.13% latency, 134.25 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 236.51 us, 0.33% latency, 152.51 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 231.74 us, 0.32% latency, 155.65 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 224.59 us, 0.31% latency, 160.61 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.52 us, 0.04% latency, 144.28 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.41 us, 0.13% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 83.21 us, 0.12% latency, 0.0 FLOPS, )
      )
      (4): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 82.26 GMACs, 3.06% MACs, 2.0 ms, 2.80% latency, 82.43 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.15 GMACs, 1.05% MACs, 919.82 us, 1.29% latency, 61.22 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 123.74 us, 0.17% latency, 108.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 115.39 us, 0.16% latency, 116.31 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 113.49 us, 0.16% latency, 118.27 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 119.69 us, 0.17% latency, 112.14 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.33 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 54.11 GMACs, 2.02% MACs, 810.62 us, 1.14% latency, 133.5 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 235.32 us, 0.33% latency, 153.29 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 236.27 us, 0.33% latency, 152.67 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 224.83 us, 0.32% latency, 160.44 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.76 us, 0.04% latency, 143.17 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.41 us, 0.13% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 81.78 us, 0.11% latency, 0.0 FLOPS, )
      )
      (5): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 82.26 GMACs, 3.06% MACs, 2.01 ms, 2.81% latency, 81.98 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.15 GMACs, 1.05% MACs, 931.5 us, 1.31% latency, 60.45 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 126.12 us, 0.18% latency, 106.42 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 115.87 us, 0.16% latency, 115.83 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 115.39 us, 0.16% latency, 116.31 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 121.12 us, 0.17% latency, 110.82 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 54.11 GMACs, 2.02% MACs, 802.28 us, 1.13% latency, 134.89 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 235.56 us, 0.33% latency, 153.13 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 231.03 us, 0.32% latency, 156.13 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 223.64 us, 0.31% latency, 161.29 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.52 us, 0.04% latency, 144.28 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.03 us, 0.13% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 83.68 us, 0.12% latency, 0.0 FLOPS, )
      )
      (6): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 82.26 GMACs, 3.06% MACs, 1.99 ms, 2.79% latency, 82.75 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.15 GMACs, 1.05% MACs, 919.1 us, 1.29% latency, 61.27 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 123.5 us, 0.17% latency, 108.68 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 114.44 us, 0.16% latency, 117.28 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 112.77 us, 0.16% latency, 119.02 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 120.4 us, 0.17% latency, 111.48 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 54.11 GMACs, 2.02% MACs, 804.66 us, 1.13% latency, 134.49 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 235.08 us, 0.33% latency, 153.44 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 230.79 us, 0.32% latency, 156.29 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 224.11 us, 0.31% latency, 160.95 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.76 us, 0.04% latency, 143.17 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 85.83 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 83.68 us, 0.12% latency, 0.0 FLOPS, )
      )
      (7): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 82.26 GMACs, 3.06% MACs, 1.99 ms, 2.79% latency, 82.67 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.15 GMACs, 1.05% MACs, 921.01 us, 1.29% latency, 61.14 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 123.26 us, 0.17% latency, 108.89 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 114.68 us, 0.16% latency, 117.04 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 113.25 us, 0.16% latency, 118.52 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 120.88 us, 0.17% latency, 111.04 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 54.11 GMACs, 2.02% MACs, 803.47 us, 1.13% latency, 134.69 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 234.6 us, 0.33% latency, 153.75 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 231.74 us, 0.32% latency, 155.65 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 223.16 us, 0.31% latency, 161.64 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.28 us, 0.04% latency, 145.42 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.55 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 81.54 us, 0.11% latency, 0.0 FLOPS, )
      )
      (8): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 82.26 GMACs, 3.06% MACs, 2.01 ms, 2.81% latency, 82.0 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.15 GMACs, 1.05% MACs, 927.21 us, 1.30% latency, 60.73 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 123.5 us, 0.17% latency, 108.68 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 115.39 us, 0.16% latency, 116.31 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 114.44 us, 0.16% latency, 117.28 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 120.64 us, 0.17% latency, 111.25 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 54.11 GMACs, 2.02% MACs, 799.89 us, 1.12% latency, 135.29 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 235.08 us, 0.33% latency, 153.44 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 231.03 us, 0.32% latency, 156.13 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 222.92 us, 0.31% latency, 161.81 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.52 us, 0.04% latency, 144.28 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.31 us, 0.13% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.51 us, 0.13% latency, 0.0 FLOPS, )
      )
      (9): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 82.26 GMACs, 3.06% MACs, 1.99 ms, 2.78% latency, 82.87 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.15 GMACs, 1.05% MACs, 919.58 us, 1.29% latency, 61.24 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 122.79 us, 0.17% latency, 109.31 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 114.92 us, 0.16% latency, 116.79 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 113.49 us, 0.16% latency, 118.27 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 120.64 us, 0.17% latency, 111.25 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 54.11 GMACs, 2.02% MACs, 800.85 us, 1.12% latency, 135.13 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 234.13 us, 0.33% latency, 154.07 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 230.79 us, 0.32% latency, 156.29 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 224.59 us, 0.31% latency, 160.61 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.04 us, 0.04% latency, 146.57 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.69 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 83.45 us, 0.12% latency, 0.0 FLOPS, )
      )
      (10): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 82.26 GMACs, 3.06% MACs, 1.99 ms, 2.79% latency, 82.68 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.15 GMACs, 1.05% MACs, 924.83 us, 1.30% latency, 60.89 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 123.74 us, 0.17% latency, 108.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 114.44 us, 0.16% latency, 117.28 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 113.49 us, 0.16% latency, 118.27 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 122.79 us, 0.17% latency, 109.31 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 54.11 GMACs, 2.02% MACs, 799.18 us, 1.12% latency, 135.41 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 235.08 us, 0.33% latency, 153.44 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 229.6 us, 0.32% latency, 157.11 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 222.92 us, 0.31% latency, 161.81 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.52 us, 0.04% latency, 144.28 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.98 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 83.68 us, 0.12% latency, 0.0 FLOPS, )
      )
      (11): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 82.26 GMACs, 3.06% MACs, 2.0 ms, 2.81% latency, 82.16 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.15 GMACs, 1.05% MACs, 934.36 us, 1.31% latency, 60.27 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 123.5 us, 0.17% latency, 108.68 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 114.68 us, 0.16% latency, 117.04 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 114.2 us, 0.16% latency, 117.53 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 121.36 us, 0.17% latency, 110.6 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 36.48 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 54.11 GMACs, 2.02% MACs, 801.56 us, 1.12% latency, 135.01 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 234.84 us, 0.33% latency, 153.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 231.27 us, 0.32% latency, 155.97 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 223.88 us, 0.31% latency, 161.12 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.04 us, 0.04% latency, 146.57 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.93 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 82.73 us, 0.12% latency, 0.0 FLOPS, )
      )
      (12): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 82.26 GMACs, 3.06% MACs, 2.0 ms, 2.80% latency, 82.39 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.15 GMACs, 1.05% MACs, 932.22 us, 1.31% latency, 60.41 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 123.26 us, 0.17% latency, 108.89 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 115.39 us, 0.16% latency, 116.31 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 112.53 us, 0.16% latency, 119.27 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 121.59 us, 0.17% latency, 110.38 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.81 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 54.11 GMACs, 2.02% MACs, 800.13 us, 1.12% latency, 135.25 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 234.37 us, 0.33% latency, 153.91 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 231.74 us, 0.32% latency, 155.65 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 222.21 us, 0.31% latency, 162.33 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.28 us, 0.04% latency, 145.42 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.17 us, 0.13% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 82.02 us, 0.12% latency, 0.0 FLOPS, )
      )
      (13): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 82.26 GMACs, 3.06% MACs, 2.01 ms, 2.83% latency, 81.66 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.15 GMACs, 1.05% MACs, 938.89 us, 1.32% latency, 59.98 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 127.32 us, 0.18% latency, 105.42 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 117.3 us, 0.16% latency, 114.42 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 114.44 us, 0.16% latency, 117.28 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 122.31 us, 0.17% latency, 109.74 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 54.11 GMACs, 2.02% MACs, 802.76 us, 1.13% latency, 134.81 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 234.84 us, 0.33% latency, 153.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 230.55 us, 0.32% latency, 156.46 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 223.88 us, 0.31% latency, 161.12 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.52 us, 0.04% latency, 144.28 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.74 us, 0.13% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 84.64 us, 0.12% latency, 0.0 FLOPS, )
      )
      (14): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 82.26 GMACs, 3.06% MACs, 1.99 ms, 2.79% latency, 82.83 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.15 GMACs, 1.05% MACs, 923.63 us, 1.30% latency, 60.97 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 123.5 us, 0.17% latency, 108.68 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 114.2 us, 0.16% latency, 117.53 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 114.2 us, 0.16% latency, 117.53 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 120.88 us, 0.17% latency, 111.04 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 54.11 GMACs, 2.02% MACs, 799.66 us, 1.12% latency, 135.33 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 234.13 us, 0.33% latency, 154.07 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 231.27 us, 0.32% latency, 155.97 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 222.92 us, 0.31% latency, 161.81 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 29.8 us, 0.04% latency, 147.75 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.74 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 81.3 us, 0.11% latency, 0.0 FLOPS, )
      )
      (15): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 82.26 GMACs, 3.06% MACs, 1.99 ms, 2.79% latency, 82.61 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.15 GMACs, 1.05% MACs, 924.11 us, 1.30% latency, 60.94 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 123.5 us, 0.17% latency, 108.68 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 115.63 us, 0.16% latency, 116.07 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 112.77 us, 0.16% latency, 119.02 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 122.07 us, 0.17% latency, 109.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.62 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 54.11 GMACs, 2.02% MACs, 802.52 us, 1.13% latency, 134.85 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 234.6 us, 0.33% latency, 153.75 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 231.27 us, 0.32% latency, 155.97 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 223.88 us, 0.31% latency, 161.12 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.04 us, 0.04% latency, 146.57 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 85.59 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 83.68 us, 0.12% latency, 0.0 FLOPS, )
      )
      (16): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 82.26 GMACs, 3.06% MACs, 2.0 ms, 2.81% latency, 82.19 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.15 GMACs, 1.05% MACs, 928.16 us, 1.30% latency, 60.67 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 123.26 us, 0.17% latency, 108.89 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 114.44 us, 0.16% latency, 117.28 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 113.25 us, 0.16% latency, 118.52 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 120.4 us, 0.17% latency, 111.48 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 36.0 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 54.11 GMACs, 2.02% MACs, 800.13 us, 1.12% latency, 135.25 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 235.08 us, 0.33% latency, 153.44 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 231.03 us, 0.32% latency, 156.13 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 222.44 us, 0.31% latency, 162.16 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.76 us, 0.04% latency, 143.17 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.45 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 82.73 us, 0.12% latency, 0.0 FLOPS, )
      )
      (17): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 82.26 GMACs, 3.06% MACs, 1.99 ms, 2.78% latency, 82.85 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.15 GMACs, 1.05% MACs, 922.92 us, 1.29% latency, 61.02 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 122.55 us, 0.17% latency, 109.52 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 114.44 us, 0.16% latency, 117.28 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 113.73 us, 0.16% latency, 118.02 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 119.92 us, 0.17% latency, 111.92 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.29 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 54.11 GMACs, 2.02% MACs, 797.27 us, 1.12% latency, 135.73 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 233.65 us, 0.33% latency, 154.38 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 230.55 us, 0.32% latency, 156.46 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 222.92 us, 0.31% latency, 161.81 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.28 us, 0.04% latency, 145.42 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.55 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 82.73 us, 0.12% latency, 0.0 FLOPS, )
      )
      (18): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 82.26 GMACs, 3.06% MACs, 1.99 ms, 2.79% latency, 82.58 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.15 GMACs, 1.05% MACs, 926.49 us, 1.30% latency, 60.78 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 124.45 us, 0.17% latency, 107.84 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 115.87 us, 0.16% latency, 115.83 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 113.25 us, 0.16% latency, 118.52 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 119.45 us, 0.17% latency, 112.37 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.76 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 54.11 GMACs, 2.02% MACs, 799.89 us, 1.12% latency, 135.29 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 235.8 us, 0.33% latency, 152.98 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 231.5 us, 0.32% latency, 155.81 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 223.88 us, 0.31% latency, 161.12 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.04 us, 0.04% latency, 146.57 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.93 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 82.73 us, 0.12% latency, 0.0 FLOPS, )
      )
      (19): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 82.26 GMACs, 3.06% MACs, 1.99 ms, 2.80% latency, 82.52 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.15 GMACs, 1.05% MACs, 924.83 us, 1.30% latency, 60.89 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 123.26 us, 0.17% latency, 108.89 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 114.68 us, 0.16% latency, 117.04 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 113.25 us, 0.16% latency, 118.52 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 121.36 us, 0.17% latency, 110.6 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 54.11 GMACs, 2.02% MACs, 804.66 us, 1.13% latency, 134.49 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 234.6 us, 0.33% latency, 153.75 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 230.55 us, 0.32% latency, 156.46 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 229.36 us, 0.32% latency, 157.27 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 29.8 us, 0.04% latency, 147.75 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.69 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 82.97 us, 0.12% latency, 0.0 FLOPS, )
      )
      (20): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 82.26 GMACs, 3.06% MACs, 1.99 ms, 2.79% latency, 82.62 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.15 GMACs, 1.05% MACs, 924.11 us, 1.30% latency, 60.94 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 123.26 us, 0.17% latency, 108.89 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 115.63 us, 0.16% latency, 116.07 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 113.49 us, 0.16% latency, 118.27 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 120.64 us, 0.17% latency, 111.25 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 54.11 GMACs, 2.02% MACs, 801.8 us, 1.12% latency, 134.97 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 234.6 us, 0.33% latency, 153.75 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 231.74 us, 0.32% latency, 155.65 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 222.92 us, 0.31% latency, 161.81 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.28 us, 0.04% latency, 145.42 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.17 us, 0.13% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 81.54 us, 0.11% latency, 0.0 FLOPS, )
      )
      (21): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 82.26 GMACs, 3.06% MACs, 2.01 ms, 2.82% latency, 81.88 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.15 GMACs, 1.05% MACs, 931.26 us, 1.31% latency, 60.47 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 125.65 us, 0.18% latency, 106.82 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 115.16 us, 0.16% latency, 116.55 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 113.73 us, 0.16% latency, 118.02 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 121.59 us, 0.17% latency, 110.38 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 54.11 GMACs, 2.02% MACs, 809.67 us, 1.14% latency, 133.66 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 235.08 us, 0.33% latency, 153.44 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 237.46 us, 0.33% latency, 151.9 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 223.88 us, 0.31% latency, 161.12 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.28 us, 0.04% latency, 145.42 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.84 us, 0.13% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 82.73 us, 0.12% latency, 0.0 FLOPS, )
      )
      (22): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 82.26 GMACs, 3.06% MACs, 2.0 ms, 2.80% latency, 82.4 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.15 GMACs, 1.05% MACs, 927.69 us, 1.30% latency, 60.7 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 123.02 us, 0.17% latency, 109.1 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 116.59 us, 0.16% latency, 115.12 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 115.16 us, 0.16% latency, 116.55 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 120.88 us, 0.17% latency, 111.04 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 54.11 GMACs, 2.02% MACs, 802.52 us, 1.13% latency, 134.85 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 234.84 us, 0.33% latency, 153.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 232.22 us, 0.33% latency, 155.33 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 224.35 us, 0.31% latency, 160.78 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.28 us, 0.04% latency, 145.42 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.17 us, 0.13% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 82.73 us, 0.12% latency, 0.0 FLOPS, )
      )
      (23): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 82.26 GMACs, 3.06% MACs, 2.0 ms, 2.81% latency, 82.16 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.15 GMACs, 1.05% MACs, 930.07 us, 1.30% latency, 60.55 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 123.98 us, 0.17% latency, 108.26 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 114.92 us, 0.16% latency, 116.79 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 113.96 us, 0.16% latency, 117.77 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 120.64 us, 0.17% latency, 111.25 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 54.11 GMACs, 2.02% MACs, 800.85 us, 1.12% latency, 135.13 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 234.84 us, 0.33% latency, 153.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 231.5 us, 0.32% latency, 155.81 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 222.68 us, 0.31% latency, 161.98 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.04 us, 0.04% latency, 146.57 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.27 us, 0.13% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 84.4 us, 0.12% latency, 0.0 FLOPS, )
      )
      (24): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 82.26 GMACs, 3.06% MACs, 2.0 ms, 2.80% latency, 82.44 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.15 GMACs, 1.05% MACs, 928.16 us, 1.30% latency, 60.67 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 123.98 us, 0.17% latency, 108.26 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 116.11 us, 0.16% latency, 115.6 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 112.77 us, 0.16% latency, 119.02 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 120.16 us, 0.17% latency, 111.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.81 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 54.11 GMACs, 2.02% MACs, 802.52 us, 1.13% latency, 134.85 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 234.84 us, 0.33% latency, 153.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 231.5 us, 0.32% latency, 155.81 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 223.16 us, 0.31% latency, 161.64 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.04 us, 0.04% latency, 146.57 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.74 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 83.68 us, 0.12% latency, 0.0 FLOPS, )
      )
      (25): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 82.26 GMACs, 3.06% MACs, 2.0 ms, 2.80% latency, 82.37 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.15 GMACs, 1.05% MACs, 926.26 us, 1.30% latency, 60.8 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 123.5 us, 0.17% latency, 108.68 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 113.96 us, 0.16% latency, 117.77 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 113.49 us, 0.16% latency, 118.27 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 120.64 us, 0.17% latency, 111.25 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.62 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 54.11 GMACs, 2.02% MACs, 802.99 us, 1.13% latency, 134.77 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 235.8 us, 0.33% latency, 152.98 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 231.03 us, 0.32% latency, 156.13 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 224.83 us, 0.32% latency, 160.44 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 29.56 us, 0.04% latency, 148.94 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.17 us, 0.13% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 82.97 us, 0.12% latency, 0.0 FLOPS, )
      )
      (26): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 82.26 GMACs, 3.06% MACs, 1.99 ms, 2.80% latency, 82.55 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.15 GMACs, 1.05% MACs, 924.11 us, 1.30% latency, 60.94 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 123.98 us, 0.17% latency, 108.26 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 114.92 us, 0.16% latency, 116.79 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 113.01 us, 0.16% latency, 118.77 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 120.16 us, 0.17% latency, 111.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 54.11 GMACs, 2.02% MACs, 802.04 us, 1.12% latency, 134.93 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 234.13 us, 0.33% latency, 154.07 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 231.5 us, 0.32% latency, 155.81 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 223.64 us, 0.31% latency, 161.29 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.52 us, 0.04% latency, 144.28 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.74 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 84.16 us, 0.12% latency, 0.0 FLOPS, )
      )
      (27): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 82.26 GMACs, 3.06% MACs, 1.99 ms, 2.80% latency, 82.5 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.15 GMACs, 1.05% MACs, 924.35 us, 1.30% latency, 60.92 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 123.26 us, 0.17% latency, 108.89 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 115.16 us, 0.16% latency, 116.55 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 113.73 us, 0.16% latency, 118.02 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 120.16 us, 0.17% latency, 111.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 54.11 GMACs, 2.02% MACs, 799.42 us, 1.12% latency, 135.37 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 234.6 us, 0.33% latency, 153.75 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 231.27 us, 0.32% latency, 155.97 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 223.64 us, 0.31% latency, 161.29 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.76 us, 0.04% latency, 143.17 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.55 us, 0.13% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 83.45 us, 0.12% latency, 0.0 FLOPS, )
      )
      (28): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 82.26 GMACs, 3.06% MACs, 1.98 ms, 2.77% latency, 83.14 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.15 GMACs, 1.05% MACs, 915.77 us, 1.28% latency, 61.49 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 122.79 us, 0.17% latency, 109.31 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 114.92 us, 0.16% latency, 116.79 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 113.01 us, 0.16% latency, 118.77 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 119.45 us, 0.17% latency, 112.37 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.62 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 54.11 GMACs, 2.02% MACs, 799.66 us, 1.12% latency, 135.33 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 234.13 us, 0.33% latency, 154.07 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 231.74 us, 0.32% latency, 155.65 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 223.64 us, 0.31% latency, 161.29 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 29.56 us, 0.04% latency, 148.94 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.5 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 81.54 us, 0.11% latency, 0.0 FLOPS, )
      )
      (29): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 82.26 GMACs, 3.06% MACs, 2.03 ms, 2.84% latency, 81.24 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.15 GMACs, 1.05% MACs, 944.61 us, 1.32% latency, 59.62 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 127.79 us, 0.18% latency, 105.03 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 117.06 us, 0.16% latency, 114.65 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 114.68 us, 0.16% latency, 117.04 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 121.83 us, 0.17% latency, 110.17 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 36.48 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 54.11 GMACs, 2.02% MACs, 803.23 us, 1.13% latency, 134.73 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 235.08 us, 0.33% latency, 153.44 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 230.79 us, 0.32% latency, 156.29 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 222.92 us, 0.31% latency, 161.81 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.52 us, 0.04% latency, 144.28 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.98 us, 0.13% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 85.35 us, 0.12% latency, 0.0 FLOPS, )
      )
      (30): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 82.26 GMACs, 3.06% MACs, 2.0 ms, 2.80% latency, 82.42 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.15 GMACs, 1.05% MACs, 924.83 us, 1.30% latency, 60.89 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 124.22 us, 0.17% latency, 108.05 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 115.16 us, 0.16% latency, 116.55 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 112.77 us, 0.16% latency, 119.02 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 120.88 us, 0.17% latency, 111.04 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 54.11 GMACs, 2.02% MACs, 801.09 us, 1.12% latency, 135.09 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 234.13 us, 0.33% latency, 154.07 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 231.27 us, 0.32% latency, 155.97 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 222.44 us, 0.31% latency, 162.16 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.52 us, 0.04% latency, 144.28 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.84 us, 0.13% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 83.92 us, 0.12% latency, 0.0 FLOPS, )
      )
      (31): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 82.26 GMACs, 3.06% MACs, 2.01 ms, 2.82% latency, 81.8 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.15 GMACs, 1.05% MACs, 939.85 us, 1.32% latency, 59.92 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 127.32 us, 0.18% latency, 105.42 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 117.3 us, 0.16% latency, 114.42 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 113.73 us, 0.16% latency, 118.02 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.71 GMACs, 0.25% MACs, 121.12 us, 0.17% latency, 110.82 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 54.11 GMACs, 2.02% MACs, 800.13 us, 1.12% latency, 135.25 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 235.8 us, 0.33% latency, 152.98 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 230.55 us, 0.32% latency, 156.46 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 18.04 GMACs, 0.67% MACs, 222.44 us, 0.31% latency, 162.16 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.04 us, 0.04% latency, 146.57 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.31 us, 0.13% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 83.92 us, 0.12% latency, 0.0 FLOPS, )
      )
    )
    (norm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 94.18 us, 0.13% latency, 0.0 FLOPS, )
  )
  (lm_head): Linear(131.08 M, 1.95% Params, 52.43 GMACs, 1.95% MACs, 1.85 ms, 2.60% latency, 56.58 TFLOPS, in_features=4096, out_features=32001, bias=False)
)
------------------------------------------------------------------------------

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 128:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

world size:                                                   4       
data parallel size:                                           4       
model parallel size:                                          1       
batch size per GPU:                                           1       
params per gpu:                                               6738.42 M
params of model = params per GPU * mp_size:                   6738.42 M
fwd MACs per GPU:                                             2978.38 GMACs
fwd flops per GPU:                                            5957.12 G
fwd flops of model = fwd flops per GPU * mp_size:             5957.12 G
fwd latency:                                                  75.51 ms
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:          78.89 TFLOPS
bwd latency:                                                  224.62 ms
bwd FLOPS per GPU = 2.0 * fwd flops per GPU / bwd latency:    53.04 TFLOPS
fwd+bwd FLOPS per GPU = 3.0 * fwd flops per GPU / (fwd+bwd latency):   59.54 TFLOPS
step latency:                                                 30.72 us
iter latency:                                                 300.16 ms
FLOPS per GPU = 3.0 * fwd flops per GPU / iter latency:       59.54 TFLOPS
samples/second:                                               13.33   

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'LlamaForCausalLM': '6738.42 M'}
    MACs        - {'LlamaForCausalLM': '2978.38 GMACs'}
    fwd latency - {'LlamaForCausalLM': '75.42 ms'}
depth 1:
    params      - {'LlamaModel': '6607.35 M'}
    MACs        - {'LlamaModel': '2920.32 GMACs'}
    fwd latency - {'LlamaModel': '73.22 ms'}
depth 2:
    params      - {'ModuleList': '6476.27 M'}
    MACs        - {'ModuleList': '2920.32 GMACs'}
    fwd latency - {'ModuleList': '68.26 ms'}
depth 3:
    params      - {'LlamaDecoderLayer': '6476.27 M'}
    MACs        - {'LlamaDecoderLayer': '2920.32 GMACs'}
    fwd latency - {'LlamaDecoderLayer': '68.26 ms'}
depth 4:
    params      - {'LlamaMLP': '4328.52 M'}
    MACs        - {'LlamaMLP': '1917.54 GMACs'}
    fwd latency - {'LlamaAttention': '33.52 ms'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

LlamaForCausalLM(
  6738.42 M, 100.00% Params, 2978.38 GMACs, 100.00% MACs, 75.42 ms, 100.00% latency, 78.98 TFLOPS, 
  (model): LlamaModel(
    6607.35 M, 98.05% Params, 2920.32 GMACs, 98.05% MACs, 73.22 ms, 97.08% latency, 79.77 TFLOPS, 
    (embed_tokens): Embedding(131.08 M, 1.95% Params, 0 MACs, 0.00% MACs, 67.95 us, 0.09% latency, 0.0 FLOPS, 32001, 4096)
    (layers): ModuleList(
      (0): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 91.26 GMACs, 3.06% MACs, 2.34 ms, 3.11% latency, 77.84 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 31.34 GMACs, 1.05% MACs, 1.12 ms, 1.49% latency, 55.94 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 157.83 us, 0.21% latency, 94.18 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 120.88 us, 0.16% latency, 122.97 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 116.59 us, 0.15% latency, 127.5 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 124.93 us, 0.17% latency, 118.98 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 36.48 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 59.92 GMACs, 2.01% MACs, 822.31 us, 1.09% latency, 145.75 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 239.13 us, 0.32% latency, 167.06 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 233.89 us, 0.31% latency, 170.8 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 227.93 us, 0.30% latency, 175.27 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 148.22 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 199.79 us, 0.26% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 95.13 us, 0.13% latency, 0.0 FLOPS, )
      )
      (1): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 91.26 GMACs, 3.06% MACs, 2.16 ms, 2.86% latency, 84.49 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 31.34 GMACs, 1.05% MACs, 1.06 ms, 1.41% latency, 59.09 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 125.17 us, 0.17% latency, 118.76 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 117.54 us, 0.16% latency, 126.46 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 113.96 us, 0.15% latency, 130.43 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 128.98 us, 0.17% latency, 115.24 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.29 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 59.92 GMACs, 2.01% MACs, 814.44 us, 1.08% latency, 147.16 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 237.7 us, 0.32% latency, 168.06 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 233.41 us, 0.31% latency, 171.15 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 226.26 us, 0.30% latency, 176.56 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.42 us, 0.04% latency, 150.39 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 96.8 us, 0.13% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.84 us, 0.12% latency, 0.0 FLOPS, )
      )
      (2): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 91.26 GMACs, 3.06% MACs, 2.14 ms, 2.84% latency, 85.34 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 31.34 GMACs, 1.05% MACs, 1.05 ms, 1.39% latency, 59.64 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 123.98 us, 0.16% latency, 119.9 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 116.11 us, 0.15% latency, 128.02 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 114.68 us, 0.15% latency, 129.62 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 122.31 us, 0.16% latency, 121.53 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.05 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 59.92 GMACs, 2.01% MACs, 811.82 us, 1.08% latency, 147.63 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 237.7 us, 0.32% latency, 168.06 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 232.93 us, 0.31% latency, 171.5 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 225.07 us, 0.30% latency, 177.5 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.19 us, 0.04% latency, 151.51 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.22 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.78 us, 0.12% latency, 0.0 FLOPS, )
      )
      (3): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 91.26 GMACs, 3.06% MACs, 2.12 ms, 2.81% latency, 86.2 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 31.34 GMACs, 1.05% MACs, 1.04 ms, 1.38% latency, 60.2 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 122.79 us, 0.16% latency, 121.06 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 115.63 us, 0.15% latency, 128.55 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 114.2 us, 0.15% latency, 130.16 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 121.36 us, 0.16% latency, 122.49 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.62 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 59.92 GMACs, 2.01% MACs, 807.05 us, 1.07% latency, 148.51 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 236.27 us, 0.31% latency, 169.08 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 232.46 us, 0.31% latency, 171.85 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 225.31 us, 0.30% latency, 177.31 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.71 us, 0.04% latency, 153.79 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.12 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 83.68 us, 0.11% latency, 0.0 FLOPS, )
      )
      (4): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 91.26 GMACs, 3.06% MACs, 2.15 ms, 2.85% latency, 84.84 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 31.34 GMACs, 1.05% MACs, 1.06 ms, 1.41% latency, 59.07 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 126.6 us, 0.17% latency, 117.41 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 115.87 us, 0.15% latency, 128.29 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 115.63 us, 0.15% latency, 128.55 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 121.83 us, 0.16% latency, 122.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.81 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 59.92 GMACs, 2.01% MACs, 811.34 us, 1.08% latency, 147.72 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 236.51 us, 0.31% latency, 168.91 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 232.7 us, 0.31% latency, 171.68 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 225.54 us, 0.30% latency, 177.12 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.66 us, 0.04% latency, 149.3 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 95.37 us, 0.13% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.31 us, 0.11% latency, 0.0 FLOPS, )
      )
      (5): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 91.26 GMACs, 3.06% MACs, 2.13 ms, 2.83% latency, 85.6 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 31.34 GMACs, 1.05% MACs, 1.04 ms, 1.39% latency, 60.0 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 124.22 us, 0.16% latency, 119.67 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 115.16 us, 0.15% latency, 129.08 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 114.44 us, 0.15% latency, 129.89 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 120.64 us, 0.16% latency, 123.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.29 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 59.92 GMACs, 2.01% MACs, 814.44 us, 1.08% latency, 147.16 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 236.99 us, 0.31% latency, 168.57 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 233.17 us, 0.31% latency, 171.33 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 225.31 us, 0.30% latency, 177.31 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.95 us, 0.04% latency, 152.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.55 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.55 us, 0.11% latency, 0.0 FLOPS, )
      )
      (6): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 91.26 GMACs, 3.06% MACs, 2.13 ms, 2.83% latency, 85.51 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 31.34 GMACs, 1.05% MACs, 1.04 ms, 1.38% latency, 60.05 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 123.02 us, 0.16% latency, 120.83 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 115.87 us, 0.15% latency, 128.29 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 112.77 us, 0.15% latency, 131.81 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 122.79 us, 0.16% latency, 121.06 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 59.92 GMACs, 2.01% MACs, 811.58 us, 1.08% latency, 147.68 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 236.75 us, 0.31% latency, 168.74 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 233.17 us, 0.31% latency, 171.33 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 226.97 us, 0.30% latency, 176.01 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.71 us, 0.04% latency, 153.79 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.17 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.5 us, 0.12% latency, 0.0 FLOPS, )
      )
      (7): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 91.26 GMACs, 3.06% MACs, 2.12 ms, 2.81% latency, 85.98 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 31.34 GMACs, 1.05% MACs, 1.04 ms, 1.38% latency, 60.11 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 124.45 us, 0.17% latency, 119.44 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 115.87 us, 0.15% latency, 128.29 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 115.16 us, 0.15% latency, 129.08 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 120.88 us, 0.16% latency, 122.97 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 59.92 GMACs, 2.01% MACs, 811.58 us, 1.08% latency, 147.68 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 236.75 us, 0.31% latency, 168.74 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 232.93 us, 0.31% latency, 171.5 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 224.83 us, 0.30% latency, 177.68 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.95 us, 0.04% latency, 152.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.88 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 84.16 us, 0.11% latency, 0.0 FLOPS, )
      )
      (8): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 91.26 GMACs, 3.06% MACs, 2.11 ms, 2.80% latency, 86.51 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 31.34 GMACs, 1.05% MACs, 1.04 ms, 1.37% latency, 60.51 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 123.02 us, 0.16% latency, 120.83 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 115.63 us, 0.15% latency, 128.55 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 114.68 us, 0.15% latency, 129.62 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 119.21 us, 0.16% latency, 124.69 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.62 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 59.92 GMACs, 2.01% MACs, 808.24 us, 1.07% latency, 148.29 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 235.8 us, 0.31% latency, 169.42 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 232.22 us, 0.31% latency, 172.03 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 225.78 us, 0.30% latency, 176.93 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 148.22 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.12 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 83.21 us, 0.11% latency, 0.0 FLOPS, )
      )
      (9): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 91.26 GMACs, 3.06% MACs, 2.15 ms, 2.85% latency, 84.93 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 31.34 GMACs, 1.05% MACs, 1.06 ms, 1.40% latency, 59.35 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 127.08 us, 0.17% latency, 116.97 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 116.59 us, 0.15% latency, 127.5 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 114.92 us, 0.15% latency, 129.35 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 120.4 us, 0.16% latency, 123.46 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.52 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 59.92 GMACs, 2.01% MACs, 808.24 us, 1.07% latency, 148.29 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 237.23 us, 0.31% latency, 168.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 232.22 us, 0.31% latency, 172.03 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 225.31 us, 0.30% latency, 177.31 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.95 us, 0.04% latency, 152.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.98 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.22 us, 0.12% latency, 0.0 FLOPS, )
      )
      (10): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 91.26 GMACs, 3.06% MACs, 2.12 ms, 2.81% latency, 86.01 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 31.34 GMACs, 1.05% MACs, 1.04 ms, 1.38% latency, 60.01 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 121.83 us, 0.16% latency, 122.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 115.16 us, 0.15% latency, 129.08 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 113.96 us, 0.15% latency, 130.43 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 120.64 us, 0.16% latency, 123.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.33 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 59.92 GMACs, 2.01% MACs, 811.34 us, 1.08% latency, 147.72 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 236.51 us, 0.31% latency, 168.91 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 232.7 us, 0.31% latency, 171.68 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 225.54 us, 0.30% latency, 177.12 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.23 us, 0.04% latency, 156.14 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.93 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 84.16 us, 0.11% latency, 0.0 FLOPS, )
      )
      (11): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 91.26 GMACs, 3.06% MACs, 2.11 ms, 2.80% latency, 86.4 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 31.34 GMACs, 1.05% MACs, 1.04 ms, 1.38% latency, 60.39 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 121.83 us, 0.16% latency, 122.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 116.59 us, 0.15% latency, 127.5 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 113.96 us, 0.15% latency, 130.43 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 120.88 us, 0.16% latency, 122.97 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.62 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 59.92 GMACs, 2.01% MACs, 808.48 us, 1.07% latency, 148.24 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 235.8 us, 0.31% latency, 169.42 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 231.98 us, 0.31% latency, 172.21 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 225.31 us, 0.30% latency, 177.31 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 148.22 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.74 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 84.88 us, 0.11% latency, 0.0 FLOPS, )
      )
      (12): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 91.26 GMACs, 3.06% MACs, 2.13 ms, 2.82% latency, 85.78 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 31.34 GMACs, 1.05% MACs, 1.05 ms, 1.39% latency, 59.97 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 125.17 us, 0.17% latency, 118.76 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 116.83 us, 0.15% latency, 127.24 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 113.73 us, 0.15% latency, 130.71 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 120.64 us, 0.16% latency, 123.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.81 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 59.92 GMACs, 2.01% MACs, 810.86 us, 1.08% latency, 147.81 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 237.46 us, 0.31% latency, 168.23 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 233.41 us, 0.31% latency, 171.15 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 225.07 us, 0.30% latency, 177.5 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.19 us, 0.04% latency, 151.51 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.36 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 85.12 us, 0.11% latency, 0.0 FLOPS, )
      )
      (13): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 91.26 GMACs, 3.06% MACs, 2.13 ms, 2.82% latency, 85.86 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 31.34 GMACs, 1.05% MACs, 1.05 ms, 1.39% latency, 59.93 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 123.5 us, 0.16% latency, 120.36 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 115.87 us, 0.15% latency, 128.29 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 114.44 us, 0.15% latency, 129.89 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 121.12 us, 0.16% latency, 122.73 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.33 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 59.92 GMACs, 2.01% MACs, 810.62 us, 1.07% latency, 147.85 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 236.27 us, 0.31% latency, 169.08 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 233.17 us, 0.31% latency, 171.33 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 225.54 us, 0.30% latency, 177.12 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 146.1 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.45 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.55 us, 0.11% latency, 0.0 FLOPS, )
      )
      (14): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 91.26 GMACs, 3.06% MACs, 2.12 ms, 2.81% latency, 86.09 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 31.34 GMACs, 1.05% MACs, 1.04 ms, 1.38% latency, 60.31 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 121.83 us, 0.16% latency, 122.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 116.59 us, 0.15% latency, 127.5 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 114.44 us, 0.15% latency, 129.89 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 120.64 us, 0.16% latency, 123.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 59.92 GMACs, 2.01% MACs, 811.58 us, 1.08% latency, 147.68 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 237.46 us, 0.31% latency, 168.23 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 232.93 us, 0.31% latency, 171.5 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 225.31 us, 0.30% latency, 177.31 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.19 us, 0.04% latency, 151.51 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.93 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 84.16 us, 0.11% latency, 0.0 FLOPS, )
      )
      (15): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 91.26 GMACs, 3.06% MACs, 2.13 ms, 2.82% latency, 85.88 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 31.34 GMACs, 1.05% MACs, 1.05 ms, 1.39% latency, 59.82 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 124.22 us, 0.16% latency, 119.67 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 116.35 us, 0.15% latency, 127.76 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 113.96 us, 0.15% latency, 130.43 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 120.4 us, 0.16% latency, 123.46 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.05 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 59.92 GMACs, 2.01% MACs, 807.29 us, 1.07% latency, 148.46 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 236.75 us, 0.31% latency, 168.74 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 231.5 us, 0.31% latency, 172.56 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 225.07 us, 0.30% latency, 177.5 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.47 us, 0.04% latency, 154.95 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.88 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 83.68 us, 0.11% latency, 0.0 FLOPS, )
      )
      (16): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 91.26 GMACs, 3.06% MACs, 2.11 ms, 2.80% latency, 86.51 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 31.34 GMACs, 1.05% MACs, 1.04 ms, 1.38% latency, 60.34 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 121.59 us, 0.16% latency, 122.25 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 114.44 us, 0.15% latency, 129.89 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 112.77 us, 0.15% latency, 131.81 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 121.59 us, 0.16% latency, 122.25 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 59.92 GMACs, 2.01% MACs, 806.33 us, 1.07% latency, 148.64 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 235.56 us, 0.31% latency, 169.59 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 232.46 us, 0.31% latency, 171.85 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 224.59 us, 0.30% latency, 177.87 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.23 us, 0.04% latency, 156.14 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.45 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 83.68 us, 0.11% latency, 0.0 FLOPS, )
      )
      (17): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 91.26 GMACs, 3.06% MACs, 2.11 ms, 2.80% latency, 86.34 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 31.34 GMACs, 1.05% MACs, 1.04 ms, 1.38% latency, 60.44 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 122.31 us, 0.16% latency, 121.53 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 115.39 us, 0.15% latency, 128.82 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 114.44 us, 0.15% latency, 129.89 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 120.88 us, 0.16% latency, 122.97 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 59.92 GMACs, 2.01% MACs, 810.86 us, 1.08% latency, 147.81 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 236.51 us, 0.31% latency, 168.91 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 233.41 us, 0.31% latency, 171.15 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 226.02 us, 0.30% latency, 176.75 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.19 us, 0.04% latency, 151.51 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.45 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 84.16 us, 0.11% latency, 0.0 FLOPS, )
      )
      (18): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 91.26 GMACs, 3.06% MACs, 2.12 ms, 2.81% latency, 86.01 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 31.34 GMACs, 1.05% MACs, 1.05 ms, 1.39% latency, 59.97 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 124.69 us, 0.17% latency, 119.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 116.11 us, 0.15% latency, 128.02 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 115.39 us, 0.15% latency, 128.82 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 122.55 us, 0.16% latency, 121.3 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 59.92 GMACs, 2.01% MACs, 807.76 us, 1.07% latency, 148.37 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 237.23 us, 0.31% latency, 168.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 232.7 us, 0.31% latency, 171.68 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 225.31 us, 0.30% latency, 177.31 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.47 us, 0.04% latency, 154.95 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.41 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 84.64 us, 0.11% latency, 0.0 FLOPS, )
      )
      (19): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 91.26 GMACs, 3.06% MACs, 2.11 ms, 2.80% latency, 86.51 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 31.34 GMACs, 1.05% MACs, 1.04 ms, 1.38% latency, 60.38 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 123.74 us, 0.16% latency, 120.13 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 114.92 us, 0.15% latency, 129.35 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 113.01 us, 0.15% latency, 131.53 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 121.12 us, 0.16% latency, 122.73 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 59.92 GMACs, 2.01% MACs, 807.52 us, 1.07% latency, 148.42 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 235.32 us, 0.31% latency, 169.76 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 232.22 us, 0.31% latency, 172.03 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 226.02 us, 0.30% latency, 176.75 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.47 us, 0.04% latency, 154.95 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.5 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 84.4 us, 0.11% latency, 0.0 FLOPS, )
      )
      (20): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 91.26 GMACs, 3.06% MACs, 2.11 ms, 2.80% latency, 86.43 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 31.34 GMACs, 1.05% MACs, 1.04 ms, 1.38% latency, 60.39 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 122.79 us, 0.16% latency, 121.06 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 114.68 us, 0.15% latency, 129.62 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 113.73 us, 0.15% latency, 130.71 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 121.12 us, 0.16% latency, 122.73 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.66 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 59.92 GMACs, 2.01% MACs, 808.95 us, 1.07% latency, 148.16 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 236.75 us, 0.31% latency, 168.74 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 232.22 us, 0.31% latency, 172.03 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 225.54 us, 0.30% latency, 177.12 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.95 us, 0.04% latency, 152.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.93 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 84.4 us, 0.11% latency, 0.0 FLOPS, )
      )
      (21): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 91.26 GMACs, 3.06% MACs, 2.13 ms, 2.83% latency, 85.65 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 31.34 GMACs, 1.05% MACs, 1.05 ms, 1.39% latency, 59.67 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 126.6 us, 0.17% latency, 117.41 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 116.83 us, 0.15% latency, 127.24 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 114.2 us, 0.15% latency, 130.16 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 121.36 us, 0.16% latency, 122.49 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.81 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 59.92 GMACs, 2.01% MACs, 808.0 us, 1.07% latency, 148.33 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 236.75 us, 0.31% latency, 168.74 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 232.93 us, 0.31% latency, 171.5 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 225.31 us, 0.30% latency, 177.31 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.99 us, 0.04% latency, 157.34 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.03 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.31 us, 0.11% latency, 0.0 FLOPS, )
      )
      (22): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 91.26 GMACs, 3.06% MACs, 2.12 ms, 2.81% latency, 86.07 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 31.34 GMACs, 1.05% MACs, 1.04 ms, 1.38% latency, 60.02 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 124.22 us, 0.16% latency, 119.67 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 119.21 us, 0.16% latency, 124.69 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 113.73 us, 0.15% latency, 130.71 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 120.16 us, 0.16% latency, 123.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 59.92 GMACs, 2.01% MACs, 808.24 us, 1.07% latency, 148.29 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 236.27 us, 0.31% latency, 169.08 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 233.17 us, 0.31% latency, 171.33 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 225.31 us, 0.30% latency, 177.31 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.23 us, 0.04% latency, 156.14 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.12 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 85.12 us, 0.11% latency, 0.0 FLOPS, )
      )
      (23): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 91.26 GMACs, 3.06% MACs, 2.14 ms, 2.83% latency, 85.39 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 31.34 GMACs, 1.05% MACs, 1.05 ms, 1.39% latency, 59.67 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 126.36 us, 0.17% latency, 117.64 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 118.26 us, 0.16% latency, 125.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 113.96 us, 0.15% latency, 130.43 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 119.92 us, 0.16% latency, 123.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.81 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 59.92 GMACs, 2.01% MACs, 809.19 us, 1.07% latency, 148.11 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 237.7 us, 0.32% latency, 168.06 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 232.46 us, 0.31% latency, 171.85 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 226.02 us, 0.30% latency, 176.75 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.99 us, 0.04% latency, 157.34 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.74 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.31 us, 0.11% latency, 0.0 FLOPS, )
      )
      (24): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 91.26 GMACs, 3.06% MACs, 2.13 ms, 2.82% latency, 85.82 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 31.34 GMACs, 1.05% MACs, 1.04 ms, 1.38% latency, 60.01 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 123.02 us, 0.16% latency, 120.83 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 115.87 us, 0.15% latency, 128.29 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 115.39 us, 0.15% latency, 128.82 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 121.36 us, 0.16% latency, 122.49 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.05 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 59.92 GMACs, 2.01% MACs, 808.48 us, 1.07% latency, 148.24 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 236.75 us, 0.31% latency, 168.74 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 231.98 us, 0.31% latency, 172.21 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 225.78 us, 0.30% latency, 176.93 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.47 us, 0.04% latency, 154.95 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.08 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 85.12 us, 0.11% latency, 0.0 FLOPS, )
      )
      (25): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 91.26 GMACs, 3.06% MACs, 2.13 ms, 2.83% latency, 85.67 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 31.34 GMACs, 1.05% MACs, 1.04 ms, 1.38% latency, 60.22 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 122.07 us, 0.16% latency, 121.77 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 114.92 us, 0.15% latency, 129.35 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 114.68 us, 0.15% latency, 129.62 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 121.83 us, 0.16% latency, 122.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 59.92 GMACs, 2.01% MACs, 808.48 us, 1.07% latency, 148.24 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 236.75 us, 0.31% latency, 168.74 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 232.93 us, 0.31% latency, 171.5 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 225.07 us, 0.30% latency, 177.5 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.23 us, 0.04% latency, 156.14 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.12 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.94 us, 0.12% latency, 0.0 FLOPS, )
      )
      (26): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 91.26 GMACs, 3.06% MACs, 2.12 ms, 2.81% latency, 86.08 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 31.34 GMACs, 1.05% MACs, 1.04 ms, 1.38% latency, 60.11 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 122.79 us, 0.16% latency, 121.06 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 115.39 us, 0.15% latency, 128.82 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 113.73 us, 0.15% latency, 130.71 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 119.92 us, 0.16% latency, 123.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.29 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 59.92 GMACs, 2.01% MACs, 807.76 us, 1.07% latency, 148.37 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 235.8 us, 0.31% latency, 169.42 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 232.22 us, 0.31% latency, 172.03 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 226.02 us, 0.30% latency, 176.75 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.95 us, 0.04% latency, 152.64 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.41 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 85.59 us, 0.11% latency, 0.0 FLOPS, )
      )
      (27): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 91.26 GMACs, 3.06% MACs, 2.12 ms, 2.81% latency, 86.24 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 31.34 GMACs, 1.05% MACs, 1.04 ms, 1.38% latency, 60.31 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 122.55 us, 0.16% latency, 121.3 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 116.35 us, 0.15% latency, 127.76 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 113.96 us, 0.15% latency, 130.43 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 120.88 us, 0.16% latency, 122.97 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.66 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 59.92 GMACs, 2.01% MACs, 807.29 us, 1.07% latency, 148.46 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 235.56 us, 0.31% latency, 169.59 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 232.7 us, 0.31% latency, 171.68 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 224.59 us, 0.30% latency, 177.87 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.23 us, 0.04% latency, 156.14 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.93 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 85.35 us, 0.11% latency, 0.0 FLOPS, )
      )
      (28): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 91.26 GMACs, 3.06% MACs, 2.14 ms, 2.83% latency, 85.45 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 31.34 GMACs, 1.05% MACs, 1.05 ms, 1.40% latency, 59.51 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 125.65 us, 0.17% latency, 118.3 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 116.35 us, 0.15% latency, 127.76 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 114.2 us, 0.15% latency, 130.16 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 122.31 us, 0.16% latency, 121.53 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.05 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 59.92 GMACs, 2.01% MACs, 808.95 us, 1.07% latency, 148.16 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 236.03 us, 0.31% latency, 169.25 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 233.41 us, 0.31% latency, 171.15 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 226.26 us, 0.30% latency, 176.56 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.99 us, 0.04% latency, 157.34 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.98 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.07 us, 0.11% latency, 0.0 FLOPS, )
      )
      (29): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 91.26 GMACs, 3.06% MACs, 2.12 ms, 2.81% latency, 86.15 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 31.34 GMACs, 1.05% MACs, 1.04 ms, 1.38% latency, 60.02 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 122.79 us, 0.16% latency, 121.06 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 117.06 us, 0.16% latency, 126.98 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 113.73 us, 0.15% latency, 130.71 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 121.12 us, 0.16% latency, 122.73 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 59.92 GMACs, 2.01% MACs, 806.57 us, 1.07% latency, 148.59 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 235.56 us, 0.31% latency, 169.59 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 233.17 us, 0.31% latency, 171.33 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 224.11 us, 0.30% latency, 178.25 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.99 us, 0.04% latency, 157.34 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.88 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 85.12 us, 0.11% latency, 0.0 FLOPS, )
      )
      (30): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 91.26 GMACs, 3.06% MACs, 2.13 ms, 2.83% latency, 85.51 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 31.34 GMACs, 1.05% MACs, 1.05 ms, 1.40% latency, 59.56 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 126.12 us, 0.17% latency, 117.86 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 116.35 us, 0.15% latency, 127.76 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 113.49 us, 0.15% latency, 130.98 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 122.31 us, 0.16% latency, 121.53 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.33 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 59.92 GMACs, 2.01% MACs, 806.81 us, 1.07% latency, 148.55 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 236.03 us, 0.31% latency, 169.25 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 232.22 us, 0.31% latency, 172.03 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 225.78 us, 0.30% latency, 176.93 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.47 us, 0.04% latency, 154.95 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.74 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 85.83 us, 0.11% latency, 0.0 FLOPS, )
      )
      (31): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 91.26 GMACs, 3.06% MACs, 2.12 ms, 2.81% latency, 85.99 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 31.34 GMACs, 1.05% MACs, 1.05 ms, 1.39% latency, 59.95 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 123.98 us, 0.16% latency, 119.9 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 115.63 us, 0.15% latency, 128.55 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 115.87 us, 0.15% latency, 128.29 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 7.43 GMACs, 0.25% MACs, 121.12 us, 0.16% latency, 122.73 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.04% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 59.92 GMACs, 2.01% MACs, 808.72 us, 1.07% latency, 148.2 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 236.27 us, 0.31% latency, 169.08 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 232.7 us, 0.31% latency, 171.68 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 19.97 GMACs, 0.67% MACs, 225.07 us, 0.30% latency, 177.5 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.19 us, 0.04% latency, 151.51 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.41 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 84.88 us, 0.11% latency, 0.0 FLOPS, )
      )
    )
    (norm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 96.32 us, 0.13% latency, 0.0 FLOPS, )
  )
  (lm_head): Linear(131.08 M, 1.95% Params, 58.07 GMACs, 1.95% MACs, 1.89 ms, 2.50% latency, 61.49 TFLOPS, in_features=4096, out_features=32001, bias=False)
)
------------------------------------------------------------------------------

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 128:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

world size:                                                   4       
data parallel size:                                           4       
model parallel size:                                          1       
batch size per GPU:                                           1       
params per gpu:                                               6738.42 M
params of model = params per GPU * mp_size:                   6738.42 M
fwd MACs per GPU:                                             2358.17 GMACs
fwd flops per GPU:                                            4716.6 G
fwd flops of model = fwd flops per GPU * mp_size:             4716.6 G
fwd latency:                                                  61.14 ms
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:          77.14 TFLOPS
bwd latency:                                                  188.4 ms
bwd FLOPS per GPU = 2.0 * fwd flops per GPU / bwd latency:    50.07 TFLOPS
fwd+bwd FLOPS per GPU = 3.0 * fwd flops per GPU / (fwd+bwd latency):   56.7 TFLOPS
step latency:                                                 30.72 us
iter latency:                                                 249.57 ms
FLOPS per GPU = 3.0 * fwd flops per GPU / iter latency:       56.7 TFLOPS
samples/second:                                               16.03   

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'LlamaForCausalLM': '6738.42 M'}
    MACs        - {'LlamaForCausalLM': '2358.17 GMACs'}
    fwd latency - {'LlamaForCausalLM': '61.05 ms'}
depth 1:
    params      - {'LlamaModel': '6607.35 M'}
    MACs        - {'LlamaModel': '2312.03 GMACs'}
    fwd latency - {'LlamaModel': '59.22 ms'}
depth 2:
    params      - {'ModuleList': '6476.27 M'}
    MACs        - {'ModuleList': '2312.03 GMACs'}
    fwd latency - {'ModuleList': '54.28 ms'}
depth 3:
    params      - {'LlamaDecoderLayer': '6476.27 M'}
    MACs        - {'LlamaDecoderLayer': '2312.03 GMACs'}
    fwd latency - {'LlamaDecoderLayer': '54.28 ms'}
depth 4:
    params      - {'LlamaMLP': '4328.52 M'}
    MACs        - {'LlamaMLP': '1523.64 GMACs'}
    fwd latency - {'LlamaAttention': '24.07 ms'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

LlamaForCausalLM(
  6738.42 M, 100.00% Params, 2358.17 GMACs, 100.00% MACs, 61.05 ms, 100.00% latency, 77.26 TFLOPS, 
  (model): LlamaModel(
    6607.35 M, 98.05% Params, 2312.03 GMACs, 98.04% MACs, 59.22 ms, 97.01% latency, 78.08 TFLOPS, 
    (embed_tokens): Embedding(131.08 M, 1.95% Params, 0 MACs, 0.00% MACs, 64.61 us, 0.11% latency, 0.0 FLOPS, 32001, 4096)
    (layers): ModuleList(
      (0): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 72.25 GMACs, 3.06% MACs, 1.93 ms, 3.16% latency, 74.82 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 24.64 GMACs, 1.04% MACs, 834.47 us, 1.37% latency, 59.05 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 125.41 us, 0.21% latency, 94.18 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 86.55 us, 0.14% latency, 136.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 81.54 us, 0.13% latency, 144.85 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 88.69 us, 0.15% latency, 133.17 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 36.95 us, 0.06% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 47.61 GMACs, 2.02% MACs, 687.36 us, 1.13% latency, 138.55 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 203.61 us, 0.33% latency, 155.9 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 194.55 us, 0.32% latency, 163.16 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 170.95 us, 0.28% latency, 185.69 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.99 us, 0.05% latency, 125.02 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 209.57 us, 0.34% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.6 us, 0.15% latency, 0.0 FLOPS, )
      )
      (1): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 72.25 GMACs, 3.06% MACs, 1.7 ms, 2.79% latency, 84.78 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 24.64 GMACs, 1.04% MACs, 754.12 us, 1.24% latency, 65.35 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 90.12 us, 0.15% latency, 131.06 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 82.73 us, 0.14% latency, 142.77 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 80.35 us, 0.13% latency, 147.0 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 86.78 us, 0.14% latency, 136.1 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.06% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 47.61 GMACs, 2.02% MACs, 677.82 us, 1.11% latency, 140.5 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 200.27 us, 0.33% latency, 158.5 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 193.83 us, 0.32% latency, 163.76 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 171.9 us, 0.28% latency, 184.66 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 29.33 us, 0.05% latency, 132.13 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.74 us, 0.15% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 84.16 us, 0.14% latency, 0.0 FLOPS, )
      )
      (2): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 72.25 GMACs, 3.06% MACs, 1.7 ms, 2.78% latency, 85.01 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 24.64 GMACs, 1.04% MACs, 751.02 us, 1.23% latency, 65.62 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 88.45 us, 0.14% latency, 133.53 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 82.49 us, 0.14% latency, 143.18 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 79.87 us, 0.13% latency, 147.88 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 89.17 us, 0.15% latency, 132.46 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 47.61 GMACs, 2.02% MACs, 677.82 us, 1.11% latency, 140.5 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 200.99 us, 0.33% latency, 157.93 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 193.6 us, 0.32% latency, 163.96 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 171.42 us, 0.28% latency, 185.17 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 29.33 us, 0.05% latency, 132.13 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.12 us, 0.15% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 83.92 us, 0.14% latency, 0.0 FLOPS, )
      )
      (3): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 72.25 GMACs, 3.06% MACs, 1.7 ms, 2.78% latency, 85.18 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 24.64 GMACs, 1.04% MACs, 749.11 us, 1.23% latency, 65.78 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 89.88 us, 0.15% latency, 131.4 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 81.3 us, 0.13% latency, 145.28 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 79.63 us, 0.13% latency, 148.32 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 86.07 us, 0.14% latency, 137.23 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.81 us, 0.06% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 47.61 GMACs, 2.02% MACs, 683.31 us, 1.12% latency, 139.37 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 200.27 us, 0.33% latency, 158.5 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 192.88 us, 0.32% latency, 164.57 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 180.01 us, 0.29% latency, 176.34 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 28.85 us, 0.05% latency, 134.32 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.21 us, 0.14% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 80.59 us, 0.13% latency, 0.0 FLOPS, )
      )
      (4): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 72.25 GMACs, 3.06% MACs, 1.68 ms, 2.75% latency, 86.21 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 24.64 GMACs, 1.04% MACs, 742.67 us, 1.22% latency, 66.35 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 88.21 us, 0.14% latency, 133.89 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 81.06 us, 0.13% latency, 145.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 79.87 us, 0.13% latency, 147.88 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 86.31 us, 0.14% latency, 136.85 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.06% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 47.61 GMACs, 2.02% MACs, 668.53 us, 1.10% latency, 142.45 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 198.36 us, 0.32% latency, 160.02 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 192.4 us, 0.32% latency, 164.98 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 170.71 us, 0.28% latency, 185.95 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 28.61 us, 0.05% latency, 135.43 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 90.36 us, 0.15% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 79.63 us, 0.13% latency, 0.0 FLOPS, )
      )
      (5): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 72.25 GMACs, 3.06% MACs, 1.68 ms, 2.75% latency, 86.16 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 24.64 GMACs, 1.04% MACs, 738.86 us, 1.21% latency, 66.7 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 88.45 us, 0.14% latency, 133.53 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 81.3 us, 0.13% latency, 145.28 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 79.87 us, 0.13% latency, 147.88 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 84.64 us, 0.14% latency, 139.55 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.06% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 47.61 GMACs, 2.02% MACs, 673.77 us, 1.10% latency, 141.34 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 200.75 us, 0.33% latency, 158.12 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 192.17 us, 0.31% latency, 165.18 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 171.42 us, 0.28% latency, 185.17 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 28.61 us, 0.05% latency, 135.43 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.55 us, 0.14% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 81.54 us, 0.13% latency, 0.0 FLOPS, )
      )
      (6): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 72.25 GMACs, 3.06% MACs, 1.7 ms, 2.78% latency, 85.18 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 24.64 GMACs, 1.04% MACs, 754.12 us, 1.24% latency, 65.35 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 90.6 us, 0.15% latency, 130.37 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 82.49 us, 0.14% latency, 143.18 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 80.35 us, 0.13% latency, 147.0 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 86.31 us, 0.14% latency, 136.85 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.52 us, 0.06% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 47.61 GMACs, 2.02% MACs, 672.58 us, 1.10% latency, 141.59 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 199.79 us, 0.33% latency, 158.88 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 192.88 us, 0.32% latency, 164.57 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 169.75 us, 0.28% latency, 186.99 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 28.85 us, 0.05% latency, 134.32 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 97.04 us, 0.16% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 80.59 us, 0.13% latency, 0.0 FLOPS, )
      )
      (7): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 72.25 GMACs, 3.06% MACs, 1.67 ms, 2.74% latency, 86.34 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 24.64 GMACs, 1.04% MACs, 741.24 us, 1.21% latency, 66.48 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 87.98 us, 0.14% latency, 134.25 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 80.82 us, 0.13% latency, 146.13 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 80.11 us, 0.13% latency, 147.44 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 86.07 us, 0.14% latency, 137.23 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.06% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 47.61 GMACs, 2.02% MACs, 672.58 us, 1.10% latency, 141.59 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 198.6 us, 0.33% latency, 159.83 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 193.6 us, 0.32% latency, 163.96 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 170.47 us, 0.28% latency, 186.21 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 28.85 us, 0.05% latency, 134.32 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.02 us, 0.14% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 78.92 us, 0.13% latency, 0.0 FLOPS, )
      )
      (8): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 72.25 GMACs, 3.06% MACs, 1.71 ms, 2.80% latency, 84.54 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 24.64 GMACs, 1.04% MACs, 761.75 us, 1.25% latency, 64.69 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 91.79 us, 0.15% latency, 128.67 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 81.54 us, 0.13% latency, 144.85 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 80.82 us, 0.13% latency, 146.13 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 87.02 us, 0.14% latency, 135.72 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 36.0 us, 0.06% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 47.61 GMACs, 2.02% MACs, 676.39 us, 1.11% latency, 140.79 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 201.23 us, 0.33% latency, 157.75 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 192.64 us, 0.32% latency, 164.77 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 170.23 us, 0.28% latency, 186.47 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 29.09 us, 0.05% latency, 133.21 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.79 us, 0.15% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 82.02 us, 0.13% latency, 0.0 FLOPS, )
      )
      (9): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 72.25 GMACs, 3.06% MACs, 1.69 ms, 2.76% latency, 85.74 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 24.64 GMACs, 1.04% MACs, 747.68 us, 1.22% latency, 65.91 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 88.93 us, 0.15% latency, 132.81 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 81.06 us, 0.13% latency, 145.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 79.87 us, 0.13% latency, 147.88 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 86.07 us, 0.14% latency, 137.23 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.29 us, 0.06% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 47.61 GMACs, 2.02% MACs, 674.25 us, 1.10% latency, 141.24 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 199.56 us, 0.33% latency, 159.07 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 193.36 us, 0.32% latency, 164.16 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 171.9 us, 0.28% latency, 184.66 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 27.89 us, 0.05% latency, 138.91 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.26 us, 0.14% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 81.3 us, 0.13% latency, 0.0 FLOPS, )
      )
      (10): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 72.25 GMACs, 3.06% MACs, 1.68 ms, 2.76% latency, 85.79 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 24.64 GMACs, 1.04% MACs, 749.35 us, 1.23% latency, 65.76 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 87.5 us, 0.14% latency, 134.99 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 81.54 us, 0.13% latency, 144.85 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 79.39 us, 0.13% latency, 148.77 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 94.18 us, 0.15% latency, 125.42 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 47.61 GMACs, 2.02% MACs, 672.1 us, 1.10% latency, 141.69 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 199.32 us, 0.33% latency, 159.26 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 193.36 us, 0.32% latency, 164.16 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 170.71 us, 0.28% latency, 185.95 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 29.56 us, 0.05% latency, 131.07 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.21 us, 0.14% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 81.06 us, 0.13% latency, 0.0 FLOPS, )
      )
      (11): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 72.25 GMACs, 3.06% MACs, 1.7 ms, 2.79% latency, 84.78 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 24.64 GMACs, 1.04% MACs, 758.89 us, 1.24% latency, 64.94 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 91.08 us, 0.15% latency, 129.68 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 82.25 us, 0.13% latency, 143.59 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 79.15 us, 0.13% latency, 149.22 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 87.5 us, 0.14% latency, 134.99 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.29 us, 0.06% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 47.61 GMACs, 2.02% MACs, 675.44 us, 1.11% latency, 140.99 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 199.32 us, 0.33% latency, 159.26 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 193.36 us, 0.32% latency, 164.16 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 171.18 us, 0.28% latency, 185.43 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 28.61 us, 0.05% latency, 135.43 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.74 us, 0.15% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 81.06 us, 0.13% latency, 0.0 FLOPS, )
      )
      (12): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 72.25 GMACs, 3.06% MACs, 1.7 ms, 2.78% latency, 85.01 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 24.64 GMACs, 1.04% MACs, 754.12 us, 1.24% latency, 65.35 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 88.45 us, 0.14% latency, 133.53 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 81.3 us, 0.13% latency, 145.28 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 80.35 us, 0.13% latency, 147.0 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 87.02 us, 0.14% latency, 135.72 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 36.24 us, 0.06% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 47.61 GMACs, 2.02% MACs, 680.69 us, 1.11% latency, 139.91 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 200.51 us, 0.33% latency, 158.31 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 194.55 us, 0.32% latency, 163.16 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 173.33 us, 0.28% latency, 183.13 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 29.33 us, 0.05% latency, 132.13 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.31 us, 0.14% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 81.06 us, 0.13% latency, 0.0 FLOPS, )
      )
      (13): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 72.25 GMACs, 3.06% MACs, 1.69 ms, 2.77% latency, 85.4 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 24.64 GMACs, 1.04% MACs, 749.35 us, 1.23% latency, 65.76 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 88.69 us, 0.15% latency, 133.17 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 82.02 us, 0.13% latency, 144.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 79.39 us, 0.13% latency, 148.77 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 87.26 us, 0.14% latency, 135.35 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.06% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 47.61 GMACs, 2.02% MACs, 673.53 us, 1.10% latency, 141.39 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 199.79 us, 0.33% latency, 158.88 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 192.64 us, 0.32% latency, 164.77 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 170.23 us, 0.28% latency, 186.47 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 29.09 us, 0.05% latency, 133.21 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.93 us, 0.15% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 83.45 us, 0.14% latency, 0.0 FLOPS, )
      )
      (14): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 72.25 GMACs, 3.06% MACs, 1.69 ms, 2.78% latency, 85.26 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 24.64 GMACs, 1.04% MACs, 753.16 us, 1.23% latency, 65.43 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 88.69 us, 0.15% latency, 133.17 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 81.54 us, 0.13% latency, 144.85 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 79.63 us, 0.13% latency, 148.32 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 87.02 us, 0.14% latency, 135.72 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.76 us, 0.06% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 47.61 GMACs, 2.02% MACs, 677.11 us, 1.11% latency, 140.64 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 198.6 us, 0.33% latency, 159.83 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 192.4 us, 0.32% latency, 164.98 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 169.99 us, 0.28% latency, 186.73 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 28.85 us, 0.05% latency, 134.32 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.74 us, 0.14% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 81.78 us, 0.13% latency, 0.0 FLOPS, )
      )
      (15): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 72.25 GMACs, 3.06% MACs, 1.68 ms, 2.76% latency, 85.82 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 24.64 GMACs, 1.04% MACs, 751.02 us, 1.23% latency, 65.62 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 89.41 us, 0.15% latency, 132.11 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 81.54 us, 0.13% latency, 144.85 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 79.39 us, 0.13% latency, 148.77 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 85.59 us, 0.14% latency, 137.99 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.52 us, 0.06% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 47.61 GMACs, 2.02% MACs, 670.67 us, 1.10% latency, 141.99 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 198.36 us, 0.32% latency, 160.02 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 194.07 us, 0.32% latency, 163.56 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 170.71 us, 0.28% latency, 185.95 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 28.13 us, 0.05% latency, 137.73 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.26 us, 0.14% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 80.11 us, 0.13% latency, 0.0 FLOPS, )
      )
      (16): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 72.25 GMACs, 3.06% MACs, 1.67 ms, 2.74% latency, 86.45 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 24.64 GMACs, 1.04% MACs, 740.77 us, 1.21% latency, 66.52 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 88.21 us, 0.14% latency, 133.89 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 82.25 us, 0.13% latency, 143.59 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 80.11 us, 0.13% latency, 147.44 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 86.07 us, 0.14% latency, 137.23 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.06% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 47.61 GMACs, 2.02% MACs, 672.34 us, 1.10% latency, 141.64 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 198.36 us, 0.32% latency, 160.02 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 193.83 us, 0.32% latency, 163.76 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 170.71 us, 0.28% latency, 185.95 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 28.13 us, 0.05% latency, 137.73 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 85.83 us, 0.14% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 78.92 us, 0.13% latency, 0.0 FLOPS, )
      )
      (17): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 72.25 GMACs, 3.06% MACs, 1.7 ms, 2.79% latency, 84.87 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 24.64 GMACs, 1.04% MACs, 756.03 us, 1.24% latency, 65.18 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 90.12 us, 0.15% latency, 131.06 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 82.02 us, 0.13% latency, 144.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 79.87 us, 0.13% latency, 147.88 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 85.83 us, 0.14% latency, 137.61 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.52 us, 0.06% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 47.61 GMACs, 2.02% MACs, 668.05 us, 1.09% latency, 142.55 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 198.36 us, 0.32% latency, 160.02 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 191.45 us, 0.31% latency, 165.8 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 170.71 us, 0.28% latency, 185.95 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 27.66 us, 0.05% latency, 140.1 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.22 us, 0.15% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 82.02 us, 0.13% latency, 0.0 FLOPS, )
      )
      (18): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 72.25 GMACs, 3.06% MACs, 1.69 ms, 2.76% latency, 85.73 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 24.64 GMACs, 1.04% MACs, 749.83 us, 1.23% latency, 65.72 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 89.41 us, 0.15% latency, 132.11 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 81.78 us, 0.13% latency, 144.43 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 79.87 us, 0.13% latency, 147.88 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 85.83 us, 0.14% latency, 137.61 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.81 us, 0.06% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 47.61 GMACs, 2.02% MACs, 672.82 us, 1.10% latency, 141.54 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 198.6 us, 0.33% latency, 159.83 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 193.12 us, 0.32% latency, 164.37 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 170.95 us, 0.28% latency, 185.69 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 28.61 us, 0.05% latency, 135.43 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.26 us, 0.14% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 81.54 us, 0.13% latency, 0.0 FLOPS, )
      )
      (19): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 72.25 GMACs, 3.06% MACs, 1.7 ms, 2.78% latency, 85.01 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 24.64 GMACs, 1.04% MACs, 760.56 us, 1.25% latency, 64.79 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 90.6 us, 0.15% latency, 130.37 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 82.73 us, 0.14% latency, 142.77 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 80.59 us, 0.13% latency, 146.57 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 86.55 us, 0.14% latency, 136.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.52 us, 0.06% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 47.61 GMACs, 2.02% MACs, 673.06 us, 1.10% latency, 141.49 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 199.79 us, 0.33% latency, 158.88 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 192.64 us, 0.32% latency, 164.77 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 170.47 us, 0.28% latency, 186.21 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 28.61 us, 0.05% latency, 135.43 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.69 us, 0.15% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 80.59 us, 0.13% latency, 0.0 FLOPS, )
      )
      (20): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 72.25 GMACs, 3.06% MACs, 1.68 ms, 2.75% latency, 86.15 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 24.64 GMACs, 1.04% MACs, 748.4 us, 1.23% latency, 65.85 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 88.93 us, 0.15% latency, 132.81 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 81.54 us, 0.13% latency, 144.85 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 80.35 us, 0.13% latency, 147.0 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 86.31 us, 0.14% latency, 136.85 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.06% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 47.61 GMACs, 2.02% MACs, 669.24 us, 1.10% latency, 142.3 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 199.08 us, 0.33% latency, 159.45 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 191.21 us, 0.31% latency, 166.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 169.52 us, 0.28% latency, 187.25 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 28.61 us, 0.05% latency, 135.43 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.07 us, 0.14% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 79.63 us, 0.13% latency, 0.0 FLOPS, )
      )
      (21): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 72.25 GMACs, 3.06% MACs, 1.68 ms, 2.74% latency, 86.27 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 24.64 GMACs, 1.04% MACs, 741.0 us, 1.21% latency, 66.5 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 87.98 us, 0.14% latency, 134.25 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 81.3 us, 0.13% latency, 145.28 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 79.63 us, 0.13% latency, 148.32 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 86.78 us, 0.14% latency, 136.1 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 47.61 GMACs, 2.02% MACs, 672.58 us, 1.10% latency, 141.59 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 199.32 us, 0.33% latency, 159.26 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 192.4 us, 0.32% latency, 164.98 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 171.18 us, 0.28% latency, 185.43 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 29.09 us, 0.05% latency, 133.21 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 84.4 us, 0.14% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 82.97 us, 0.14% latency, 0.0 FLOPS, )
      )
      (22): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 72.25 GMACs, 3.06% MACs, 1.69 ms, 2.77% latency, 85.57 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 24.64 GMACs, 1.04% MACs, 753.16 us, 1.23% latency, 65.43 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 89.17 us, 0.15% latency, 132.46 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 81.78 us, 0.13% latency, 144.43 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 80.59 us, 0.13% latency, 146.57 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 85.83 us, 0.14% latency, 137.61 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.52 us, 0.06% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 47.61 GMACs, 2.02% MACs, 672.58 us, 1.10% latency, 141.59 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 199.32 us, 0.33% latency, 159.26 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 193.12 us, 0.32% latency, 164.37 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 170.95 us, 0.28% latency, 185.69 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 28.85 us, 0.05% latency, 134.32 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.21 us, 0.14% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 81.3 us, 0.13% latency, 0.0 FLOPS, )
      )
      (23): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 72.25 GMACs, 3.06% MACs, 1.67 ms, 2.74% latency, 86.35 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 24.64 GMACs, 1.04% MACs, 744.58 us, 1.22% latency, 66.18 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 88.69 us, 0.15% latency, 133.17 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 81.54 us, 0.13% latency, 144.85 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 79.63 us, 0.13% latency, 148.32 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 86.31 us, 0.14% latency, 136.85 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.06% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 47.61 GMACs, 2.02% MACs, 669.24 us, 1.10% latency, 142.3 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 199.32 us, 0.33% latency, 159.26 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 192.88 us, 0.32% latency, 164.57 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 169.99 us, 0.28% latency, 186.73 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 28.85 us, 0.05% latency, 134.32 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.31 us, 0.14% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 79.39 us, 0.13% latency, 0.0 FLOPS, )
      )
      (24): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 72.25 GMACs, 3.06% MACs, 1.69 ms, 2.76% latency, 85.62 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 24.64 GMACs, 1.04% MACs, 753.16 us, 1.23% latency, 65.43 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 90.12 us, 0.15% latency, 131.06 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 81.78 us, 0.13% latency, 144.43 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 79.39 us, 0.13% latency, 148.77 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 86.78 us, 0.14% latency, 136.1 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.29 us, 0.06% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 47.61 GMACs, 2.02% MACs, 672.58 us, 1.10% latency, 141.59 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 199.79 us, 0.33% latency, 158.88 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 192.17 us, 0.31% latency, 165.18 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 170.71 us, 0.28% latency, 185.95 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 29.09 us, 0.05% latency, 133.21 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.21 us, 0.14% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 79.87 us, 0.13% latency, 0.0 FLOPS, )
      )
      (25): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 72.25 GMACs, 3.06% MACs, 1.69 ms, 2.76% latency, 85.63 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 24.64 GMACs, 1.04% MACs, 746.97 us, 1.22% latency, 65.97 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 94.18 us, 0.15% latency, 125.42 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 82.25 us, 0.13% latency, 143.59 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 79.39 us, 0.13% latency, 148.77 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 85.59 us, 0.14% latency, 137.99 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.06% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 47.61 GMACs, 2.02% MACs, 679.97 us, 1.11% latency, 140.05 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 199.56 us, 0.33% latency, 159.07 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 191.93 us, 0.31% latency, 165.39 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 179.29 us, 0.29% latency, 177.04 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 28.13 us, 0.05% latency, 137.73 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.55 us, 0.14% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 80.35 us, 0.13% latency, 0.0 FLOPS, )
      )
      (26): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 72.25 GMACs, 3.06% MACs, 1.67 ms, 2.74% latency, 86.5 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 24.64 GMACs, 1.04% MACs, 742.91 us, 1.22% latency, 66.33 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 88.93 us, 0.15% latency, 132.81 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 82.02 us, 0.13% latency, 144.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 80.11 us, 0.13% latency, 147.44 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 84.16 us, 0.14% latency, 140.34 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.06% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 47.61 GMACs, 2.02% MACs, 669.96 us, 1.10% latency, 142.15 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 197.89 us, 0.32% latency, 160.41 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 191.93 us, 0.31% latency, 165.39 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 169.99 us, 0.28% latency, 186.73 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 28.37 us, 0.05% latency, 136.57 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.31 us, 0.14% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 78.92 us, 0.13% latency, 0.0 FLOPS, )
      )
      (27): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 72.25 GMACs, 3.06% MACs, 1.69 ms, 2.77% latency, 85.6 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 24.64 GMACs, 1.04% MACs, 752.21 us, 1.23% latency, 65.51 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 88.69 us, 0.15% latency, 133.17 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 83.21 us, 0.14% latency, 141.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 79.87 us, 0.13% latency, 147.88 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 86.78 us, 0.14% latency, 136.1 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.33 us, 0.06% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 47.61 GMACs, 2.02% MACs, 672.34 us, 1.10% latency, 141.64 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 199.32 us, 0.33% latency, 159.26 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 192.88 us, 0.32% latency, 164.57 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 170.47 us, 0.28% latency, 186.21 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 28.85 us, 0.05% latency, 134.32 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 85.83 us, 0.14% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 83.92 us, 0.14% latency, 0.0 FLOPS, )
      )
      (28): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 72.25 GMACs, 3.06% MACs, 1.7 ms, 2.79% latency, 84.84 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 24.64 GMACs, 1.04% MACs, 749.83 us, 1.23% latency, 65.72 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 90.12 us, 0.15% latency, 131.06 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 82.25 us, 0.13% latency, 143.59 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 79.39 us, 0.13% latency, 148.77 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 85.12 us, 0.14% latency, 138.77 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.29 us, 0.06% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 47.61 GMACs, 2.02% MACs, 670.19 us, 1.10% latency, 142.1 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 199.08 us, 0.33% latency, 159.45 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 192.88 us, 0.32% latency, 164.57 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 170.23 us, 0.28% latency, 186.47 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 28.85 us, 0.05% latency, 134.32 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 105.86 us, 0.17% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 81.3 us, 0.13% latency, 0.0 FLOPS, )
      )
      (29): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 72.25 GMACs, 3.06% MACs, 1.67 ms, 2.74% latency, 86.34 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 24.64 GMACs, 1.04% MACs, 738.62 us, 1.21% latency, 66.72 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 88.21 us, 0.14% latency, 133.89 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 81.54 us, 0.13% latency, 144.85 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 80.11 us, 0.13% latency, 147.44 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 86.31 us, 0.14% latency, 136.85 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.9 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 47.61 GMACs, 2.02% MACs, 673.53 us, 1.10% latency, 141.39 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 198.36 us, 0.32% latency, 160.02 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 193.12 us, 0.32% latency, 164.37 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 171.9 us, 0.28% latency, 184.66 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 28.85 us, 0.05% latency, 134.32 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.78 us, 0.14% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 81.06 us, 0.13% latency, 0.0 FLOPS, )
      )
      (30): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 72.25 GMACs, 3.06% MACs, 1.7 ms, 2.79% latency, 84.8 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 24.64 GMACs, 1.04% MACs, 759.6 us, 1.24% latency, 64.87 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 91.31 us, 0.15% latency, 129.35 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 82.02 us, 0.13% latency, 144.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 79.63 us, 0.13% latency, 148.32 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 87.5 us, 0.14% latency, 134.99 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.29 us, 0.06% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 47.61 GMACs, 2.02% MACs, 673.06 us, 1.10% latency, 141.49 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 200.75 us, 0.33% latency, 158.12 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 192.4 us, 0.32% latency, 164.98 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 170.23 us, 0.28% latency, 186.47 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 29.56 us, 0.05% latency, 131.07 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.46 us, 0.15% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 81.78 us, 0.13% latency, 0.0 FLOPS, )
      )
      (31): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 72.25 GMACs, 3.06% MACs, 1.68 ms, 2.75% latency, 86.13 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 24.64 GMACs, 1.04% MACs, 745.53 us, 1.22% latency, 66.1 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 88.21 us, 0.14% latency, 133.89 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 81.06 us, 0.13% latency, 145.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 79.87 us, 0.13% latency, 147.88 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 5.91 GMACs, 0.25% MACs, 85.35 us, 0.14% latency, 138.38 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.06% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 47.61 GMACs, 2.02% MACs, 671.39 us, 1.10% latency, 141.84 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 198.84 us, 0.33% latency, 159.64 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 193.36 us, 0.32% latency, 164.16 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 15.87 GMACs, 0.67% MACs, 170.23 us, 0.28% latency, 186.47 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 28.61 us, 0.05% latency, 135.43 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.74 us, 0.14% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 79.15 us, 0.13% latency, 0.0 FLOPS, )
      )
    )
    (norm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.74 us, 0.15% latency, 0.0 FLOPS, )
  )
  (lm_head): Linear(131.08 M, 1.95% Params, 46.14 GMACs, 1.96% MACs, 1.56 ms, 2.56% latency, 59.0 TFLOPS, in_features=4096, out_features=32001, bias=False)
)
------------------------------------------------------------------------------

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 129:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

world size:                                                   4       
data parallel size:                                           4       
model parallel size:                                          1       
batch size per GPU:                                           1       
params per gpu:                                               6738.42 M
params of model = params per GPU * mp_size:                   6738.42 M
fwd MACs per GPU:                                             2671.14 GMACs
fwd flops per GPU:                                            5342.59 G
fwd flops of model = fwd flops per GPU * mp_size:             5342.59 G
fwd latency:                                                  72.31 ms
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:          73.88 TFLOPS
bwd latency:                                                  211.66 ms
bwd FLOPS per GPU = 2.0 * fwd flops per GPU / bwd latency:    50.48 TFLOPS
fwd+bwd FLOPS per GPU = 3.0 * fwd flops per GPU / (fwd+bwd latency):   56.44 TFLOPS
step latency:                                                 182.3 ms
iter latency:                                                 466.27 ms
FLOPS per GPU = 3.0 * fwd flops per GPU / iter latency:       34.37 TFLOPS
samples/second:                                               8.58    

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'LlamaForCausalLM': '6738.42 M'}
    MACs        - {'LlamaForCausalLM': '2671.14 GMACs'}
    fwd latency - {'LlamaForCausalLM': '72.22 ms'}
depth 1:
    params      - {'LlamaModel': '6607.35 M'}
    MACs        - {'LlamaModel': '2618.97 GMACs'}
    fwd latency - {'LlamaModel': '70.09 ms'}
depth 2:
    params      - {'ModuleList': '6476.27 M'}
    MACs        - {'ModuleList': '2618.97 GMACs'}
    fwd latency - {'ModuleList': '65.14 ms'}
depth 3:
    params      - {'LlamaDecoderLayer': '6476.27 M'}
    MACs        - {'LlamaDecoderLayer': '2618.97 GMACs'}
    fwd latency - {'LlamaDecoderLayer': '65.14 ms'}
depth 4:
    params      - {'LlamaMLP': '4328.52 M'}
    MACs        - {'LlamaMLP': '1722.75 GMACs'}
    fwd latency - {'LlamaAttention': '30.66 ms'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

LlamaForCausalLM(
  6738.42 M, 100.00% Params, 2671.14 GMACs, 100.00% MACs, 72.22 ms, 100.00% latency, 73.98 TFLOPS, 
  (model): LlamaModel(
    6607.35 M, 98.05% Params, 2618.97 GMACs, 98.05% MACs, 70.09 ms, 97.05% latency, 74.74 TFLOPS, 
    (embed_tokens): Embedding(131.08 M, 1.95% Params, 0 MACs, 0.00% MACs, 65.33 us, 0.09% latency, 0.0 FLOPS, 32001, 4096)
    (layers): ModuleList(
      (0): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 81.84 GMACs, 3.06% MACs, 2.28 ms, 3.15% latency, 71.87 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.01 GMACs, 1.05% MACs, 1.04 ms, 1.45% latency, 53.64 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 163.08 us, 0.23% latency, 81.89 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 120.16 us, 0.17% latency, 111.14 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 115.87 us, 0.16% latency, 115.25 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 125.17 us, 0.17% latency, 106.69 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 38.86 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 53.84 GMACs, 2.02% MACs, 816.58 us, 1.13% latency, 131.86 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 238.42 us, 0.33% latency, 150.54 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 232.93 us, 0.32% latency, 154.08 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 226.26 us, 0.31% latency, 158.63 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 32.42 us, 0.04% latency, 135.12 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 212.19 us, 0.29% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 93.94 us, 0.13% latency, 0.0 FLOPS, )
      )
      (1): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 81.84 GMACs, 3.06% MACs, 2.06 ms, 2.86% latency, 79.28 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.01 GMACs, 1.05% MACs, 964.4 us, 1.34% latency, 58.09 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 125.65 us, 0.17% latency, 106.29 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 115.63 us, 0.16% latency, 115.49 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 113.01 us, 0.16% latency, 118.17 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 124.93 us, 0.17% latency, 106.9 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.29 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 53.84 GMACs, 2.02% MACs, 815.39 us, 1.13% latency, 132.05 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 243.19 us, 0.34% latency, 147.58 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 232.7 us, 0.32% latency, 154.24 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 224.83 us, 0.31% latency, 159.64 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.23 us, 0.04% latency, 140.27 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 95.61 us, 0.13% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.69 us, 0.12% latency, 0.0 FLOPS, )
      )
      (2): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 81.84 GMACs, 3.06% MACs, 2.04 ms, 2.82% latency, 80.28 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.01 GMACs, 1.05% MACs, 963.21 us, 1.33% latency, 58.16 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 124.93 us, 0.17% latency, 106.9 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 116.83 us, 0.16% latency, 114.31 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 114.68 us, 0.16% latency, 116.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 121.36 us, 0.17% latency, 110.05 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 36.95 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 53.84 GMACs, 2.02% MACs, 804.9 us, 1.11% latency, 133.78 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 236.03 us, 0.33% latency, 152.06 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 231.5 us, 0.32% latency, 155.03 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 223.88 us, 0.31% latency, 160.32 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.76 us, 0.04% latency, 142.45 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.79 us, 0.13% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 84.4 us, 0.12% latency, 0.0 FLOPS, )
      )
      (3): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 81.84 GMACs, 3.06% MACs, 2.04 ms, 2.83% latency, 80.19 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.01 GMACs, 1.05% MACs, 950.34 us, 1.32% latency, 58.95 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 123.5 us, 0.17% latency, 108.13 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 115.63 us, 0.16% latency, 115.49 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 114.2 us, 0.16% latency, 116.94 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 120.4 us, 0.17% latency, 110.92 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 53.84 GMACs, 2.02% MACs, 807.29 us, 1.12% latency, 133.38 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 236.51 us, 0.33% latency, 151.75 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 232.22 us, 0.32% latency, 154.55 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 223.88 us, 0.31% latency, 160.32 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.99 us, 0.04% latency, 141.35 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.03 us, 0.13% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 83.92 us, 0.12% latency, 0.0 FLOPS, )
      )
      (4): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 81.84 GMACs, 3.06% MACs, 2.02 ms, 2.80% latency, 80.92 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.01 GMACs, 1.05% MACs, 951.53 us, 1.32% latency, 58.87 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 123.26 us, 0.17% latency, 108.34 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 114.92 us, 0.16% latency, 116.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 114.2 us, 0.16% latency, 116.94 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 123.26 us, 0.17% latency, 108.34 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 53.84 GMACs, 2.02% MACs, 802.28 us, 1.11% latency, 134.21 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 235.08 us, 0.33% latency, 152.67 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 231.5 us, 0.32% latency, 155.03 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 223.4 us, 0.31% latency, 160.66 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.52 us, 0.04% latency, 143.56 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.69 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.07 us, 0.12% latency, 0.0 FLOPS, )
      )
      (5): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 81.84 GMACs, 3.06% MACs, 2.03 ms, 2.81% latency, 80.77 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.01 GMACs, 1.05% MACs, 950.81 us, 1.32% latency, 58.92 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 123.98 us, 0.17% latency, 107.72 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 115.63 us, 0.16% latency, 115.49 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 113.49 us, 0.16% latency, 117.68 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 121.12 us, 0.17% latency, 110.26 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 53.84 GMACs, 2.02% MACs, 802.99 us, 1.11% latency, 134.09 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 235.8 us, 0.33% latency, 152.21 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 230.79 us, 0.32% latency, 155.51 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 224.35 us, 0.31% latency, 159.97 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.99 us, 0.04% latency, 141.35 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.08 us, 0.13% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 84.64 us, 0.12% latency, 0.0 FLOPS, )
      )
      (6): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 81.84 GMACs, 3.06% MACs, 2.01 ms, 2.79% latency, 81.33 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.01 GMACs, 1.05% MACs, 949.38 us, 1.31% latency, 59.01 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 123.26 us, 0.17% latency, 108.34 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 116.11 us, 0.16% latency, 115.02 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 114.92 us, 0.16% latency, 116.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 120.88 us, 0.17% latency, 110.48 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.52 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 53.84 GMACs, 2.02% MACs, 800.61 us, 1.11% latency, 134.49 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 235.08 us, 0.33% latency, 152.67 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 230.79 us, 0.32% latency, 155.51 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 222.92 us, 0.31% latency, 161.0 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.28 us, 0.04% latency, 144.69 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.78 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 82.49 us, 0.11% latency, 0.0 FLOPS, )
      )
      (7): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 81.84 GMACs, 3.06% MACs, 2.03 ms, 2.81% latency, 80.55 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.01 GMACs, 1.05% MACs, 957.25 us, 1.33% latency, 58.52 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 123.5 us, 0.17% latency, 108.13 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 115.39 us, 0.16% latency, 115.73 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 114.68 us, 0.16% latency, 116.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 123.74 us, 0.17% latency, 107.93 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 53.84 GMACs, 2.02% MACs, 806.09 us, 1.12% latency, 133.58 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 236.03 us, 0.33% latency, 152.06 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 231.74 us, 0.32% latency, 154.87 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 224.11 us, 0.31% latency, 160.15 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.76 us, 0.04% latency, 142.45 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.5 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 85.12 us, 0.12% latency, 0.0 FLOPS, )
      )
      (8): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 81.84 GMACs, 3.06% MACs, 2.04 ms, 2.82% latency, 80.29 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.01 GMACs, 1.05% MACs, 960.59 us, 1.33% latency, 58.32 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 124.22 us, 0.17% latency, 107.51 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 115.39 us, 0.16% latency, 115.73 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 114.68 us, 0.16% latency, 116.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 121.59 us, 0.17% latency, 109.83 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.52 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 53.84 GMACs, 2.02% MACs, 807.29 us, 1.12% latency, 133.38 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 236.03 us, 0.33% latency, 152.06 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 231.98 us, 0.32% latency, 154.71 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 223.4 us, 0.31% latency, 160.66 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 31.47 us, 0.04% latency, 139.21 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.65 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 83.68 us, 0.12% latency, 0.0 FLOPS, )
      )
      (9): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 81.84 GMACs, 3.06% MACs, 2.03 ms, 2.82% latency, 80.45 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.01 GMACs, 1.05% MACs, 962.97 us, 1.33% latency, 58.17 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 125.17 us, 0.17% latency, 106.69 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 116.59 us, 0.16% latency, 114.55 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 113.96 us, 0.16% latency, 117.18 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 121.59 us, 0.17% latency, 109.83 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.05 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 53.84 GMACs, 2.02% MACs, 805.85 us, 1.12% latency, 133.62 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 235.32 us, 0.33% latency, 152.52 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 231.74 us, 0.32% latency, 154.87 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 224.35 us, 0.31% latency, 159.97 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.28 us, 0.04% latency, 144.69 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.26 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 82.97 us, 0.11% latency, 0.0 FLOPS, )
      )
      (10): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 81.84 GMACs, 3.06% MACs, 2.05 ms, 2.84% latency, 79.71 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.01 GMACs, 1.05% MACs, 970.6 us, 1.34% latency, 57.72 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 128.27 us, 0.18% latency, 104.11 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 115.87 us, 0.16% latency, 115.25 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 113.49 us, 0.16% latency, 117.68 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 123.02 us, 0.17% latency, 108.55 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.05 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 53.84 GMACs, 2.02% MACs, 805.62 us, 1.12% latency, 133.66 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 236.03 us, 0.33% latency, 152.06 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 230.55 us, 0.32% latency, 155.67 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 224.83 us, 0.31% latency, 159.64 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.52 us, 0.04% latency, 143.56 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 94.41 us, 0.13% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 84.64 us, 0.12% latency, 0.0 FLOPS, )
      )
      (11): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 81.84 GMACs, 3.06% MACs, 2.02 ms, 2.79% latency, 81.17 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.01 GMACs, 1.05% MACs, 951.53 us, 1.32% latency, 58.87 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 124.93 us, 0.17% latency, 106.9 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 115.16 us, 0.16% latency, 115.97 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 114.2 us, 0.16% latency, 116.94 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 120.64 us, 0.17% latency, 110.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 53.84 GMACs, 2.02% MACs, 800.13 us, 1.11% latency, 134.57 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 234.13 us, 0.32% latency, 153.3 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 231.74 us, 0.32% latency, 154.87 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 224.11 us, 0.31% latency, 160.15 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.04 us, 0.04% latency, 145.84 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.93 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 82.49 us, 0.11% latency, 0.0 FLOPS, )
      )
      (12): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 81.84 GMACs, 3.06% MACs, 2.01 ms, 2.79% latency, 81.32 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.01 GMACs, 1.05% MACs, 947.24 us, 1.31% latency, 59.14 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 123.74 us, 0.17% latency, 107.93 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 115.63 us, 0.16% latency, 115.49 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 113.25 us, 0.16% latency, 117.92 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 121.12 us, 0.17% latency, 110.26 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 53.84 GMACs, 2.02% MACs, 802.04 us, 1.11% latency, 134.25 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 234.84 us, 0.33% latency, 152.83 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 231.27 us, 0.32% latency, 155.19 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 223.64 us, 0.31% latency, 160.49 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.28 us, 0.04% latency, 144.69 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.98 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 82.25 us, 0.11% latency, 0.0 FLOPS, )
      )
      (13): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 81.84 GMACs, 3.06% MACs, 2.04 ms, 2.83% latency, 80.22 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.01 GMACs, 1.05% MACs, 962.5 us, 1.33% latency, 58.2 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 123.26 us, 0.17% latency, 108.34 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 115.63 us, 0.16% latency, 115.49 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 113.49 us, 0.16% latency, 117.68 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 122.07 us, 0.17% latency, 109.4 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 53.84 GMACs, 2.02% MACs, 804.19 us, 1.11% latency, 133.89 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 234.84 us, 0.33% latency, 152.83 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 231.5 us, 0.32% latency, 155.03 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 223.4 us, 0.31% latency, 160.66 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.76 us, 0.04% latency, 142.45 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.74 us, 0.13% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 84.88 us, 0.12% latency, 0.0 FLOPS, )
      )
      (14): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 81.84 GMACs, 3.06% MACs, 2.03 ms, 2.81% latency, 80.69 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.01 GMACs, 1.05% MACs, 953.67 us, 1.32% latency, 58.74 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 124.22 us, 0.17% latency, 107.51 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 115.63 us, 0.16% latency, 115.49 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 113.25 us, 0.16% latency, 117.92 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 122.07 us, 0.17% latency, 109.4 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 53.84 GMACs, 2.02% MACs, 802.28 us, 1.11% latency, 134.21 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 235.32 us, 0.33% latency, 152.52 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 231.74 us, 0.32% latency, 154.87 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 222.92 us, 0.31% latency, 161.0 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.52 us, 0.04% latency, 143.56 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.5 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 85.59 us, 0.12% latency, 0.0 FLOPS, )
      )
      (15): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 81.84 GMACs, 3.06% MACs, 2.03 ms, 2.82% latency, 80.44 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.01 GMACs, 1.05% MACs, 961.3 us, 1.33% latency, 58.27 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 123.98 us, 0.17% latency, 107.72 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 115.87 us, 0.16% latency, 115.25 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 114.2 us, 0.16% latency, 116.94 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 121.59 us, 0.17% latency, 109.83 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.52 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 53.84 GMACs, 2.02% MACs, 803.47 us, 1.11% latency, 134.01 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 234.6 us, 0.32% latency, 152.98 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 231.98 us, 0.32% latency, 154.71 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 223.4 us, 0.31% latency, 160.66 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.52 us, 0.04% latency, 143.56 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 89.41 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 84.16 us, 0.12% latency, 0.0 FLOPS, )
      )
      (16): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 81.84 GMACs, 3.06% MACs, 2.01 ms, 2.79% latency, 81.36 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.01 GMACs, 1.05% MACs, 948.43 us, 1.31% latency, 59.07 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 123.5 us, 0.17% latency, 108.13 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 114.68 us, 0.16% latency, 116.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 112.77 us, 0.16% latency, 118.42 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 122.07 us, 0.17% latency, 109.4 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 53.84 GMACs, 2.02% MACs, 799.42 us, 1.11% latency, 134.69 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 234.6 us, 0.32% latency, 152.98 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 231.27 us, 0.32% latency, 155.19 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 222.68 us, 0.31% latency, 161.17 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.04 us, 0.04% latency, 145.84 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.21 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 82.49 us, 0.11% latency, 0.0 FLOPS, )
      )
      (17): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 81.84 GMACs, 3.06% MACs, 2.03 ms, 2.81% latency, 80.61 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.01 GMACs, 1.05% MACs, 951.77 us, 1.32% latency, 58.86 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 124.22 us, 0.17% latency, 107.51 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 116.35 us, 0.16% latency, 114.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 113.25 us, 0.16% latency, 117.92 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 120.4 us, 0.17% latency, 110.92 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 53.84 GMACs, 2.02% MACs, 805.14 us, 1.11% latency, 133.74 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 236.51 us, 0.33% latency, 151.75 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 231.5 us, 0.32% latency, 155.03 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 222.92 us, 0.31% latency, 161.0 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.52 us, 0.04% latency, 143.56 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.74 us, 0.13% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 83.68 us, 0.12% latency, 0.0 FLOPS, )
      )
      (18): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 81.84 GMACs, 3.06% MACs, 2.03 ms, 2.81% latency, 80.56 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.01 GMACs, 1.05% MACs, 965.12 us, 1.34% latency, 58.04 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 124.69 us, 0.17% latency, 107.1 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 114.68 us, 0.16% latency, 116.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 113.25 us, 0.16% latency, 117.92 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 122.31 us, 0.17% latency, 109.19 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 35.05 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 53.84 GMACs, 2.02% MACs, 801.56 us, 1.11% latency, 134.33 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 234.84 us, 0.33% latency, 152.83 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 231.27 us, 0.32% latency, 155.19 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 224.35 us, 0.31% latency, 159.97 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.52 us, 0.04% latency, 143.56 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.26 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 82.97 us, 0.11% latency, 0.0 FLOPS, )
      )
      (19): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 81.84 GMACs, 3.06% MACs, 2.01 ms, 2.78% latency, 81.52 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.01 GMACs, 1.05% MACs, 948.43 us, 1.31% latency, 59.07 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 122.79 us, 0.17% latency, 108.76 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 114.44 us, 0.16% latency, 116.69 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 112.53 us, 0.16% latency, 118.67 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 119.69 us, 0.17% latency, 111.58 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.81 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 53.84 GMACs, 2.02% MACs, 798.7 us, 1.11% latency, 134.81 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 234.6 us, 0.32% latency, 152.98 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 230.07 us, 0.32% latency, 156.0 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 223.16 us, 0.31% latency, 160.83 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 29.56 us, 0.04% latency, 148.19 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.78 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 81.78 us, 0.11% latency, 0.0 FLOPS, )
      )
      (20): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 81.84 GMACs, 3.06% MACs, 2.02 ms, 2.80% latency, 80.9 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.01 GMACs, 1.05% MACs, 957.97 us, 1.33% latency, 58.48 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 122.31 us, 0.17% latency, 109.19 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 114.68 us, 0.16% latency, 116.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 113.49 us, 0.16% latency, 117.68 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 124.22 us, 0.17% latency, 107.51 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 53.84 GMACs, 2.02% MACs, 801.32 us, 1.11% latency, 134.37 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 235.08 us, 0.33% latency, 152.67 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 231.27 us, 0.32% latency, 155.19 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 223.64 us, 0.31% latency, 160.49 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.28 us, 0.04% latency, 144.69 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.07 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 85.59 us, 0.12% latency, 0.0 FLOPS, )
      )
      (21): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 81.84 GMACs, 3.06% MACs, 2.02 ms, 2.80% latency, 80.97 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.01 GMACs, 1.05% MACs, 952.96 us, 1.32% latency, 58.78 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 124.69 us, 0.17% latency, 107.1 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 114.68 us, 0.16% latency, 116.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 113.25 us, 0.16% latency, 117.92 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 121.36 us, 0.17% latency, 110.05 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.09 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 53.84 GMACs, 2.02% MACs, 802.04 us, 1.11% latency, 134.25 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 235.08 us, 0.33% latency, 152.67 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 230.55 us, 0.32% latency, 155.67 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 222.44 us, 0.31% latency, 161.35 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.04 us, 0.04% latency, 145.84 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.93 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 85.12 us, 0.12% latency, 0.0 FLOPS, )
      )
      (22): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 81.84 GMACs, 3.06% MACs, 2.02 ms, 2.80% latency, 80.93 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.01 GMACs, 1.05% MACs, 947.95 us, 1.31% latency, 59.09 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 123.5 us, 0.17% latency, 108.13 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 115.16 us, 0.16% latency, 115.97 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 113.49 us, 0.16% latency, 117.68 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 120.88 us, 0.17% latency, 110.48 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 53.84 GMACs, 2.02% MACs, 800.13 us, 1.11% latency, 134.57 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 234.84 us, 0.33% latency, 152.83 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 231.03 us, 0.32% latency, 155.35 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 223.4 us, 0.31% latency, 160.66 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 29.8 us, 0.04% latency, 147.01 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 95.37 us, 0.13% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 83.92 us, 0.12% latency, 0.0 FLOPS, )
      )
      (23): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 81.84 GMACs, 3.06% MACs, 2.03 ms, 2.80% latency, 80.83 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.01 GMACs, 1.05% MACs, 956.77 us, 1.32% latency, 58.55 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 123.5 us, 0.17% latency, 108.13 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 115.63 us, 0.16% latency, 115.49 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 113.96 us, 0.16% latency, 117.18 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 122.55 us, 0.17% latency, 108.98 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 53.84 GMACs, 2.02% MACs, 799.66 us, 1.11% latency, 134.65 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 235.56 us, 0.33% latency, 152.36 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 230.79 us, 0.32% latency, 155.51 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 222.68 us, 0.31% latency, 161.17 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 29.56 us, 0.04% latency, 148.19 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.08 us, 0.13% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 83.68 us, 0.12% latency, 0.0 FLOPS, )
      )
      (24): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 81.84 GMACs, 3.06% MACs, 2.02 ms, 2.79% latency, 81.2 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.01 GMACs, 1.05% MACs, 948.91 us, 1.31% latency, 59.04 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 123.98 us, 0.17% latency, 107.72 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 114.2 us, 0.16% latency, 116.94 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 112.53 us, 0.16% latency, 118.67 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 121.12 us, 0.17% latency, 110.26 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 53.84 GMACs, 2.02% MACs, 802.52 us, 1.11% latency, 134.17 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 235.8 us, 0.33% latency, 152.21 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 230.55 us, 0.32% latency, 155.67 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 223.4 us, 0.31% latency, 160.66 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 29.56 us, 0.04% latency, 148.19 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.45 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 82.73 us, 0.11% latency, 0.0 FLOPS, )
      )
      (25): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 81.84 GMACs, 3.06% MACs, 2.0 ms, 2.77% latency, 81.69 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.01 GMACs, 1.05% MACs, 945.81 us, 1.31% latency, 59.23 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 123.26 us, 0.17% latency, 108.34 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 114.44 us, 0.16% latency, 116.69 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 113.25 us, 0.16% latency, 117.92 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 121.12 us, 0.17% latency, 110.26 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.38 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 53.84 GMACs, 2.02% MACs, 796.79 us, 1.10% latency, 135.14 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 233.17 us, 0.32% latency, 153.92 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 230.07 us, 0.32% latency, 156.0 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 223.4 us, 0.31% latency, 160.66 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 29.8 us, 0.04% latency, 147.01 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 85.83 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 82.73 us, 0.11% latency, 0.0 FLOPS, )
      )
      (26): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 81.84 GMACs, 3.06% MACs, 2.04 ms, 2.83% latency, 80.12 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.01 GMACs, 1.05% MACs, 962.73 us, 1.33% latency, 58.19 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 128.27 us, 0.18% latency, 104.11 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 115.16 us, 0.16% latency, 115.97 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 113.73 us, 0.16% latency, 117.43 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 122.55 us, 0.17% latency, 108.98 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.57 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 53.84 GMACs, 2.02% MACs, 805.38 us, 1.12% latency, 133.7 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 235.56 us, 0.33% latency, 152.36 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 231.5 us, 0.32% latency, 155.03 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 224.59 us, 0.31% latency, 159.81 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.52 us, 0.04% latency, 143.56 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 92.98 us, 0.13% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 84.4 us, 0.12% latency, 0.0 FLOPS, )
      )
      (27): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 81.84 GMACs, 3.06% MACs, 2.01 ms, 2.79% latency, 81.24 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.01 GMACs, 1.05% MACs, 947.48 us, 1.31% latency, 59.12 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 123.26 us, 0.17% latency, 108.34 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 115.39 us, 0.16% latency, 115.73 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 113.49 us, 0.16% latency, 117.68 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 121.59 us, 0.17% latency, 109.83 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 53.84 GMACs, 2.02% MACs, 802.52 us, 1.11% latency, 134.17 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 235.32 us, 0.33% latency, 152.52 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 231.27 us, 0.32% latency, 155.19 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 222.68 us, 0.31% latency, 161.17 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.76 us, 0.04% latency, 142.45 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 86.78 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 82.97 us, 0.11% latency, 0.0 FLOPS, )
      )
      (28): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 81.84 GMACs, 3.06% MACs, 2.04 ms, 2.82% latency, 80.37 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.01 GMACs, 1.05% MACs, 961.07 us, 1.33% latency, 58.29 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 126.12 us, 0.17% latency, 105.89 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 114.92 us, 0.16% latency, 116.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 114.44 us, 0.16% latency, 116.69 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 123.5 us, 0.17% latency, 108.13 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.33 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 53.84 GMACs, 2.02% MACs, 804.42 us, 1.11% latency, 133.86 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 235.32 us, 0.33% latency, 152.52 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 231.74 us, 0.32% latency, 154.87 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 223.16 us, 0.31% latency, 160.83 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.52 us, 0.04% latency, 143.56 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 91.79 us, 0.13% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 84.16 us, 0.12% latency, 0.0 FLOPS, )
      )
      (29): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 81.84 GMACs, 3.06% MACs, 2.02 ms, 2.80% latency, 80.9 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.01 GMACs, 1.05% MACs, 949.14 us, 1.31% latency, 59.02 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 123.26 us, 0.17% latency, 108.34 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 114.68 us, 0.16% latency, 116.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 113.73 us, 0.16% latency, 117.43 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 120.88 us, 0.17% latency, 110.48 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.86 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 53.84 GMACs, 2.02% MACs, 801.32 us, 1.11% latency, 134.37 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 234.84 us, 0.33% latency, 152.83 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 231.5 us, 0.32% latency, 155.03 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 223.88 us, 0.31% latency, 160.32 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.28 us, 0.04% latency, 144.69 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 88.21 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.98 us, 0.12% latency, 0.0 FLOPS, )
      )
      (30): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 81.84 GMACs, 3.06% MACs, 2.02 ms, 2.79% latency, 81.17 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.01 GMACs, 1.05% MACs, 950.57 us, 1.32% latency, 58.93 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 124.45 us, 0.17% latency, 107.31 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 113.96 us, 0.16% latency, 117.18 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 112.53 us, 0.16% latency, 118.67 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 122.31 us, 0.17% latency, 109.19 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 33.14 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 53.84 GMACs, 2.02% MACs, 800.13 us, 1.11% latency, 134.57 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 235.32 us, 0.33% latency, 152.52 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 231.27 us, 0.32% latency, 155.19 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 222.44 us, 0.31% latency, 161.35 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.52 us, 0.04% latency, 143.56 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.98 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 83.68 us, 0.12% latency, 0.0 FLOPS, )
      )
      (31): LlamaDecoderLayer(
        202.38 M, 3.00% Params, 81.84 GMACs, 3.06% MACs, 2.03 ms, 2.81% latency, 80.74 TFLOPS, 
        (self_attn): LlamaAttention(
          67.11 M, 1.00% Params, 28.01 GMACs, 1.05% MACs, 962.26 us, 1.33% latency, 58.22 TFLOPS, 
          (q_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 123.98 us, 0.17% latency, 107.72 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 114.68 us, 0.16% latency, 116.45 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 114.92 us, 0.16% latency, 116.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(16.78 M, 0.25% Params, 6.68 GMACs, 0.25% MACs, 120.88 us, 0.17% latency, 110.48 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 34.81 us, 0.05% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          135.27 M, 2.01% Params, 53.84 GMACs, 2.02% MACs, 799.42 us, 1.11% latency, 134.69 TFLOPS, 
          (gate_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 234.37 us, 0.32% latency, 153.14 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 230.55 us, 0.32% latency, 155.67 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(45.09 M, 0.67% Params, 17.95 GMACs, 0.67% MACs, 222.68 us, 0.31% latency, 161.17 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 30.76 us, 0.04% latency, 142.45 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 87.98 us, 0.12% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 83.45 us, 0.12% latency, 0.0 FLOPS, )
      )
    )
    (norm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 96.32 us, 0.13% latency, 0.0 FLOPS, )
  )
  (lm_head): Linear(131.08 M, 1.95% Params, 52.17 GMACs, 1.95% MACs, 1.85 ms, 2.56% latency, 56.33 TFLOPS, in_features=4096, out_features=32001, bias=False)
)
------------------------------------------------------------------------------
  6%|▋         | 129/2048 [19:22<96:26:18, 180.92s/it]  6%|▋         | 130/2048 [19:27<68:14:21, 128.08s/it]  6%|▋         | 131/2048 [19:31<48:28:19, 91.03s/it]   6%|▋         | 132/2048 [19:36<34:38:11, 65.08s/it]  6%|▋         | 133/2048 [19:40<24:53:53, 46.81s/it]  7%|▋         | 134/2048 [19:44<18:05:31, 34.03s/it]  7%|▋         | 135/2048 [19:48<13:13:22, 24.88s/it]  7%|▋         | 136/2048 [19:51<9:48:35, 18.47s/it]   7%|▋         | 137/2048 [19:55<7:22:46, 13.90s/it]  7%|▋         | 138/2048 [19:59<5:48:52, 10.96s/it]  7%|▋         | 139/2048 [20:04<4:53:27,  9.22s/it]  7%|▋         | 140/2048 [20:09<4:14:34,  8.01s/it]  7%|▋         | 141/2048 [20:14<3:47:12,  7.15s/it]  7%|▋         | 142/2048 [20:19<3:26:28,  6.50s/it]  7%|▋         | 143/2048 [20:24<3:10:19,  5.99s/it]  7%|▋         | 144/2048 [20:29<2:58:45,  5.63s/it]                                                    {'loss': 1.4469, 'learning_rate': 0.0001991599020126803, 'epoch': 1.04}
  7%|▋         | 144/2048 [20:29<2:58:45,  5.63s/it]  7%|▋         | 145/2048 [20:33<2:46:59,  5.27s/it]  7%|▋         | 146/2048 [20:37<2:33:48,  4.85s/it]  7%|▋         | 147/2048 [20:41<2:28:01,  4.67s/it]  7%|▋         | 148/2048 [20:45<2:23:51,  4.54s/it]  7%|▋         | 149/2048 [20:49<2:18:19,  4.37s/it]  7%|▋         | 150/2048 [20:53<2:06:56,  4.01s/it]  7%|▋         | 151/2048 [20:57<2:09:59,  4.11s/it]  7%|▋         | 152/2048 [21:02<2:19:58,  4.43s/it]  7%|▋         | 153/2048 [21:07<2:26:46,  4.65s/it]  8%|▊         | 154/2048 [21:12<2:31:19,  4.79s/it]  8%|▊         | 155/2048 [21:17<2:32:25,  4.83s/it]  8%|▊         | 156/2048 [21:22<2:30:26,  4.77s/it]  8%|▊         | 157/2048 [21:26<2:27:05,  4.67s/it]  8%|▊         | 158/2048 [21:31<2:22:18,  4.52s/it]  8%|▊         | 159/2048 [21:34<2:15:11,  4.29s/it]  8%|▊         | 160/2048 [21:38<2:11:26,  4.18s/it]                                                    {'loss': 1.2346, 'learning_rate': 0.00019880079485128365, 'epoch': 1.16}
  8%|▊         | 160/2048 [21:38<2:11:26,  4.18s/it]  8%|▊         | 161/2048 [21:42<2:10:22,  4.15s/it]  8%|▊         | 162/2048 [21:46<2:06:47,  4.03s/it]  8%|▊         | 163/2048 [21:49<2:00:42,  3.84s/it]  8%|▊         | 164/2048 [21:55<2:13:11,  4.24s/it]  8%|▊         | 165/2048 [22:00<2:21:51,  4.52s/it]  8%|▊         | 166/2048 [22:05<2:27:40,  4.71s/it]  8%|▊         | 167/2048 [22:10<2:31:11,  4.82s/it]  8%|▊         | 168/2048 [22:15<2:31:32,  4.84s/it]  8%|▊         | 169/2048 [22:20<2:29:51,  4.79s/it]  8%|▊         | 170/2048 [22:24<2:25:32,  4.65s/it]  8%|▊         | 171/2048 [22:28<2:19:34,  4.46s/it]  8%|▊         | 172/2048 [22:32<2:15:17,  4.33s/it]  8%|▊         | 173/2048 [22:36<2:09:00,  4.13s/it]  8%|▊         | 174/2048 [22:40<2:07:39,  4.09s/it]  9%|▊         | 175/2048 [22:43<2:00:39,  3.86s/it]  9%|▊         | 176/2048 [22:47<2:05:03,  4.01s/it]                                                    {'loss': 1.2306, 'learning_rate': 0.00019837840011169438, 'epoch': 1.28}
  9%|▊         | 176/2048 [22:47<2:05:03,  4.01s/it]  9%|▊         | 177/2048 [22:52<2:15:52,  4.36s/it]  9%|▊         | 178/2048 [22:58<2:24:14,  4.63s/it]  9%|▊         | 179/2048 [23:03<2:28:54,  4.78s/it]  9%|▉         | 180/2048 [23:08<2:29:19,  4.80s/it]  9%|▉         | 181/2048 [23:12<2:28:32,  4.77s/it]  9%|▉         | 182/2048 [23:17<2:24:49,  4.66s/it]  9%|▉         | 183/2048 [23:21<2:23:01,  4.60s/it]  9%|▉         | 184/2048 [23:25<2:17:38,  4.43s/it]  9%|▉         | 185/2048 [23:29<2:11:49,  4.25s/it]  9%|▉         | 186/2048 [23:33<2:09:00,  4.16s/it]  9%|▉         | 187/2048 [23:37<2:06:06,  4.07s/it]  9%|▉         | 188/2048 [23:40<1:59:16,  3.85s/it]  9%|▉         | 189/2048 [23:45<2:11:30,  4.24s/it]  9%|▉         | 190/2048 [23:51<2:20:03,  4.52s/it]  9%|▉         | 191/2048 [23:56<2:25:56,  4.72s/it]  9%|▉         | 192/2048 [24:01<2:28:54,  4.81s/it]                                                    {'loss': 1.1626, 'learning_rate': 0.00019789298836197982, 'epoch': 1.39}
  9%|▉         | 192/2048 [24:01<2:28:54,  4.81s/it]  9%|▉         | 193/2048 [24:06<2:28:53,  4.82s/it]  9%|▉         | 194/2048 [24:10<2:26:10,  4.73s/it] 10%|▉         | 195/2048 [24:15<2:23:01,  4.63s/it] 10%|▉         | 196/2048 [24:19<2:17:29,  4.45s/it] 10%|▉         | 197/2048 [24:22<2:10:40,  4.24s/it] 10%|▉         | 198/2048 [24:26<2:06:03,  4.09s/it] 10%|▉         | 199/2048 [24:30<2:03:41,  4.01s/it] 10%|▉         | 200/2048 [24:34<1:59:58,  3.90s/it] 10%|▉         | 201/2048 [24:38<2:03:50,  4.02s/it] 10%|▉         | 202/2048 [24:43<2:14:22,  4.37s/it] 10%|▉         | 203/2048 [24:48<2:21:38,  4.61s/it] 10%|▉         | 204/2048 [24:53<2:26:31,  4.77s/it] 10%|█         | 205/2048 [24:58<2:28:45,  4.84s/it] 10%|█         | 206/2048 [25:03<2:28:14,  4.83s/it] 10%|█         | 207/2048 [25:08<2:25:35,  4.74s/it] 10%|█         | 208/2048 [25:12<2:21:08,  4.60s/it]                                                    {'loss': 1.2015, 'learning_rate': 0.00019734487053621816, 'epoch': 1.51}
 10%|█         | 208/2048 [25:12<2:21:08,  4.60s/it] 10%|█         | 209/2048 [25:16<2:15:09,  4.41s/it] 10%|█         | 210/2048 [25:20<2:10:05,  4.25s/it] 10%|█         | 211/2048 [25:23<2:04:14,  4.06s/it] 10%|█         | 212/2048 [25:27<1:58:26,  3.87s/it] 10%|█         | 213/2048 [25:30<1:53:26,  3.71s/it] 10%|█         | 214/2048 [25:35<2:06:48,  4.15s/it] 10%|█         | 215/2048 [25:41<2:16:05,  4.45s/it] 11%|█         | 216/2048 [25:46<2:22:24,  4.66s/it] 11%|█         | 217/2048 [25:51<2:26:03,  4.79s/it] 11%|█         | 218/2048 [25:56<2:26:53,  4.82s/it] 11%|█         | 219/2048 [26:00<2:25:16,  4.77s/it] 11%|█         | 220/2048 [26:05<2:21:28,  4.64s/it] 11%|█         | 221/2048 [26:09<2:17:21,  4.51s/it] 11%|█         | 222/2048 [26:13<2:11:52,  4.33s/it] 11%|█         | 223/2048 [26:17<2:08:10,  4.21s/it] 11%|█         | 224/2048 [26:20<2:03:52,  4.08s/it]                                                    {'loss': 1.2854, 'learning_rate': 0.00019673439773532713, 'epoch': 1.62}
 11%|█         | 224/2048 [26:20<2:03:52,  4.08s/it] 11%|█         | 225/2048 [26:24<1:57:02,  3.85s/it] 11%|█         | 226/2048 [26:28<2:01:36,  4.00s/it] 11%|█         | 227/2048 [26:33<2:12:08,  4.35s/it] 11%|█         | 228/2048 [26:38<2:19:20,  4.59s/it] 11%|█         | 229/2048 [26:44<2:24:07,  4.75s/it] 11%|█         | 230/2048 [26:49<2:26:20,  4.83s/it] 11%|█▏        | 231/2048 [26:53<2:25:45,  4.81s/it] 11%|█▏        | 232/2048 [26:58<2:22:36,  4.71s/it] 11%|█▏        | 233/2048 [27:02<2:18:35,  4.58s/it] 11%|█▏        | 234/2048 [27:06<2:13:04,  4.40s/it] 11%|█▏        | 235/2048 [27:10<2:07:56,  4.23s/it] 12%|█▏        | 236/2048 [27:14<2:05:05,  4.14s/it] 12%|█▏        | 237/2048 [27:18<2:01:24,  4.02s/it] 12%|█▏        | 238/2048 [27:21<1:55:31,  3.83s/it] 12%|█▏        | 239/2048 [27:26<2:07:36,  4.23s/it] 12%|█▏        | 240/2048 [27:31<2:16:00,  4.51s/it]                                                    {'loss': 1.2159, 'learning_rate': 0.0001960619610021641, 'epoch': 1.74}
 12%|█▏        | 240/2048 [27:31<2:16:00,  4.51s/it] 12%|█▏        | 241/2048 [27:37<2:21:43,  4.71s/it] 12%|█▏        | 242/2048 [27:42<2:25:27,  4.83s/it] 12%|█▏        | 243/2048 [27:47<2:25:47,  4.85s/it] 12%|█▏        | 244/2048 [27:51<2:23:07,  4.76s/it] 12%|█▏        | 245/2048 [27:55<2:19:49,  4.65s/it] 12%|█▏        | 246/2048 [27:59<2:13:38,  4.45s/it] 12%|█▏        | 247/2048 [28:04<2:11:01,  4.37s/it] 12%|█▏        | 248/2048 [28:07<2:06:23,  4.21s/it] 12%|█▏        | 249/2048 [28:11<2:04:02,  4.14s/it] 12%|█▏        | 250/2048 [28:15<1:56:23,  3.88s/it] 12%|█▏        | 251/2048 [28:19<2:00:32,  4.02s/it] 12%|█▏        | 252/2048 [28:24<2:10:46,  4.37s/it] 12%|█▏        | 253/2048 [28:29<2:17:50,  4.61s/it] 12%|█▏        | 254/2048 [28:35<2:22:33,  4.77s/it] 12%|█▏        | 255/2048 [28:40<2:24:17,  4.83s/it] 12%|█▎        | 256/2048 [28:44<2:23:56,  4.82s/it]                                                    {'loss': 1.2187, 'learning_rate': 0.00019532799107104042, 'epoch': 1.86}
 12%|█▎        | 256/2048 [28:44<2:23:56,  4.82s/it]
  0%|          | 0/256 [00:00<?, ?it/s][A
  2%|▏         | 4/256 [00:00<00:09, 27.36it/s][A
  3%|▎         | 7/256 [00:00<00:11, 21.12it/s][A
  4%|▍         | 10/256 [00:00<00:12, 19.37it/s][A
  5%|▍         | 12/256 [00:00<00:13, 18.42it/s][A
  5%|▌         | 14/256 [00:00<00:13, 17.87it/s][A
  6%|▋         | 16/256 [00:00<00:13, 18.21it/s][A
  7%|▋         | 18/256 [00:00<00:13, 17.81it/s][A
  8%|▊         | 21/256 [00:01<00:12, 18.54it/s][A
  9%|▉         | 23/256 [00:01<00:12, 18.04it/s][A
 10%|▉         | 25/256 [00:01<00:13, 17.68it/s][A
 11%|█         | 27/256 [00:01<00:12, 17.76it/s][A
 11%|█▏        | 29/256 [00:01<00:12, 17.46it/s][A
 12%|█▏        | 31/256 [00:01<00:13, 17.27it/s][A
 13%|█▎        | 34/256 [00:01<00:11, 18.72it/s][A
 14%|█▍        | 36/256 [00:01<00:12, 18.16it/s][A
 15%|█▍        | 38/256 [00:02<00:12, 17.63it/s][A
 16%|█▌        | 40/256 [00:02<00:12, 17.37it/s][A
 16%|█▋        | 42/256 [00:02<00:12, 17.74it/s][A
 17%|█▋        | 44/256 [00:02<00:12, 17.49it/s][A
 18%|█▊        | 46/256 [00:02<00:12, 17.28it/s][A
 19%|█▉        | 48/256 [00:02<00:11, 17.39it/s][A
 20%|█▉        | 50/256 [00:02<00:12, 17.09it/s][A
 20%|██        | 52/256 [00:02<00:12, 16.99it/s][A
 21%|██        | 54/256 [00:02<00:11, 17.77it/s][A
 22%|██▏       | 56/256 [00:03<00:11, 17.59it/s][A
 23%|██▎       | 58/256 [00:03<00:11, 17.51it/s][A
 23%|██▎       | 60/256 [00:03<00:11, 17.27it/s][A
 24%|██▍       | 62/256 [00:03<00:11, 16.94it/s][A
 25%|██▌       | 64/256 [00:03<00:11, 17.23it/s][A
 26%|██▌       | 66/256 [00:03<00:11, 17.09it/s][A
 27%|██▋       | 68/256 [00:03<00:11, 16.99it/s][A
 27%|██▋       | 70/256 [00:03<00:10, 17.13it/s][A
 28%|██▊       | 72/256 [00:04<00:10, 17.01it/s][A
 29%|██▉       | 74/256 [00:04<00:10, 16.98it/s][A
 30%|██▉       | 76/256 [00:04<00:10, 16.99it/s][A
 30%|███       | 78/256 [00:04<00:10, 16.97it/s][A
 31%|███▏      | 80/256 [00:04<00:10, 16.92it/s][A
 32%|███▏      | 82/256 [00:04<00:10, 16.99it/s][A
 33%|███▎      | 84/256 [00:04<00:10, 17.16it/s][A
 34%|███▎      | 86/256 [00:04<00:09, 17.06it/s][A
 34%|███▍      | 88/256 [00:04<00:09, 17.29it/s][A
 35%|███▌      | 90/256 [00:05<00:09, 17.03it/s][A
 36%|███▌      | 92/256 [00:05<00:09, 16.92it/s][A
 37%|███▋      | 94/256 [00:05<00:09, 17.00it/s][A
 38%|███▊      | 96/256 [00:05<00:09, 16.91it/s][A
 39%|███▊      | 99/256 [00:05<00:08, 17.74it/s][A
 39%|███▉      | 101/256 [00:05<00:08, 17.38it/s][A
 40%|████      | 103/256 [00:05<00:08, 17.48it/s][A
 41%|████▏     | 106/256 [00:06<00:08, 18.35it/s][A
 42%|████▏     | 108/256 [00:06<00:08, 17.97it/s][A
 43%|████▎     | 110/256 [00:06<00:08, 17.74it/s][A
 44%|████▍     | 112/256 [00:06<00:08, 17.46it/s][A
 45%|████▍     | 114/256 [00:06<00:08, 17.58it/s][A
 46%|████▌     | 117/256 [00:06<00:07, 18.04it/s][A
 46%|████▋     | 119/256 [00:06<00:07, 17.83it/s][A
 47%|████▋     | 121/256 [00:06<00:07, 17.46it/s][A
 48%|████▊     | 123/256 [00:06<00:07, 17.33it/s][A
 49%|████▉     | 125/256 [00:07<00:07, 17.14it/s][A
 50%|█████     | 128/256 [00:07<00:06, 18.65it/s][A
 51%|█████     | 130/256 [00:07<00:06, 18.23it/s][A
 52%|█████▏    | 132/256 [00:07<00:07, 17.68it/s][A
 52%|█████▏    | 134/256 [00:07<00:06, 17.54it/s][A
 53%|█████▎    | 136/256 [00:07<00:06, 17.39it/s][A
 54%|█████▍    | 138/256 [00:07<00:06, 17.20it/s][A
 55%|█████▍    | 140/256 [00:07<00:06, 17.07it/s][A
 56%|█████▌    | 143/256 [00:08<00:06, 18.20it/s][A
 57%|█████▋    | 145/256 [00:08<00:06, 17.64it/s][A
 57%|█████▋    | 147/256 [00:08<00:06, 17.44it/s][A
 58%|█████▊    | 149/256 [00:08<00:06, 17.24it/s][A
 59%|█████▉    | 151/256 [00:08<00:06, 17.11it/s][A
 60%|██████    | 154/256 [00:08<00:05, 17.73it/s][A
 61%|██████    | 156/256 [00:08<00:05, 17.60it/s][A
 62%|██████▏   | 158/256 [00:08<00:05, 17.23it/s][A
 63%|██████▎   | 161/256 [00:09<00:05, 17.98it/s][A
 64%|██████▎   | 163/256 [00:09<00:05, 17.52it/s][A
 64%|██████▍   | 165/256 [00:09<00:05, 17.42it/s][A
 65%|██████▌   | 167/256 [00:09<00:05, 17.08it/s][A
 66%|██████▌   | 169/256 [00:09<00:05, 17.33it/s][A
 67%|██████▋   | 171/256 [00:09<00:04, 17.08it/s][A
 68%|██████▊   | 173/256 [00:09<00:04, 16.94it/s][A
 68%|██████▊   | 175/256 [00:09<00:04, 16.99it/s][A
 69%|██████▉   | 177/256 [00:10<00:04, 16.83it/s][A
 70%|██████▉   | 179/256 [00:10<00:04, 16.92it/s][A
 71%|███████   | 182/256 [00:10<00:03, 19.22it/s][A
 72%|███████▏  | 184/256 [00:10<00:03, 18.60it/s][A
 73%|███████▎  | 186/256 [00:10<00:03, 17.93it/s][A
 74%|███████▍  | 189/256 [00:10<00:03, 18.43it/s][A
 75%|███████▍  | 191/256 [00:10<00:03, 17.85it/s][A
 75%|███████▌  | 193/256 [00:10<00:03, 17.68it/s][A
 76%|███████▌  | 195/256 [00:11<00:03, 17.28it/s][A
 77%|███████▋  | 197/256 [00:11<00:03, 17.58it/s][A
 78%|███████▊  | 199/256 [00:11<00:03, 18.19it/s][A
 79%|███████▊  | 201/256 [00:11<00:03, 17.88it/s][A
 79%|███████▉  | 203/256 [00:11<00:03, 17.41it/s][A
 80%|████████  | 205/256 [00:11<00:02, 17.36it/s][A
 81%|████████  | 207/256 [00:11<00:02, 17.05it/s][A
 82%|████████▏ | 209/256 [00:11<00:02, 16.95it/s][A
 82%|████████▏ | 211/256 [00:12<00:02, 16.92it/s][A
 83%|████████▎ | 213/256 [00:12<00:02, 16.88it/s][A
 84%|████████▍ | 215/256 [00:12<00:02, 16.84it/s][A
 85%|████████▍ | 217/256 [00:12<00:02, 16.84it/s][A
 86%|████████▌ | 219/256 [00:12<00:02, 16.93it/s][A
 86%|████████▋ | 221/256 [00:12<00:02, 16.77it/s][A
 87%|████████▋ | 223/256 [00:12<00:01, 16.78it/s][A
 88%|████████▊ | 225/256 [00:12<00:01, 17.21it/s][A
 89%|████████▊ | 227/256 [00:12<00:01, 16.94it/s][A
 89%|████████▉ | 229/256 [00:13<00:01, 16.86it/s][A
 90%|█████████ | 231/256 [00:13<00:01, 17.09it/s][A
 91%|█████████ | 233/256 [00:13<00:01, 17.05it/s][A
 92%|█████████▏| 235/256 [00:13<00:01, 17.03it/s][A
 93%|█████████▎| 238/256 [00:13<00:01, 17.94it/s][A
 94%|█████████▍| 240/256 [00:13<00:00, 17.73it/s][A
 95%|█████████▍| 242/256 [00:13<00:00, 17.45it/s][A
 95%|█████████▌| 244/256 [00:13<00:00, 17.77it/s][A
 96%|█████████▋| 247/256 [00:14<00:00, 18.27it/s][A
 97%|█████████▋| 249/256 [00:14<00:00, 17.96it/s][A
 98%|█████████▊| 252/256 [00:14<00:00, 19.02it/s][A
 99%|█████████▉| 254/256 [00:14<00:00, 18.54it/s][A
100%|██████████| 256/256 [00:14<00:00, 17.18it/s][A                                                    
                                                 [A{'eval_loss': 1.5285062789916992, 'eval_runtime': 14.7994, 'eval_samples_per_second': 69.192, 'eval_steps_per_second': 17.298, 'epoch': 1.86}
 12%|█▎        | 256/2048 [28:59<2:23:56,  4.82s/it]
100%|██████████| 256/256 [00:14<00:00, 17.18it/s][A
                                                 [A  0%|          | 0/383 [00:00<?, ?it/s]  0%|          | 0/383 [00:00<?, ?it/s]  0%|          | 0/383 [00:00<?, ?it/s]
  0%|          | 0/383 [00:00<?, ?it/s][A  0%|          | 1/383 [00:00<00:41,  9.18it/s]
  0%|          | 1/383 [00:00<00:52,  7.32it/s][A  1%|          | 2/383 [00:00<00:26, 14.31it/s]  1%|          | 2/383 [00:00<00:27, 13.87it/s]  1%|          | 3/383 [00:00<00:24, 15.20it/s]
  1%|          | 4/383 [00:00<00:23, 16.24it/s][A  1%|▏         | 5/383 [00:00<00:19, 19.25it/s]  1%|▏         | 5/383 [00:00<00:20, 18.23it/s]  1%|▏         | 5/383 [00:00<00:21, 17.26it/s]  2%|▏         | 7/383 [00:00<00:21, 17.85it/s]
  2%|▏         | 7/383 [00:00<00:20, 18.62it/s][A  2%|▏         | 7/383 [00:00<00:21, 17.09it/s]  2%|▏         | 7/383 [00:00<00:22, 16.71it/s]  2%|▏         | 9/383 [00:00<00:23, 16.06it/s]  2%|▏         | 9/383 [00:00<00:24, 15.40it/s]
  2%|▏         | 9/383 [00:00<00:23, 16.19it/s][A  2%|▏         | 9/383 [00:00<00:23, 15.98it/s]  3%|▎         | 11/383 [00:00<00:23, 15.51it/s]
  3%|▎         | 11/383 [00:00<00:24, 15.46it/s][A  3%|▎         | 11/383 [00:00<00:25, 14.61it/s]  3%|▎         | 11/383 [00:00<00:24, 15.02it/s]  3%|▎         | 13/383 [00:00<00:24, 15.06it/s]  3%|▎         | 13/383 [00:00<00:24, 15.33it/s]
  3%|▎         | 13/383 [00:00<00:24, 14.88it/s][A  3%|▎         | 13/383 [00:00<00:25, 14.42it/s]  4%|▍         | 15/383 [00:00<00:22, 16.12it/s]  4%|▍         | 15/383 [00:00<00:22, 16.25it/s]
  4%|▍         | 15/383 [00:00<00:23, 15.91it/s][A  4%|▍         | 15/383 [00:00<00:23, 15.44it/s]  4%|▍         | 17/383 [00:01<00:21, 16.77it/s]  4%|▍         | 17/383 [00:01<00:21, 16.99it/s]
  4%|▍         | 17/383 [00:01<00:21, 16.81it/s][A  4%|▍         | 17/383 [00:01<00:22, 16.24it/s]  5%|▍         | 19/383 [00:01<00:20, 17.35it/s]  5%|▍         | 19/383 [00:01<00:20, 17.53it/s]
  5%|▍         | 19/383 [00:01<00:20, 17.43it/s][A  5%|▍         | 19/383 [00:01<00:22, 16.51it/s]  5%|▌         | 21/383 [00:01<00:20, 17.36it/s]  5%|▌         | 21/383 [00:01<00:20, 17.69it/s]
  5%|▌         | 21/383 [00:01<00:20, 17.72it/s][A  5%|▌         | 21/383 [00:01<00:21, 16.81it/s]  6%|▌         | 23/383 [00:01<00:20, 17.23it/s]
  6%|▌         | 23/383 [00:01<00:20, 17.61it/s][A  6%|▌         | 23/383 [00:01<00:20, 17.36it/s]  6%|▌         | 23/383 [00:01<00:21, 17.01it/s]  7%|▋         | 25/383 [00:01<00:21, 16.75it/s]
  7%|▋         | 25/383 [00:01<00:20, 17.39it/s][A  7%|▋         | 25/383 [00:01<00:21, 17.02it/s]  7%|▋         | 25/383 [00:01<00:21, 16.72it/s]
  7%|▋         | 27/383 [00:01<00:20, 17.11it/s][A  7%|▋         | 27/383 [00:01<00:24, 14.62it/s]  7%|▋         | 27/383 [00:01<00:24, 14.69it/s]  7%|▋         | 27/383 [00:01<00:24, 14.42it/s]
  8%|▊         | 29/383 [00:01<00:25, 13.69it/s][A  8%|▊         | 29/383 [00:01<00:27, 12.83it/s]  8%|▊         | 29/383 [00:01<00:27, 12.83it/s]  8%|▊         | 29/383 [00:01<00:28, 12.45it/s]
  8%|▊         | 31/383 [00:01<00:25, 13.59it/s][A  8%|▊         | 31/383 [00:02<00:27, 13.00it/s]  8%|▊         | 31/383 [00:02<00:26, 13.09it/s]  8%|▊         | 31/383 [00:02<00:26, 13.18it/s]
  9%|▊         | 33/383 [00:02<00:25, 13.79it/s][A  9%|▊         | 33/383 [00:02<00:25, 13.69it/s]  9%|▊         | 33/383 [00:02<00:25, 13.80it/s]  9%|▊         | 33/383 [00:02<00:24, 14.05it/s]
  9%|▉         | 35/383 [00:02<00:23, 14.51it/s][A  9%|▉         | 35/383 [00:02<00:24, 14.12it/s]  9%|▉         | 35/383 [00:02<00:24, 14.38it/s]  9%|▉         | 35/383 [00:02<00:23, 14.64it/s]
 10%|▉         | 37/383 [00:02<00:23, 14.83it/s][A 10%|▉         | 37/383 [00:02<00:23, 14.92it/s] 10%|▉         | 37/383 [00:02<00:23, 14.97it/s] 10%|▉         | 37/383 [00:02<00:22, 15.23it/s]
 10%|█         | 39/383 [00:02<00:22, 15.34it/s][A 10%|█         | 39/383 [00:02<00:22, 15.48it/s] 10%|█         | 39/383 [00:02<00:22, 15.56it/s] 10%|█         | 39/383 [00:02<00:21, 15.72it/s]
 11%|█         | 41/383 [00:02<00:21, 15.96it/s][A 11%|█         | 41/383 [00:02<00:21, 15.91it/s] 11%|█         | 41/383 [00:02<00:21, 16.04it/s] 11%|█         | 41/383 [00:02<00:21, 15.87it/s] 11%|█         | 43/383 [00:02<00:20, 16.92it/s]
 11%|█▏        | 44/383 [00:02<00:18, 17.85it/s][A 11%|█▏        | 44/383 [00:02<00:18, 18.35it/s] 11%|█         | 43/383 [00:02<00:20, 16.51it/s] 12%|█▏        | 46/383 [00:02<00:17, 19.55it/s]
 12%|█▏        | 47/383 [00:02<00:16, 20.03it/s][A 12%|█▏        | 47/383 [00:02<00:16, 20.62it/s] 12%|█▏        | 46/383 [00:02<00:17, 19.16it/s] 13%|█▎        | 49/383 [00:02<00:15, 21.45it/s]
 13%|█▎        | 50/383 [00:02<00:15, 21.59it/s][A 13%|█▎        | 49/383 [00:03<00:15, 21.20it/s] 13%|█▎        | 50/383 [00:03<00:16, 20.81it/s]
 14%|█▍        | 53/383 [00:03<00:17, 18.41it/s][A 14%|█▎        | 52/383 [00:03<00:18, 17.44it/s] 14%|█▍        | 53/383 [00:03<00:17, 18.46it/s] 14%|█▎        | 52/383 [00:03<00:18, 18.13it/s]
 14%|█▍        | 55/383 [00:03<00:17, 18.32it/s][A 14%|█▍        | 54/383 [00:03<00:18, 17.77it/s] 14%|█▍        | 55/383 [00:03<00:17, 18.38it/s] 14%|█▍        | 54/383 [00:03<00:18, 17.88it/s]
 15%|█▍        | 57/383 [00:03<00:18, 18.02it/s][A 15%|█▍        | 56/383 [00:03<00:18, 17.99it/s] 15%|█▍        | 57/383 [00:03<00:17, 18.24it/s] 15%|█▍        | 56/383 [00:03<00:18, 17.97it/s] 15%|█▌        | 58/383 [00:03<00:18, 17.40it/s]
 15%|█▌        | 59/383 [00:03<00:18, 17.15it/s][A 15%|█▌        | 59/383 [00:03<00:18, 17.19it/s] 15%|█▌        | 58/383 [00:03<00:18, 17.21it/s] 16%|█▌        | 60/383 [00:03<00:19, 16.83it/s]
 16%|█▌        | 61/383 [00:03<00:19, 16.19it/s][A 16%|█▌        | 61/383 [00:03<00:19, 16.77it/s] 16%|█▌        | 60/383 [00:03<00:19, 16.84it/s] 16%|█▌        | 62/383 [00:03<00:19, 16.09it/s]
 16%|█▋        | 63/383 [00:03<00:19, 16.15it/s][A 16%|█▋        | 63/383 [00:03<00:19, 16.40it/s] 16%|█▌        | 62/383 [00:03<00:19, 16.23it/s] 17%|█▋        | 64/383 [00:03<00:19, 15.98it/s]
 17%|█▋        | 65/383 [00:03<00:19, 16.13it/s][A 17%|█▋        | 65/383 [00:03<00:19, 15.91it/s] 17%|█▋        | 64/383 [00:03<00:19, 16.25it/s] 17%|█▋        | 66/383 [00:04<00:20, 15.73it/s]
 17%|█▋        | 67/383 [00:04<00:20, 15.74it/s][A 17%|█▋        | 66/383 [00:04<00:19, 16.25it/s] 17%|█▋        | 67/383 [00:04<00:20, 15.69it/s] 18%|█▊        | 68/383 [00:04<00:20, 15.64it/s] 18%|█▊        | 68/383 [00:04<00:20, 15.04it/s]
 18%|█▊        | 69/383 [00:04<00:21, 14.94it/s][A 18%|█▊        | 69/383 [00:04<00:20, 14.96it/s]
 19%|█▊        | 71/383 [00:04<00:20, 15.10it/s][A 18%|█▊        | 70/383 [00:04<00:20, 15.05it/s] 18%|█▊        | 70/383 [00:04<00:21, 14.63it/s] 19%|█▊        | 71/383 [00:04<00:20, 15.11it/s]
 19%|█▉        | 73/383 [00:04<00:19, 16.06it/s][A 19%|█▉        | 72/383 [00:04<00:20, 15.53it/s] 19%|█▉        | 72/383 [00:04<00:19, 15.71it/s] 19%|█▉        | 73/383 [00:04<00:19, 15.72it/s]
 20%|█▉        | 75/383 [00:04<00:19, 15.93it/s][A 19%|█▉        | 74/383 [00:04<00:19, 16.21it/s] 20%|█▉        | 75/383 [00:04<00:19, 15.70it/s] 19%|█▉        | 74/383 [00:04<00:20, 15.29it/s]
 20%|██        | 77/383 [00:04<00:19, 15.95it/s][A 20%|█▉        | 76/383 [00:04<00:18, 16.26it/s] 20%|█▉        | 76/383 [00:04<00:19, 15.66it/s] 20%|██        | 77/383 [00:04<00:19, 15.82it/s]
 21%|██        | 79/383 [00:04<00:19, 15.94it/s][A 20%|██        | 78/383 [00:04<00:18, 16.17it/s] 20%|██        | 78/383 [00:04<00:19, 15.83it/s] 21%|██        | 79/383 [00:04<00:19, 15.85it/s] 21%|██        | 80/383 [00:04<00:18, 16.09it/s]
 21%|██        | 81/383 [00:04<00:19, 15.74it/s][A 21%|██        | 80/383 [00:04<00:19, 15.76it/s] 21%|██        | 81/383 [00:05<00:19, 15.60it/s]
 22%|██▏       | 83/383 [00:05<00:18, 15.92it/s][A 21%|██▏       | 82/383 [00:05<00:18, 15.95it/s] 21%|██▏       | 82/383 [00:05<00:18, 15.97it/s] 22%|██▏       | 83/383 [00:05<00:19, 15.75it/s] 22%|██▏       | 84/383 [00:05<00:18, 15.99it/s]
 22%|██▏       | 85/383 [00:05<00:18, 15.72it/s][A 22%|██▏       | 84/383 [00:05<00:18, 16.12it/s] 22%|██▏       | 85/383 [00:05<00:18, 16.06it/s] 22%|██▏       | 86/383 [00:05<00:18, 16.23it/s] 22%|██▏       | 86/383 [00:05<00:18, 16.08it/s]
 23%|██▎       | 87/383 [00:05<00:20, 14.25it/s][A 23%|██▎       | 87/383 [00:05<00:20, 14.18it/s] 23%|██▎       | 88/383 [00:05<00:22, 13.00it/s] 23%|██▎       | 88/383 [00:05<00:23, 12.54it/s]
 23%|██▎       | 89/383 [00:05<00:24, 11.83it/s][A 23%|██▎       | 89/383 [00:05<00:32,  9.03it/s] 23%|██▎       | 90/383 [00:05<00:34,  8.60it/s]
 24%|██▍       | 91/383 [00:06<00:39,  7.39it/s][A 23%|██▎       | 90/383 [00:06<00:42,  6.89it/s] 24%|██▍       | 91/383 [00:06<00:48,  6.02it/s] 24%|██▍       | 92/383 [00:06<00:48,  5.99it/s] 24%|██▍       | 92/383 [00:06<00:47,  6.09it/s] 24%|██▍       | 92/383 [00:06<00:52,  5.56it/s]
 24%|██▍       | 93/383 [00:06<00:50,  5.71it/s][A 25%|██▍       | 94/383 [00:06<00:39,  7.30it/s]
 25%|██▌       | 96/383 [00:06<00:36,  7.92it/s][A 24%|██▍       | 93/383 [00:06<00:55,  5.26it/s] 25%|██▌       | 96/383 [00:06<00:31,  9.18it/s] 24%|██▍       | 93/383 [00:06<00:53,  5.43it/s]
 26%|██▌       | 98/383 [00:06<00:30,  9.45it/s][A 25%|██▍       | 95/383 [00:06<00:41,  6.93it/s] 26%|██▌       | 98/383 [00:06<00:25, 11.06it/s] 25%|██▍       | 95/383 [00:06<00:40,  7.16it/s]
 26%|██▌       | 100/383 [00:07<00:25, 10.88it/s][A 25%|██▌       | 97/383 [00:07<00:32,  8.78it/s] 26%|██▌       | 100/383 [00:07<00:22, 12.40it/s] 25%|██▌       | 97/383 [00:07<00:31,  9.01it/s]
 27%|██▋       | 102/383 [00:07<00:22, 12.24it/s][A 26%|██▌       | 99/383 [00:07<00:27, 10.48it/s] 27%|██▋       | 102/383 [00:07<00:20, 13.77it/s] 26%|██▌       | 99/383 [00:07<00:26, 10.69it/s]
 27%|██▋       | 104/383 [00:07<00:20, 13.36it/s][A 26%|██▋       | 101/383 [00:07<00:23, 11.80it/s] 27%|██▋       | 104/383 [00:07<00:18, 15.03it/s] 26%|██▋       | 101/383 [00:07<00:23, 12.11it/s]
 28%|██▊       | 106/383 [00:07<00:18, 14.74it/s][A 28%|██▊       | 106/383 [00:07<00:17, 16.15it/s] 27%|██▋       | 103/383 [00:07<00:21, 13.08it/s] 27%|██▋       | 103/383 [00:07<00:20, 13.49it/s]
 28%|██▊       | 108/383 [00:07<00:17, 15.91it/s][A 28%|██▊       | 108/383 [00:07<00:16, 16.87it/s] 27%|██▋       | 105/383 [00:07<00:19, 14.33it/s] 27%|██▋       | 105/383 [00:07<00:18, 14.82it/s]
 29%|██▊       | 110/383 [00:07<00:16, 16.79it/s][A 29%|██▊       | 110/383 [00:07<00:15, 17.56it/s] 28%|██▊       | 107/383 [00:07<00:17, 15.46it/s] 28%|██▊       | 107/383 [00:07<00:17, 15.99it/s]
 29%|██▉       | 112/383 [00:07<00:15, 17.41it/s][A 29%|██▉       | 112/383 [00:07<00:15, 18.06it/s] 28%|██▊       | 109/383 [00:07<00:16, 16.40it/s] 28%|██▊       | 109/383 [00:07<00:16, 16.88it/s]
 30%|██▉       | 114/383 [00:07<00:15, 17.92it/s][A 30%|██▉       | 114/383 [00:07<00:14, 18.44it/s] 29%|██▉       | 111/383 [00:07<00:15, 17.14it/s] 29%|██▉       | 111/383 [00:07<00:15, 17.51it/s]
 30%|███       | 116/383 [00:07<00:14, 17.93it/s][A 30%|██▉       | 113/383 [00:07<00:15, 17.84it/s] 30%|███       | 116/383 [00:07<00:15, 17.76it/s] 30%|██▉       | 113/383 [00:07<00:15, 17.90it/s]
 31%|███       | 118/383 [00:08<00:15, 17.51it/s][A 30%|███       | 115/383 [00:08<00:15, 17.79it/s] 30%|███       | 115/383 [00:08<00:15, 17.85it/s] 31%|███       | 118/383 [00:08<00:15, 17.21it/s]
 31%|███▏      | 120/383 [00:08<00:15, 17.06it/s][A 31%|███       | 117/383 [00:08<00:15, 16.99it/s] 31%|███▏      | 120/383 [00:08<00:15, 17.17it/s] 31%|███       | 117/383 [00:08<00:15, 17.39it/s]
 32%|███▏      | 122/383 [00:08<00:15, 16.88it/s][A 32%|███▏      | 122/383 [00:08<00:15, 17.39it/s] 31%|███       | 119/383 [00:08<00:15, 16.85it/s] 31%|███       | 119/383 [00:08<00:15, 16.93it/s]
 32%|███▏      | 124/383 [00:08<00:14, 17.45it/s][A 32%|███▏      | 124/383 [00:08<00:14, 18.08it/s] 32%|███▏      | 121/383 [00:08<00:15, 16.61it/s] 32%|███▏      | 121/383 [00:08<00:15, 16.58it/s]
 33%|███▎      | 126/383 [00:08<00:14, 18.03it/s][A 33%|███▎      | 126/383 [00:08<00:14, 18.35it/s] 32%|███▏      | 123/383 [00:08<00:15, 16.92it/s] 32%|███▏      | 123/383 [00:08<00:14, 17.39it/s]
 33%|███▎      | 128/383 [00:08<00:13, 18.56it/s][A 33%|███▎      | 128/383 [00:08<00:13, 18.70it/s] 33%|███▎      | 125/383 [00:08<00:14, 17.43it/s] 33%|███▎      | 125/383 [00:08<00:14, 17.87it/s]
 34%|███▍      | 130/383 [00:08<00:14, 17.62it/s][A 34%|███▍      | 130/383 [00:08<00:14, 17.09it/s] 33%|███▎      | 127/383 [00:08<00:13, 18.30it/s] 33%|███▎      | 127/383 [00:08<00:14, 17.85it/s]
 34%|███▍      | 132/383 [00:08<00:14, 16.76it/s][A 34%|███▎      | 129/383 [00:08<00:13, 18.21it/s] 34%|███▎      | 129/383 [00:08<00:14, 17.33it/s] 34%|███▍      | 132/383 [00:08<00:15, 16.54it/s]
 35%|███▍      | 134/383 [00:08<00:15, 16.42it/s][A 34%|███▍      | 131/383 [00:08<00:14, 17.61it/s] 35%|███▍      | 134/383 [00:09<00:15, 16.48it/s] 34%|███▍      | 131/383 [00:09<00:14, 16.89it/s]
 36%|███▌      | 136/383 [00:09<00:15, 16.42it/s][A 35%|███▍      | 133/383 [00:09<00:14, 16.89it/s] 36%|███▌      | 136/383 [00:09<00:15, 16.32it/s] 35%|███▍      | 133/383 [00:09<00:15, 16.56it/s]
 36%|███▌      | 138/383 [00:09<00:14, 16.44it/s][A 35%|███▌      | 135/383 [00:09<00:14, 16.67it/s] 36%|███▌      | 138/383 [00:09<00:14, 16.43it/s] 35%|███▌      | 135/383 [00:09<00:15, 16.31it/s]
 37%|███▋      | 140/383 [00:09<00:14, 16.33it/s][A 36%|███▌      | 137/383 [00:09<00:14, 16.48it/s] 37%|███▋      | 140/383 [00:09<00:14, 16.43it/s] 36%|███▌      | 137/383 [00:09<00:15, 16.03it/s]
 37%|███▋      | 142/383 [00:09<00:14, 16.26it/s][A 36%|███▋      | 139/383 [00:09<00:14, 16.57it/s] 37%|███▋      | 142/383 [00:09<00:14, 16.38it/s] 36%|███▋      | 139/383 [00:09<00:15, 15.98it/s]
 38%|███▊      | 144/383 [00:09<00:14, 16.21it/s][A 38%|███▊      | 144/383 [00:09<00:14, 16.45it/s] 37%|███▋      | 141/383 [00:09<00:14, 16.18it/s] 37%|███▋      | 141/383 [00:09<00:14, 16.16it/s]
 38%|███▊      | 146/383 [00:09<00:14, 16.12it/s][A 38%|███▊      | 146/383 [00:09<00:14, 16.40it/s] 37%|███▋      | 143/383 [00:09<00:14, 16.22it/s] 37%|███▋      | 143/383 [00:09<00:14, 16.23it/s]
 39%|███▊      | 148/383 [00:09<00:14, 16.07it/s][A 38%|███▊      | 145/383 [00:09<00:14, 16.15it/s] 38%|███▊      | 145/383 [00:09<00:14, 16.07it/s] 39%|███▊      | 148/383 [00:09<00:15, 14.91it/s] 38%|███▊      | 147/383 [00:09<00:14, 16.16it/s] 38%|███▊      | 147/383 [00:10<00:14, 16.18it/s]
 39%|███▉      | 150/383 [00:10<00:17, 13.13it/s][A 39%|███▉      | 150/383 [00:10<00:18, 12.58it/s] 39%|███▉      | 149/383 [00:10<00:17, 13.39it/s] 39%|███▉      | 149/383 [00:10<00:17, 13.35it/s]
 40%|███▉      | 152/383 [00:10<00:19, 12.01it/s][A 40%|███▉      | 152/383 [00:10<00:19, 11.75it/s] 39%|███▉      | 151/383 [00:10<00:19, 12.01it/s] 39%|███▉      | 151/383 [00:10<00:19, 11.83it/s]
 40%|████      | 154/383 [00:10<00:25,  9.16it/s][A 40%|███▉      | 153/383 [00:10<00:20, 11.50it/s] 40%|███▉      | 153/383 [00:10<00:21, 10.88it/s] 40%|████      | 154/383 [00:10<00:26,  8.59it/s] 40%|████      | 155/383 [00:11<00:30,  7.41it/s]
 41%|████      | 156/383 [00:11<00:35,  6.34it/s][A 40%|████      | 155/383 [00:11<00:32,  7.11it/s] 41%|████      | 156/383 [00:11<00:36,  6.24it/s] 41%|████      | 156/383 [00:11<00:36,  6.30it/s] 41%|████      | 156/383 [00:11<00:36,  6.18it/s]
 41%|████      | 157/383 [00:11<00:41,  5.45it/s][A 41%|████      | 157/383 [00:11<00:39,  5.71it/s] 41%|████      | 157/383 [00:11<00:39,  5.73it/s] 41%|████      | 157/383 [00:11<00:41,  5.44it/s] 41%|████▏     | 158/383 [00:11<00:43,  5.19it/s]
 41%|████▏     | 158/383 [00:11<00:46,  4.80it/s][A 41%|████▏     | 158/383 [00:11<00:44,  5.11it/s] 42%|████▏     | 159/383 [00:11<00:42,  5.28it/s] 41%|████▏     | 158/383 [00:11<00:45,  4.98it/s]
 42%|████▏     | 159/383 [00:12<00:49,  4.54it/s][A 42%|████▏     | 159/383 [00:12<00:42,  5.23it/s] 42%|████▏     | 160/383 [00:12<00:45,  4.89it/s] 42%|████▏     | 159/383 [00:12<00:48,  4.61it/s]
 42%|████▏     | 160/383 [00:12<00:51,  4.31it/s][A 42%|████▏     | 160/383 [00:12<00:45,  4.95it/s] 42%|████▏     | 161/383 [00:12<00:44,  5.00it/s] 42%|████▏     | 160/383 [00:12<00:47,  4.71it/s]
 42%|████▏     | 161/383 [00:12<00:48,  4.60it/s][A 42%|████▏     | 161/383 [00:12<00:46,  4.79it/s] 42%|████▏     | 162/383 [00:12<00:42,  5.21it/s] 42%|████▏     | 161/383 [00:12<00:49,  4.51it/s]
 42%|████▏     | 162/383 [00:12<00:49,  4.49it/s][A 42%|████▏     | 162/383 [00:12<00:47,  4.62it/s] 43%|████▎     | 163/383 [00:12<00:46,  4.73it/s] 42%|████▏     | 162/383 [00:12<00:46,  4.71it/s]
 43%|████▎     | 163/383 [00:12<00:46,  4.70it/s][A 43%|████▎     | 163/383 [00:12<00:47,  4.67it/s] 43%|████▎     | 164/383 [00:13<00:50,  4.38it/s] 43%|████▎     | 163/383 [00:13<00:46,  4.78it/s]
 43%|████▎     | 164/383 [00:13<00:50,  4.38it/s][A 43%|████▎     | 164/383 [00:13<00:49,  4.41it/s] 43%|████▎     | 165/383 [00:13<00:46,  4.68it/s] 43%|████▎     | 164/383 [00:13<00:48,  4.50it/s] 44%|████▍     | 168/383 [00:13<00:25,  8.53it/s]
 43%|████▎     | 165/383 [00:13<00:48,  4.52it/s][A 43%|████▎     | 165/383 [00:13<00:47,  4.57it/s]
 44%|████▍     | 168/383 [00:13<00:25,  8.29it/s][A 45%|████▍     | 171/383 [00:13<00:17, 11.83it/s] 44%|████▍     | 168/383 [00:13<00:25,  8.39it/s] 45%|████▌     | 173/383 [00:13<00:15, 13.42it/s] 43%|████▎     | 165/383 [00:13<00:51,  4.22it/s]
 45%|████▍     | 171/383 [00:13<00:17, 11.83it/s][A 45%|████▍     | 171/383 [00:13<00:17, 11.94it/s] 44%|████▍     | 168/383 [00:13<00:27,  7.88it/s]
 45%|████▌     | 174/383 [00:13<00:14, 14.53it/s][A 46%|████▌     | 175/383 [00:13<00:15, 13.40it/s] 45%|████▌     | 174/383 [00:13<00:14, 14.69it/s] 45%|████▍     | 171/383 [00:13<00:18, 11.31it/s]
 46%|████▌     | 176/383 [00:13<00:14, 14.23it/s][A 46%|████▌     | 177/383 [00:13<00:15, 13.19it/s] 46%|████▌     | 176/383 [00:13<00:14, 14.34it/s] 45%|████▌     | 174/383 [00:13<00:15, 13.62it/s] 47%|████▋     | 179/383 [00:13<00:14, 14.54it/s]
 46%|████▋     | 178/383 [00:13<00:14, 14.56it/s][A 46%|████▋     | 178/383 [00:14<00:13, 14.66it/s] 47%|████▋     | 181/383 [00:14<00:13, 15.35it/s]
 47%|████▋     | 180/383 [00:14<00:13, 15.57it/s][A 46%|████▌     | 176/383 [00:14<00:15, 13.52it/s] 47%|████▋     | 180/383 [00:14<00:12, 15.70it/s] 48%|████▊     | 183/383 [00:14<00:12, 16.21it/s]
 48%|████▊     | 182/383 [00:14<00:12, 16.06it/s][A 46%|████▋     | 178/383 [00:14<00:14, 13.98it/s] 48%|████▊     | 182/383 [00:14<00:12, 16.31it/s]
 48%|████▊     | 184/383 [00:14<00:11, 16.70it/s][A 48%|████▊     | 185/383 [00:14<00:12, 15.98it/s] 47%|████▋     | 180/383 [00:14<00:13, 15.08it/s] 48%|████▊     | 184/383 [00:14<00:11, 16.85it/s]
 49%|████▊     | 186/383 [00:14<00:12, 16.05it/s][A 48%|████▊     | 182/383 [00:14<00:12, 15.76it/s] 49%|████▉     | 187/383 [00:14<00:12, 15.15it/s] 49%|████▊     | 186/383 [00:14<00:12, 15.57it/s]
 49%|████▉     | 188/383 [00:14<00:11, 16.58it/s][A 48%|████▊     | 184/383 [00:14<00:12, 16.56it/s] 50%|████▉     | 190/383 [00:14<00:10, 17.59it/s] 49%|████▉     | 188/383 [00:14<00:12, 16.11it/s]
 50%|████▉     | 191/383 [00:14<00:10, 18.60it/s][A 50%|█████     | 192/383 [00:14<00:10, 17.93it/s] 49%|████▊     | 186/383 [00:14<00:12, 15.42it/s] 50%|████▉     | 191/383 [00:14<00:10, 18.32it/s]
 50%|█████     | 193/383 [00:14<00:10, 18.52it/s][A 51%|█████     | 194/383 [00:14<00:10, 18.14it/s] 49%|████▉     | 188/383 [00:14<00:12, 15.38it/s] 50%|█████     | 193/383 [00:14<00:10, 18.46it/s]
 51%|█████     | 195/383 [00:14<00:10, 18.43it/s][A 51%|█████     | 196/383 [00:14<00:10, 18.44it/s] 51%|█████     | 195/383 [00:14<00:10, 18.53it/s] 50%|████▉     | 191/383 [00:14<00:10, 17.79it/s]
 51%|█████▏    | 197/383 [00:15<00:09, 18.69it/s][A 52%|█████▏    | 199/383 [00:15<00:09, 20.04it/s] 51%|█████▏    | 197/383 [00:15<00:09, 18.84it/s] 50%|█████     | 193/383 [00:15<00:10, 18.10it/s]
 52%|█████▏    | 199/383 [00:15<00:09, 19.04it/s][A 53%|█████▎    | 202/383 [00:15<00:08, 21.83it/s] 52%|█████▏    | 199/383 [00:15<00:09, 19.10it/s] 51%|█████     | 195/383 [00:15<00:10, 18.32it/s]
 53%|█████▎    | 202/383 [00:15<00:08, 21.13it/s][A 54%|█████▎    | 205/383 [00:15<00:07, 23.05it/s] 51%|█████▏    | 197/383 [00:15<00:10, 18.48it/s] 53%|█████▎    | 202/383 [00:15<00:08, 21.19it/s]
 54%|█████▎    | 205/383 [00:15<00:07, 22.77it/s][A 54%|█████▍    | 208/383 [00:15<00:07, 23.89it/s] 54%|█████▎    | 205/383 [00:15<00:07, 22.49it/s] 52%|█████▏    | 200/383 [00:15<00:09, 19.62it/s]
 54%|█████▍    | 208/383 [00:15<00:07, 23.61it/s][A 55%|█████▌    | 211/383 [00:15<00:07, 24.55it/s] 54%|█████▍    | 208/383 [00:15<00:07, 23.46it/s] 53%|█████▎    | 203/383 [00:15<00:08, 21.42it/s]
 55%|█████▌    | 211/383 [00:15<00:07, 24.30it/s][A 56%|█████▌    | 214/383 [00:15<00:06, 24.32it/s] 55%|█████▌    | 211/383 [00:15<00:07, 24.21it/s] 54%|█████▍    | 206/383 [00:15<00:07, 22.76it/s]
 56%|█████▌    | 214/383 [00:15<00:06, 24.72it/s][A 57%|█████▋    | 217/383 [00:15<00:06, 24.81it/s] 56%|█████▌    | 214/383 [00:15<00:06, 24.66it/s] 55%|█████▍    | 209/383 [00:15<00:07, 23.57it/s]
 57%|█████▋    | 217/383 [00:15<00:06, 24.31it/s][A 57%|█████▋    | 220/383 [00:15<00:06, 25.01it/s] 57%|█████▋    | 217/383 [00:15<00:06, 24.85it/s] 55%|█████▌    | 212/383 [00:15<00:07, 24.08it/s]
 57%|█████▋    | 220/383 [00:15<00:06, 24.81it/s][A 57%|█████▋    | 220/383 [00:15<00:06, 25.14it/s] 56%|█████▌    | 215/383 [00:16<00:06, 24.57it/s] 58%|█████▊    | 223/383 [00:16<00:07, 22.18it/s]
 58%|█████▊    | 223/383 [00:16<00:07, 22.78it/s][A 57%|█████▋    | 218/383 [00:16<00:06, 24.92it/s] 58%|█████▊    | 223/383 [00:16<00:07, 22.13it/s] 59%|█████▉    | 226/383 [00:16<00:07, 20.43it/s]
 59%|█████▉    | 226/383 [00:16<00:07, 20.64it/s][A 58%|█████▊    | 221/383 [00:16<00:06, 23.77it/s] 59%|█████▉    | 226/383 [00:16<00:07, 20.13it/s] 60%|█████▉    | 229/383 [00:16<00:07, 19.26it/s] 58%|█████▊    | 224/383 [00:16<00:07, 20.89it/s]
 60%|█████▉    | 229/383 [00:16<00:08, 19.09it/s][A 60%|█████▉    | 229/383 [00:16<00:08, 18.92it/s] 60%|██████    | 231/383 [00:16<00:08, 17.32it/s]
 60%|██████    | 231/383 [00:16<00:08, 17.83it/s][A 59%|█████▉    | 227/383 [00:16<00:08, 19.22it/s] 60%|██████    | 231/383 [00:16<00:08, 18.02it/s] 61%|██████    | 233/383 [00:16<00:09, 15.98it/s]
 61%|██████    | 233/383 [00:16<00:09, 16.35it/s][A 60%|██████    | 230/383 [00:16<00:08, 18.43it/s] 61%|██████    | 233/383 [00:16<00:09, 16.26it/s] 61%|██████▏   | 235/383 [00:16<00:09, 15.02it/s]
 61%|██████▏   | 235/383 [00:16<00:09, 15.32it/s][A 61%|██████    | 232/383 [00:16<00:08, 16.78it/s] 61%|██████▏   | 235/383 [00:16<00:09, 14.82it/s] 62%|██████▏   | 237/383 [00:17<00:10, 14.22it/s]
 62%|██████▏   | 237/383 [00:17<00:10, 14.47it/s][A 61%|██████    | 234/383 [00:17<00:09, 15.76it/s] 62%|██████▏   | 237/383 [00:17<00:10, 14.22it/s] 62%|██████▏   | 239/383 [00:17<00:10, 13.69it/s]
 62%|██████▏   | 239/383 [00:17<00:10, 13.98it/s][A 62%|██████▏   | 236/383 [00:17<00:09, 14.98it/s] 62%|██████▏   | 239/383 [00:17<00:10, 13.81it/s] 63%|██████▎   | 241/383 [00:17<00:10, 13.32it/s]
 63%|██████▎   | 241/383 [00:17<00:10, 13.64it/s][A 62%|██████▏   | 238/383 [00:17<00:10, 14.37it/s] 63%|██████▎   | 241/383 [00:17<00:10, 13.34it/s] 63%|██████▎   | 243/383 [00:17<00:10, 13.18it/s]
 63%|██████▎   | 243/383 [00:17<00:10, 13.29it/s][A 63%|██████▎   | 240/383 [00:17<00:10, 14.05it/s] 63%|██████▎   | 243/383 [00:17<00:10, 13.20it/s] 64%|██████▍   | 245/383 [00:17<00:10, 12.96it/s]
 64%|██████▍   | 245/383 [00:17<00:10, 13.27it/s][A 63%|██████▎   | 242/383 [00:17<00:10, 13.69it/s] 64%|██████▍   | 245/383 [00:17<00:10, 13.06it/s] 64%|██████▍   | 247/383 [00:17<00:10, 13.08it/s]
 64%|██████▍   | 247/383 [00:17<00:10, 13.27it/s][A 64%|██████▎   | 244/383 [00:17<00:10, 13.60it/s] 64%|██████▍   | 247/383 [00:17<00:10, 12.86it/s] 65%|██████▌   | 249/383 [00:17<00:10, 13.01it/s]
 65%|██████▌   | 249/383 [00:18<00:10, 12.83it/s][A 64%|██████▍   | 246/383 [00:18<00:10, 13.28it/s] 65%|██████▌   | 249/383 [00:18<00:10, 12.86it/s] 66%|██████▌   | 251/383 [00:18<00:10, 12.64it/s]
 66%|██████▌   | 251/383 [00:18<00:10, 12.97it/s][A 65%|██████▍   | 248/383 [00:18<00:10, 13.14it/s] 66%|██████▌   | 251/383 [00:18<00:10, 12.96it/s] 66%|██████▌   | 253/383 [00:18<00:10, 12.70it/s]
 66%|██████▌   | 253/383 [00:18<00:10, 12.96it/s][A 65%|██████▌   | 250/383 [00:18<00:10, 13.03it/s] 66%|██████▌   | 253/383 [00:18<00:10, 12.67it/s] 67%|██████▋   | 255/383 [00:18<00:09, 12.91it/s]
 67%|██████▋   | 255/383 [00:18<00:09, 13.06it/s][A 66%|██████▌   | 252/383 [00:18<00:10, 12.87it/s] 67%|██████▋   | 255/383 [00:18<00:10, 12.63it/s] 67%|██████▋   | 257/383 [00:18<00:09, 13.10it/s]
 67%|██████▋   | 257/383 [00:18<00:09, 13.16it/s][A 66%|██████▋   | 254/383 [00:18<00:10, 12.76it/s] 67%|██████▋   | 257/383 [00:18<00:09, 12.94it/s] 68%|██████▊   | 259/383 [00:18<00:09, 13.37it/s]
 68%|██████▊   | 259/383 [00:18<00:09, 13.28it/s][A 67%|██████▋   | 256/383 [00:18<00:09, 12.87it/s] 68%|██████▊   | 259/383 [00:18<00:09, 13.06it/s] 68%|██████▊   | 261/383 [00:18<00:09, 13.23it/s]
 68%|██████▊   | 261/383 [00:18<00:09, 13.48it/s][A 67%|██████▋   | 258/383 [00:18<00:09, 12.97it/s] 68%|██████▊   | 261/383 [00:19<00:09, 13.09it/s] 69%|██████▊   | 263/383 [00:19<00:08, 13.44it/s]
 69%|██████▊   | 263/383 [00:19<00:08, 13.47it/s][A 68%|██████▊   | 260/383 [00:19<00:09, 13.14it/s] 69%|██████▉   | 266/383 [00:19<00:07, 16.41it/s] 69%|██████▊   | 263/383 [00:19<00:09, 13.17it/s]
 69%|██████▉   | 266/383 [00:19<00:07, 16.05it/s][A 70%|███████   | 269/383 [00:19<00:06, 18.78it/s] 68%|██████▊   | 262/383 [00:19<00:09, 13.28it/s]
 70%|███████   | 269/383 [00:19<00:06, 18.55it/s][A 69%|██████▉   | 266/383 [00:19<00:07, 15.67it/s] 69%|██████▉   | 264/383 [00:19<00:08, 14.39it/s] 71%|███████   | 272/383 [00:19<00:05, 19.02it/s] 70%|███████   | 269/383 [00:19<00:06, 18.02it/s]
 71%|███████   | 272/383 [00:19<00:05, 19.77it/s][A 70%|██████▉   | 267/383 [00:19<00:07, 16.46it/s] 72%|███████▏  | 274/383 [00:19<00:06, 17.69it/s] 71%|███████   | 272/383 [00:19<00:05, 19.08it/s]
 72%|███████▏  | 275/383 [00:19<00:05, 18.50it/s][A 70%|███████   | 270/383 [00:19<00:05, 18.84it/s] 72%|███████▏  | 276/383 [00:19<00:06, 16.51it/s] 72%|███████▏  | 274/383 [00:19<00:06, 17.83it/s]
 72%|███████▏  | 277/383 [00:19<00:05, 17.72it/s][A 71%|███████▏  | 273/383 [00:19<00:05, 19.42it/s] 73%|███████▎  | 278/383 [00:19<00:06, 16.33it/s] 72%|███████▏  | 276/383 [00:19<00:06, 17.41it/s]
 73%|███████▎  | 279/383 [00:19<00:06, 16.86it/s][A 72%|███████▏  | 275/383 [00:19<00:05, 18.47it/s] 73%|███████▎  | 280/383 [00:19<00:06, 16.17it/s] 73%|███████▎  | 278/383 [00:19<00:06, 16.55it/s]
 73%|███████▎  | 281/383 [00:20<00:06, 16.17it/s][A 72%|███████▏  | 277/383 [00:20<00:05, 17.75it/s] 73%|███████▎  | 280/383 [00:20<00:06, 16.26it/s] 74%|███████▎  | 282/383 [00:20<00:07, 13.86it/s] 73%|███████▎  | 279/383 [00:20<00:06, 16.43it/s]
 74%|███████▍  | 283/383 [00:20<00:07, 14.27it/s][A 74%|███████▎  | 282/383 [00:20<00:07, 14.11it/s] 74%|███████▍  | 284/383 [00:20<00:07, 12.94it/s] 73%|███████▎  | 281/383 [00:20<00:06, 14.68it/s]
 74%|███████▍  | 285/383 [00:20<00:07, 12.89it/s][A 74%|███████▍  | 284/383 [00:20<00:07, 12.76it/s] 75%|███████▍  | 286/383 [00:20<00:07, 12.21it/s] 74%|███████▍  | 283/383 [00:20<00:07, 13.05it/s]
 75%|███████▍  | 287/383 [00:20<00:07, 12.07it/s][A 75%|███████▍  | 286/383 [00:20<00:08, 12.12it/s] 75%|███████▌  | 288/383 [00:20<00:07, 11.90it/s] 74%|███████▍  | 285/383 [00:20<00:07, 12.67it/s] 75%|███████▌  | 288/383 [00:20<00:08, 11.37it/s] 75%|███████▍  | 287/383 [00:20<00:07, 12.33it/s]
 75%|███████▌  | 289/383 [00:20<00:10,  8.76it/s][A 76%|███████▌  | 290/383 [00:21<00:12,  7.74it/s] 75%|███████▌  | 289/383 [00:21<00:10,  9.27it/s] 76%|███████▌  | 290/383 [00:21<00:12,  7.31it/s] 76%|███████▌  | 291/383 [00:21<00:13,  6.62it/s]
 76%|███████▌  | 291/383 [00:21<00:14,  6.29it/s][A 76%|███████▌  | 291/383 [00:21<00:14,  6.22it/s] 76%|███████▌  | 292/383 [00:21<00:15,  5.84it/s]
 76%|███████▌  | 292/383 [00:21<00:16,  5.59it/s][A 76%|███████▌  | 291/383 [00:21<00:14,  6.39it/s] 76%|███████▌  | 292/383 [00:21<00:17,  5.33it/s] 77%|███████▋  | 293/383 [00:21<00:17,  5.12it/s] 76%|███████▌  | 292/383 [00:22<00:16,  5.62it/s]
 77%|███████▋  | 293/383 [00:22<00:18,  4.92it/s][A 77%|███████▋  | 293/383 [00:22<00:18,  4.91it/s] 77%|███████▋  | 294/383 [00:22<00:18,  4.79it/s]
 77%|███████▋  | 294/383 [00:22<00:19,  4.60it/s][A 77%|███████▋  | 293/383 [00:22<00:17,  5.10it/s] 77%|███████▋  | 295/383 [00:22<00:18,  4.71it/s] 77%|███████▋  | 294/383 [00:22<00:19,  4.49it/s]
 77%|███████▋  | 295/383 [00:22<00:19,  4.42it/s][A 77%|███████▋  | 294/383 [00:22<00:18,  4.73it/s] 77%|███████▋  | 296/383 [00:22<00:20,  4.27it/s] 77%|███████▋  | 295/383 [00:22<00:20,  4.20it/s] 77%|███████▋  | 295/383 [00:22<00:19,  4.54it/s]
 77%|███████▋  | 296/383 [00:22<00:20,  4.18it/s][A 78%|███████▊  | 297/383 [00:22<00:19,  4.31it/s] 77%|███████▋  | 296/383 [00:23<00:21,  4.08it/s] 77%|███████▋  | 296/383 [00:23<00:20,  4.26it/s]
 78%|███████▊  | 297/383 [00:23<00:21,  3.93it/s][A 78%|███████▊  | 298/383 [00:23<00:20,  4.10it/s] 78%|███████▊  | 297/383 [00:23<00:21,  4.08it/s]
 78%|███████▊  | 298/383 [00:23<00:21,  4.04it/s][A 78%|███████▊  | 297/383 [00:23<00:21,  4.06it/s] 78%|███████▊  | 298/383 [00:23<00:19,  4.28it/s] 78%|███████▊  | 299/383 [00:23<00:21,  3.95it/s]
 78%|███████▊  | 299/383 [00:23<00:20,  4.01it/s][A 78%|███████▊  | 298/383 [00:23<00:22,  3.80it/s] 78%|███████▊  | 300/383 [00:23<00:20,  4.06it/s] 78%|███████▊  | 299/383 [00:23<00:20,  4.06it/s]
 78%|███████▊  | 300/383 [00:23<00:21,  3.86it/s][A 78%|███████▊  | 299/383 [00:23<00:22,  3.78it/s] 79%|███████▊  | 301/383 [00:24<00:20,  3.92it/s] 78%|███████▊  | 300/383 [00:24<00:21,  3.89it/s]
 79%|███████▊  | 301/383 [00:24<00:21,  3.78it/s][A 79%|███████▊  | 301/383 [00:24<00:20,  4.03it/s] 78%|███████▊  | 300/383 [00:24<00:22,  3.71it/s] 79%|███████▉  | 302/383 [00:24<00:21,  3.85it/s]
 79%|███████▉  | 302/383 [00:24<00:21,  3.74it/s][A 79%|███████▉  | 303/383 [00:24<00:21,  3.80it/s] 79%|███████▊  | 301/383 [00:24<00:22,  3.65it/s] 79%|███████▉  | 302/383 [00:24<00:21,  3.81it/s]
 79%|███████▉  | 303/383 [00:24<00:20,  3.93it/s][A 79%|███████▉  | 303/383 [00:24<00:20,  3.97it/s] 79%|███████▉  | 302/383 [00:24<00:22,  3.62it/s] 79%|███████▉  | 304/383 [00:24<00:21,  3.68it/s]
 79%|███████▉  | 304/383 [00:25<00:21,  3.76it/s][A 79%|███████▉  | 304/383 [00:25<00:19,  4.00it/s] 80%|███████▉  | 305/383 [00:25<00:20,  3.83it/s] 79%|███████▉  | 303/383 [00:25<00:22,  3.63it/s] 80%|███████▉  | 306/383 [00:25<00:18,  4.09it/s]
 80%|███████▉  | 305/383 [00:25<00:21,  3.63it/s][A 80%|███████▉  | 305/383 [00:25<00:20,  3.85it/s] 79%|███████▉  | 304/383 [00:25<00:21,  3.62it/s] 80%|████████  | 307/383 [00:25<00:19,  3.96it/s]
 80%|███████▉  | 306/383 [00:25<00:20,  3.67it/s][A 80%|███████▉  | 306/383 [00:25<00:20,  3.81it/s] 80%|███████▉  | 305/383 [00:25<00:21,  3.63it/s] 80%|████████  | 308/383 [00:25<00:19,  3.81it/s]
 80%|████████  | 307/383 [00:25<00:20,  3.65it/s][A 80%|████████  | 307/383 [00:25<00:20,  3.73it/s] 80%|███████▉  | 306/383 [00:25<00:21,  3.62it/s] 81%|████████  | 309/383 [00:26<00:19,  3.85it/s] 80%|████████  | 308/383 [00:26<00:19,  3.87it/s]
 80%|████████  | 308/383 [00:26<00:20,  3.67it/s][A 80%|████████  | 307/383 [00:26<00:21,  3.59it/s]
 81%|████████  | 309/383 [00:26<00:19,  3.76it/s][A 81%|████████  | 309/383 [00:26<00:19,  3.79it/s] 81%|████████  | 310/383 [00:26<00:19,  3.70it/s] 80%|████████  | 308/383 [00:26<00:20,  3.57it/s]
 81%|████████  | 310/383 [00:26<00:18,  3.86it/s][A 81%|████████  | 310/383 [00:26<00:19,  3.71it/s] 81%|████████  | 311/383 [00:26<00:19,  3.64it/s] 81%|████████  | 309/383 [00:26<00:19,  3.80it/s]
 81%|████████  | 311/383 [00:26<00:18,  3.83it/s][A 81%|████████  | 311/383 [00:26<00:19,  3.69it/s] 81%|████████▏ | 312/383 [00:26<00:19,  3.64it/s] 81%|████████  | 310/383 [00:26<00:18,  3.90it/s]
 81%|████████▏ | 312/383 [00:27<00:18,  3.86it/s][A 81%|████████▏ | 312/383 [00:27<00:19,  3.64it/s] 82%|████████▏ | 313/383 [00:27<00:19,  3.64it/s] 81%|████████  | 311/383 [00:27<00:18,  3.81it/s]
 82%|████████▏ | 313/383 [00:27<00:19,  3.66it/s][A 82%|████████▏ | 313/383 [00:27<00:18,  3.69it/s] 81%|████████▏ | 312/383 [00:27<00:18,  3.88it/s] 82%|████████▏ | 314/383 [00:27<00:19,  3.61it/s]
 82%|████████▏ | 314/383 [00:27<00:18,  3.68it/s][A 82%|████████▏ | 314/383 [00:27<00:18,  3.79it/s] 82%|████████▏ | 313/383 [00:27<00:17,  3.91it/s] 82%|████████▏ | 315/383 [00:27<00:19,  3.51it/s]
 82%|████████▏ | 315/383 [00:27<00:18,  3.67it/s][A 82%|████████▏ | 315/383 [00:27<00:17,  3.84it/s] 82%|████████▏ | 314/383 [00:27<00:17,  3.96it/s] 83%|████████▎ | 316/383 [00:28<00:18,  3.64it/s] 82%|████████▏ | 315/383 [00:28<00:16,  4.09it/s]
 83%|████████▎ | 316/383 [00:28<00:18,  3.72it/s][A 83%|████████▎ | 316/383 [00:28<00:17,  3.81it/s] 83%|████████▎ | 317/383 [00:28<00:18,  3.64it/s]
 83%|████████▎ | 317/383 [00:28<00:17,  3.71it/s][A 83%|████████▎ | 316/383 [00:28<00:17,  3.80it/s] 83%|████████▎ | 317/383 [00:28<00:18,  3.63it/s] 83%|████████▎ | 318/383 [00:28<00:18,  3.51it/s] 83%|████████▎ | 317/383 [00:28<00:17,  3.75it/s]
 83%|████████▎ | 318/383 [00:28<00:18,  3.56it/s][A 83%|████████▎ | 318/383 [00:28<00:17,  3.61it/s] 83%|████████▎ | 319/383 [00:28<00:17,  3.70it/s]
 83%|████████▎ | 319/383 [00:29<00:17,  3.68it/s][A 83%|████████▎ | 318/383 [00:29<00:17,  3.71it/s] 83%|████████▎ | 319/383 [00:29<00:17,  3.67it/s] 84%|████████▎ | 320/383 [00:29<00:17,  3.69it/s]
 84%|████████▎ | 320/383 [00:29<00:16,  3.77it/s][A 83%|████████▎ | 319/383 [00:29<00:17,  3.71it/s] 84%|████████▎ | 320/383 [00:29<00:17,  3.69it/s] 84%|████████▍ | 321/383 [00:29<00:17,  3.60it/s]
 84%|████████▍ | 321/383 [00:29<00:16,  3.78it/s][A 84%|████████▎ | 320/383 [00:29<00:17,  3.68it/s] 84%|████████▍ | 321/383 [00:29<00:16,  3.73it/s] 84%|████████▍ | 322/383 [00:29<00:16,  3.63it/s]
 84%|████████▍ | 322/383 [00:29<00:15,  3.85it/s][A 84%|████████▍ | 321/383 [00:29<00:17,  3.59it/s] 84%|████████▍ | 322/383 [00:29<00:16,  3.59it/s] 84%|████████▍ | 323/383 [00:29<00:16,  3.75it/s]
 84%|████████▍ | 323/383 [00:30<00:15,  3.99it/s][A 84%|████████▍ | 322/383 [00:30<00:16,  3.78it/s] 84%|████████▍ | 323/383 [00:30<00:16,  3.73it/s] 85%|████████▍ | 324/383 [00:30<00:15,  3.71it/s]
 85%|████████▍ | 324/383 [00:30<00:15,  3.77it/s][A 84%|████████▍ | 323/383 [00:30<00:16,  3.72it/s] 85%|████████▍ | 324/383 [00:30<00:15,  3.72it/s] 85%|████████▍ | 325/383 [00:30<00:15,  3.80it/s]
 85%|████████▍ | 325/383 [00:30<00:15,  3.86it/s][A 85%|████████▍ | 325/383 [00:30<00:15,  3.82it/s] 85%|████████▍ | 324/383 [00:30<00:16,  3.68it/s] 85%|████████▌ | 326/383 [00:30<00:14,  3.96it/s]
 85%|████████▌ | 326/383 [00:30<00:15,  3.76it/s][A 85%|████████▌ | 326/383 [00:30<00:14,  3.85it/s] 85%|████████▌ | 327/383 [00:30<00:14,  3.89it/s] 85%|████████▍ | 325/383 [00:30<00:15,  3.67it/s]
 85%|████████▌ | 327/383 [00:31<00:14,  3.74it/s][A 85%|████████▌ | 326/383 [00:31<00:14,  3.82it/s] 85%|████████▌ | 327/383 [00:31<00:15,  3.67it/s] 86%|████████▌ | 328/383 [00:31<00:14,  3.79it/s]
 86%|████████▌ | 328/383 [00:31<00:14,  3.68it/s][A 85%|████████▌ | 327/383 [00:31<00:14,  3.89it/s] 86%|████████▌ | 329/383 [00:31<00:14,  3.80it/s] 86%|████████▌ | 328/383 [00:31<00:15,  3.64it/s]
 86%|████████▌ | 329/383 [00:31<00:14,  3.74it/s][A 86%|████████▌ | 328/383 [00:31<00:14,  3.88it/s] 86%|████████▌ | 329/383 [00:31<00:14,  3.76it/s] 86%|████████▌ | 330/383 [00:31<00:14,  3.74it/s] 86%|████████▋ | 331/383 [00:31<00:12,  4.25it/s]
 86%|████████▌ | 330/383 [00:31<00:14,  3.76it/s][A 86%|████████▌ | 329/383 [00:31<00:13,  3.89it/s] 86%|████████▌ | 330/383 [00:32<00:14,  3.64it/s] 87%|████████▋ | 332/383 [00:32<00:10,  4.84it/s] 86%|████████▌ | 330/383 [00:32<00:12,  4.13it/s] 86%|████████▋ | 331/383 [00:32<00:12,  4.27it/s]
 86%|████████▋ | 331/383 [00:32<00:13,  3.73it/s][A 87%|████████▋ | 333/383 [00:32<00:09,  5.34it/s] 87%|████████▋ | 332/383 [00:32<00:11,  4.60it/s]
 87%|████████▋ | 332/383 [00:32<00:12,  4.22it/s][A 87%|████████▋ | 334/383 [00:32<00:08,  5.62it/s] 86%|████████▋ | 331/383 [00:32<00:13,  3.99it/s]
 87%|████████▋ | 333/383 [00:32<00:10,  4.70it/s][A 87%|████████▋ | 335/383 [00:32<00:08,  5.81it/s] 87%|████████▋ | 333/383 [00:32<00:10,  4.85it/s] 87%|████████▋ | 332/383 [00:32<00:11,  4.37it/s]
 87%|████████▋ | 334/383 [00:32<00:09,  5.06it/s][A 88%|████████▊ | 336/383 [00:32<00:07,  5.94it/s] 87%|████████▋ | 334/383 [00:32<00:09,  5.09it/s] 87%|████████▋ | 333/383 [00:32<00:10,  4.96it/s] 88%|████████▊ | 337/383 [00:32<00:07,  6.11it/s] 87%|████████▋ | 335/383 [00:32<00:08,  5.63it/s]
 87%|████████▋ | 335/383 [00:32<00:09,  5.25it/s][A 87%|████████▋ | 334/383 [00:32<00:09,  5.43it/s] 88%|████████▊ | 336/383 [00:33<00:08,  5.73it/s] 88%|████████▊ | 338/383 [00:33<00:07,  5.92it/s]
 88%|████████▊ | 336/383 [00:33<00:08,  5.25it/s][A 87%|████████▋ | 335/383 [00:33<00:08,  5.49it/s] 88%|████████▊ | 337/383 [00:33<00:07,  5.89it/s] 89%|████████▉ | 340/383 [00:33<00:05,  7.69it/s] 88%|████████▊ | 336/383 [00:33<00:07,  5.89it/s]
 88%|████████▊ | 337/383 [00:33<00:08,  5.39it/s][A 89%|████████▉ | 342/383 [00:33<00:04,  9.47it/s] 88%|████████▊ | 338/383 [00:33<00:07,  6.01it/s] 88%|████████▊ | 337/383 [00:33<00:07,  6.10it/s]
 88%|████████▊ | 338/383 [00:33<00:08,  5.49it/s][A 89%|████████▉ | 340/383 [00:33<00:05,  8.40it/s] 90%|████████▉ | 344/383 [00:33<00:03, 10.79it/s] 88%|████████▊ | 338/383 [00:33<00:07,  6.17it/s]
 89%|████████▊ | 339/383 [00:33<00:07,  5.73it/s][A 89%|████████▉ | 342/383 [00:33<00:04,  9.82it/s] 90%|█████████ | 346/383 [00:33<00:03, 11.54it/s] 89%|████████▉ | 340/383 [00:33<00:05,  8.26it/s]
 89%|████████▉ | 341/383 [00:33<00:05,  7.90it/s][A 90%|████████▉ | 344/383 [00:33<00:03, 11.11it/s] 91%|█████████ | 348/383 [00:33<00:02, 12.47it/s] 89%|████████▉ | 342/383 [00:33<00:04,  9.96it/s]
 90%|████████▉ | 343/383 [00:33<00:04,  9.62it/s][A 91%|█████████▏| 350/383 [00:33<00:02, 13.35it/s] 90%|█████████ | 346/383 [00:33<00:03, 12.21it/s] 90%|████████▉ | 344/383 [00:33<00:03, 10.54it/s]
 90%|█████████ | 345/383 [00:33<00:03, 11.04it/s][A 91%|█████████ | 348/383 [00:34<00:02, 12.99it/s] 92%|█████████▏| 352/383 [00:34<00:02, 13.38it/s] 90%|█████████ | 346/383 [00:34<00:03, 11.91it/s]
 91%|█████████ | 347/383 [00:34<00:02, 12.18it/s][A 91%|█████████▏| 350/383 [00:34<00:02, 13.07it/s] 92%|█████████▏| 354/383 [00:34<00:02, 13.70it/s] 91%|█████████ | 348/383 [00:34<00:02, 12.86it/s]
 91%|█████████ | 349/383 [00:34<00:02, 12.89it/s][A 93%|█████████▎| 356/383 [00:34<00:01, 14.48it/s] 92%|█████████▏| 352/383 [00:34<00:02, 13.79it/s] 91%|█████████▏| 350/383 [00:34<00:02, 13.39it/s]
 92%|█████████▏| 351/383 [00:34<00:02, 13.29it/s][A 93%|█████████▎| 358/383 [00:34<00:01, 15.29it/s] 92%|█████████▏| 354/383 [00:34<00:02, 14.07it/s] 92%|█████████▏| 352/383 [00:34<00:02, 13.67it/s]
 92%|█████████▏| 353/383 [00:34<00:02, 13.27it/s][A 93%|█████████▎| 356/383 [00:34<00:01, 14.37it/s] 92%|█████████▏| 354/383 [00:34<00:02, 13.76it/s]
 93%|█████████▎| 355/383 [00:34<00:02, 13.58it/s][A 93%|█████████▎| 358/383 [00:34<00:01, 14.75it/s] 94%|█████████▍| 360/383 [00:34<00:02,  9.73it/s]
 93%|█████████▎| 357/383 [00:34<00:01, 14.40it/s][A 93%|█████████▎| 356/383 [00:34<00:01, 13.63it/s]
 94%|█████████▎| 359/383 [00:34<00:01, 15.45it/s][A 93%|█████████▎| 358/383 [00:34<00:01, 14.87it/s] 94%|█████████▍| 360/383 [00:35<00:02,  9.97it/s] 95%|█████████▍| 362/383 [00:35<00:02,  8.33it/s] 94%|█████████▍| 360/383 [00:35<00:01, 11.85it/s]
 94%|█████████▍| 361/383 [00:35<00:02, 10.16it/s][A 95%|█████████▍| 362/383 [00:35<00:02,  8.17it/s] 95%|█████████▌| 364/383 [00:35<00:02,  7.20it/s] 95%|█████████▍| 362/383 [00:35<00:02,  9.10it/s]
 95%|█████████▍| 363/383 [00:35<00:02,  8.47it/s][A 95%|█████████▌| 365/383 [00:35<00:02,  6.86it/s] 95%|█████████▌| 364/383 [00:35<00:02,  7.14it/s] 96%|█████████▌| 367/383 [00:35<00:01,  8.69it/s] 95%|█████████▌| 364/383 [00:35<00:02,  7.81it/s] 96%|█████████▋| 369/383 [00:35<00:01, 10.47it/s] 95%|█████████▌| 365/383 [00:35<00:02,  6.85it/s]
 95%|█████████▌| 365/383 [00:35<00:02,  7.44it/s][A 97%|█████████▋| 371/383 [00:35<00:00, 12.16it/s] 95%|█████████▌| 365/383 [00:36<00:02,  7.36it/s] 96%|█████████▌| 367/383 [00:36<00:01,  8.61it/s]
 96%|█████████▌| 366/383 [00:36<00:02,  7.38it/s][A 97%|█████████▋| 373/383 [00:36<00:00, 13.66it/s] 96%|█████████▌| 367/383 [00:36<00:01,  9.24it/s] 96%|█████████▋| 369/383 [00:36<00:01, 10.40it/s]
 96%|█████████▌| 368/383 [00:36<00:01,  9.20it/s][A 98%|█████████▊| 375/383 [00:36<00:00, 14.64it/s] 96%|█████████▋| 369/383 [00:36<00:01, 10.96it/s] 97%|█████████▋| 371/383 [00:36<00:00, 12.04it/s]
 97%|█████████▋| 370/383 [00:36<00:01, 10.89it/s][A 98%|█████████▊| 377/383 [00:36<00:00, 15.82it/s] 97%|█████████▋| 371/383 [00:36<00:00, 12.44it/s] 97%|█████████▋| 373/383 [00:36<00:00, 13.42it/s]
 97%|█████████▋| 372/383 [00:36<00:00, 12.34it/s][A 99%|█████████▉| 380/383 [00:36<00:00, 18.26it/s] 97%|█████████▋| 373/383 [00:36<00:00, 13.83it/s] 98%|█████████▊| 375/383 [00:36<00:00, 14.87it/s]
 98%|█████████▊| 374/383 [00:36<00:00, 13.79it/s][A100%|██████████| 383/383 [00:36<00:00, 19.37it/s]100%|██████████| 383/383 [00:36<00:00, 10.47it/s]
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
 98%|█████████▊| 376/383 [00:36<00:00, 15.79it/s]/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
 99%|█████████▊| 378/383 [00:36<00:00, 16.91it/s]/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)

 98%|█████████▊| 377/383 [00:36<00:00, 15.74it/s][A/home/bagus/pytorch/torch/utils/checkpoint.py:426: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 99%|█████████▉| 379/383 [00:36<00:00, 18.38it/s] 99%|█████████▉| 381/383 [00:36<00:00, 19.49it/s]
 99%|█████████▉| 380/383 [00:36<00:00, 17.68it/s][A100%|██████████| 383/383 [00:36<00:00, 10.40it/s]
100%|█████████▉| 382/383 [00:36<00:00, 20.59it/s]/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
100%|██████████| 383/383 [00:36<00:00, 10.38it/s]
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/pytorch/torch/utils/checkpoint.py:426: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

100%|██████████| 383/383 [00:36<00:00, 18.84it/s][A100%|██████████| 383/383 [00:36<00:00, 10.36it/s]
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
                                                    {'mmlu_loss': 1.8073724063196008, 'mmlu_eval_accuracy_conceptual_physics': 0.4230769230769231, 'mmlu_eval_accuracy_logical_fallacies': nan, 'mmlu_eval_accuracy_professional_psychology': nan, 'mmlu_eval_accuracy_public_relations': nan, 'mmlu_eval_accuracy_security_studies': nan, 'mmlu_eval_accuracy_high_school_physics': nan, 'mmlu_eval_accuracy_formal_logic': 0.14285714285714285, 'mmlu_eval_accuracy_anatomy': 0.2857142857142857, 'mmlu_eval_accuracy_moral_scenarios': nan, 'mmlu_eval_accuracy_moral_disputes': nan, 'mmlu_eval_accuracy_management': nan, 'mmlu_eval_accuracy_high_school_european_history': 0.2222222222222222, 'mmlu_eval_accuracy_professional_law': nan, 'mmlu_eval_accuracy_elementary_mathematics': 0.2682926829268293, 'mmlu_eval_accuracy_high_school_computer_science': 0.2222222222222222, 'mmlu_eval_accuracy_high_school_geography': 0.4166666666666667, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy_high_school_biology': 0.25, 'mmlu_eval_accuracy_philosophy': nan, 'mmlu_eval_accuracy_astronomy': 0.125, 'mmlu_eval_accuracy_nutrition': nan, 'mmlu_eval_accuracy_high_school_government_and_politics': nan, 'mmlu_eval_accuracy_medical_genetics': nan, 'mmlu_eval_accuracy_human_aging': nan, 'mmlu_eval_accuracy_human_sexuality': nan, 'mmlu_eval_accuracy_college_medicine': 0.22727272727272727, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_college_physics': 0.18181818181818182, 'mmlu_eval_accuracy_us_foreign_policy': nan, 'mmlu_eval_accuracy_college_computer_science': 0.2727272727272727, 'mmlu_eval_accuracy_machine_learning': nan, 'mmlu_eval_accuracy_professional_accounting': nan, 'mmlu_eval_accuracy_world_religions': nan, 'mmlu_eval_accuracy_miscellaneous': nan, 'mmlu_eval_accuracy_jurisprudence': nan, 'mmlu_eval_accuracy_high_school_chemistry': 0.22727272727272727, 'mmlu_eval_accuracy_virology': nan, 'mmlu_eval_accuracy_high_school_mathematics': nan, 'mmlu_eval_accuracy_electrical_engineering': 0.3125, 'mmlu_eval_accuracy_high_school_world_history': nan, 'mmlu_eval_accuracy_prehistory': nan, 'mmlu_eval_accuracy_high_school_statistics': nan, 'mmlu_eval_accuracy_high_school_psychology': nan, 'mmlu_eval_accuracy_high_school_microeconomics': nan, 'mmlu_eval_accuracy_high_school_us_history': nan, 'mmlu_eval_accuracy_college_biology': 0.3125, 'mmlu_eval_accuracy_high_school_macroeconomics': nan, 'mmlu_eval_accuracy_professional_medicine': nan, 'mmlu_eval_accuracy_clinical_knowledge': 0.3103448275862069, 'mmlu_eval_accuracy_business_ethics': 0.09090909090909091, 'mmlu_eval_accuracy_college_chemistry': 0.5, 'mmlu_eval_accuracy_econometrics': 0.5, 'mmlu_eval_accuracy_marketing': nan, 'mmlu_eval_accuracy_international_law': nan, 'mmlu_eval_accuracy_sociology': nan, 'mmlu_eval_accuracy_computer_security': 0.18181818181818182, 'mmlu_eval_accuracy_college_mathematics': 0.18181818181818182, 'mmlu_eval_accuracy': nan, 'epoch': 1.86}
 12%|█▎        | 256/2048 [29:36<2:23:56,  4.82s/it]/home/bagus/pytorch/torch/utils/checkpoint.py:426: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/pytorch/torch/utils/checkpoint.py:426: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 13%|█▎        | 257/2048 [29:41<10:05:43, 20.29s/it] 13%|█▎        | 258/2048 [29:45<7:41:30, 15.47s/it]  13%|█▎        | 259/2048 [29:49<6:00:40, 12.10s/it] 13%|█▎        | 260/2048 [29:53<4:48:11,  9.67s/it] 13%|█▎        | 261/2048 [29:57<3:55:42,  7.91s/it] 13%|█▎        | 262/2048 [30:01<3:16:22,  6.60s/it] 13%|█▎        | 263/2048 [30:04<2:46:16,  5.59s/it] 13%|█▎        | 264/2048 [30:09<2:42:27,  5.46s/it] 13%|█▎        | 265/2048 [30:14<2:39:43,  5.37s/it] 13%|█▎        | 266/2048 [30:19<2:37:32,  5.30s/it] 13%|█▎        | 267/2048 [30:24<2:35:37,  5.24s/it] 13%|█▎        | 268/2048 [30:29<2:32:26,  5.14s/it] 13%|█▎        | 269/2048 [30:34<2:27:33,  4.98s/it] 13%|█▎        | 270/2048 [30:38<2:22:00,  4.79s/it] 13%|█▎        | 271/2048 [30:42<2:15:50,  4.59s/it] 13%|█▎        | 272/2048 [30:46<2:10:05,  4.40s/it]                                                    {'loss': 1.236, 'learning_rate': 0.00019453295809181143, 'epoch': 1.97}
 13%|█▎        | 272/2048 [30:46<2:10:05,  4.40s/it] 13%|█▎        | 273/2048 [30:50<2:04:32,  4.21s/it] 13%|█▎        | 274/2048 [30:54<2:01:28,  4.11s/it] 13%|█▎        | 275/2048 [30:57<1:54:37,  3.88s/it] 13%|█▎        | 276/2048 [31:01<1:57:41,  3.98s/it] 14%|█▎        | 277/2048 [31:07<2:08:07,  4.34s/it] 14%|█▎        | 278/2048 [31:12<2:15:23,  4.59s/it] 14%|█▎        | 279/2048 [31:17<2:20:08,  4.75s/it] 14%|█▎        | 280/2048 [31:22<2:23:03,  4.85s/it] 14%|█▎        | 281/2048 [31:27<2:23:26,  4.87s/it] 14%|█▍        | 282/2048 [31:32<2:21:37,  4.81s/it] 14%|█▍        | 283/2048 [31:36<2:16:33,  4.64s/it] 14%|█▍        | 284/2048 [31:40<2:13:36,  4.54s/it] 14%|█▍        | 285/2048 [31:44<2:08:38,  4.38s/it] 14%|█▍        | 286/2048 [31:48<2:03:52,  4.22s/it] 14%|█▍        | 287/2048 [31:52<2:03:05,  4.19s/it] 14%|█▍        | 288/2048 [31:55<1:54:02,  3.89s/it]                                                    {'loss': 0.9365, 'learning_rate': 0.00019367737132871843, 'epoch': 2.09}
 14%|█▍        | 288/2048 [31:55<1:54:02,  3.89s/it] 14%|█▍        | 289/2048 [32:00<2:00:08,  4.10s/it] 14%|█▍        | 290/2048 [32:05<2:09:30,  4.42s/it] 14%|█▍        | 291/2048 [32:10<2:15:53,  4.64s/it] 14%|█▍        | 292/2048 [32:15<2:19:25,  4.76s/it] 14%|█▍        | 293/2048 [32:20<2:20:10,  4.79s/it] 14%|█▍        | 294/2048 [32:25<2:20:25,  4.80s/it] 14%|█▍        | 295/2048 [32:29<2:16:24,  4.67s/it] 14%|█▍        | 296/2048 [32:34<2:13:01,  4.56s/it] 15%|█▍        | 297/2048 [32:38<2:08:42,  4.41s/it] 15%|█▍        | 298/2048 [32:42<2:05:36,  4.31s/it] 15%|█▍        | 299/2048 [32:46<2:02:19,  4.20s/it] 15%|█▍        | 300/2048 [32:50<1:59:16,  4.09s/it] 15%|█▍        | 301/2048 [32:53<1:54:04,  3.92s/it] 15%|█▍        | 302/2048 [32:58<2:04:57,  4.29s/it] 15%|█▍        | 303/2048 [33:03<2:12:30,  4.56s/it] 15%|█▍        | 304/2048 [33:09<2:17:37,  4.74s/it]                                                    {'loss': 0.7707, 'learning_rate': 0.00019276177883417598, 'epoch': 2.2}
 15%|█▍        | 304/2048 [33:09<2:17:37,  4.74s/it] 15%|█▍        | 305/2048 [33:14<2:20:00,  4.82s/it] 15%|█▍        | 306/2048 [33:19<2:20:57,  4.85s/it] 15%|█▍        | 307/2048 [33:23<2:17:58,  4.75s/it] 15%|█▌        | 308/2048 [33:27<2:12:09,  4.56s/it] 15%|█▌        | 309/2048 [33:31<2:08:25,  4.43s/it] 15%|█▌        | 310/2048 [33:35<2:03:29,  4.26s/it] 15%|█▌        | 311/2048 [33:39<1:58:02,  4.08s/it] 15%|█▌        | 312/2048 [33:43<1:55:11,  3.98s/it] 15%|█▌        | 313/2048 [33:46<1:48:19,  3.75s/it] 15%|█▌        | 314/2048 [33:50<1:55:38,  4.00s/it] 15%|█▌        | 315/2048 [33:56<2:05:41,  4.35s/it] 15%|█▌        | 316/2048 [34:01<2:12:38,  4.60s/it] 15%|█▌        | 317/2048 [34:06<2:17:10,  4.75s/it] 16%|█▌        | 318/2048 [34:11<2:18:51,  4.82s/it] 16%|█▌        | 319/2048 [34:15<2:17:30,  4.77s/it] 16%|█▌        | 320/2048 [34:20<2:15:07,  4.69s/it]                                                    {'loss': 0.8252, 'learning_rate': 0.00019178676709771258, 'epoch': 2.32}
 16%|█▌        | 320/2048 [34:20<2:15:07,  4.69s/it] 16%|█▌        | 321/2048 [34:24<2:12:11,  4.59s/it] 16%|█▌        | 322/2048 [34:29<2:09:10,  4.49s/it] 16%|█▌        | 323/2048 [34:32<2:03:59,  4.31s/it] 16%|█▌        | 324/2048 [34:36<1:59:50,  4.17s/it] 16%|█▌        | 325/2048 [34:40<1:54:25,  3.98s/it] 16%|█▌        | 326/2048 [34:44<1:51:56,  3.90s/it] 16%|█▌        | 327/2048 [34:49<2:02:47,  4.28s/it] 16%|█▌        | 328/2048 [34:54<2:10:21,  4.55s/it] 16%|█▌        | 329/2048 [34:59<2:15:25,  4.73s/it] 16%|█▌        | 330/2048 [35:04<2:17:39,  4.81s/it] 16%|█▌        | 331/2048 [35:09<2:16:54,  4.78s/it] 16%|█▌        | 332/2048 [35:13<2:14:25,  4.70s/it] 16%|█▋        | 333/2048 [35:18<2:11:10,  4.59s/it] 16%|█▋        | 334/2048 [35:22<2:06:10,  4.42s/it] 16%|█▋        | 335/2048 [35:26<2:02:40,  4.30s/it] 16%|█▋        | 336/2048 [35:30<1:59:17,  4.18s/it]                                                    {'loss': 0.8154, 'learning_rate': 0.0001907529606702906, 'epoch': 2.44}
 16%|█▋        | 336/2048 [35:30<1:59:17,  4.18s/it] 16%|█▋        | 337/2048 [35:33<1:56:11,  4.07s/it] 17%|█▋        | 338/2048 [35:37<1:49:16,  3.83s/it] 17%|█▋        | 339/2048 [35:41<1:55:51,  4.07s/it] 17%|█▋        | 340/2048 [35:46<2:05:12,  4.40s/it] 17%|█▋        | 341/2048 [35:52<2:11:37,  4.63s/it] 17%|█▋        | 342/2048 [35:57<2:15:32,  4.77s/it] 17%|█▋        | 343/2048 [36:02<2:16:23,  4.80s/it] 17%|█▋        | 344/2048 [36:06<2:14:38,  4.74s/it] 17%|█▋        | 345/2048 [36:11<2:11:52,  4.65s/it] 17%|█▋        | 346/2048 [36:14<2:05:43,  4.43s/it] 17%|█▋        | 347/2048 [36:19<2:03:04,  4.34s/it] 17%|█▋        | 348/2048 [36:23<2:00:37,  4.26s/it] 17%|█▋        | 349/2048 [36:27<1:57:04,  4.13s/it] 17%|█▋        | 350/2048 [36:30<1:53:21,  4.01s/it] 17%|█▋        | 351/2048 [36:34<1:50:14,  3.90s/it] 17%|█▋        | 352/2048 [36:39<2:00:58,  4.28s/it]                                                    {'loss': 0.8178, 'learning_rate': 0.00018966102176424564, 'epoch': 2.55}
 17%|█▋        | 352/2048 [36:39<2:00:58,  4.28s/it] 17%|█▋        | 353/2048 [36:44<2:08:24,  4.55s/it] 17%|█▋        | 354/2048 [36:49<2:13:19,  4.72s/it] 17%|█▋        | 355/2048 [36:54<2:15:45,  4.81s/it] 17%|█▋        | 356/2048 [36:59<2:15:13,  4.79s/it] 17%|█▋        | 357/2048 [37:04<2:11:33,  4.67s/it] 17%|█▋        | 358/2048 [37:08<2:07:00,  4.51s/it] 18%|█▊        | 359/2048 [37:12<2:02:32,  4.35s/it] 18%|█▊        | 360/2048 [37:15<1:57:52,  4.19s/it] 18%|█▊        | 361/2048 [37:19<1:56:13,  4.13s/it] 18%|█▊        | 362/2048 [37:23<1:50:54,  3.95s/it] 18%|█▊        | 363/2048 [37:26<1:44:27,  3.72s/it] 18%|█▊        | 364/2048 [37:31<1:51:51,  3.99s/it] 18%|█▊        | 365/2048 [37:36<2:01:44,  4.34s/it] 18%|█▊        | 366/2048 [37:41<2:08:27,  4.58s/it] 18%|█▊        | 367/2048 [37:46<2:13:01,  4.75s/it] 18%|█▊        | 368/2048 [37:51<2:15:17,  4.83s/it]                                                    {'loss': 0.7925, 'learning_rate': 0.00018851164982910135, 'epoch': 2.67}
 18%|█▊        | 368/2048 [37:51<2:15:17,  4.83s/it] 18%|█▊        | 369/2048 [37:56<2:13:42,  4.78s/it] 18%|█▊        | 370/2048 [38:00<2:10:47,  4.68s/it] 18%|█▊        | 371/2048 [38:05<2:07:12,  4.55s/it] 18%|█▊        | 372/2048 [38:08<2:01:05,  4.34s/it] 18%|█▊        | 373/2048 [38:12<1:57:14,  4.20s/it] 18%|█▊        | 374/2048 [38:16<1:53:36,  4.07s/it] 18%|█▊        | 375/2048 [38:20<1:48:26,  3.89s/it] 18%|█▊        | 376/2048 [38:23<1:45:59,  3.80s/it] 18%|█▊        | 377/2048 [38:28<1:57:20,  4.21s/it] 18%|█▊        | 378/2048 [38:33<2:05:14,  4.50s/it] 19%|█▊        | 379/2048 [38:39<2:10:37,  4.70s/it] 19%|█▊        | 380/2048 [38:44<2:14:13,  4.83s/it] 19%|█▊        | 381/2048 [38:49<2:14:29,  4.84s/it] 19%|█▊        | 382/2048 [38:53<2:12:30,  4.77s/it] 19%|█▊        | 383/2048 [38:58<2:10:03,  4.69s/it] 19%|█▉        | 384/2048 [39:02<2:04:08,  4.48s/it]                                                    {'loss': 0.8617, 'learning_rate': 0.000187305581103532, 'epoch': 2.79}
 19%|█▉        | 384/2048 [39:02<2:04:08,  4.48s/it] 19%|█▉        | 385/2048 [39:05<1:57:25,  4.24s/it] 19%|█▉        | 386/2048 [39:10<1:58:04,  4.26s/it] 19%|█▉        | 387/2048 [39:14<1:54:07,  4.12s/it] 19%|█▉        | 388/2048 [39:17<1:47:08,  3.87s/it] 19%|█▉        | 389/2048 [39:21<1:53:11,  4.09s/it] 19%|█▉        | 390/2048 [39:27<2:02:03,  4.42s/it] 19%|█▉        | 391/2048 [39:32<2:08:07,  4.64s/it] 19%|█▉        | 392/2048 [39:37<2:11:48,  4.78s/it] 19%|█▉        | 393/2048 [39:42<2:13:24,  4.84s/it] 19%|█▉        | 394/2048 [39:47<2:13:17,  4.84s/it] 19%|█▉        | 395/2048 [39:51<2:09:29,  4.70s/it] 19%|█▉        | 396/2048 [39:55<2:03:20,  4.48s/it] 19%|█▉        | 397/2048 [39:59<1:59:19,  4.34s/it] 19%|█▉        | 398/2048 [40:03<1:58:29,  4.31s/it] 19%|█▉        | 399/2048 [40:07<1:53:45,  4.14s/it] 20%|█▉        | 400/2048 [40:10<1:48:24,  3.95s/it]                                                    {'loss': 0.8681, 'learning_rate': 0.0001860435881437593, 'epoch': 2.9}
 20%|█▉        | 400/2048 [40:10<1:48:24,  3.95s/it] 20%|█▉        | 401/2048 [40:14<1:45:30,  3.84s/it] 20%|█▉        | 402/2048 [40:19<1:56:22,  4.24s/it] 20%|█▉        | 403/2048 [40:24<2:03:53,  4.52s/it] 20%|█▉        | 404/2048 [40:30<2:08:56,  4.71s/it] 20%|█▉        | 405/2048 [40:35<2:11:56,  4.82s/it] 20%|█▉        | 406/2048 [40:39<2:11:42,  4.81s/it] 20%|█▉        | 407/2048 [40:44<2:10:47,  4.78s/it] 20%|█▉        | 408/2048 [40:49<2:07:14,  4.66s/it] 20%|█▉        | 409/2048 [40:53<2:01:42,  4.46s/it] 20%|██        | 410/2048 [40:56<1:57:32,  4.31s/it] 20%|██        | 411/2048 [41:01<1:56:13,  4.26s/it] 20%|██        | 412/2048 [41:05<1:54:27,  4.20s/it] 20%|██        | 413/2048 [41:08<1:46:04,  3.89s/it] 20%|██        | 414/2048 [41:12<1:50:50,  4.07s/it] 20%|██        | 415/2048 [41:18<1:59:46,  4.40s/it] 20%|██        | 416/2048 [41:23<2:05:53,  4.63s/it]                                                    {'loss': 0.7703, 'learning_rate': 0.0001847264793286859, 'epoch': 3.02}
 20%|██        | 416/2048 [41:23<2:05:53,  4.63s/it] 20%|██        | 417/2048 [41:28<2:10:04,  4.79s/it] 20%|██        | 418/2048 [41:33<2:11:08,  4.83s/it] 20%|██        | 419/2048 [41:38<2:11:19,  4.84s/it] 21%|██        | 420/2048 [41:42<2:07:52,  4.71s/it] 21%|██        | 421/2048 [41:46<2:03:06,  4.54s/it] 21%|██        | 422/2048 [41:50<1:57:58,  4.35s/it] 21%|██        | 423/2048 [41:54<1:53:18,  4.18s/it] 21%|██        | 424/2048 [41:58<1:49:07,  4.03s/it] 21%|██        | 425/2048 [42:01<1:45:41,  3.91s/it] 21%|██        | 426/2048 [42:04<1:39:39,  3.69s/it] 21%|██        | 427/2048 [42:09<1:49:14,  4.04s/it] 21%|██        | 428/2048 [42:14<1:58:17,  4.38s/it] 21%|██        | 429/2048 [42:20<2:04:24,  4.61s/it] 21%|██        | 430/2048 [42:25<2:08:20,  4.76s/it] 21%|██        | 431/2048 [42:30<2:09:30,  4.81s/it] 21%|██        | 432/2048 [42:34<2:08:49,  4.78s/it]                                                    {'loss': 0.5361, 'learning_rate': 0.00018335509834208223, 'epoch': 3.13}
 21%|██        | 432/2048 [42:34<2:08:49,  4.78s/it] 21%|██        | 433/2048 [42:39<2:05:30,  4.66s/it] 21%|██        | 434/2048 [42:43<2:00:27,  4.48s/it] 21%|██        | 435/2048 [42:47<1:55:44,  4.31s/it] 21%|██▏       | 436/2048 [42:51<1:54:31,  4.26s/it] 21%|██▏       | 437/2048 [42:55<1:50:41,  4.12s/it] 21%|██▏       | 438/2048 [42:58<1:45:03,  3.92s/it] 21%|██▏       | 439/2048 [43:02<1:44:20,  3.89s/it] 21%|██▏       | 440/2048 [43:07<1:54:33,  4.27s/it] 22%|██▏       | 441/2048 [43:12<2:01:39,  4.54s/it] 22%|██▏       | 442/2048 [43:17<2:06:24,  4.72s/it] 22%|██▏       | 443/2048 [43:22<2:09:27,  4.84s/it] 22%|██▏       | 444/2048 [43:27<2:09:34,  4.85s/it] 22%|██▏       | 445/2048 [43:32<2:06:49,  4.75s/it] 22%|██▏       | 446/2048 [43:36<2:04:19,  4.66s/it] 22%|██▏       | 447/2048 [43:40<1:59:48,  4.49s/it] 22%|██▏       | 448/2048 [43:44<1:54:33,  4.30s/it]                                                    {'loss': 0.5247, 'learning_rate': 0.00018193032363215868, 'epoch': 3.25}
 22%|██▏       | 448/2048 [43:44<1:54:33,  4.30s/it] 22%|██▏       | 449/2048 [43:48<1:49:04,  4.09s/it] 22%|██▏       | 450/2048 [43:52<1:47:29,  4.04s/it] 22%|██▏       | 451/2048 [43:55<1:41:52,  3.83s/it] 22%|██▏       | 452/2048 [44:00<1:50:16,  4.15s/it] 22%|██▏       | 453/2048 [44:05<1:58:22,  4.45s/it] 22%|██▏       | 454/2048 [44:10<2:03:47,  4.66s/it] 22%|██▏       | 455/2048 [44:15<2:06:46,  4.78s/it] 22%|██▏       | 456/2048 [44:20<2:08:13,  4.83s/it] 22%|██▏       | 457/2048 [44:25<2:07:11,  4.80s/it] 22%|██▏       | 458/2048 [44:29<2:04:08,  4.68s/it] 22%|██▏       | 459/2048 [44:34<1:59:51,  4.53s/it] 22%|██▏       | 460/2048 [44:37<1:53:32,  4.29s/it] 23%|██▎       | 461/2048 [44:41<1:50:28,  4.18s/it] 23%|██▎       | 462/2048 [44:46<1:51:27,  4.22s/it] 23%|██▎       | 463/2048 [44:49<1:45:19,  3.99s/it] 23%|██▎       | 464/2048 [44:53<1:44:15,  3.95s/it]                                                    {'loss': 0.5605, 'learning_rate': 0.0001804530678488691, 'epoch': 3.37}
 23%|██▎       | 464/2048 [44:53<1:44:15,  3.95s/it] 23%|██▎       | 465/2048 [44:58<1:53:51,  4.32s/it] 23%|██▎       | 466/2048 [45:03<2:00:31,  4.57s/it] 23%|██▎       | 467/2048 [45:08<2:04:59,  4.74s/it] 23%|██▎       | 468/2048 [45:13<2:07:40,  4.85s/it] 23%|██▎       | 469/2048 [45:18<2:07:13,  4.83s/it] 23%|██▎       | 470/2048 [45:23<2:04:14,  4.72s/it] 23%|██▎       | 471/2048 [45:27<2:00:24,  4.58s/it] 23%|██▎       | 472/2048 [45:31<1:56:29,  4.43s/it] 23%|██▎       | 473/2048 [45:35<1:53:40,  4.33s/it] 23%|██▎       | 474/2048 [45:39<1:53:32,  4.33s/it] 23%|██▎       | 475/2048 [45:43<1:47:48,  4.11s/it] 23%|██▎       | 476/2048 [45:46<1:40:45,  3.85s/it] 23%|██▎       | 477/2048 [45:51<1:48:42,  4.15s/it] 23%|██▎       | 478/2048 [45:56<1:56:38,  4.46s/it] 23%|██▎       | 479/2048 [46:01<2:02:03,  4.67s/it] 23%|██▎       | 480/2048 [46:07<2:05:37,  4.81s/it]                                                    {'loss': 0.5027, 'learning_rate': 0.00017892427725930613, 'epoch': 3.48}
 23%|██▎       | 480/2048 [46:07<2:05:37,  4.81s/it] 23%|██▎       | 481/2048 [46:12<2:06:50,  4.86s/it] 24%|██▎       | 482/2048 [46:16<2:06:43,  4.86s/it] 24%|██▎       | 483/2048 [46:21<2:02:55,  4.71s/it] 24%|██▎       | 484/2048 [46:25<1:58:14,  4.54s/it] 24%|██▎       | 485/2048 [46:29<1:53:39,  4.36s/it] 24%|██▎       | 486/2048 [46:33<1:49:42,  4.21s/it] 24%|██▍       | 487/2048 [46:37<1:49:44,  4.22s/it] 24%|██▍       | 488/2048 [46:40<1:43:48,  3.99s/it] 24%|██▍       | 489/2048 [46:44<1:42:42,  3.95s/it] 24%|██▍       | 490/2048 [46:49<1:52:08,  4.32s/it] 24%|██▍       | 491/2048 [46:55<1:58:39,  4.57s/it] 24%|██▍       | 492/2048 [47:00<2:03:04,  4.75s/it] 24%|██▍       | 493/2048 [47:05<2:05:31,  4.84s/it] 24%|██▍       | 494/2048 [47:10<2:05:40,  4.85s/it] 24%|██▍       | 495/2048 [47:14<2:03:43,  4.78s/it] 24%|██▍       | 496/2048 [47:19<2:00:46,  4.67s/it]                                                    {'loss': 0.5722, 'learning_rate': 0.00017734493114156282, 'epoch': 3.6}
 24%|██▍       | 496/2048 [47:19<2:00:46,  4.67s/it] 24%|██▍       | 497/2048 [47:23<1:56:51,  4.52s/it] 24%|██▍       | 498/2048 [47:27<1:51:59,  4.33s/it] 24%|██▍       | 499/2048 [47:31<1:47:38,  4.17s/it] 24%|██▍       | 500/2048 [47:34<1:43:50,  4.03s/it] 24%|██▍       | 501/2048 [47:37<1:36:57,  3.76s/it] 25%|██▍       | 502/2048 [47:42<1:45:29,  4.09s/it] 25%|██▍       | 503/2048 [47:47<1:53:41,  4.42s/it] 25%|██▍       | 504/2048 [47:53<1:59:18,  4.64s/it] 25%|██▍       | 505/2048 [47:58<2:01:43,  4.73s/it] 25%|██▍       | 506/2048 [48:02<2:02:06,  4.75s/it] 25%|██▍       | 507/2048 [48:07<2:00:30,  4.69s/it] 25%|██▍       | 508/2048 [48:11<1:57:17,  4.57s/it] 25%|██▍       | 509/2048 [48:15<1:53:03,  4.41s/it] 25%|██▍       | 510/2048 [48:19<1:49:11,  4.26s/it] 25%|██▍       | 511/2048 [48:23<1:46:50,  4.17s/it] 25%|██▌       | 512/2048 [48:27<1:47:42,  4.21s/it]                                                    {'loss': 0.5633, 'learning_rate': 0.00017571604115744892, 'epoch': 3.71}
 25%|██▌       | 512/2048 [48:27<1:47:42,  4.21s/it]
  0%|          | 0/256 [00:00<?, ?it/s][A
  2%|▏         | 4/256 [00:00<00:09, 27.47it/s][A
  3%|▎         | 7/256 [00:00<00:11, 21.16it/s][A
  4%|▍         | 10/256 [00:00<00:12, 19.38it/s][A
  5%|▍         | 12/256 [00:00<00:13, 18.42it/s][A
  5%|▌         | 14/256 [00:00<00:13, 17.87it/s][A
  6%|▋         | 16/256 [00:00<00:13, 18.21it/s][A
  7%|▋         | 18/256 [00:00<00:13, 17.81it/s][A
  8%|▊         | 21/256 [00:01<00:12, 18.54it/s][A
  9%|▉         | 23/256 [00:01<00:12, 18.04it/s][A
 10%|▉         | 25/256 [00:01<00:13, 17.68it/s][A
 11%|█         | 27/256 [00:01<00:12, 17.76it/s][A
 11%|█▏        | 29/256 [00:01<00:13, 17.41it/s][A
 12%|█▏        | 31/256 [00:01<00:13, 17.28it/s][A
 13%|█▎        | 34/256 [00:01<00:11, 18.73it/s][A
 14%|█▍        | 36/256 [00:01<00:12, 18.16it/s][A
 15%|█▍        | 38/256 [00:02<00:12, 17.64it/s][A
 16%|█▌        | 40/256 [00:02<00:12, 17.38it/s][A
 16%|█▋        | 42/256 [00:02<00:12, 17.75it/s][A
 17%|█▋        | 44/256 [00:02<00:12, 17.49it/s][A
 18%|█▊        | 46/256 [00:02<00:12, 17.28it/s][A
 19%|█▉        | 48/256 [00:02<00:11, 17.39it/s][A
 20%|█▉        | 50/256 [00:02<00:12, 17.10it/s][A
 20%|██        | 52/256 [00:02<00:12, 16.99it/s][A
 21%|██        | 54/256 [00:02<00:11, 17.77it/s][A
 22%|██▏       | 56/256 [00:03<00:11, 17.59it/s][A
 23%|██▎       | 58/256 [00:03<00:11, 17.53it/s][A
 23%|██▎       | 60/256 [00:03<00:11, 17.28it/s][A
 24%|██▍       | 62/256 [00:03<00:11, 16.96it/s][A
 25%|██▌       | 64/256 [00:03<00:11, 17.25it/s][A
 26%|██▌       | 66/256 [00:03<00:11, 17.10it/s][A
 27%|██▋       | 68/256 [00:03<00:11, 17.01it/s][A
 27%|██▋       | 70/256 [00:03<00:10, 17.15it/s][A
 28%|██▊       | 72/256 [00:04<00:10, 17.03it/s][A
 29%|██▉       | 74/256 [00:04<00:10, 16.99it/s][A
 30%|██▉       | 76/256 [00:04<00:10, 17.00it/s][A
 30%|███       | 78/256 [00:04<00:10, 16.97it/s][A
 31%|███▏      | 80/256 [00:04<00:10, 16.93it/s][A
 32%|███▏      | 82/256 [00:04<00:10, 16.98it/s][A
 33%|███▎      | 84/256 [00:04<00:10, 17.15it/s][A
 34%|███▎      | 86/256 [00:04<00:09, 17.05it/s][A
 34%|███▍      | 88/256 [00:04<00:09, 17.28it/s][A
 35%|███▌      | 90/256 [00:05<00:09, 17.02it/s][A
 36%|███▌      | 92/256 [00:05<00:09, 16.92it/s][A
 37%|███▋      | 94/256 [00:05<00:09, 17.00it/s][A
 38%|███▊      | 96/256 [00:05<00:09, 16.91it/s][A
 39%|███▊      | 99/256 [00:05<00:08, 17.74it/s][A
 39%|███▉      | 101/256 [00:05<00:08, 17.38it/s][A
 40%|████      | 103/256 [00:05<00:08, 17.48it/s][A
 41%|████▏     | 106/256 [00:06<00:08, 18.35it/s][A
 42%|████▏     | 108/256 [00:06<00:08, 17.97it/s][A
 43%|████▎     | 110/256 [00:06<00:08, 17.73it/s][A
 44%|████▍     | 112/256 [00:06<00:08, 17.46it/s][A
 45%|████▍     | 114/256 [00:06<00:08, 17.58it/s][A
 46%|████▌     | 117/256 [00:06<00:07, 18.04it/s][A
 46%|████▋     | 119/256 [00:06<00:07, 17.84it/s][A
 47%|████▋     | 121/256 [00:06<00:07, 17.47it/s][A
 48%|████▊     | 123/256 [00:06<00:07, 17.32it/s][A
 49%|████▉     | 125/256 [00:07<00:07, 17.15it/s][A
 50%|█████     | 128/256 [00:07<00:06, 18.66it/s][A
 51%|█████     | 130/256 [00:07<00:06, 18.23it/s][A
 52%|█████▏    | 132/256 [00:07<00:07, 17.69it/s][A
 52%|█████▏    | 134/256 [00:07<00:06, 17.55it/s][A
 53%|█████▎    | 136/256 [00:07<00:06, 17.39it/s][A
 54%|█████▍    | 138/256 [00:07<00:06, 17.20it/s][A
 55%|█████▍    | 140/256 [00:07<00:06, 17.07it/s][A
 56%|█████▌    | 143/256 [00:08<00:06, 18.20it/s][A
 57%|█████▋    | 145/256 [00:08<00:06, 17.63it/s][A
 57%|█████▋    | 147/256 [00:08<00:06, 17.43it/s][A
 58%|█████▊    | 149/256 [00:08<00:06, 17.24it/s][A
 59%|█████▉    | 151/256 [00:08<00:06, 17.10it/s][A
 60%|██████    | 154/256 [00:08<00:05, 17.72it/s][A
 61%|██████    | 156/256 [00:08<00:05, 17.59it/s][A
 62%|██████▏   | 158/256 [00:08<00:05, 17.24it/s][A
 63%|██████▎   | 161/256 [00:09<00:05, 17.96it/s][A
 64%|██████▎   | 163/256 [00:09<00:05, 17.52it/s][A
 64%|██████▍   | 165/256 [00:09<00:05, 17.43it/s][A
 65%|██████▌   | 167/256 [00:09<00:05, 17.07it/s][A
 66%|██████▌   | 169/256 [00:09<00:05, 17.32it/s][A
 67%|██████▋   | 171/256 [00:09<00:04, 17.08it/s][A
 68%|██████▊   | 173/256 [00:09<00:04, 16.94it/s][A
 68%|██████▊   | 175/256 [00:09<00:04, 17.01it/s][A
 69%|██████▉   | 177/256 [00:10<00:04, 16.84it/s][A
 70%|██████▉   | 179/256 [00:10<00:04, 16.92it/s][A
 71%|███████   | 182/256 [00:10<00:03, 19.23it/s][A
 72%|███████▏  | 184/256 [00:10<00:03, 18.60it/s][A
 73%|███████▎  | 186/256 [00:10<00:03, 17.93it/s][A
 74%|███████▍  | 189/256 [00:10<00:03, 18.43it/s][A
 75%|███████▍  | 191/256 [00:10<00:03, 17.84it/s][A
 75%|███████▌  | 193/256 [00:10<00:03, 17.67it/s][A
 76%|███████▌  | 195/256 [00:11<00:03, 17.28it/s][A
 77%|███████▋  | 197/256 [00:11<00:03, 17.58it/s][A
 78%|███████▊  | 199/256 [00:11<00:03, 18.19it/s][A
 79%|███████▊  | 201/256 [00:11<00:03, 17.87it/s][A
 79%|███████▉  | 203/256 [00:11<00:03, 17.41it/s][A
 80%|████████  | 205/256 [00:11<00:02, 17.35it/s][A
 81%|████████  | 207/256 [00:11<00:02, 17.05it/s][A
 82%|████████▏ | 209/256 [00:11<00:02, 16.95it/s][A
 82%|████████▏ | 211/256 [00:12<00:02, 16.92it/s][A
 83%|████████▎ | 213/256 [00:12<00:02, 16.88it/s][A
 84%|████████▍ | 215/256 [00:12<00:02, 16.84it/s][A
 85%|████████▍ | 217/256 [00:12<00:02, 16.84it/s][A
 86%|████████▌ | 219/256 [00:12<00:02, 16.93it/s][A
 86%|████████▋ | 221/256 [00:12<00:02, 16.78it/s][A
 87%|████████▋ | 223/256 [00:12<00:01, 16.78it/s][A
 88%|████████▊ | 225/256 [00:12<00:01, 17.21it/s][A
 89%|████████▊ | 227/256 [00:12<00:01, 16.94it/s][A
 89%|████████▉ | 229/256 [00:13<00:01, 16.86it/s][A
 90%|█████████ | 231/256 [00:13<00:01, 17.09it/s][A
 91%|█████████ | 233/256 [00:13<00:01, 17.05it/s][A
 92%|█████████▏| 235/256 [00:13<00:01, 17.02it/s][A
 93%|█████████▎| 238/256 [00:13<00:01, 17.94it/s][A
 94%|█████████▍| 240/256 [00:13<00:00, 17.73it/s][A
 95%|█████████▍| 242/256 [00:13<00:00, 17.45it/s][A
 95%|█████████▌| 244/256 [00:13<00:00, 17.77it/s][A
 96%|█████████▋| 247/256 [00:14<00:00, 18.27it/s][A
 97%|█████████▋| 249/256 [00:14<00:00, 17.95it/s][A
 98%|█████████▊| 252/256 [00:14<00:00, 19.02it/s][A
 99%|█████████▉| 254/256 [00:14<00:00, 18.55it/s][A
100%|██████████| 256/256 [00:14<00:00, 17.06it/s][A                                                    
                                                 [A{'eval_loss': 1.811979055404663, 'eval_runtime': 14.7457, 'eval_samples_per_second': 69.444, 'eval_steps_per_second': 17.361, 'epoch': 3.71}
 25%|██▌       | 512/2048 [48:42<1:47:42,  4.21s/it]
100%|██████████| 256/256 [00:14<00:00, 17.06it/s][A
                                                 [A  0%|          | 0/383 [00:00<?, ?it/s]  0%|          | 0/383 [00:00<?, ?it/s]  0%|          | 0/383 [00:00<?, ?it/s]
  0%|          | 0/383 [00:00<?, ?it/s][A  0%|          | 1/383 [00:00<00:41,  9.10it/s]
  0%|          | 1/383 [00:00<00:46,  8.24it/s][A  1%|          | 2/383 [00:00<00:25, 14.83it/s]  1%|          | 2/383 [00:00<00:26, 14.18it/s]  1%|          | 3/383 [00:00<00:25, 15.12it/s]
  1%|          | 4/383 [00:00<00:22, 17.06it/s][A  1%|▏         | 5/383 [00:00<00:19, 19.44it/s]  1%|▏         | 5/383 [00:00<00:20, 18.50it/s]  1%|▏         | 5/383 [00:00<00:21, 17.21it/s]  2%|▏         | 7/383 [00:00<00:20, 17.95it/s]
  2%|▏         | 7/383 [00:00<00:19, 19.11it/s][A  2%|▏         | 7/383 [00:00<00:21, 17.24it/s]  2%|▏         | 7/383 [00:00<00:22, 16.69it/s]  2%|▏         | 9/383 [00:00<00:23, 16.11it/s]
  2%|▏         | 9/383 [00:00<00:22, 16.43it/s][A  2%|▏         | 9/383 [00:00<00:24, 15.48it/s]  2%|▏         | 9/383 [00:00<00:23, 15.97it/s]  3%|▎         | 11/383 [00:00<00:23, 15.54it/s]
  3%|▎         | 11/383 [00:00<00:23, 15.61it/s][A  3%|▎         | 11/383 [00:00<00:25, 14.66it/s]  3%|▎         | 11/383 [00:00<00:24, 15.01it/s]  3%|▎         | 13/383 [00:00<00:24, 15.07it/s]
  3%|▎         | 13/383 [00:00<00:24, 14.97it/s][A  3%|▎         | 13/383 [00:00<00:24, 15.32it/s]  3%|▎         | 13/383 [00:00<00:25, 14.44it/s]  4%|▍         | 15/383 [00:00<00:22, 16.13it/s]
  4%|▍         | 15/383 [00:00<00:23, 15.98it/s][A  4%|▍         | 15/383 [00:00<00:22, 16.24it/s]  4%|▍         | 15/383 [00:00<00:23, 15.46it/s]  4%|▍         | 17/383 [00:01<00:21, 16.78it/s]
  4%|▍         | 17/383 [00:01<00:21, 16.87it/s][A  4%|▍         | 17/383 [00:01<00:21, 16.98it/s]  4%|▍         | 17/383 [00:01<00:22, 16.25it/s]  5%|▍         | 19/383 [00:01<00:20, 17.35it/s]
  5%|▍         | 19/383 [00:01<00:20, 17.47it/s][A  5%|▍         | 19/383 [00:01<00:20, 17.53it/s]  5%|▍         | 19/383 [00:01<00:22, 16.52it/s]  5%|▌         | 21/383 [00:01<00:20, 17.36it/s]
  5%|▌         | 21/383 [00:01<00:20, 17.75it/s][A  5%|▌         | 21/383 [00:01<00:20, 17.69it/s]  5%|▌         | 21/383 [00:01<00:21, 16.81it/s]  6%|▌         | 23/383 [00:01<00:20, 17.23it/s]
  6%|▌         | 23/383 [00:01<00:20, 17.64it/s][A  6%|▌         | 23/383 [00:01<00:20, 17.36it/s]  6%|▌         | 23/383 [00:01<00:21, 17.01it/s]
  7%|▋         | 25/383 [00:01<00:20, 17.40it/s][A  7%|▋         | 25/383 [00:01<00:21, 16.74it/s]  7%|▋         | 25/383 [00:01<00:21, 17.02it/s]  7%|▋         | 25/383 [00:01<00:21, 16.73it/s]
  7%|▋         | 27/383 [00:01<00:20, 17.12it/s][A  7%|▋         | 27/383 [00:01<00:24, 14.62it/s]  7%|▋         | 27/383 [00:01<00:24, 14.69it/s]  7%|▋         | 27/383 [00:01<00:24, 14.42it/s]
  8%|▊         | 29/383 [00:01<00:25, 13.69it/s][A  8%|▊         | 29/383 [00:01<00:27, 12.83it/s]  8%|▊         | 29/383 [00:01<00:27, 12.83it/s]  8%|▊         | 29/383 [00:01<00:28, 12.45it/s]
  8%|▊         | 31/383 [00:01<00:25, 13.59it/s][A  8%|▊         | 31/383 [00:02<00:27, 13.00it/s]  8%|▊         | 31/383 [00:02<00:26, 13.09it/s]  8%|▊         | 31/383 [00:02<00:26, 13.18it/s]
  9%|▊         | 33/383 [00:02<00:25, 13.79it/s][A  9%|▊         | 33/383 [00:02<00:25, 13.69it/s]  9%|▊         | 33/383 [00:02<00:25, 13.80it/s]  9%|▊         | 33/383 [00:02<00:24, 14.05it/s]
  9%|▉         | 35/383 [00:02<00:23, 14.51it/s][A  9%|▉         | 35/383 [00:02<00:24, 14.12it/s]  9%|▉         | 35/383 [00:02<00:24, 14.38it/s]  9%|▉         | 35/383 [00:02<00:23, 14.64it/s]
 10%|▉         | 37/383 [00:02<00:23, 14.83it/s][A 10%|▉         | 37/383 [00:02<00:23, 14.92it/s] 10%|▉         | 37/383 [00:02<00:23, 14.97it/s] 10%|▉         | 37/383 [00:02<00:22, 15.23it/s]
 10%|█         | 39/383 [00:02<00:22, 15.35it/s][A 10%|█         | 39/383 [00:02<00:22, 15.48it/s] 10%|█         | 39/383 [00:02<00:22, 15.56it/s] 10%|█         | 39/383 [00:02<00:21, 15.72it/s]
 11%|█         | 41/383 [00:02<00:21, 15.97it/s][A 11%|█         | 41/383 [00:02<00:21, 15.91it/s] 11%|█         | 41/383 [00:02<00:21, 16.05it/s] 11%|█         | 41/383 [00:02<00:21, 15.87it/s]
 11%|█▏        | 44/383 [00:02<00:18, 17.86it/s][A 11%|█         | 43/383 [00:02<00:20, 16.92it/s] 11%|█▏        | 44/383 [00:02<00:18, 18.35it/s] 11%|█         | 43/383 [00:02<00:20, 16.51it/s]
 12%|█▏        | 47/383 [00:02<00:16, 20.04it/s][A 12%|█▏        | 46/383 [00:02<00:17, 19.55it/s] 12%|█▏        | 47/383 [00:02<00:16, 20.62it/s] 12%|█▏        | 46/383 [00:02<00:17, 19.16it/s]
 13%|█▎        | 50/383 [00:02<00:15, 21.60it/s][A 13%|█▎        | 49/383 [00:02<00:15, 21.46it/s] 13%|█▎        | 49/383 [00:03<00:15, 21.20it/s] 13%|█▎        | 50/383 [00:03<00:16, 20.80it/s]
 14%|█▍        | 53/383 [00:03<00:17, 18.41it/s][A 14%|█▍        | 53/383 [00:03<00:17, 18.46it/s] 14%|█▎        | 52/383 [00:03<00:18, 17.44it/s] 14%|█▎        | 52/383 [00:03<00:18, 18.13it/s]
 14%|█▍        | 55/383 [00:03<00:17, 18.32it/s][A 14%|█▍        | 54/383 [00:03<00:18, 17.77it/s] 14%|█▍        | 55/383 [00:03<00:17, 18.37it/s] 14%|█▍        | 54/383 [00:03<00:18, 17.88it/s]
 15%|█▍        | 57/383 [00:03<00:18, 18.02it/s][A 15%|█▍        | 56/383 [00:03<00:18, 17.99it/s] 15%|█▍        | 57/383 [00:03<00:17, 18.23it/s] 15%|█▍        | 56/383 [00:03<00:18, 17.96it/s]
 15%|█▌        | 59/383 [00:03<00:18, 17.16it/s][A 15%|█▌        | 58/383 [00:03<00:18, 17.40it/s] 15%|█▌        | 59/383 [00:03<00:18, 17.18it/s] 15%|█▌        | 58/383 [00:03<00:18, 17.21it/s]
 16%|█▌        | 61/383 [00:03<00:19, 16.20it/s][A 16%|█▌        | 60/383 [00:03<00:19, 16.83it/s] 16%|█▌        | 61/383 [00:03<00:19, 16.77it/s] 16%|█▌        | 60/383 [00:03<00:19, 16.84it/s]
 16%|█▋        | 63/383 [00:03<00:19, 16.15it/s][A 16%|█▌        | 62/383 [00:03<00:19, 16.09it/s] 16%|█▋        | 63/383 [00:03<00:19, 16.40it/s] 16%|█▌        | 62/383 [00:03<00:19, 16.23it/s]
 17%|█▋        | 65/383 [00:03<00:19, 16.13it/s][A 17%|█▋        | 64/383 [00:03<00:19, 15.98it/s] 17%|█▋        | 64/383 [00:03<00:19, 16.25it/s] 17%|█▋        | 65/383 [00:03<00:19, 15.91it/s]
 17%|█▋        | 67/383 [00:04<00:20, 15.74it/s][A 17%|█▋        | 66/383 [00:04<00:20, 15.73it/s] 17%|█▋        | 66/383 [00:04<00:19, 16.24it/s] 17%|█▋        | 67/383 [00:04<00:20, 15.68it/s]
 18%|█▊        | 69/383 [00:04<00:21, 14.94it/s][A 18%|█▊        | 68/383 [00:04<00:20, 15.64it/s] 18%|█▊        | 68/383 [00:04<00:20, 15.04it/s] 18%|█▊        | 69/383 [00:04<00:20, 14.96it/s]
 19%|█▊        | 71/383 [00:04<00:20, 15.10it/s][A 18%|█▊        | 70/383 [00:04<00:20, 15.05it/s] 19%|█▊        | 71/383 [00:04<00:20, 15.11it/s] 18%|█▊        | 70/383 [00:04<00:21, 14.63it/s]
 19%|█▉        | 73/383 [00:04<00:19, 16.06it/s][A 19%|█▉        | 72/383 [00:04<00:19, 15.71it/s] 19%|█▉        | 72/383 [00:04<00:20, 15.53it/s] 19%|█▉        | 73/383 [00:04<00:19, 15.71it/s]
 20%|█▉        | 75/383 [00:04<00:19, 15.93it/s][A 19%|█▉        | 74/383 [00:04<00:19, 16.21it/s] 20%|█▉        | 75/383 [00:04<00:19, 15.69it/s] 19%|█▉        | 74/383 [00:04<00:20, 15.29it/s]
 20%|██        | 77/383 [00:04<00:19, 15.95it/s][A 20%|█▉        | 76/383 [00:04<00:18, 16.27it/s] 20%|██        | 77/383 [00:04<00:19, 15.81it/s] 20%|█▉        | 76/383 [00:04<00:19, 15.66it/s]
 21%|██        | 79/383 [00:04<00:19, 15.94it/s][A 20%|██        | 78/383 [00:04<00:18, 16.17it/s] 20%|██        | 78/383 [00:04<00:19, 15.83it/s] 21%|██        | 79/383 [00:04<00:19, 15.84it/s]
 21%|██        | 81/383 [00:04<00:19, 15.74it/s][A 21%|██        | 80/383 [00:04<00:18, 16.10it/s] 21%|██        | 80/383 [00:04<00:19, 15.76it/s] 21%|██        | 81/383 [00:05<00:19, 15.59it/s]
 22%|██▏       | 83/383 [00:05<00:18, 15.93it/s][A 21%|██▏       | 82/383 [00:05<00:18, 15.96it/s] 21%|██▏       | 82/383 [00:05<00:18, 15.97it/s] 22%|██▏       | 83/383 [00:05<00:19, 15.75it/s]
 22%|██▏       | 85/383 [00:05<00:18, 15.72it/s][A 22%|██▏       | 84/383 [00:05<00:18, 15.99it/s] 22%|██▏       | 84/383 [00:05<00:18, 16.12it/s] 22%|██▏       | 85/383 [00:05<00:18, 16.06it/s] 22%|██▏       | 86/383 [00:05<00:18, 16.23it/s] 22%|██▏       | 86/383 [00:05<00:18, 16.08it/s]
 23%|██▎       | 87/383 [00:05<00:20, 14.25it/s][A 23%|██▎       | 87/383 [00:05<00:20, 14.18it/s] 23%|██▎       | 88/383 [00:05<00:22, 13.00it/s] 23%|██▎       | 88/383 [00:05<00:23, 12.54it/s]
 23%|██▎       | 89/383 [00:05<00:24, 11.83it/s][A 23%|██▎       | 89/383 [00:05<00:32,  9.02it/s] 23%|██▎       | 90/383 [00:05<00:34,  8.60it/s]
 24%|██▍       | 91/383 [00:06<00:39,  7.40it/s][A 23%|██▎       | 90/383 [00:06<00:42,  6.90it/s] 24%|██▍       | 91/383 [00:06<00:48,  6.02it/s] 24%|██▍       | 92/383 [00:06<00:48,  5.98it/s] 24%|██▍       | 92/383 [00:06<00:47,  6.09it/s] 24%|██▍       | 92/383 [00:06<00:52,  5.56it/s]
 24%|██▍       | 93/383 [00:06<00:50,  5.71it/s][A 25%|██▍       | 94/383 [00:06<00:39,  7.30it/s]
 25%|██▌       | 96/383 [00:06<00:36,  7.92it/s][A 24%|██▍       | 93/383 [00:06<00:55,  5.26it/s] 25%|██▌       | 96/383 [00:06<00:31,  9.18it/s] 24%|██▍       | 93/383 [00:06<00:53,  5.44it/s]
 26%|██▌       | 98/383 [00:06<00:30,  9.45it/s][A 25%|██▍       | 95/383 [00:06<00:41,  6.93it/s] 26%|██▌       | 98/383 [00:06<00:25, 11.06it/s] 25%|██▍       | 95/383 [00:06<00:40,  7.17it/s]
 26%|██▌       | 100/383 [00:07<00:25, 10.89it/s][A 25%|██▌       | 97/383 [00:07<00:32,  8.78it/s] 26%|██▌       | 100/383 [00:07<00:22, 12.40it/s] 25%|██▌       | 97/383 [00:07<00:31,  9.01it/s]
 27%|██▋       | 102/383 [00:07<00:22, 12.24it/s][A 26%|██▌       | 99/383 [00:07<00:27, 10.48it/s] 27%|██▋       | 102/383 [00:07<00:20, 13.76it/s] 26%|██▌       | 99/383 [00:07<00:26, 10.70it/s]
 27%|██▋       | 104/383 [00:07<00:20, 13.36it/s][A 26%|██▋       | 101/383 [00:07<00:23, 11.79it/s] 27%|██▋       | 104/383 [00:07<00:18, 15.03it/s] 26%|██▋       | 101/383 [00:07<00:23, 12.12it/s]
 28%|██▊       | 106/383 [00:07<00:18, 14.74it/s][A 28%|██▊       | 106/383 [00:07<00:17, 16.15it/s] 27%|██▋       | 103/383 [00:07<00:21, 13.07it/s] 27%|██▋       | 103/383 [00:07<00:20, 13.50it/s]
 28%|██▊       | 108/383 [00:07<00:17, 15.91it/s][A 28%|██▊       | 108/383 [00:07<00:16, 16.87it/s] 27%|██▋       | 105/383 [00:07<00:19, 14.33it/s] 27%|██▋       | 105/383 [00:07<00:18, 14.82it/s]
 29%|██▊       | 110/383 [00:07<00:16, 16.79it/s][A 29%|██▊       | 110/383 [00:07<00:15, 17.55it/s] 28%|██▊       | 107/383 [00:07<00:17, 15.45it/s] 28%|██▊       | 107/383 [00:07<00:17, 16.00it/s]
 29%|██▉       | 112/383 [00:07<00:15, 17.41it/s][A 29%|██▉       | 112/383 [00:07<00:15, 18.05it/s] 28%|██▊       | 109/383 [00:07<00:16, 16.39it/s] 28%|██▊       | 109/383 [00:07<00:16, 16.89it/s]
 30%|██▉       | 114/383 [00:07<00:15, 17.92it/s][A 30%|██▉       | 114/383 [00:07<00:14, 18.44it/s] 29%|██▉       | 111/383 [00:07<00:15, 17.13it/s] 29%|██▉       | 111/383 [00:07<00:15, 17.51it/s]
 30%|███       | 116/383 [00:07<00:14, 17.93it/s][A 30%|██▉       | 113/383 [00:07<00:15, 17.83it/s] 30%|███       | 116/383 [00:07<00:15, 17.75it/s] 30%|██▉       | 113/383 [00:07<00:15, 17.90it/s]
 31%|███       | 118/383 [00:08<00:15, 17.51it/s][A 30%|███       | 115/383 [00:08<00:15, 17.78it/s] 30%|███       | 115/383 [00:08<00:15, 17.85it/s] 31%|███       | 118/383 [00:08<00:15, 17.21it/s]
 31%|███▏      | 120/383 [00:08<00:15, 17.06it/s][A 31%|███       | 117/383 [00:08<00:15, 16.99it/s] 31%|███▏      | 120/383 [00:08<00:15, 17.17it/s] 31%|███       | 117/383 [00:08<00:15, 17.40it/s]
 32%|███▏      | 122/383 [00:08<00:15, 16.88it/s][A 32%|███▏      | 122/383 [00:08<00:15, 17.39it/s] 31%|███       | 119/383 [00:08<00:15, 16.84it/s] 31%|███       | 119/383 [00:08<00:15, 16.93it/s]
 32%|███▏      | 124/383 [00:08<00:14, 17.46it/s][A 32%|███▏      | 124/383 [00:08<00:14, 18.08it/s] 32%|███▏      | 121/383 [00:08<00:15, 16.61it/s] 32%|███▏      | 121/383 [00:08<00:15, 16.58it/s]
 33%|███▎      | 126/383 [00:08<00:14, 18.04it/s][A 33%|███▎      | 126/383 [00:08<00:14, 18.35it/s] 32%|███▏      | 123/383 [00:08<00:15, 16.92it/s] 32%|███▏      | 123/383 [00:08<00:14, 17.39it/s]
 33%|███▎      | 128/383 [00:08<00:13, 18.56it/s][A 33%|███▎      | 128/383 [00:08<00:13, 18.70it/s] 33%|███▎      | 125/383 [00:08<00:14, 17.42it/s] 33%|███▎      | 125/383 [00:08<00:14, 17.87it/s]
 34%|███▍      | 130/383 [00:08<00:14, 17.62it/s][A 34%|███▍      | 130/383 [00:08<00:14, 17.08it/s] 33%|███▎      | 127/383 [00:08<00:14, 17.85it/s] 33%|███▎      | 127/383 [00:08<00:13, 18.31it/s]
 34%|███▍      | 132/383 [00:08<00:14, 16.76it/s][A 34%|███▎      | 129/383 [00:08<00:13, 18.22it/s] 34%|███▎      | 129/383 [00:08<00:14, 17.33it/s] 34%|███▍      | 132/383 [00:08<00:15, 16.53it/s]
 35%|███▍      | 134/383 [00:08<00:15, 16.43it/s][A 34%|███▍      | 131/383 [00:08<00:14, 17.62it/s] 34%|███▍      | 131/383 [00:08<00:14, 16.89it/s] 35%|███▍      | 134/383 [00:09<00:15, 16.48it/s]
 36%|███▌      | 136/383 [00:09<00:15, 16.42it/s][A 35%|███▍      | 133/383 [00:09<00:14, 16.90it/s] 35%|███▍      | 133/383 [00:09<00:15, 16.56it/s] 36%|███▌      | 136/383 [00:09<00:15, 16.31it/s]
 36%|███▌      | 138/383 [00:09<00:14, 16.44it/s][A 35%|███▌      | 135/383 [00:09<00:14, 16.68it/s] 36%|███▌      | 138/383 [00:09<00:14, 16.42it/s] 35%|███▌      | 135/383 [00:09<00:15, 16.30it/s]
 37%|███▋      | 140/383 [00:09<00:14, 16.34it/s][A 36%|███▌      | 137/383 [00:09<00:14, 16.48it/s] 37%|███▋      | 140/383 [00:09<00:14, 16.42it/s] 36%|███▌      | 137/383 [00:09<00:15, 16.03it/s]
 37%|███▋      | 142/383 [00:09<00:14, 16.27it/s][A 36%|███▋      | 139/383 [00:09<00:14, 16.57it/s] 37%|███▋      | 142/383 [00:09<00:14, 16.37it/s] 36%|███▋      | 139/383 [00:09<00:15, 15.98it/s]
 38%|███▊      | 144/383 [00:09<00:14, 16.21it/s][A 38%|███▊      | 144/383 [00:09<00:14, 16.44it/s] 37%|███▋      | 141/383 [00:09<00:14, 16.18it/s] 37%|███▋      | 141/383 [00:09<00:14, 16.16it/s]
 38%|███▊      | 146/383 [00:09<00:14, 16.13it/s][A 38%|███▊      | 146/383 [00:09<00:14, 16.40it/s] 37%|███▋      | 143/383 [00:09<00:14, 16.22it/s] 37%|███▋      | 143/383 [00:09<00:14, 16.23it/s]
 39%|███▊      | 148/383 [00:09<00:14, 16.07it/s][A 38%|███▊      | 145/383 [00:09<00:14, 16.18it/s] 38%|███▊      | 145/383 [00:09<00:14, 16.07it/s] 39%|███▊      | 148/383 [00:09<00:15, 14.92it/s] 38%|███▊      | 147/383 [00:09<00:14, 16.19it/s] 38%|███▊      | 147/383 [00:09<00:14, 16.18it/s]
 39%|███▉      | 150/383 [00:10<00:17, 13.13it/s][A 39%|███▉      | 150/383 [00:10<00:18, 12.59it/s] 39%|███▉      | 149/383 [00:10<00:17, 13.41it/s] 39%|███▉      | 149/383 [00:10<00:17, 13.35it/s]
 40%|███▉      | 152/383 [00:10<00:19, 12.01it/s][A 40%|███▉      | 152/383 [00:10<00:19, 11.75it/s] 39%|███▉      | 151/383 [00:10<00:19, 12.02it/s] 39%|███▉      | 151/383 [00:10<00:19, 11.83it/s]
 40%|████      | 154/383 [00:10<00:25,  9.16it/s][A 40%|███▉      | 153/383 [00:10<00:19, 11.51it/s] 40%|███▉      | 153/383 [00:10<00:21, 10.88it/s] 40%|████      | 154/383 [00:10<00:26,  8.60it/s] 40%|████      | 155/383 [00:11<00:30,  7.41it/s]
 41%|████      | 156/383 [00:11<00:35,  6.34it/s][A 40%|████      | 155/383 [00:11<00:32,  7.11it/s] 41%|████      | 156/383 [00:11<00:36,  6.24it/s] 41%|████      | 156/383 [00:11<00:36,  6.30it/s] 41%|████      | 156/383 [00:11<00:36,  6.18it/s]
 41%|████      | 157/383 [00:11<00:41,  5.45it/s][A 41%|████      | 157/383 [00:11<00:39,  5.71it/s] 41%|████      | 157/383 [00:11<00:39,  5.73it/s] 41%|████      | 157/383 [00:11<00:41,  5.44it/s] 41%|████▏     | 158/383 [00:11<00:43,  5.19it/s]
 41%|████▏     | 158/383 [00:11<00:46,  4.80it/s][A 41%|████▏     | 158/383 [00:11<00:43,  5.11it/s] 42%|████▏     | 159/383 [00:11<00:42,  5.28it/s] 41%|████▏     | 158/383 [00:11<00:45,  4.98it/s]
 42%|████▏     | 159/383 [00:11<00:49,  4.54it/s][A 42%|████▏     | 159/383 [00:12<00:42,  5.24it/s] 42%|████▏     | 160/383 [00:12<00:45,  4.89it/s] 42%|████▏     | 159/383 [00:12<00:48,  4.61it/s]
 42%|████▏     | 160/383 [00:12<00:51,  4.32it/s][A 42%|████▏     | 160/383 [00:12<00:45,  4.95it/s] 42%|████▏     | 161/383 [00:12<00:44,  5.00it/s] 42%|████▏     | 160/383 [00:12<00:47,  4.71it/s]
 42%|████▏     | 161/383 [00:12<00:48,  4.60it/s][A 42%|████▏     | 161/383 [00:12<00:46,  4.79it/s] 42%|████▏     | 162/383 [00:12<00:42,  5.21it/s] 42%|████▏     | 161/383 [00:12<00:49,  4.51it/s]
 42%|████▏     | 162/383 [00:12<00:49,  4.49it/s][A 42%|████▏     | 162/383 [00:12<00:47,  4.62it/s] 43%|████▎     | 163/383 [00:12<00:46,  4.73it/s] 42%|████▏     | 162/383 [00:12<00:46,  4.71it/s]
 43%|████▎     | 163/383 [00:12<00:46,  4.70it/s][A 43%|████▎     | 163/383 [00:12<00:47,  4.67it/s] 43%|████▎     | 164/383 [00:13<00:50,  4.38it/s] 43%|████▎     | 163/383 [00:13<00:46,  4.78it/s]
 43%|████▎     | 164/383 [00:13<00:50,  4.38it/s][A 43%|████▎     | 164/383 [00:13<00:49,  4.41it/s] 43%|████▎     | 165/383 [00:13<00:46,  4.68it/s] 43%|████▎     | 164/383 [00:13<00:48,  4.50it/s]
 43%|████▎     | 165/383 [00:13<00:48,  4.52it/s][A 44%|████▍     | 168/383 [00:13<00:25,  8.53it/s] 43%|████▎     | 165/383 [00:13<00:47,  4.57it/s]
 44%|████▍     | 168/383 [00:13<00:25,  8.29it/s][A 45%|████▍     | 171/383 [00:13<00:17, 11.82it/s] 44%|████▍     | 168/383 [00:13<00:25,  8.40it/s]
 45%|████▍     | 171/383 [00:13<00:17, 11.83it/s][A 45%|████▌     | 173/383 [00:13<00:15, 13.41it/s] 43%|████▎     | 165/383 [00:13<00:51,  4.22it/s] 45%|████▍     | 171/383 [00:13<00:17, 11.94it/s] 44%|████▍     | 168/383 [00:13<00:27,  7.88it/s]
 45%|████▌     | 174/383 [00:13<00:14, 14.52it/s][A 46%|████▌     | 175/383 [00:13<00:15, 13.39it/s] 45%|████▌     | 174/383 [00:13<00:14, 14.69it/s] 45%|████▍     | 171/383 [00:13<00:18, 11.31it/s]
 46%|████▌     | 176/383 [00:13<00:14, 14.23it/s][A 46%|████▌     | 177/383 [00:13<00:15, 13.19it/s] 46%|████▌     | 176/383 [00:13<00:14, 14.35it/s] 45%|████▌     | 174/383 [00:13<00:15, 13.62it/s]
 46%|████▋     | 178/383 [00:13<00:14, 14.56it/s][A 47%|████▋     | 179/383 [00:13<00:14, 14.54it/s] 46%|████▋     | 178/383 [00:14<00:13, 14.68it/s]
 47%|████▋     | 180/383 [00:14<00:13, 15.57it/s][A 47%|████▋     | 181/383 [00:14<00:13, 15.35it/s] 46%|████▌     | 176/383 [00:14<00:15, 13.52it/s] 47%|████▋     | 180/383 [00:14<00:12, 15.72it/s]
 48%|████▊     | 182/383 [00:14<00:12, 16.06it/s][A 48%|████▊     | 183/383 [00:14<00:12, 16.20it/s] 46%|████▋     | 178/383 [00:14<00:14, 13.98it/s] 48%|████▊     | 182/383 [00:14<00:12, 16.33it/s]
 48%|████▊     | 184/383 [00:14<00:11, 16.70it/s][A 48%|████▊     | 185/383 [00:14<00:12, 15.98it/s] 47%|████▋     | 180/383 [00:14<00:13, 15.08it/s] 48%|████▊     | 184/383 [00:14<00:11, 16.86it/s]
 49%|████▊     | 186/383 [00:14<00:12, 16.05it/s][A 48%|████▊     | 182/383 [00:14<00:12, 15.76it/s] 49%|████▉     | 187/383 [00:14<00:12, 15.15it/s] 49%|████▊     | 186/383 [00:14<00:12, 15.58it/s]
 49%|████▉     | 188/383 [00:14<00:11, 16.58it/s][A 48%|████▊     | 184/383 [00:14<00:12, 16.56it/s] 50%|████▉     | 190/383 [00:14<00:10, 17.59it/s] 49%|████▉     | 188/383 [00:14<00:12, 16.12it/s]
 50%|████▉     | 191/383 [00:14<00:10, 18.60it/s][A 50%|█████     | 192/383 [00:14<00:10, 17.92it/s] 49%|████▊     | 186/383 [00:14<00:12, 15.41it/s] 50%|████▉     | 191/383 [00:14<00:10, 18.33it/s]
 50%|█████     | 193/383 [00:14<00:10, 18.52it/s][A 51%|█████     | 194/383 [00:14<00:10, 18.13it/s] 49%|████▉     | 188/383 [00:14<00:12, 15.38it/s] 50%|█████     | 193/383 [00:14<00:10, 18.47it/s]
 51%|█████     | 195/383 [00:14<00:10, 18.43it/s][A 51%|█████     | 196/383 [00:14<00:10, 18.44it/s] 51%|█████     | 195/383 [00:14<00:10, 18.54it/s] 50%|████▉     | 191/383 [00:14<00:10, 17.79it/s]
 51%|█████▏    | 197/383 [00:14<00:09, 18.69it/s][A 52%|█████▏    | 199/383 [00:15<00:09, 20.03it/s] 51%|█████▏    | 197/383 [00:15<00:09, 18.85it/s] 50%|█████     | 193/383 [00:15<00:10, 18.10it/s]
 52%|█████▏    | 199/383 [00:15<00:09, 19.04it/s][A 53%|█████▎    | 202/383 [00:15<00:08, 21.82it/s] 52%|█████▏    | 199/383 [00:15<00:09, 19.12it/s] 51%|█████     | 195/383 [00:15<00:10, 18.32it/s]
 53%|█████▎    | 202/383 [00:15<00:08, 21.13it/s][A 54%|█████▎    | 205/383 [00:15<00:07, 23.04it/s] 53%|█████▎    | 202/383 [00:15<00:08, 21.22it/s] 51%|█████▏    | 197/383 [00:15<00:10, 18.47it/s]
 54%|█████▎    | 205/383 [00:15<00:07, 22.78it/s][A 54%|█████▍    | 208/383 [00:15<00:07, 23.89it/s] 54%|█████▎    | 205/383 [00:15<00:07, 22.51it/s] 52%|█████▏    | 200/383 [00:15<00:09, 19.61it/s]
 54%|█████▍    | 208/383 [00:15<00:07, 23.62it/s][A 55%|█████▌    | 211/383 [00:15<00:07, 24.53it/s] 54%|█████▍    | 208/383 [00:15<00:07, 23.49it/s] 53%|█████▎    | 203/383 [00:15<00:08, 21.41it/s]
 55%|█████▌    | 211/383 [00:15<00:07, 24.30it/s][A 56%|█████▌    | 214/383 [00:15<00:06, 24.31it/s] 55%|█████▌    | 211/383 [00:15<00:07, 24.23it/s] 54%|█████▍    | 206/383 [00:15<00:07, 22.76it/s]
 56%|█████▌    | 214/383 [00:15<00:06, 24.72it/s][A 57%|█████▋    | 217/383 [00:15<00:06, 24.80it/s] 56%|█████▌    | 214/383 [00:15<00:06, 24.67it/s] 55%|█████▍    | 209/383 [00:15<00:07, 23.56it/s]
 57%|█████▋    | 217/383 [00:15<00:06, 24.31it/s][A 57%|█████▋    | 220/383 [00:15<00:06, 25.00it/s] 57%|█████▋    | 217/383 [00:15<00:06, 24.86it/s] 55%|█████▌    | 212/383 [00:15<00:07, 24.07it/s]
 57%|█████▋    | 220/383 [00:15<00:06, 24.81it/s][A 57%|█████▋    | 220/383 [00:15<00:06, 25.16it/s] 56%|█████▌    | 215/383 [00:16<00:06, 24.56it/s] 58%|█████▊    | 223/383 [00:16<00:07, 22.17it/s]
 58%|█████▊    | 223/383 [00:16<00:07, 22.78it/s][A 57%|█████▋    | 218/383 [00:16<00:06, 24.91it/s] 58%|█████▊    | 223/383 [00:16<00:07, 22.17it/s] 59%|█████▉    | 226/383 [00:16<00:07, 20.43it/s]
 59%|█████▉    | 226/383 [00:16<00:07, 20.64it/s][A 58%|█████▊    | 221/383 [00:16<00:06, 23.76it/s] 59%|█████▉    | 226/383 [00:16<00:07, 20.16it/s] 60%|█████▉    | 229/383 [00:16<00:07, 19.26it/s]
 60%|█████▉    | 229/383 [00:16<00:08, 19.10it/s][A 58%|█████▊    | 224/383 [00:16<00:07, 20.88it/s] 60%|█████▉    | 229/383 [00:16<00:08, 18.93it/s] 60%|██████    | 231/383 [00:16<00:08, 17.31it/s]
 60%|██████    | 231/383 [00:16<00:08, 17.83it/s][A 59%|█████▉    | 227/383 [00:16<00:08, 19.22it/s] 60%|██████    | 231/383 [00:16<00:08, 18.03it/s] 61%|██████    | 233/383 [00:16<00:09, 15.98it/s]
 61%|██████    | 233/383 [00:16<00:09, 16.35it/s][A 61%|██████    | 233/383 [00:16<00:09, 16.27it/s] 60%|██████    | 230/383 [00:16<00:08, 18.42it/s] 61%|██████▏   | 235/383 [00:16<00:09, 15.02it/s]
 61%|██████▏   | 235/383 [00:16<00:09, 15.32it/s][A 61%|██████    | 232/383 [00:16<00:08, 16.78it/s] 61%|██████▏   | 235/383 [00:16<00:09, 14.83it/s] 62%|██████▏   | 237/383 [00:17<00:10, 14.22it/s]
 62%|██████▏   | 237/383 [00:17<00:10, 14.47it/s][A 61%|██████    | 234/383 [00:17<00:09, 15.75it/s] 62%|██████▏   | 237/383 [00:17<00:10, 14.22it/s] 62%|██████▏   | 239/383 [00:17<00:10, 13.69it/s]
 62%|██████▏   | 239/383 [00:17<00:10, 13.98it/s][A 62%|██████▏   | 236/383 [00:17<00:09, 14.98it/s] 62%|██████▏   | 239/383 [00:17<00:10, 13.82it/s] 63%|██████▎   | 241/383 [00:17<00:10, 13.32it/s]
 63%|██████▎   | 241/383 [00:17<00:10, 13.64it/s][A 62%|██████▏   | 238/383 [00:17<00:10, 14.36it/s] 63%|██████▎   | 241/383 [00:17<00:10, 13.34it/s] 63%|██████▎   | 243/383 [00:17<00:10, 13.18it/s]
 63%|██████▎   | 243/383 [00:17<00:10, 13.29it/s][A 63%|██████▎   | 240/383 [00:17<00:10, 14.05it/s] 63%|██████▎   | 243/383 [00:17<00:10, 13.20it/s] 64%|██████▍   | 245/383 [00:17<00:10, 12.96it/s]
 64%|██████▍   | 245/383 [00:17<00:10, 13.27it/s][A 63%|██████▎   | 242/383 [00:17<00:10, 13.69it/s] 64%|██████▍   | 245/383 [00:17<00:10, 13.06it/s] 64%|██████▍   | 247/383 [00:17<00:10, 13.08it/s]
 64%|██████▍   | 247/383 [00:17<00:10, 13.27it/s][A 64%|██████▎   | 244/383 [00:17<00:10, 13.60it/s] 64%|██████▍   | 247/383 [00:17<00:10, 12.87it/s] 65%|██████▌   | 249/383 [00:17<00:10, 13.01it/s]
 65%|██████▌   | 249/383 [00:17<00:10, 12.83it/s][A 64%|██████▍   | 246/383 [00:18<00:10, 13.28it/s] 65%|██████▌   | 249/383 [00:18<00:10, 12.87it/s] 66%|██████▌   | 251/383 [00:18<00:10, 12.64it/s]
 66%|██████▌   | 251/383 [00:18<00:10, 12.97it/s][A 65%|██████▍   | 248/383 [00:18<00:10, 13.14it/s] 66%|██████▌   | 251/383 [00:18<00:10, 12.96it/s] 66%|██████▌   | 253/383 [00:18<00:10, 12.70it/s]
 66%|██████▌   | 253/383 [00:18<00:10, 12.96it/s][A 65%|██████▌   | 250/383 [00:18<00:10, 13.03it/s] 66%|██████▌   | 253/383 [00:18<00:10, 12.67it/s] 67%|██████▋   | 255/383 [00:18<00:09, 12.91it/s]
 67%|██████▋   | 255/383 [00:18<00:09, 13.06it/s][A 66%|██████▌   | 252/383 [00:18<00:10, 12.87it/s] 67%|██████▋   | 255/383 [00:18<00:10, 12.64it/s] 67%|██████▋   | 257/383 [00:18<00:09, 13.10it/s]
 67%|██████▋   | 257/383 [00:18<00:09, 13.16it/s][A 66%|██████▋   | 254/383 [00:18<00:10, 12.76it/s] 67%|██████▋   | 257/383 [00:18<00:09, 12.94it/s] 68%|██████▊   | 259/383 [00:18<00:09, 13.37it/s]
 68%|██████▊   | 259/383 [00:18<00:09, 13.28it/s][A 67%|██████▋   | 256/383 [00:18<00:09, 12.87it/s] 68%|██████▊   | 259/383 [00:18<00:09, 13.06it/s] 68%|██████▊   | 261/383 [00:18<00:09, 13.23it/s]
 68%|██████▊   | 261/383 [00:18<00:09, 13.49it/s][A 67%|██████▋   | 258/383 [00:18<00:09, 12.97it/s] 68%|██████▊   | 261/383 [00:19<00:09, 13.09it/s] 69%|██████▊   | 263/383 [00:19<00:08, 13.43it/s]
 69%|██████▊   | 263/383 [00:19<00:08, 13.47it/s][A 68%|██████▊   | 260/383 [00:19<00:09, 13.13it/s] 69%|██████▉   | 266/383 [00:19<00:07, 16.40it/s]
 69%|██████▊   | 263/383 [00:19<00:09, 13.17it/s] 69%|██████▉   | 266/383 [00:19<00:07, 16.05it/s][A 70%|███████   | 269/383 [00:19<00:06, 18.77it/s] 68%|██████▊   | 262/383 [00:19<00:09, 13.27it/s]
 70%|███████   | 269/383 [00:19<00:06, 18.55it/s][A 69%|██████▉   | 266/383 [00:19<00:07, 15.70it/s] 69%|██████▉   | 264/383 [00:19<00:08, 14.38it/s]
 71%|███████   | 272/383 [00:19<00:05, 19.77it/s][A 71%|███████   | 272/383 [00:19<00:05, 19.01it/s] 70%|███████   | 269/383 [00:19<00:06, 18.04it/s] 70%|██████▉   | 267/383 [00:19<00:07, 16.45it/s] 72%|███████▏  | 274/383 [00:19<00:06, 17.69it/s] 71%|███████   | 272/383 [00:19<00:05, 19.10it/s]
 72%|███████▏  | 275/383 [00:19<00:05, 18.50it/s][A 70%|███████   | 270/383 [00:19<00:06, 18.83it/s] 72%|███████▏  | 274/383 [00:19<00:06, 17.84it/s] 72%|███████▏  | 276/383 [00:19<00:06, 16.51it/s]
 72%|███████▏  | 277/383 [00:19<00:05, 17.72it/s][A 71%|███████▏  | 273/383 [00:19<00:05, 19.42it/s] 72%|███████▏  | 276/383 [00:19<00:06, 17.42it/s] 73%|███████▎  | 278/383 [00:19<00:06, 16.33it/s]
 73%|███████▎  | 279/383 [00:19<00:06, 16.87it/s][A 72%|███████▏  | 275/383 [00:19<00:05, 18.46it/s] 73%|███████▎  | 280/383 [00:19<00:06, 16.17it/s] 73%|███████▎  | 278/383 [00:19<00:06, 16.56it/s]
 73%|███████▎  | 281/383 [00:19<00:06, 16.18it/s][A 72%|███████▏  | 277/383 [00:20<00:05, 17.75it/s] 73%|███████▎  | 280/383 [00:20<00:06, 16.28it/s] 74%|███████▎  | 282/383 [00:20<00:07, 13.86it/s]
 74%|███████▍  | 283/383 [00:20<00:07, 14.27it/s][A 73%|███████▎  | 279/383 [00:20<00:06, 16.43it/s] 74%|███████▎  | 282/383 [00:20<00:07, 14.12it/s] 74%|███████▍  | 284/383 [00:20<00:07, 12.93it/s] 73%|███████▎  | 281/383 [00:20<00:06, 14.67it/s]
 74%|███████▍  | 285/383 [00:20<00:07, 12.89it/s][A 74%|███████▍  | 284/383 [00:20<00:07, 12.77it/s] 75%|███████▍  | 286/383 [00:20<00:07, 12.21it/s] 74%|███████▍  | 283/383 [00:20<00:07, 13.05it/s]
 75%|███████▍  | 287/383 [00:20<00:07, 12.08it/s][A 75%|███████▍  | 286/383 [00:20<00:08, 12.12it/s] 75%|███████▌  | 288/383 [00:20<00:07, 11.90it/s] 74%|███████▍  | 285/383 [00:20<00:07, 12.67it/s] 75%|███████▌  | 288/383 [00:20<00:08, 11.38it/s] 75%|███████▍  | 287/383 [00:20<00:07, 12.33it/s]
 75%|███████▌  | 289/383 [00:20<00:10,  8.76it/s][A 76%|███████▌  | 290/383 [00:21<00:12,  7.74it/s] 75%|███████▌  | 289/383 [00:21<00:10,  9.27it/s] 76%|███████▌  | 290/383 [00:21<00:12,  7.31it/s] 76%|███████▌  | 291/383 [00:21<00:13,  6.62it/s]
 76%|███████▌  | 291/383 [00:21<00:14,  6.29it/s][A 76%|███████▌  | 291/383 [00:21<00:14,  6.22it/s] 76%|███████▌  | 292/383 [00:21<00:15,  5.83it/s]
 76%|███████▌  | 292/383 [00:21<00:16,  5.59it/s][A 76%|███████▌  | 291/383 [00:21<00:14,  6.39it/s] 76%|███████▌  | 292/383 [00:21<00:17,  5.33it/s] 77%|███████▋  | 293/383 [00:21<00:17,  5.12it/s]
 77%|███████▋  | 293/383 [00:22<00:18,  4.92it/s][A 76%|███████▌  | 292/383 [00:22<00:16,  5.62it/s] 77%|███████▋  | 293/383 [00:22<00:18,  4.91it/s] 77%|███████▋  | 294/383 [00:22<00:18,  4.79it/s]
 77%|███████▋  | 294/383 [00:22<00:19,  4.60it/s][A 77%|███████▋  | 293/383 [00:22<00:17,  5.10it/s] 77%|███████▋  | 295/383 [00:22<00:18,  4.71it/s] 77%|███████▋  | 294/383 [00:22<00:19,  4.49it/s]
 77%|███████▋  | 295/383 [00:22<00:19,  4.42it/s][A 77%|███████▋  | 294/383 [00:22<00:18,  4.73it/s] 77%|███████▋  | 296/383 [00:22<00:20,  4.26it/s] 77%|███████▋  | 295/383 [00:22<00:20,  4.20it/s] 77%|███████▋  | 295/383 [00:22<00:19,  4.54it/s]
 77%|███████▋  | 296/383 [00:22<00:20,  4.18it/s][A 78%|███████▊  | 297/383 [00:22<00:19,  4.31it/s] 77%|███████▋  | 296/383 [00:23<00:21,  4.07it/s] 77%|███████▋  | 296/383 [00:23<00:20,  4.26it/s]
 78%|███████▊  | 297/383 [00:23<00:21,  3.93it/s][A 78%|███████▊  | 298/383 [00:23<00:20,  4.10it/s] 78%|███████▊  | 297/383 [00:23<00:21,  4.08it/s]
 78%|███████▊  | 298/383 [00:23<00:21,  4.03it/s][A 78%|███████▊  | 297/383 [00:23<00:21,  4.06it/s] 78%|███████▊  | 298/383 [00:23<00:19,  4.28it/s] 78%|███████▊  | 299/383 [00:23<00:21,  3.95it/s]
 78%|███████▊  | 299/383 [00:23<00:20,  4.01it/s][A 78%|███████▊  | 298/383 [00:23<00:22,  3.80it/s] 78%|███████▊  | 300/383 [00:23<00:20,  4.06it/s] 78%|███████▊  | 299/383 [00:23<00:20,  4.06it/s]
 78%|███████▊  | 300/383 [00:23<00:21,  3.86it/s][A 78%|███████▊  | 299/383 [00:23<00:22,  3.78it/s] 79%|███████▊  | 301/383 [00:24<00:20,  3.92it/s] 78%|███████▊  | 300/383 [00:24<00:21,  3.89it/s]
 79%|███████▊  | 301/383 [00:24<00:21,  3.78it/s][A 79%|███████▊  | 301/383 [00:24<00:20,  4.03it/s] 78%|███████▊  | 300/383 [00:24<00:22,  3.71it/s] 79%|███████▉  | 302/383 [00:24<00:21,  3.85it/s]
 79%|███████▉  | 302/383 [00:24<00:21,  3.74it/s][A 79%|███████▉  | 303/383 [00:24<00:21,  3.80it/s] 79%|███████▉  | 302/383 [00:24<00:21,  3.82it/s] 79%|███████▊  | 301/383 [00:24<00:22,  3.65it/s]
 79%|███████▉  | 303/383 [00:24<00:20,  3.93it/s][A 79%|███████▉  | 303/383 [00:24<00:20,  3.97it/s] 79%|███████▉  | 302/383 [00:24<00:22,  3.62it/s] 79%|███████▉  | 304/383 [00:24<00:21,  3.68it/s]
 79%|███████▉  | 304/383 [00:24<00:21,  3.76it/s][A 79%|███████▉  | 304/383 [00:25<00:19,  4.00it/s] 80%|███████▉  | 305/383 [00:25<00:20,  3.83it/s] 79%|███████▉  | 303/383 [00:25<00:22,  3.63it/s] 80%|███████▉  | 306/383 [00:25<00:18,  4.09it/s]
 80%|███████▉  | 305/383 [00:25<00:21,  3.63it/s][A 80%|███████▉  | 305/383 [00:25<00:20,  3.85it/s] 79%|███████▉  | 304/383 [00:25<00:21,  3.62it/s]
 80%|███████▉  | 306/383 [00:25<00:20,  3.67it/s][A 80%|████████  | 307/383 [00:25<00:19,  3.95it/s] 80%|███████▉  | 306/383 [00:25<00:20,  3.81it/s] 80%|███████▉  | 305/383 [00:25<00:21,  3.63it/s]
 80%|████████  | 307/383 [00:25<00:20,  3.65it/s][A 80%|████████  | 308/383 [00:25<00:19,  3.81it/s] 80%|████████  | 307/383 [00:25<00:20,  3.73it/s] 80%|███████▉  | 306/383 [00:25<00:21,  3.62it/s] 80%|████████  | 308/383 [00:26<00:19,  3.87it/s] 81%|████████  | 309/383 [00:26<00:19,  3.85it/s]
 80%|████████  | 308/383 [00:26<00:20,  3.67it/s][A 80%|████████  | 307/383 [00:26<00:21,  3.59it/s]
 81%|████████  | 309/383 [00:26<00:19,  3.76it/s][A 81%|████████  | 309/383 [00:26<00:19,  3.79it/s] 81%|████████  | 310/383 [00:26<00:19,  3.70it/s] 80%|████████  | 308/383 [00:26<00:20,  3.57it/s]
 81%|████████  | 310/383 [00:26<00:18,  3.86it/s][A 81%|████████  | 310/383 [00:26<00:19,  3.71it/s] 81%|████████  | 311/383 [00:26<00:19,  3.64it/s] 81%|████████  | 309/383 [00:26<00:19,  3.80it/s]
 81%|████████  | 311/383 [00:26<00:18,  3.83it/s][A 81%|████████  | 311/383 [00:26<00:19,  3.69it/s] 81%|████████▏ | 312/383 [00:26<00:19,  3.63it/s] 81%|████████  | 310/383 [00:26<00:18,  3.90it/s]
 81%|████████▏ | 312/383 [00:27<00:18,  3.86it/s][A 81%|████████▏ | 312/383 [00:27<00:19,  3.64it/s] 82%|████████▏ | 313/383 [00:27<00:19,  3.64it/s] 81%|████████  | 311/383 [00:27<00:18,  3.81it/s]
 82%|████████▏ | 313/383 [00:27<00:19,  3.66it/s][A 82%|████████▏ | 313/383 [00:27<00:18,  3.69it/s] 81%|████████▏ | 312/383 [00:27<00:18,  3.88it/s] 82%|████████▏ | 314/383 [00:27<00:19,  3.61it/s]
 82%|████████▏ | 314/383 [00:27<00:18,  3.68it/s][A 82%|████████▏ | 314/383 [00:27<00:18,  3.79it/s] 82%|████████▏ | 313/383 [00:27<00:17,  3.91it/s] 82%|████████▏ | 315/383 [00:27<00:19,  3.51it/s]
 82%|████████▏ | 315/383 [00:27<00:18,  3.67it/s][A 82%|████████▏ | 315/383 [00:27<00:17,  3.84it/s] 82%|████████▏ | 314/383 [00:27<00:17,  3.96it/s] 83%|████████▎ | 316/383 [00:28<00:18,  3.64it/s] 82%|████████▏ | 315/383 [00:28<00:16,  4.09it/s]
 83%|████████▎ | 316/383 [00:28<00:18,  3.71it/s][A 83%|████████▎ | 316/383 [00:28<00:17,  3.81it/s] 83%|████████▎ | 317/383 [00:28<00:18,  3.64it/s]
 83%|████████▎ | 317/383 [00:28<00:17,  3.70it/s][A 83%|████████▎ | 316/383 [00:28<00:17,  3.80it/s] 83%|████████▎ | 317/383 [00:28<00:18,  3.63it/s] 83%|████████▎ | 318/383 [00:28<00:18,  3.51it/s] 83%|████████▎ | 317/383 [00:28<00:17,  3.75it/s]
 83%|████████▎ | 318/383 [00:28<00:18,  3.55it/s][A 83%|████████▎ | 318/383 [00:28<00:17,  3.61it/s] 83%|████████▎ | 319/383 [00:28<00:17,  3.70it/s]
 83%|████████▎ | 319/383 [00:29<00:17,  3.68it/s][A 83%|████████▎ | 318/383 [00:29<00:17,  3.71it/s] 83%|████████▎ | 319/383 [00:29<00:17,  3.67it/s] 84%|████████▎ | 320/383 [00:29<00:17,  3.69it/s]
 84%|████████▎ | 320/383 [00:29<00:16,  3.76it/s][A 83%|████████▎ | 319/383 [00:29<00:17,  3.71it/s] 84%|████████▎ | 320/383 [00:29<00:17,  3.69it/s] 84%|████████▍ | 321/383 [00:29<00:17,  3.60it/s]
 84%|████████▍ | 321/383 [00:29<00:16,  3.77it/s][A 84%|████████▍ | 321/383 [00:29<00:16,  3.73it/s] 84%|████████▎ | 320/383 [00:29<00:17,  3.68it/s] 84%|████████▍ | 322/383 [00:29<00:16,  3.63it/s]
 84%|████████▍ | 322/383 [00:29<00:15,  3.85it/s][A 84%|████████▍ | 321/383 [00:29<00:17,  3.59it/s] 84%|████████▍ | 322/383 [00:29<00:16,  3.59it/s] 84%|████████▍ | 323/383 [00:29<00:15,  3.75it/s]
 84%|████████▍ | 323/383 [00:30<00:15,  3.98it/s][A 84%|████████▍ | 322/383 [00:30<00:16,  3.78it/s] 84%|████████▍ | 323/383 [00:30<00:16,  3.73it/s] 85%|████████▍ | 324/383 [00:30<00:15,  3.71it/s]
 85%|████████▍ | 324/383 [00:30<00:15,  3.76it/s][A 84%|████████▍ | 323/383 [00:30<00:16,  3.72it/s] 85%|████████▍ | 324/383 [00:30<00:15,  3.72it/s] 85%|████████▍ | 325/383 [00:30<00:15,  3.80it/s]
 85%|████████▍ | 325/383 [00:30<00:15,  3.85it/s][A 85%|████████▍ | 325/383 [00:30<00:15,  3.82it/s] 85%|████████▍ | 324/383 [00:30<00:16,  3.68it/s] 85%|████████▌ | 326/383 [00:30<00:14,  3.96it/s]
 85%|████████▌ | 326/383 [00:30<00:15,  3.75it/s][A 85%|████████▌ | 326/383 [00:30<00:14,  3.85it/s] 85%|████████▍ | 325/383 [00:30<00:15,  3.67it/s] 85%|████████▌ | 327/383 [00:30<00:14,  3.89it/s]
 85%|████████▌ | 327/383 [00:31<00:14,  3.74it/s][A 85%|████████▌ | 326/383 [00:31<00:14,  3.82it/s] 85%|████████▌ | 327/383 [00:31<00:15,  3.67it/s] 86%|████████▌ | 328/383 [00:31<00:14,  3.79it/s]
 86%|████████▌ | 328/383 [00:31<00:14,  3.67it/s][A 85%|████████▌ | 327/383 [00:31<00:14,  3.89it/s] 86%|████████▌ | 328/383 [00:31<00:15,  3.64it/s] 86%|████████▌ | 329/383 [00:31<00:14,  3.80it/s]
 86%|████████▌ | 329/383 [00:31<00:14,  3.74it/s][A 86%|████████▌ | 328/383 [00:31<00:14,  3.88it/s] 86%|████████▌ | 329/383 [00:31<00:14,  3.76it/s] 86%|████████▌ | 330/383 [00:31<00:14,  3.74it/s]
 86%|████████▌ | 330/383 [00:31<00:14,  3.75it/s][A 86%|████████▋ | 331/383 [00:31<00:12,  4.25it/s] 86%|████████▌ | 329/383 [00:31<00:13,  3.89it/s] 86%|████████▌ | 330/383 [00:32<00:14,  3.64it/s] 87%|████████▋ | 332/383 [00:32<00:10,  4.84it/s] 86%|████████▌ | 330/383 [00:32<00:12,  4.13it/s] 86%|████████▋ | 331/383 [00:32<00:12,  4.27it/s]
 86%|████████▋ | 331/383 [00:32<00:13,  3.73it/s][A 87%|████████▋ | 333/383 [00:32<00:09,  5.33it/s] 87%|████████▋ | 332/383 [00:32<00:11,  4.60it/s]
 87%|████████▋ | 332/383 [00:32<00:12,  4.22it/s][A 87%|████████▋ | 334/383 [00:32<00:08,  5.62it/s] 86%|████████▋ | 331/383 [00:32<00:13,  3.99it/s]
 87%|████████▋ | 333/383 [00:32<00:10,  4.70it/s][A 87%|████████▋ | 333/383 [00:32<00:10,  4.85it/s] 87%|████████▋ | 335/383 [00:32<00:08,  5.81it/s] 87%|████████▋ | 332/383 [00:32<00:11,  4.37it/s]
 87%|████████▋ | 334/383 [00:32<00:09,  5.06it/s][A 88%|████████▊ | 336/383 [00:32<00:07,  5.94it/s] 87%|████████▋ | 334/383 [00:32<00:09,  5.09it/s] 87%|████████▋ | 333/383 [00:32<00:10,  4.96it/s] 87%|████████▋ | 335/383 [00:32<00:08,  5.63it/s] 88%|████████▊ | 337/383 [00:32<00:07,  6.11it/s]
 87%|████████▋ | 335/383 [00:32<00:09,  5.25it/s][A 87%|████████▋ | 334/383 [00:32<00:09,  5.43it/s] 88%|████████▊ | 336/383 [00:33<00:08,  5.73it/s] 88%|████████▊ | 338/383 [00:33<00:07,  5.92it/s]
 88%|████████▊ | 336/383 [00:33<00:08,  5.25it/s][A 87%|████████▋ | 335/383 [00:33<00:08,  5.49it/s] 88%|████████▊ | 337/383 [00:33<00:07,  5.89it/s] 89%|████████▉ | 340/383 [00:33<00:05,  7.69it/s] 88%|████████▊ | 336/383 [00:33<00:07,  5.89it/s]
 88%|████████▊ | 337/383 [00:33<00:08,  5.39it/s][A 88%|████████▊ | 338/383 [00:33<00:07,  6.01it/s] 89%|████████▉ | 342/383 [00:33<00:04,  9.47it/s] 88%|████████▊ | 337/383 [00:33<00:07,  6.11it/s]
 88%|████████▊ | 338/383 [00:33<00:08,  5.49it/s][A 89%|████████▉ | 340/383 [00:33<00:05,  8.40it/s] 90%|████████▉ | 344/383 [00:33<00:03, 10.79it/s] 88%|████████▊ | 338/383 [00:33<00:07,  6.17it/s]
 89%|████████▊ | 339/383 [00:33<00:07,  5.73it/s][A 89%|████████▉ | 342/383 [00:33<00:04,  9.83it/s] 90%|█████████ | 346/383 [00:33<00:03, 11.54it/s] 89%|████████▉ | 340/383 [00:33<00:05,  8.26it/s]
 89%|████████▉ | 341/383 [00:33<00:05,  7.90it/s][A 90%|████████▉ | 344/383 [00:33<00:03, 11.11it/s] 91%|█████████ | 348/383 [00:33<00:02, 12.47it/s] 89%|████████▉ | 342/383 [00:33<00:04,  9.96it/s]
 90%|████████▉ | 343/383 [00:33<00:04,  9.62it/s][A 90%|█████████ | 346/383 [00:33<00:03, 12.21it/s] 91%|█████████▏| 350/383 [00:33<00:02, 13.35it/s]
 90%|█████████ | 345/383 [00:33<00:03, 11.04it/s][A 90%|████████▉ | 344/383 [00:33<00:03, 10.54it/s] 91%|█████████ | 348/383 [00:34<00:02, 12.99it/s] 92%|█████████▏| 352/383 [00:34<00:02, 13.38it/s]
 91%|█████████ | 347/383 [00:34<00:02, 12.18it/s][A 90%|█████████ | 346/383 [00:34<00:03, 11.91it/s] 91%|█████████▏| 350/383 [00:34<00:02, 13.07it/s] 92%|█████████▏| 354/383 [00:34<00:02, 13.70it/s] 91%|█████████ | 348/383 [00:34<00:02, 12.86it/s]
 91%|█████████ | 349/383 [00:34<00:02, 12.89it/s][A 92%|█████████▏| 352/383 [00:34<00:02, 13.79it/s] 93%|█████████▎| 356/383 [00:34<00:01, 14.48it/s] 91%|█████████▏| 350/383 [00:34<00:02, 13.39it/s]
 92%|█████████▏| 351/383 [00:34<00:02, 13.29it/s][A 93%|█████████▎| 358/383 [00:34<00:01, 15.29it/s] 92%|█████████▏| 354/383 [00:34<00:02, 14.08it/s] 92%|█████████▏| 352/383 [00:34<00:02, 13.67it/s]
 92%|█████████▏| 353/383 [00:34<00:02, 13.27it/s][A 93%|█████████▎| 356/383 [00:34<00:01, 14.37it/s] 92%|█████████▏| 354/383 [00:34<00:02, 13.76it/s]
 93%|█████████▎| 355/383 [00:34<00:02, 13.58it/s][A 93%|█████████▎| 358/383 [00:34<00:01, 14.75it/s]
 93%|█████████▎| 357/383 [00:34<00:01, 14.40it/s][A 94%|█████████▍| 360/383 [00:34<00:02,  9.74it/s] 93%|█████████▎| 356/383 [00:34<00:01, 13.63it/s]
 94%|█████████▎| 359/383 [00:34<00:01, 15.45it/s][A 93%|█████████▎| 358/383 [00:34<00:01, 14.87it/s] 94%|█████████▍| 360/383 [00:35<00:02,  9.97it/s] 95%|█████████▍| 362/383 [00:35<00:02,  8.33it/s] 94%|█████████▍| 360/383 [00:35<00:01, 11.85it/s]
 94%|█████████▍| 361/383 [00:35<00:02, 10.16it/s][A 95%|█████████▍| 362/383 [00:35<00:02,  8.17it/s] 95%|█████████▌| 364/383 [00:35<00:02,  7.20it/s] 95%|█████████▍| 362/383 [00:35<00:02,  9.10it/s]
 95%|█████████▍| 363/383 [00:35<00:02,  8.47it/s][A 95%|█████████▌| 365/383 [00:35<00:02,  6.86it/s] 95%|█████████▌| 364/383 [00:35<00:02,  7.14it/s] 96%|█████████▌| 367/383 [00:35<00:01,  8.69it/s] 95%|█████████▌| 364/383 [00:35<00:02,  7.81it/s] 96%|█████████▋| 369/383 [00:35<00:01, 10.47it/s] 95%|█████████▌| 365/383 [00:35<00:02,  6.85it/s]
 95%|█████████▌| 365/383 [00:35<00:02,  7.44it/s][A 97%|█████████▋| 371/383 [00:35<00:00, 12.16it/s] 95%|█████████▌| 365/383 [00:36<00:02,  7.36it/s] 96%|█████████▌| 367/383 [00:36<00:01,  8.61it/s]
 96%|█████████▌| 366/383 [00:36<00:02,  7.38it/s][A 97%|█████████▋| 373/383 [00:36<00:00, 13.67it/s] 96%|█████████▌| 367/383 [00:36<00:01,  9.24it/s] 96%|█████████▋| 369/383 [00:36<00:01, 10.40it/s]
 96%|█████████▌| 368/383 [00:36<00:01,  9.20it/s][A 98%|█████████▊| 375/383 [00:36<00:00, 14.64it/s] 96%|█████████▋| 369/383 [00:36<00:01, 10.96it/s] 97%|█████████▋| 371/383 [00:36<00:00, 12.04it/s]
 97%|█████████▋| 370/383 [00:36<00:01, 10.89it/s][A 98%|█████████▊| 377/383 [00:36<00:00, 15.82it/s] 97%|█████████▋| 371/383 [00:36<00:00, 12.44it/s] 97%|█████████▋| 373/383 [00:36<00:00, 13.42it/s]
 97%|█████████▋| 372/383 [00:36<00:00, 12.34it/s][A 99%|█████████▉| 380/383 [00:36<00:00, 18.26it/s] 97%|█████████▋| 373/383 [00:36<00:00, 13.83it/s] 98%|█████████▊| 375/383 [00:36<00:00, 14.87it/s]
 98%|█████████▊| 374/383 [00:36<00:00, 13.79it/s][A100%|██████████| 383/383 [00:36<00:00, 19.08it/s]100%|██████████| 383/383 [00:36<00:00, 10.47it/s]
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
 98%|█████████▊| 376/383 [00:36<00:00, 15.79it/s] 99%|█████████▊| 378/383 [00:36<00:00, 16.91it/s]/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)

/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
 98%|█████████▊| 377/383 [00:36<00:00, 15.74it/s][A/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/pytorch/torch/utils/checkpoint.py:426: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 99%|█████████▉| 379/383 [00:36<00:00, 18.38it/s] 99%|█████████▉| 381/383 [00:36<00:00, 19.49it/s]
 99%|█████████▉| 380/383 [00:36<00:00, 17.69it/s][A100%|█████████▉| 382/383 [00:36<00:00, 20.59it/s]100%|██████████| 383/383 [00:36<00:00, 10.40it/s]
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
100%|██████████| 383/383 [00:36<00:00, 10.38it/s]
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/pytorch/torch/utils/checkpoint.py:426: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

100%|██████████| 383/383 [00:36<00:00, 18.41it/s][A100%|██████████| 383/383 [00:36<00:00, 10.36it/s]
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
                                                    {'mmlu_loss': 2.482988081775197, 'mmlu_eval_accuracy_conceptual_physics': 0.34615384615384615, 'mmlu_eval_accuracy_logical_fallacies': nan, 'mmlu_eval_accuracy_professional_psychology': nan, 'mmlu_eval_accuracy_public_relations': nan, 'mmlu_eval_accuracy_security_studies': nan, 'mmlu_eval_accuracy_high_school_physics': nan, 'mmlu_eval_accuracy_formal_logic': 0.07142857142857142, 'mmlu_eval_accuracy_anatomy': 0.21428571428571427, 'mmlu_eval_accuracy_moral_scenarios': nan, 'mmlu_eval_accuracy_moral_disputes': nan, 'mmlu_eval_accuracy_management': nan, 'mmlu_eval_accuracy_high_school_european_history': 0.2777777777777778, 'mmlu_eval_accuracy_professional_law': nan, 'mmlu_eval_accuracy_elementary_mathematics': 0.3902439024390244, 'mmlu_eval_accuracy_high_school_computer_science': 0.0, 'mmlu_eval_accuracy_high_school_geography': 0.5833333333333334, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy_high_school_biology': 0.25, 'mmlu_eval_accuracy_philosophy': nan, 'mmlu_eval_accuracy_astronomy': 0.3125, 'mmlu_eval_accuracy_nutrition': nan, 'mmlu_eval_accuracy_high_school_government_and_politics': nan, 'mmlu_eval_accuracy_medical_genetics': nan, 'mmlu_eval_accuracy_human_aging': nan, 'mmlu_eval_accuracy_human_sexuality': nan, 'mmlu_eval_accuracy_college_medicine': 0.2727272727272727, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_college_physics': 0.18181818181818182, 'mmlu_eval_accuracy_us_foreign_policy': nan, 'mmlu_eval_accuracy_college_computer_science': 0.36363636363636365, 'mmlu_eval_accuracy_machine_learning': nan, 'mmlu_eval_accuracy_professional_accounting': nan, 'mmlu_eval_accuracy_world_religions': nan, 'mmlu_eval_accuracy_miscellaneous': nan, 'mmlu_eval_accuracy_jurisprudence': nan, 'mmlu_eval_accuracy_high_school_chemistry': 0.4090909090909091, 'mmlu_eval_accuracy_virology': nan, 'mmlu_eval_accuracy_high_school_mathematics': nan, 'mmlu_eval_accuracy_electrical_engineering': 0.25, 'mmlu_eval_accuracy_high_school_world_history': nan, 'mmlu_eval_accuracy_prehistory': nan, 'mmlu_eval_accuracy_high_school_statistics': nan, 'mmlu_eval_accuracy_high_school_psychology': nan, 'mmlu_eval_accuracy_high_school_microeconomics': nan, 'mmlu_eval_accuracy_high_school_us_history': nan, 'mmlu_eval_accuracy_college_biology': 0.3125, 'mmlu_eval_accuracy_high_school_macroeconomics': nan, 'mmlu_eval_accuracy_professional_medicine': nan, 'mmlu_eval_accuracy_clinical_knowledge': 0.27586206896551724, 'mmlu_eval_accuracy_business_ethics': 0.18181818181818182, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_econometrics': 0.75, 'mmlu_eval_accuracy_marketing': nan, 'mmlu_eval_accuracy_international_law': nan, 'mmlu_eval_accuracy_sociology': nan, 'mmlu_eval_accuracy_computer_security': 0.09090909090909091, 'mmlu_eval_accuracy_college_mathematics': 0.09090909090909091, 'mmlu_eval_accuracy': nan, 'epoch': 3.71}
 25%|██▌       | 512/2048 [49:19<1:47:42,  4.21s/it]/home/bagus/pytorch/torch/utils/checkpoint.py:426: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/pytorch/torch/utils/checkpoint.py:426: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 25%|██▌       | 513/2048 [49:23<8:20:38, 19.57s/it] 25%|██▌       | 514/2048 [49:27<6:19:44, 14.85s/it] 25%|██▌       | 515/2048 [49:32<5:05:17, 11.95s/it] 25%|██▌       | 516/2048 [49:37<4:13:08,  9.91s/it] 25%|██▌       | 517/2048 [49:42<3:36:30,  8.48s/it] 25%|██▌       | 518/2048 [49:47<3:10:13,  7.46s/it] 25%|██▌       | 519/2048 [49:52<2:49:44,  6.66s/it] 25%|██▌       | 520/2048 [49:57<2:33:23,  6.02s/it] 25%|██▌       | 521/2048 [50:01<2:20:31,  5.52s/it] 25%|██▌       | 522/2048 [50:05<2:10:04,  5.11s/it] 26%|██▌       | 523/2048 [50:09<2:00:21,  4.74s/it] 26%|██▌       | 524/2048 [50:13<1:53:20,  4.46s/it] 26%|██▌       | 525/2048 [50:16<1:46:59,  4.21s/it] 26%|██▌       | 526/2048 [50:20<1:39:21,  3.92s/it] 26%|██▌       | 527/2048 [50:25<1:46:44,  4.21s/it] 26%|██▌       | 528/2048 [50:30<1:53:58,  4.50s/it]                                                    {'loss': 0.5422, 'learning_rate': 0.00017403865070446335, 'epoch': 3.83}
 26%|██▌       | 528/2048 [50:30<1:53:58,  4.50s/it] 26%|██▌       | 529/2048 [50:35<1:58:55,  4.70s/it] 26%|██▌       | 530/2048 [50:40<2:01:48,  4.81s/it] 26%|██▌       | 531/2048 [50:45<2:01:32,  4.81s/it] 26%|██▌       | 532/2048 [50:49<2:01:05,  4.79s/it] 26%|██▌       | 533/2048 [50:54<1:57:36,  4.66s/it] 26%|██▌       | 534/2048 [50:58<1:53:48,  4.51s/it] 26%|██▌       | 535/2048 [51:02<1:49:17,  4.33s/it] 26%|██▌       | 536/2048 [51:06<1:44:38,  4.15s/it] 26%|██▌       | 537/2048 [51:09<1:42:05,  4.05s/it] 26%|██▋       | 538/2048 [51:13<1:37:14,  3.86s/it] 26%|██▋       | 539/2048 [51:17<1:36:44,  3.85s/it] 26%|██▋       | 540/2048 [51:22<1:46:39,  4.24s/it] 26%|██▋       | 541/2048 [51:27<1:53:32,  4.52s/it] 26%|██▋       | 542/2048 [51:32<1:58:11,  4.71s/it] 27%|██▋       | 543/2048 [51:37<2:00:19,  4.80s/it] 27%|██▋       | 544/2048 [51:42<1:59:57,  4.79s/it]                                                    {'loss': 0.5705, 'learning_rate': 0.00017231383424743814, 'epoch': 3.95}
 27%|██▋       | 544/2048 [51:42<1:59:57,  4.79s/it] 27%|██▋       | 545/2048 [51:46<1:57:29,  4.69s/it] 27%|██▋       | 546/2048 [51:51<1:56:09,  4.64s/it] 27%|██▋       | 547/2048 [51:55<1:51:34,  4.46s/it] 27%|██▋       | 548/2048 [51:59<1:48:42,  4.35s/it] 27%|██▋       | 549/2048 [52:03<1:45:04,  4.21s/it] 27%|██▋       | 550/2048 [52:07<1:43:47,  4.16s/it] 27%|██▋       | 551/2048 [52:10<1:38:16,  3.94s/it] 27%|██▋       | 552/2048 [52:15<1:44:00,  4.17s/it] 27%|██▋       | 553/2048 [52:20<1:51:23,  4.47s/it] 27%|██▋       | 554/2048 [52:25<1:56:30,  4.68s/it] 27%|██▋       | 555/2048 [52:31<1:59:53,  4.82s/it] 27%|██▋       | 556/2048 [52:36<2:01:05,  4.87s/it] 27%|██▋       | 557/2048 [52:40<1:59:50,  4.82s/it] 27%|██▋       | 558/2048 [52:45<1:56:22,  4.69s/it] 27%|██▋       | 559/2048 [52:49<1:53:49,  4.59s/it] 27%|██▋       | 560/2048 [52:53<1:48:18,  4.37s/it]                                                    {'loss': 0.4613, 'learning_rate': 0.00017054269663028233, 'epoch': 4.06}
 27%|██▋       | 560/2048 [52:53<1:48:18,  4.37s/it] 27%|██▋       | 561/2048 [52:57<1:46:04,  4.28s/it] 27%|██▋       | 562/2048 [53:01<1:44:15,  4.21s/it] 27%|██▋       | 563/2048 [53:05<1:41:01,  4.08s/it] 28%|██▊       | 564/2048 [53:08<1:33:24,  3.78s/it] 28%|██▊       | 565/2048 [53:13<1:43:41,  4.20s/it] 28%|██▊       | 566/2048 [53:18<1:50:49,  4.49s/it] 28%|██▊       | 567/2048 [53:23<1:55:41,  4.69s/it] 28%|██▊       | 568/2048 [53:28<1:57:59,  4.78s/it] 28%|██▊       | 569/2048 [53:33<1:59:17,  4.84s/it] 28%|██▊       | 570/2048 [53:38<1:59:01,  4.83s/it] 28%|██▊       | 571/2048 [53:43<1:56:12,  4.72s/it] 28%|██▊       | 572/2048 [53:47<1:52:44,  4.58s/it] 28%|██▊       | 573/2048 [53:51<1:48:03,  4.40s/it] 28%|██▊       | 574/2048 [53:55<1:42:59,  4.19s/it] 28%|██▊       | 575/2048 [53:59<1:43:19,  4.21s/it] 28%|██▊       | 576/2048 [54:02<1:36:45,  3.94s/it]                                                    {'loss': 0.3856, 'learning_rate': 0.00016872637236826582, 'epoch': 4.18}
 28%|██▊       | 576/2048 [54:02<1:36:45,  3.94s/it] 28%|██▊       | 577/2048 [54:06<1:38:10,  4.00s/it] 28%|██▊       | 578/2048 [54:11<1:46:40,  4.35s/it] 28%|██▊       | 579/2048 [54:17<1:52:31,  4.60s/it] 28%|██▊       | 580/2048 [54:22<1:56:26,  4.76s/it] 28%|██▊       | 581/2048 [54:27<1:58:47,  4.86s/it] 28%|██▊       | 582/2048 [54:32<1:58:35,  4.85s/it] 28%|██▊       | 583/2048 [54:36<1:56:05,  4.75s/it] 29%|██▊       | 584/2048 [54:40<1:51:23,  4.57s/it] 29%|██▊       | 585/2048 [54:44<1:47:35,  4.41s/it] 29%|██▊       | 586/2048 [54:48<1:43:32,  4.25s/it] 29%|██▊       | 587/2048 [54:52<1:39:50,  4.10s/it] 29%|██▊       | 588/2048 [54:56<1:35:48,  3.94s/it] 29%|██▉       | 589/2048 [54:59<1:30:09,  3.71s/it] 29%|██▉       | 590/2048 [55:04<1:40:45,  4.15s/it] 29%|██▉       | 591/2048 [55:09<1:48:09,  4.45s/it] 29%|██▉       | 592/2048 [55:14<1:53:04,  4.66s/it]                                                    {'loss': 0.3004, 'learning_rate': 0.00016686602492129717, 'epoch': 4.29}
 29%|██▉       | 592/2048 [55:14<1:53:04,  4.66s/it] 29%|██▉       | 593/2048 [55:19<1:56:23,  4.80s/it] 29%|██▉       | 594/2048 [55:24<1:56:25,  4.80s/it] 29%|██▉       | 595/2048 [55:29<1:55:42,  4.78s/it] 29%|██▉       | 596/2048 [55:33<1:52:43,  4.66s/it] 29%|██▉       | 597/2048 [55:37<1:48:34,  4.49s/it] 29%|██▉       | 598/2048 [55:41<1:45:06,  4.35s/it] 29%|██▉       | 599/2048 [55:45<1:41:50,  4.22s/it] 29%|██▉       | 600/2048 [55:49<1:39:46,  4.13s/it] 29%|██▉       | 601/2048 [55:53<1:34:36,  3.92s/it] 29%|██▉       | 602/2048 [55:57<1:35:42,  3.97s/it] 29%|██▉       | 603/2048 [56:02<1:44:18,  4.33s/it] 29%|██▉       | 604/2048 [56:07<1:50:13,  4.58s/it] 30%|██▉       | 605/2048 [56:12<1:54:14,  4.75s/it] 30%|██▉       | 606/2048 [56:17<1:56:25,  4.84s/it] 30%|██▉       | 607/2048 [56:22<1:55:40,  4.82s/it] 30%|██▉       | 608/2048 [56:26<1:53:14,  4.72s/it]                                                    {'loss': 0.4029, 'learning_rate': 0.00016496284594866113, 'epoch': 4.41}
 30%|██▉       | 608/2048 [56:26<1:53:14,  4.72s/it] 30%|██▉       | 609/2048 [56:31<1:50:11,  4.59s/it] 30%|██▉       | 610/2048 [56:35<1:47:41,  4.49s/it] 30%|██▉       | 611/2048 [56:39<1:43:39,  4.33s/it] 30%|██▉       | 612/2048 [56:43<1:41:24,  4.24s/it] 30%|██▉       | 613/2048 [56:47<1:38:30,  4.12s/it] 30%|██▉       | 614/2048 [56:50<1:32:07,  3.85s/it] 30%|███       | 615/2048 [56:55<1:41:29,  4.25s/it] 30%|███       | 616/2048 [57:00<1:48:00,  4.53s/it] 30%|███       | 617/2048 [57:06<1:52:24,  4.71s/it] 30%|███       | 618/2048 [57:11<1:54:53,  4.82s/it] 30%|███       | 619/2048 [57:16<1:55:26,  4.85s/it] 30%|███       | 620/2048 [57:20<1:53:08,  4.75s/it] 30%|███       | 621/2048 [57:24<1:50:26,  4.64s/it] 30%|███       | 622/2048 [57:29<1:46:10,  4.47s/it] 30%|███       | 623/2048 [57:32<1:41:49,  4.29s/it] 30%|███       | 624/2048 [57:37<1:41:44,  4.29s/it]                                                    {'loss': 0.3872, 'learning_rate': 0.00016301805454569217, 'epoch': 4.53}
 30%|███       | 624/2048 [57:37<1:41:44,  4.29s/it] 31%|███       | 625/2048 [57:41<1:40:10,  4.22s/it] 31%|███       | 626/2048 [57:44<1:34:24,  3.98s/it] 31%|███       | 627/2048 [57:48<1:35:03,  4.01s/it] 31%|███       | 628/2048 [57:53<1:43:11,  4.36s/it] 31%|███       | 629/2048 [57:59<1:48:44,  4.60s/it] 31%|███       | 630/2048 [58:04<1:52:17,  4.75s/it] 31%|███       | 631/2048 [58:09<1:53:30,  4.81s/it] 31%|███       | 632/2048 [58:13<1:53:07,  4.79s/it] 31%|███       | 633/2048 [58:18<1:49:59,  4.66s/it] 31%|███       | 634/2048 [58:22<1:46:41,  4.53s/it] 31%|███       | 635/2048 [58:26<1:42:15,  4.34s/it] 31%|███       | 636/2048 [58:30<1:37:05,  4.13s/it] 31%|███       | 637/2048 [58:33<1:33:34,  3.98s/it] 31%|███       | 638/2048 [58:37<1:33:41,  3.99s/it] 31%|███       | 639/2048 [58:40<1:27:16,  3.72s/it] 31%|███▏      | 640/2048 [58:45<1:37:26,  4.15s/it]                                                    {'loss': 0.3945, 'learning_rate': 0.00016103289646287404, 'epoch': 4.64}
 31%|███▏      | 640/2048 [58:45<1:37:26,  4.15s/it] 31%|███▏      | 641/2048 [58:51<1:44:32,  4.46s/it] 31%|███▏      | 642/2048 [58:56<1:49:23,  4.67s/it] 31%|███▏      | 643/2048 [59:01<1:52:29,  4.80s/it] 31%|███▏      | 644/2048 [59:06<1:52:31,  4.81s/it] 31%|███▏      | 645/2048 [59:11<1:52:49,  4.83s/it] 32%|███▏      | 646/2048 [59:15<1:49:34,  4.69s/it] 32%|███▏      | 647/2048 [59:19<1:46:51,  4.58s/it] 32%|███▏      | 648/2048 [59:23<1:40:38,  4.31s/it] 32%|███▏      | 649/2048 [59:27<1:36:21,  4.13s/it] 32%|███▏      | 650/2048 [59:30<1:31:31,  3.93s/it] 32%|███▏      | 651/2048 [59:33<1:26:41,  3.72s/it] 32%|███▏      | 652/2048 [59:37<1:28:57,  3.82s/it] 32%|███▏      | 653/2048 [59:43<1:38:17,  4.23s/it] 32%|███▏      | 654/2048 [59:48<1:44:47,  4.51s/it] 32%|███▏      | 655/2048 [59:53<1:49:10,  4.70s/it] 32%|███▏      | 656/2048 [59:58<1:51:28,  4.81s/it]                                                    {'loss': 0.3611, 'learning_rate': 0.00015900864330786518, 'epoch': 4.76}
 32%|███▏      | 656/2048 [59:58<1:51:28,  4.81s/it] 32%|███▏      | 657/2048 [1:00:03<1:52:01,  4.83s/it] 32%|███▏      | 658/2048 [1:00:07<1:49:52,  4.74s/it] 32%|███▏      | 659/2048 [1:00:12<1:46:32,  4.60s/it] 32%|███▏      | 660/2048 [1:00:16<1:42:59,  4.45s/it] 32%|███▏      | 661/2048 [1:00:20<1:39:24,  4.30s/it] 32%|███▏      | 662/2048 [1:00:23<1:35:20,  4.13s/it] 32%|███▏      | 663/2048 [1:00:27<1:34:24,  4.09s/it] 32%|███▏      | 664/2048 [1:00:31<1:27:48,  3.81s/it] 32%|███▏      | 665/2048 [1:00:36<1:37:10,  4.22s/it] 33%|███▎      | 666/2048 [1:00:41<1:43:40,  4.50s/it] 33%|███▎      | 667/2048 [1:00:46<1:48:00,  4.69s/it] 33%|███▎      | 668/2048 [1:00:51<1:50:25,  4.80s/it] 33%|███▎      | 669/2048 [1:00:56<1:49:55,  4.78s/it] 33%|███▎      | 670/2048 [1:01:01<1:49:21,  4.76s/it] 33%|███▎      | 671/2048 [1:01:05<1:46:05,  4.62s/it] 33%|███▎      | 672/2048 [1:01:09<1:42:25,  4.47s/it]                                                      {'loss': 0.4303, 'learning_rate': 0.00015694659173096098, 'epoch': 4.87}
 33%|███▎      | 672/2048 [1:01:09<1:42:25,  4.47s/it] 33%|███▎      | 673/2048 [1:01:13<1:37:40,  4.26s/it] 33%|███▎      | 674/2048 [1:01:17<1:38:10,  4.29s/it] 33%|███▎      | 675/2048 [1:01:21<1:33:17,  4.08s/it] 33%|███▎      | 676/2048 [1:01:24<1:29:54,  3.93s/it] 33%|███▎      | 677/2048 [1:01:28<1:31:03,  3.98s/it] 33%|███▎      | 678/2048 [1:01:34<1:39:05,  4.34s/it] 33%|███▎      | 679/2048 [1:01:39<1:44:37,  4.59s/it] 33%|███▎      | 680/2048 [1:01:44<1:47:57,  4.74s/it] 33%|███▎      | 681/2048 [1:01:49<1:49:03,  4.79s/it] 33%|███▎      | 682/2048 [1:01:53<1:48:13,  4.75s/it] 33%|███▎      | 683/2048 [1:01:58<1:46:22,  4.68s/it] 33%|███▎      | 684/2048 [1:02:02<1:42:17,  4.50s/it] 33%|███▎      | 685/2048 [1:02:06<1:39:39,  4.39s/it] 33%|███▎      | 686/2048 [1:02:10<1:35:49,  4.22s/it] 34%|███▎      | 687/2048 [1:02:14<1:34:34,  4.17s/it] 34%|███▎      | 688/2048 [1:02:18<1:31:00,  4.01s/it]                                                      {'loss': 0.3846, 'learning_rate': 0.00015484806259451485, 'epoch': 4.99}
 34%|███▎      | 688/2048 [1:02:18<1:31:00,  4.01s/it] 34%|███▎      | 689/2048 [1:02:21<1:24:31,  3.73s/it] 34%|███▎      | 690/2048 [1:02:26<1:33:53,  4.15s/it] 34%|███▎      | 691/2048 [1:02:31<1:40:45,  4.45s/it] 34%|███▍      | 692/2048 [1:02:36<1:45:24,  4.66s/it] 34%|███▍      | 693/2048 [1:02:41<1:48:33,  4.81s/it] 34%|███▍      | 694/2048 [1:02:46<1:48:56,  4.83s/it] 34%|███▍      | 695/2048 [1:02:51<1:48:13,  4.80s/it] 34%|███▍      | 696/2048 [1:02:55<1:46:02,  4.71s/it] 34%|███▍      | 697/2048 [1:03:00<1:43:29,  4.60s/it] 34%|███▍      | 698/2048 [1:03:04<1:39:39,  4.43s/it] 34%|███▍      | 699/2048 [1:03:08<1:35:34,  4.25s/it] 34%|███▍      | 700/2048 [1:03:12<1:33:33,  4.16s/it] 34%|███▍      | 701/2048 [1:03:15<1:29:44,  4.00s/it] 34%|███▍      | 702/2048 [1:03:18<1:24:59,  3.79s/it] 34%|███▍      | 703/2048 [1:03:24<1:34:13,  4.20s/it] 34%|███▍      | 704/2048 [1:03:29<1:40:40,  4.49s/it]                                                      {'loss': 0.2647, 'learning_rate': 0.00015271440012685025, 'epoch': 5.11}
 34%|███▍      | 704/2048 [1:03:29<1:40:40,  4.49s/it] 34%|███▍      | 705/2048 [1:03:34<1:45:03,  4.69s/it] 34%|███▍      | 706/2048 [1:03:39<1:47:34,  4.81s/it] 35%|███▍      | 707/2048 [1:03:44<1:47:47,  4.82s/it] 35%|███▍      | 708/2048 [1:03:49<1:46:50,  4.78s/it] 35%|███▍      | 709/2048 [1:03:53<1:44:11,  4.67s/it] 35%|███▍      | 710/2048 [1:03:57<1:39:47,  4.48s/it] 35%|███▍      | 711/2048 [1:04:01<1:35:22,  4.28s/it] 35%|███▍      | 712/2048 [1:04:05<1:32:36,  4.16s/it] 35%|███▍      | 713/2048 [1:04:08<1:29:38,  4.03s/it] 35%|███▍      | 714/2048 [1:04:12<1:25:29,  3.84s/it] 35%|███▍      | 715/2048 [1:04:16<1:28:56,  4.00s/it] 35%|███▍      | 716/2048 [1:04:21<1:36:38,  4.35s/it] 35%|███▌      | 717/2048 [1:04:27<1:41:57,  4.60s/it] 35%|███▌      | 718/2048 [1:04:32<1:45:25,  4.76s/it] 35%|███▌      | 719/2048 [1:04:37<1:46:38,  4.81s/it] 35%|███▌      | 720/2048 [1:04:41<1:45:02,  4.75s/it]                                                      {'loss': 0.2718, 'learning_rate': 0.00015054697106120507, 'epoch': 5.22}
 35%|███▌      | 720/2048 [1:04:41<1:45:02,  4.75s/it] 35%|███▌      | 721/2048 [1:04:46<1:43:00,  4.66s/it] 35%|███▌      | 722/2048 [1:04:50<1:39:23,  4.50s/it] 35%|███▌      | 723/2048 [1:04:54<1:35:22,  4.32s/it] 35%|███▌      | 724/2048 [1:04:58<1:34:11,  4.27s/it] 35%|███▌      | 725/2048 [1:05:02<1:30:27,  4.10s/it] 35%|███▌      | 726/2048 [1:05:05<1:28:00,  3.99s/it] 35%|███▌      | 727/2048 [1:05:09<1:23:29,  3.79s/it] 36%|███▌      | 728/2048 [1:05:14<1:32:31,  4.21s/it] 36%|███▌      | 729/2048 [1:05:19<1:38:48,  4.49s/it] 36%|███▌      | 730/2048 [1:05:24<1:43:06,  4.69s/it] 36%|███▌      | 731/2048 [1:05:29<1:45:29,  4.81s/it] 36%|███▌      | 732/2048 [1:05:34<1:45:07,  4.79s/it] 36%|███▌      | 733/2048 [1:05:38<1:43:21,  4.72s/it] 36%|███▌      | 734/2048 [1:05:43<1:39:27,  4.54s/it] 36%|███▌      | 735/2048 [1:05:47<1:38:25,  4.50s/it] 36%|███▌      | 736/2048 [1:05:51<1:33:36,  4.28s/it]                                                      {'loss': 0.2884, 'learning_rate': 0.00014834716376026084, 'epoch': 5.34}
 36%|███▌      | 736/2048 [1:05:51<1:33:36,  4.28s/it] 36%|███▌      | 737/2048 [1:05:55<1:31:33,  4.19s/it] 36%|███▌      | 738/2048 [1:05:58<1:28:26,  4.05s/it] 36%|███▌      | 739/2048 [1:06:02<1:23:00,  3.81s/it] 36%|███▌      | 740/2048 [1:06:06<1:26:25,  3.96s/it] 36%|███▌      | 741/2048 [1:06:11<1:34:14,  4.33s/it] 36%|███▌      | 742/2048 [1:06:16<1:39:34,  4.57s/it] 36%|███▋      | 743/2048 [1:06:21<1:42:58,  4.73s/it] 36%|███▋      | 744/2048 [1:06:26<1:44:40,  4.82s/it] 36%|███▋      | 745/2048 [1:06:31<1:42:47,  4.73s/it] 36%|███▋      | 746/2048 [1:06:35<1:40:23,  4.63s/it] 36%|███▋      | 747/2048 [1:06:40<1:38:15,  4.53s/it] 37%|███▋      | 748/2048 [1:06:44<1:33:56,  4.34s/it] 37%|███▋      | 749/2048 [1:06:48<1:31:22,  4.22s/it] 37%|███▋      | 750/2048 [1:06:51<1:29:00,  4.11s/it] 37%|███▋      | 751/2048 [1:06:55<1:25:03,  3.93s/it] 37%|███▋      | 752/2048 [1:06:58<1:20:56,  3.75s/it]                                                      {'loss': 0.2807, 'learning_rate': 0.0001461163873268164, 'epoch': 5.45}
 37%|███▋      | 752/2048 [1:06:58<1:20:56,  3.75s/it] 37%|███▋      | 753/2048 [1:07:03<1:30:05,  4.17s/it] 37%|███▋      | 754/2048 [1:07:09<1:36:28,  4.47s/it] 37%|███▋      | 755/2048 [1:07:14<1:40:45,  4.68s/it] 37%|███▋      | 756/2048 [1:07:19<1:43:14,  4.79s/it] 37%|███▋      | 757/2048 [1:07:24<1:43:38,  4.82s/it] 37%|███▋      | 758/2048 [1:07:28<1:42:35,  4.77s/it] 37%|███▋      | 759/2048 [1:07:33<1:40:22,  4.67s/it] 37%|███▋      | 760/2048 [1:07:37<1:35:41,  4.46s/it] 37%|███▋      | 761/2048 [1:07:41<1:31:30,  4.27s/it] 37%|███▋      | 762/2048 [1:07:45<1:31:34,  4.27s/it] 37%|███▋      | 763/2048 [1:07:49<1:30:12,  4.21s/it] 37%|███▋      | 764/2048 [1:07:52<1:25:32,  4.00s/it] 37%|███▋      | 765/2048 [1:07:57<1:27:42,  4.10s/it] 37%|███▋      | 766/2048 [1:08:02<1:34:29,  4.42s/it] 37%|███▋      | 767/2048 [1:08:07<1:39:07,  4.64s/it] 38%|███▊      | 768/2048 [1:08:12<1:42:02,  4.78s/it]                                                      {'loss': 0.2233, 'learning_rate': 0.0001438560707011772, 'epoch': 5.57}
 38%|███▊      | 768/2048 [1:08:12<1:42:02,  4.78s/it]
  0%|          | 0/256 [00:00<?, ?it/s][A
  2%|▏         | 4/256 [00:00<00:09, 26.76it/s][A
  3%|▎         | 7/256 [00:00<00:11, 20.95it/s][A
  4%|▍         | 10/256 [00:00<00:12, 19.27it/s][A
  5%|▍         | 12/256 [00:00<00:13, 18.35it/s][A
  5%|▌         | 14/256 [00:00<00:13, 17.82it/s][A
  6%|▋         | 16/256 [00:00<00:13, 18.16it/s][A
  7%|▋         | 18/256 [00:00<00:13, 17.77it/s][A
  8%|▊         | 21/256 [00:01<00:12, 18.52it/s][A
  9%|▉         | 23/256 [00:01<00:12, 18.03it/s][A
 10%|▉         | 25/256 [00:01<00:13, 17.67it/s][A
 11%|█         | 27/256 [00:01<00:12, 17.75it/s][A
 11%|█▏        | 29/256 [00:01<00:13, 17.45it/s][A
 12%|█▏        | 31/256 [00:01<00:13, 17.26it/s][A
 13%|█▎        | 34/256 [00:01<00:11, 18.72it/s][A
 14%|█▍        | 36/256 [00:01<00:12, 18.16it/s][A
 15%|█▍        | 38/256 [00:02<00:12, 17.63it/s][A
 16%|█▌        | 40/256 [00:02<00:12, 17.37it/s][A
 16%|█▋        | 42/256 [00:02<00:12, 17.74it/s][A
 17%|█▋        | 44/256 [00:02<00:12, 17.49it/s][A
 18%|█▊        | 46/256 [00:02<00:12, 17.28it/s][A
 19%|█▉        | 48/256 [00:02<00:11, 17.39it/s][A
 20%|█▉        | 50/256 [00:02<00:12, 17.10it/s][A
 20%|██        | 52/256 [00:02<00:12, 17.00it/s][A
 21%|██        | 54/256 [00:02<00:11, 17.78it/s][A
 22%|██▏       | 56/256 [00:03<00:11, 17.57it/s][A
 23%|██▎       | 58/256 [00:03<00:11, 17.53it/s][A
 23%|██▎       | 60/256 [00:03<00:11, 17.27it/s][A
 24%|██▍       | 62/256 [00:03<00:11, 16.94it/s][A
 25%|██▌       | 64/256 [00:03<00:11, 17.24it/s][A
 26%|██▌       | 66/256 [00:03<00:11, 17.09it/s][A
 27%|██▋       | 68/256 [00:03<00:11, 17.01it/s][A
 27%|██▋       | 70/256 [00:03<00:10, 17.15it/s][A
 28%|██▊       | 72/256 [00:04<00:10, 17.03it/s][A
 29%|██▉       | 74/256 [00:04<00:10, 16.99it/s][A
 30%|██▉       | 76/256 [00:04<00:10, 17.01it/s][A
 30%|███       | 78/256 [00:04<00:10, 16.98it/s][A
 31%|███▏      | 80/256 [00:04<00:10, 16.94it/s][A
 32%|███▏      | 82/256 [00:04<00:10, 16.99it/s][A
 33%|███▎      | 84/256 [00:04<00:10, 17.16it/s][A
 34%|███▎      | 86/256 [00:04<00:09, 17.06it/s][A
 34%|███▍      | 88/256 [00:04<00:09, 17.28it/s][A
 35%|███▌      | 90/256 [00:05<00:09, 17.03it/s][A
 36%|███▌      | 92/256 [00:05<00:09, 16.93it/s][A
 37%|███▋      | 94/256 [00:05<00:09, 17.01it/s][A
 38%|███▊      | 96/256 [00:05<00:09, 16.91it/s][A
 39%|███▊      | 99/256 [00:05<00:08, 17.72it/s][A
 39%|███▉      | 101/256 [00:05<00:08, 17.37it/s][A
 40%|████      | 103/256 [00:05<00:08, 17.48it/s][A
 41%|████▏     | 106/256 [00:06<00:08, 18.35it/s][A
 42%|████▏     | 108/256 [00:06<00:08, 17.97it/s][A
 43%|████▎     | 110/256 [00:06<00:08, 17.74it/s][A
 44%|████▍     | 112/256 [00:06<00:08, 17.45it/s][A
 45%|████▍     | 114/256 [00:06<00:08, 17.58it/s][A
 46%|████▌     | 117/256 [00:06<00:07, 18.03it/s][A
 46%|████▋     | 119/256 [00:06<00:07, 17.83it/s][A
 47%|████▋     | 121/256 [00:06<00:07, 17.46it/s][A
 48%|████▊     | 123/256 [00:06<00:07, 17.32it/s][A
 49%|████▉     | 125/256 [00:07<00:07, 17.15it/s][A
 50%|█████     | 128/256 [00:07<00:06, 18.67it/s][A
 51%|█████     | 130/256 [00:07<00:06, 18.24it/s][A
 52%|█████▏    | 132/256 [00:07<00:07, 17.69it/s][A
 52%|█████▏    | 134/256 [00:07<00:06, 17.55it/s][A
 53%|█████▎    | 136/256 [00:07<00:06, 17.40it/s][A
 54%|█████▍    | 138/256 [00:07<00:06, 17.20it/s][A
 55%|█████▍    | 140/256 [00:07<00:06, 17.07it/s][A
 56%|█████▌    | 143/256 [00:08<00:06, 18.20it/s][A
 57%|█████▋    | 145/256 [00:08<00:06, 17.64it/s][A
 57%|█████▋    | 147/256 [00:08<00:06, 17.44it/s][A
 58%|█████▊    | 149/256 [00:08<00:06, 17.25it/s][A
 59%|█████▉    | 151/256 [00:08<00:06, 17.11it/s][A
 60%|██████    | 154/256 [00:08<00:05, 17.73it/s][A
 61%|██████    | 156/256 [00:08<00:05, 17.59it/s][A
 62%|██████▏   | 158/256 [00:08<00:05, 17.24it/s][A
 63%|██████▎   | 161/256 [00:09<00:05, 17.98it/s][A
 64%|██████▎   | 163/256 [00:09<00:05, 17.53it/s][A
 64%|██████▍   | 165/256 [00:09<00:05, 17.44it/s][A
 65%|██████▌   | 167/256 [00:09<00:05, 17.07it/s][A
 66%|██████▌   | 169/256 [00:09<00:05, 17.33it/s][A
 67%|██████▋   | 171/256 [00:09<00:04, 17.08it/s][A
 68%|██████▊   | 173/256 [00:09<00:04, 16.94it/s][A
 68%|██████▊   | 175/256 [00:09<00:04, 17.00it/s][A
 69%|██████▉   | 177/256 [00:10<00:04, 16.83it/s][A
 70%|██████▉   | 179/256 [00:10<00:04, 16.92it/s][A
 71%|███████   | 182/256 [00:10<00:03, 19.23it/s][A
 72%|███████▏  | 184/256 [00:10<00:03, 18.60it/s][A
 73%|███████▎  | 186/256 [00:10<00:03, 17.94it/s][A
 74%|███████▍  | 189/256 [00:10<00:03, 18.43it/s][A
 75%|███████▍  | 191/256 [00:10<00:03, 17.84it/s][A
 75%|███████▌  | 193/256 [00:10<00:03, 17.68it/s][A
 76%|███████▌  | 195/256 [00:11<00:03, 17.27it/s][A
 77%|███████▋  | 197/256 [00:11<00:03, 17.58it/s][A
 78%|███████▊  | 199/256 [00:11<00:03, 18.19it/s][A
 79%|███████▊  | 201/256 [00:11<00:03, 17.88it/s][A
 79%|███████▉  | 203/256 [00:11<00:03, 17.41it/s][A
 80%|████████  | 205/256 [00:11<00:02, 17.36it/s][A
 81%|████████  | 207/256 [00:11<00:02, 17.05it/s][A
 82%|████████▏ | 209/256 [00:11<00:02, 16.96it/s][A
 82%|████████▏ | 211/256 [00:12<00:02, 16.92it/s][A
 83%|████████▎ | 213/256 [00:12<00:02, 16.89it/s][A
 84%|████████▍ | 215/256 [00:12<00:02, 16.85it/s][A
 85%|████████▍ | 217/256 [00:12<00:02, 16.84it/s][A
 86%|████████▌ | 219/256 [00:12<00:02, 16.94it/s][A
 86%|████████▋ | 221/256 [00:12<00:02, 16.78it/s][A
 87%|████████▋ | 223/256 [00:12<00:01, 16.79it/s][A
 88%|████████▊ | 225/256 [00:12<00:01, 17.22it/s][A
 89%|████████▊ | 227/256 [00:12<00:01, 16.95it/s][A
 89%|████████▉ | 229/256 [00:13<00:01, 16.86it/s][A
 90%|█████████ | 231/256 [00:13<00:01, 17.09it/s][A
 91%|█████████ | 233/256 [00:13<00:01, 17.06it/s][A
 92%|█████████▏| 235/256 [00:13<00:01, 17.04it/s][A
 93%|█████████▎| 238/256 [00:13<00:01, 17.96it/s][A
 94%|█████████▍| 240/256 [00:13<00:00, 17.74it/s][A
 95%|█████████▍| 242/256 [00:13<00:00, 17.46it/s][A
 95%|█████████▌| 244/256 [00:13<00:00, 17.78it/s][A
 96%|█████████▋| 247/256 [00:14<00:00, 18.28it/s][A
 97%|█████████▋| 249/256 [00:14<00:00, 17.96it/s][A
 98%|█████████▊| 252/256 [00:14<00:00, 19.04it/s][A
 99%|█████████▉| 254/256 [00:14<00:00, 18.56it/s][A
100%|██████████| 256/256 [00:14<00:00, 17.21it/s][A                                                      
                                                 [A{'eval_loss': 1.918195366859436, 'eval_runtime': 14.7448, 'eval_samples_per_second': 69.448, 'eval_steps_per_second': 17.362, 'epoch': 5.57}
 38%|███▊      | 768/2048 [1:08:27<1:42:02,  4.78s/it]
100%|██████████| 256/256 [00:14<00:00, 17.21it/s][A
                                                 [A  0%|          | 0/383 [00:00<?, ?it/s]  0%|          | 0/383 [00:00<?, ?it/s]  0%|          | 0/383 [00:00<?, ?it/s]
  0%|          | 0/383 [00:00<?, ?it/s][A  0%|          | 1/383 [00:00<00:41,  9.31it/s]
  0%|          | 1/383 [00:00<00:48,  7.80it/s][A  1%|          | 2/383 [00:00<00:25, 14.96it/s]  1%|          | 2/383 [00:00<00:26, 14.18it/s]  1%|          | 3/383 [00:00<00:25, 15.18it/s]
  1%|          | 4/383 [00:00<00:22, 16.58it/s][A  1%|▏         | 5/383 [00:00<00:19, 19.26it/s]  1%|▏         | 5/383 [00:00<00:20, 18.58it/s]  1%|▏         | 5/383 [00:00<00:21, 17.25it/s]  2%|▏         | 7/383 [00:00<00:21, 17.85it/s]
  2%|▏         | 7/383 [00:00<00:19, 18.82it/s][A  2%|▏         | 7/383 [00:00<00:21, 17.29it/s]  2%|▏         | 7/383 [00:00<00:22, 16.71it/s]  2%|▏         | 9/383 [00:00<00:23, 16.06it/s]  2%|▏         | 9/383 [00:00<00:24, 15.52it/s]
  2%|▏         | 9/383 [00:00<00:22, 16.29it/s][A  2%|▏         | 9/383 [00:00<00:23, 15.98it/s]  3%|▎         | 11/383 [00:00<00:23, 15.52it/s]
  3%|▎         | 11/383 [00:00<00:23, 15.52it/s][A  3%|▎         | 11/383 [00:00<00:25, 14.68it/s]  3%|▎         | 11/383 [00:00<00:24, 15.01it/s]  3%|▎         | 13/383 [00:00<00:24, 15.06it/s]  3%|▎         | 13/383 [00:00<00:24, 15.32it/s]
  3%|▎         | 13/383 [00:00<00:24, 14.92it/s][A  3%|▎         | 13/383 [00:00<00:25, 14.47it/s]  4%|▍         | 15/383 [00:00<00:22, 16.12it/s]
  4%|▍         | 15/383 [00:00<00:23, 15.94it/s][A  4%|▍         | 15/383 [00:00<00:22, 16.24it/s]  4%|▍         | 15/383 [00:00<00:23, 15.49it/s]  4%|▍         | 17/383 [00:01<00:21, 16.77it/s]
  4%|▍         | 17/383 [00:01<00:21, 16.84it/s][A  4%|▍         | 17/383 [00:01<00:21, 16.98it/s]  4%|▍         | 17/383 [00:01<00:22, 16.28it/s]  5%|▍         | 19/383 [00:01<00:20, 17.35it/s]
  5%|▍         | 19/383 [00:01<00:20, 17.45it/s][A  5%|▍         | 19/383 [00:01<00:20, 17.53it/s]  5%|▍         | 19/383 [00:01<00:21, 16.55it/s]  5%|▌         | 21/383 [00:01<00:20, 17.36it/s]
  5%|▌         | 21/383 [00:01<00:20, 17.73it/s][A  5%|▌         | 21/383 [00:01<00:20, 17.69it/s]  5%|▌         | 21/383 [00:01<00:21, 16.84it/s]  6%|▌         | 23/383 [00:01<00:20, 17.22it/s]
  6%|▌         | 23/383 [00:01<00:20, 17.62it/s][A  6%|▌         | 23/383 [00:01<00:20, 17.36it/s]  6%|▌         | 23/383 [00:01<00:21, 17.03it/s]  7%|▋         | 25/383 [00:01<00:21, 16.75it/s]
  7%|▋         | 25/383 [00:01<00:20, 17.39it/s][A  7%|▋         | 25/383 [00:01<00:21, 17.01it/s]  7%|▋         | 25/383 [00:01<00:21, 16.75it/s]
  7%|▋         | 27/383 [00:01<00:20, 17.12it/s][A  7%|▋         | 27/383 [00:01<00:24, 14.62it/s]  7%|▋         | 27/383 [00:01<00:24, 14.69it/s]  7%|▋         | 27/383 [00:01<00:24, 14.44it/s]
  8%|▊         | 29/383 [00:01<00:25, 13.69it/s][A  8%|▊         | 29/383 [00:01<00:27, 12.83it/s]  8%|▊         | 29/383 [00:01<00:27, 12.83it/s]  8%|▊         | 29/383 [00:01<00:28, 12.46it/s]
  8%|▊         | 31/383 [00:01<00:25, 13.59it/s][A  8%|▊         | 31/383 [00:02<00:27, 13.00it/s]  8%|▊         | 31/383 [00:02<00:26, 13.09it/s]  8%|▊         | 31/383 [00:02<00:26, 13.19it/s]
  9%|▊         | 33/383 [00:02<00:25, 13.79it/s][A  9%|▊         | 33/383 [00:02<00:25, 13.69it/s]  9%|▊         | 33/383 [00:02<00:25, 13.80it/s]  9%|▊         | 33/383 [00:02<00:24, 14.06it/s]
  9%|▉         | 35/383 [00:02<00:23, 14.51it/s][A  9%|▉         | 35/383 [00:02<00:24, 14.12it/s]  9%|▉         | 35/383 [00:02<00:24, 14.37it/s]  9%|▉         | 35/383 [00:02<00:23, 14.66it/s]
 10%|▉         | 37/383 [00:02<00:23, 14.83it/s][A 10%|▉         | 37/383 [00:02<00:23, 14.93it/s] 10%|▉         | 37/383 [00:02<00:23, 14.97it/s] 10%|▉         | 37/383 [00:02<00:22, 15.24it/s]
 10%|█         | 39/383 [00:02<00:22, 15.35it/s][A 10%|█         | 39/383 [00:02<00:22, 15.48it/s] 10%|█         | 39/383 [00:02<00:22, 15.56it/s] 10%|█         | 39/383 [00:02<00:21, 15.73it/s]
 11%|█         | 41/383 [00:02<00:21, 15.96it/s][A 11%|█         | 41/383 [00:02<00:21, 15.91it/s] 11%|█         | 41/383 [00:02<00:21, 16.04it/s] 11%|█         | 41/383 [00:02<00:21, 15.89it/s]
 11%|█▏        | 44/383 [00:02<00:18, 17.85it/s][A 11%|█         | 43/383 [00:02<00:20, 16.90it/s] 11%|█▏        | 44/383 [00:02<00:18, 18.35it/s] 11%|█         | 43/383 [00:02<00:20, 16.53it/s]
 12%|█▏        | 47/383 [00:02<00:16, 20.03it/s][A 12%|█▏        | 46/383 [00:02<00:17, 19.54it/s] 12%|█▏        | 47/383 [00:02<00:16, 20.62it/s] 12%|█▏        | 46/383 [00:02<00:17, 19.18it/s]
 13%|█▎        | 50/383 [00:02<00:15, 21.58it/s][A 13%|█▎        | 49/383 [00:02<00:15, 21.45it/s] 13%|█▎        | 49/383 [00:03<00:15, 21.23it/s] 13%|█▎        | 50/383 [00:03<00:16, 20.80it/s]
 14%|█▍        | 53/383 [00:03<00:17, 18.41it/s][A 14%|█▍        | 53/383 [00:03<00:17, 18.46it/s] 14%|█▎        | 52/383 [00:03<00:18, 17.43it/s] 14%|█▎        | 52/383 [00:03<00:18, 18.15it/s]
 14%|█▍        | 55/383 [00:03<00:17, 18.32it/s][A 14%|█▍        | 54/383 [00:03<00:18, 17.76it/s] 14%|█▍        | 55/383 [00:03<00:17, 18.38it/s] 14%|█▍        | 54/383 [00:03<00:18, 17.91it/s]
 15%|█▍        | 57/383 [00:03<00:18, 18.02it/s][A 15%|█▍        | 56/383 [00:03<00:18, 17.99it/s] 15%|█▍        | 57/383 [00:03<00:17, 18.23it/s] 15%|█▍        | 56/383 [00:03<00:18, 17.99it/s]
 15%|█▌        | 59/383 [00:03<00:18, 17.16it/s][A 15%|█▌        | 58/383 [00:03<00:18, 17.40it/s] 15%|█▌        | 58/383 [00:03<00:18, 17.23it/s] 15%|█▌        | 59/383 [00:03<00:18, 17.18it/s] 16%|█▌        | 60/383 [00:03<00:19, 16.83it/s]
 16%|█▌        | 61/383 [00:03<00:19, 16.20it/s][A 16%|█▌        | 60/383 [00:03<00:19, 16.86it/s] 16%|█▌        | 61/383 [00:03<00:19, 16.77it/s]
 16%|█▋        | 63/383 [00:03<00:19, 16.15it/s][A 16%|█▌        | 62/383 [00:03<00:19, 16.09it/s] 16%|█▋        | 63/383 [00:03<00:19, 16.40it/s] 16%|█▌        | 62/383 [00:03<00:19, 16.25it/s]
 17%|█▋        | 65/383 [00:03<00:19, 16.14it/s][A 17%|█▋        | 64/383 [00:03<00:19, 15.97it/s] 17%|█▋        | 64/383 [00:03<00:19, 16.27it/s] 17%|█▋        | 65/383 [00:03<00:19, 15.91it/s]
 17%|█▋        | 67/383 [00:04<00:20, 15.74it/s][A 17%|█▋        | 66/383 [00:04<00:19, 16.26it/s] 17%|█▋        | 66/383 [00:04<00:20, 15.72it/s] 17%|█▋        | 67/383 [00:04<00:20, 15.68it/s] 18%|█▊        | 68/383 [00:04<00:20, 15.66it/s]
 18%|█▊        | 69/383 [00:04<00:21, 14.94it/s][A 18%|█▊        | 68/383 [00:04<00:20, 15.03it/s] 18%|█▊        | 69/383 [00:04<00:20, 14.95it/s]
 19%|█▊        | 71/383 [00:04<00:20, 15.10it/s][A 18%|█▊        | 70/383 [00:04<00:20, 15.06it/s] 19%|█▊        | 71/383 [00:04<00:20, 15.11it/s] 18%|█▊        | 70/383 [00:04<00:21, 14.63it/s]
 19%|█▉        | 73/383 [00:04<00:19, 16.06it/s][A 19%|█▉        | 72/383 [00:04<00:19, 15.73it/s] 19%|█▉        | 72/383 [00:04<00:20, 15.53it/s] 19%|█▉        | 73/383 [00:04<00:19, 15.71it/s] 19%|█▉        | 74/383 [00:04<00:19, 16.23it/s]
 20%|█▉        | 75/383 [00:04<00:19, 15.93it/s][A 20%|█▉        | 75/383 [00:04<00:19, 15.70it/s] 19%|█▉        | 74/383 [00:04<00:20, 15.29it/s] 20%|█▉        | 76/383 [00:04<00:18, 16.28it/s]
 20%|██        | 77/383 [00:04<00:19, 15.95it/s][A 20%|█▉        | 76/383 [00:04<00:19, 15.65it/s] 20%|██        | 77/383 [00:04<00:19, 15.82it/s] 20%|██        | 78/383 [00:04<00:18, 16.19it/s]
 21%|██        | 79/383 [00:04<00:19, 15.94it/s][A 20%|██        | 78/383 [00:04<00:19, 15.82it/s] 21%|██        | 79/383 [00:04<00:19, 15.85it/s] 21%|██        | 80/383 [00:04<00:18, 16.11it/s]
 21%|██        | 81/383 [00:04<00:19, 15.74it/s][A 21%|██        | 80/383 [00:04<00:19, 15.76it/s] 21%|██        | 81/383 [00:05<00:19, 15.60it/s] 21%|██▏       | 82/383 [00:05<00:18, 15.97it/s]
 22%|██▏       | 83/383 [00:05<00:18, 15.92it/s][A 21%|██▏       | 82/383 [00:05<00:18, 15.96it/s] 22%|██▏       | 83/383 [00:05<00:19, 15.75it/s] 22%|██▏       | 84/383 [00:05<00:18, 16.01it/s]
 22%|██▏       | 85/383 [00:05<00:18, 15.72it/s][A 22%|██▏       | 84/383 [00:05<00:18, 16.11it/s] 22%|██▏       | 85/383 [00:05<00:18, 16.06it/s] 22%|██▏       | 86/383 [00:05<00:18, 16.25it/s] 22%|██▏       | 86/383 [00:05<00:18, 16.08it/s]
 23%|██▎       | 87/383 [00:05<00:20, 14.25it/s][A 23%|██▎       | 87/383 [00:05<00:20, 14.18it/s] 23%|██▎       | 88/383 [00:05<00:22, 13.02it/s] 23%|██▎       | 88/383 [00:05<00:23, 12.53it/s]
 23%|██▎       | 89/383 [00:05<00:24, 11.83it/s][A 23%|██▎       | 89/383 [00:05<00:32,  9.02it/s] 23%|██▎       | 90/383 [00:05<00:34,  8.61it/s]
 24%|██▍       | 91/383 [00:06<00:39,  7.39it/s][A 23%|██▎       | 90/383 [00:06<00:42,  6.90it/s] 24%|██▍       | 91/383 [00:06<00:48,  6.02it/s] 24%|██▍       | 92/383 [00:06<00:48,  5.99it/s] 24%|██▍       | 92/383 [00:06<00:47,  6.09it/s] 24%|██▍       | 92/383 [00:06<00:52,  5.56it/s]
 24%|██▍       | 93/383 [00:06<00:50,  5.71it/s][A 25%|██▍       | 94/383 [00:06<00:39,  7.30it/s]
 25%|██▌       | 96/383 [00:06<00:36,  7.92it/s][A 24%|██▍       | 93/383 [00:06<00:55,  5.26it/s] 25%|██▌       | 96/383 [00:06<00:31,  9.18it/s] 24%|██▍       | 93/383 [00:06<00:53,  5.43it/s]
 26%|██▌       | 98/383 [00:06<00:30,  9.45it/s][A 25%|██▍       | 95/383 [00:06<00:41,  6.94it/s] 26%|██▌       | 98/383 [00:06<00:25, 11.06it/s] 25%|██▍       | 95/383 [00:06<00:40,  7.17it/s]
 26%|██▌       | 100/383 [00:07<00:26, 10.88it/s][A 25%|██▌       | 97/383 [00:07<00:32,  8.79it/s] 26%|██▌       | 100/383 [00:07<00:22, 12.40it/s] 25%|██▌       | 97/383 [00:07<00:31,  9.01it/s]
 27%|██▋       | 102/383 [00:07<00:22, 12.23it/s][A 26%|██▌       | 99/383 [00:07<00:27, 10.49it/s] 27%|██▋       | 102/383 [00:07<00:20, 13.77it/s] 26%|██▌       | 99/383 [00:07<00:26, 10.70it/s]
 27%|██▋       | 104/383 [00:07<00:20, 13.36it/s][A 26%|██▋       | 101/383 [00:07<00:23, 11.80it/s] 27%|██▋       | 104/383 [00:07<00:18, 15.03it/s] 26%|██▋       | 101/383 [00:07<00:23, 12.11it/s]
 28%|██▊       | 106/383 [00:07<00:18, 14.74it/s][A 27%|██▋       | 103/383 [00:07<00:21, 13.09it/s] 28%|██▊       | 106/383 [00:07<00:17, 16.15it/s] 27%|██▋       | 103/383 [00:07<00:20, 13.49it/s]
 28%|██▊       | 108/383 [00:07<00:17, 15.91it/s][A 27%|██▋       | 105/383 [00:07<00:19, 14.34it/s] 28%|██▊       | 108/383 [00:07<00:16, 16.87it/s] 27%|██▋       | 105/383 [00:07<00:18, 14.82it/s]
 29%|██▊       | 110/383 [00:07<00:16, 16.79it/s][A 28%|██▊       | 107/383 [00:07<00:17, 15.47it/s] 29%|██▊       | 110/383 [00:07<00:15, 17.56it/s] 28%|██▊       | 107/383 [00:07<00:17, 15.99it/s]
 29%|██▉       | 112/383 [00:07<00:15, 17.41it/s][A 28%|██▊       | 109/383 [00:07<00:16, 16.41it/s] 29%|██▉       | 112/383 [00:07<00:15, 18.05it/s] 28%|██▊       | 109/383 [00:07<00:16, 16.88it/s]
 30%|██▉       | 114/383 [00:07<00:15, 17.92it/s][A 29%|██▉       | 111/383 [00:07<00:15, 17.15it/s] 30%|██▉       | 114/383 [00:07<00:14, 18.44it/s] 29%|██▉       | 111/383 [00:07<00:15, 17.51it/s]
 30%|███       | 116/383 [00:07<00:14, 17.93it/s][A 30%|██▉       | 113/383 [00:07<00:15, 17.85it/s] 30%|███       | 116/383 [00:07<00:15, 17.76it/s] 30%|██▉       | 113/383 [00:07<00:15, 17.90it/s]
 31%|███       | 118/383 [00:08<00:15, 17.51it/s][A 30%|███       | 115/383 [00:08<00:15, 17.80it/s] 30%|███       | 115/383 [00:08<00:15, 17.85it/s] 31%|███       | 118/383 [00:08<00:15, 17.21it/s]
 31%|███▏      | 120/383 [00:08<00:15, 17.06it/s][A 31%|███       | 117/383 [00:08<00:15, 17.01it/s] 31%|███▏      | 120/383 [00:08<00:15, 17.17it/s] 31%|███       | 117/383 [00:08<00:15, 17.39it/s]
 32%|███▏      | 122/383 [00:08<00:15, 16.88it/s][A 31%|███       | 119/383 [00:08<00:15, 16.86it/s] 32%|███▏      | 122/383 [00:08<00:15, 17.38it/s] 31%|███       | 119/383 [00:08<00:15, 16.93it/s]
 32%|███▏      | 124/383 [00:08<00:14, 17.45it/s][A 32%|███▏      | 124/383 [00:08<00:14, 18.07it/s] 32%|███▏      | 121/383 [00:08<00:15, 16.63it/s] 32%|███▏      | 121/383 [00:08<00:15, 16.58it/s]
 33%|███▎      | 126/383 [00:08<00:14, 18.03it/s][A 33%|███▎      | 126/383 [00:08<00:14, 18.34it/s] 32%|███▏      | 123/383 [00:08<00:15, 16.93it/s] 32%|███▏      | 123/383 [00:08<00:14, 17.39it/s]
 33%|███▎      | 128/383 [00:08<00:13, 18.55it/s][A 33%|███▎      | 128/383 [00:08<00:13, 18.69it/s] 33%|███▎      | 125/383 [00:08<00:14, 17.44it/s] 33%|███▎      | 125/383 [00:08<00:14, 17.87it/s]
 34%|███▍      | 130/383 [00:08<00:14, 17.62it/s][A 33%|███▎      | 127/383 [00:08<00:14, 17.87it/s] 34%|███▍      | 130/383 [00:08<00:14, 17.08it/s] 33%|███▎      | 127/383 [00:08<00:13, 18.30it/s]
 34%|███▍      | 132/383 [00:08<00:14, 16.76it/s][A 34%|███▎      | 129/383 [00:08<00:14, 17.35it/s] 34%|███▎      | 129/383 [00:08<00:13, 18.21it/s] 34%|███▍      | 132/383 [00:08<00:15, 16.53it/s]
 35%|███▍      | 134/383 [00:08<00:15, 16.42it/s][A 34%|███▍      | 131/383 [00:08<00:14, 17.61it/s] 34%|███▍      | 131/383 [00:08<00:14, 16.90it/s] 35%|███▍      | 134/383 [00:09<00:15, 16.48it/s]
 36%|███▌      | 136/383 [00:09<00:15, 16.41it/s][A 35%|███▍      | 133/383 [00:09<00:15, 16.58it/s] 35%|███▍      | 133/383 [00:09<00:14, 16.89it/s] 36%|███▌      | 136/383 [00:09<00:15, 16.31it/s]
 36%|███▌      | 138/383 [00:09<00:14, 16.44it/s][A 35%|███▌      | 135/383 [00:09<00:14, 16.67it/s] 35%|███▌      | 135/383 [00:09<00:15, 16.32it/s] 36%|███▌      | 138/383 [00:09<00:14, 16.42it/s]
 37%|███▋      | 140/383 [00:09<00:14, 16.33it/s][A 36%|███▌      | 137/383 [00:09<00:14, 16.48it/s] 37%|███▋      | 140/383 [00:09<00:14, 16.42it/s] 36%|███▌      | 137/383 [00:09<00:15, 16.04it/s]
 37%|███▋      | 142/383 [00:09<00:14, 16.26it/s][A 36%|███▋      | 139/383 [00:09<00:14, 16.57it/s] 37%|███▋      | 142/383 [00:09<00:14, 16.37it/s] 36%|███▋      | 139/383 [00:09<00:15, 15.99it/s]
 38%|███▊      | 144/383 [00:09<00:14, 16.21it/s][A 38%|███▊      | 144/383 [00:09<00:14, 16.44it/s] 37%|███▋      | 141/383 [00:09<00:14, 16.18it/s] 37%|███▋      | 141/383 [00:09<00:14, 16.18it/s]
 38%|███▊      | 146/383 [00:09<00:14, 16.12it/s][A 38%|███▊      | 146/383 [00:09<00:14, 16.40it/s] 37%|███▋      | 143/383 [00:09<00:14, 16.22it/s] 37%|███▋      | 143/383 [00:09<00:14, 16.24it/s]
 39%|███▊      | 148/383 [00:09<00:14, 16.07it/s][A 38%|███▊      | 145/383 [00:09<00:14, 16.18it/s] 38%|███▊      | 145/383 [00:09<00:14, 16.08it/s] 39%|███▊      | 148/383 [00:09<00:15, 14.92it/s] 38%|███▊      | 147/383 [00:09<00:14, 16.19it/s] 38%|███▊      | 147/383 [00:09<00:14, 16.19it/s]
 39%|███▉      | 150/383 [00:10<00:17, 13.13it/s][A 39%|███▉      | 150/383 [00:10<00:18, 12.58it/s] 39%|███▉      | 149/383 [00:10<00:17, 13.41it/s] 39%|███▉      | 149/383 [00:10<00:17, 13.36it/s]
 40%|███▉      | 152/383 [00:10<00:19, 12.01it/s][A 40%|███▉      | 152/383 [00:10<00:19, 11.75it/s] 39%|███▉      | 151/383 [00:10<00:19, 12.02it/s] 39%|███▉      | 151/383 [00:10<00:19, 11.84it/s]
 40%|████      | 154/383 [00:10<00:25,  9.16it/s][A 40%|███▉      | 153/383 [00:10<00:19, 11.51it/s] 40%|███▉      | 153/383 [00:10<00:21, 10.89it/s] 40%|████      | 154/383 [00:10<00:26,  8.60it/s] 40%|████      | 155/383 [00:11<00:30,  7.41it/s]
 41%|████      | 156/383 [00:11<00:35,  6.34it/s][A 40%|████      | 155/383 [00:11<00:32,  7.11it/s] 41%|████      | 156/383 [00:11<00:36,  6.24it/s] 41%|████      | 156/383 [00:11<00:36,  6.30it/s] 41%|████      | 156/383 [00:11<00:36,  6.18it/s]
 41%|████      | 157/383 [00:11<00:41,  5.45it/s][A 41%|████      | 157/383 [00:11<00:39,  5.71it/s] 41%|████      | 157/383 [00:11<00:39,  5.73it/s] 41%|████      | 157/383 [00:11<00:41,  5.44it/s] 41%|████▏     | 158/383 [00:11<00:43,  5.19it/s]
 41%|████▏     | 158/383 [00:11<00:46,  4.80it/s][A 41%|████▏     | 158/383 [00:11<00:43,  5.11it/s] 42%|████▏     | 159/383 [00:11<00:42,  5.28it/s] 41%|████▏     | 158/383 [00:11<00:45,  4.98it/s]
 42%|████▏     | 159/383 [00:11<00:49,  4.54it/s][A 42%|████▏     | 159/383 [00:12<00:42,  5.24it/s] 42%|████▏     | 160/383 [00:12<00:45,  4.89it/s] 42%|████▏     | 159/383 [00:12<00:48,  4.61it/s]
 42%|████▏     | 160/383 [00:12<00:51,  4.32it/s][A 42%|████▏     | 160/383 [00:12<00:45,  4.95it/s] 42%|████▏     | 161/383 [00:12<00:44,  5.00it/s] 42%|████▏     | 160/383 [00:12<00:47,  4.72it/s]
 42%|████▏     | 161/383 [00:12<00:48,  4.60it/s][A 42%|████▏     | 161/383 [00:12<00:46,  4.79it/s] 42%|████▏     | 162/383 [00:12<00:42,  5.21it/s] 42%|████▏     | 161/383 [00:12<00:49,  4.51it/s]
 42%|████▏     | 162/383 [00:12<00:49,  4.49it/s][A 42%|████▏     | 162/383 [00:12<00:47,  4.62it/s] 43%|████▎     | 163/383 [00:12<00:46,  4.73it/s] 42%|████▏     | 162/383 [00:12<00:46,  4.71it/s]
 43%|████▎     | 163/383 [00:12<00:46,  4.70it/s][A 43%|████▎     | 163/383 [00:12<00:47,  4.67it/s] 43%|████▎     | 163/383 [00:13<00:45,  4.78it/s] 43%|████▎     | 164/383 [00:13<00:50,  4.38it/s]
 43%|████▎     | 164/383 [00:13<00:50,  4.38it/s][A 43%|████▎     | 164/383 [00:13<00:49,  4.41it/s] 43%|████▎     | 165/383 [00:13<00:46,  4.68it/s] 43%|████▎     | 164/383 [00:13<00:48,  4.50it/s]
 43%|████▎     | 165/383 [00:13<00:48,  4.52it/s][A 44%|████▍     | 168/383 [00:13<00:25,  8.53it/s] 43%|████▎     | 165/383 [00:13<00:47,  4.57it/s]
 44%|████▍     | 168/383 [00:13<00:25,  8.30it/s][A 45%|████▍     | 171/383 [00:13<00:17, 11.83it/s] 44%|████▍     | 168/383 [00:13<00:25,  8.39it/s] 43%|████▎     | 165/383 [00:13<00:51,  4.22it/s] 45%|████▌     | 173/383 [00:13<00:15, 13.42it/s]
 45%|████▍     | 171/383 [00:13<00:17, 11.83it/s][A 45%|████▍     | 171/383 [00:13<00:17, 11.94it/s] 44%|████▍     | 168/383 [00:13<00:27,  7.89it/s]
 45%|████▌     | 174/383 [00:13<00:14, 14.52it/s][A 46%|████▌     | 175/383 [00:13<00:15, 13.40it/s] 45%|████▌     | 174/383 [00:13<00:14, 14.69it/s] 45%|████▍     | 171/383 [00:13<00:18, 11.32it/s]
 46%|████▌     | 176/383 [00:13<00:14, 14.23it/s][A 46%|████▌     | 177/383 [00:13<00:15, 13.19it/s] 46%|████▌     | 176/383 [00:13<00:14, 14.34it/s] 45%|████▌     | 174/383 [00:13<00:15, 13.63it/s] 47%|████▋     | 179/383 [00:13<00:14, 14.54it/s]
 46%|████▋     | 178/383 [00:13<00:14, 14.56it/s][A 46%|████▋     | 178/383 [00:14<00:13, 14.68it/s]
 47%|████▋     | 180/383 [00:14<00:13, 15.57it/s][A 47%|████▋     | 181/383 [00:14<00:13, 15.35it/s] 46%|████▌     | 176/383 [00:14<00:15, 13.54it/s] 47%|████▋     | 180/383 [00:14<00:12, 15.71it/s] 48%|████▊     | 183/383 [00:14<00:12, 16.21it/s]
 48%|████▊     | 182/383 [00:14<00:12, 16.06it/s][A 46%|████▋     | 178/383 [00:14<00:14, 14.00it/s] 48%|████▊     | 182/383 [00:14<00:12, 16.32it/s]
 48%|████▊     | 184/383 [00:14<00:11, 16.69it/s][A 48%|████▊     | 185/383 [00:14<00:12, 15.98it/s] 47%|████▋     | 180/383 [00:14<00:13, 15.10it/s] 48%|████▊     | 184/383 [00:14<00:11, 16.86it/s] 48%|████▊     | 182/383 [00:14<00:12, 15.78it/s]
 49%|████▊     | 186/383 [00:14<00:12, 16.04it/s][A 49%|████▉     | 187/383 [00:14<00:12, 15.15it/s] 49%|████▊     | 186/383 [00:14<00:12, 15.58it/s] 48%|████▊     | 184/383 [00:14<00:12, 16.58it/s]
 49%|████▉     | 188/383 [00:14<00:11, 16.58it/s][A 50%|████▉     | 190/383 [00:14<00:10, 17.59it/s] 49%|████▉     | 188/383 [00:14<00:12, 16.12it/s]
 50%|████▉     | 191/383 [00:14<00:10, 18.59it/s][A 49%|████▊     | 186/383 [00:14<00:12, 15.43it/s] 50%|█████     | 192/383 [00:14<00:10, 17.93it/s] 50%|████▉     | 191/383 [00:14<00:10, 18.33it/s]
 50%|█████     | 193/383 [00:14<00:10, 18.50it/s][A 51%|█████     | 194/383 [00:14<00:10, 18.14it/s] 49%|████▉     | 188/383 [00:14<00:12, 15.40it/s] 50%|█████     | 193/383 [00:14<00:10, 18.47it/s]
 51%|█████     | 195/383 [00:14<00:10, 18.42it/s][A 51%|█████     | 196/383 [00:14<00:10, 18.44it/s] 50%|████▉     | 191/383 [00:14<00:10, 17.80it/s] 51%|█████     | 195/383 [00:14<00:10, 18.54it/s]
 51%|█████▏    | 197/383 [00:15<00:09, 18.69it/s][A 52%|█████▏    | 199/383 [00:15<00:09, 20.04it/s] 50%|█████     | 193/383 [00:15<00:10, 18.11it/s] 51%|█████▏    | 197/383 [00:15<00:09, 18.85it/s]
 52%|█████▏    | 199/383 [00:15<00:09, 19.04it/s][A 53%|█████▎    | 202/383 [00:15<00:08, 21.83it/s] 51%|█████     | 195/383 [00:15<00:10, 18.34it/s] 52%|█████▏    | 199/383 [00:15<00:09, 19.11it/s]
 53%|█████▎    | 202/383 [00:15<00:08, 21.12it/s][A 51%|█████▏    | 197/383 [00:15<00:10, 18.49it/s] 54%|█████▎    | 205/383 [00:15<00:07, 23.05it/s] 53%|█████▎    | 202/383 [00:15<00:08, 21.21it/s]
 54%|█████▎    | 205/383 [00:15<00:07, 22.77it/s][A 54%|█████▍    | 208/383 [00:15<00:07, 23.89it/s] 54%|█████▎    | 205/383 [00:15<00:07, 22.50it/s] 52%|█████▏    | 200/383 [00:15<00:09, 19.62it/s]
 54%|█████▍    | 208/383 [00:15<00:07, 23.61it/s][A 55%|█████▌    | 211/383 [00:15<00:07, 24.54it/s] 54%|█████▍    | 208/383 [00:15<00:07, 23.48it/s] 53%|█████▎    | 203/383 [00:15<00:08, 21.43it/s]
 55%|█████▌    | 211/383 [00:15<00:07, 24.30it/s][A 56%|█████▌    | 214/383 [00:15<00:06, 24.32it/s] 54%|█████▍    | 206/383 [00:15<00:07, 22.78it/s] 55%|█████▌    | 211/383 [00:15<00:07, 24.22it/s]
 56%|█████▌    | 214/383 [00:15<00:06, 24.72it/s][A 57%|█████▋    | 217/383 [00:15<00:06, 24.81it/s] 56%|█████▌    | 214/383 [00:15<00:06, 24.65it/s] 55%|█████▍    | 209/383 [00:15<00:07, 23.58it/s]
 57%|█████▋    | 217/383 [00:15<00:06, 24.31it/s][A 57%|█████▋    | 220/383 [00:15<00:06, 25.00it/s] 57%|█████▋    | 217/383 [00:15<00:06, 24.85it/s] 55%|█████▌    | 212/383 [00:15<00:07, 24.10it/s]
 57%|█████▋    | 220/383 [00:15<00:06, 24.81it/s][A 57%|█████▋    | 220/383 [00:15<00:06, 25.14it/s] 56%|█████▌    | 215/383 [00:15<00:06, 24.59it/s] 58%|█████▊    | 223/383 [00:16<00:07, 22.18it/s]
 58%|█████▊    | 223/383 [00:16<00:07, 22.78it/s][A 57%|█████▋    | 218/383 [00:16<00:06, 24.94it/s] 58%|█████▊    | 223/383 [00:16<00:07, 22.16it/s] 59%|█████▉    | 226/383 [00:16<00:07, 20.43it/s] 58%|█████▊    | 221/383 [00:16<00:06, 23.79it/s]
 59%|█████▉    | 226/383 [00:16<00:07, 20.63it/s][A 59%|█████▉    | 226/383 [00:16<00:07, 20.15it/s] 60%|█████▉    | 229/383 [00:16<00:07, 19.26it/s] 58%|█████▊    | 224/383 [00:16<00:07, 20.90it/s]
 60%|█████▉    | 229/383 [00:16<00:08, 19.09it/s][A 60%|█████▉    | 229/383 [00:16<00:08, 18.93it/s] 60%|██████    | 231/383 [00:16<00:08, 17.31it/s]
 60%|██████    | 231/383 [00:16<00:08, 17.83it/s][A 59%|█████▉    | 227/383 [00:16<00:08, 19.24it/s] 60%|██████    | 231/383 [00:16<00:08, 18.03it/s] 61%|██████    | 233/383 [00:16<00:09, 15.98it/s]
 61%|██████    | 233/383 [00:16<00:09, 16.35it/s][A 60%|██████    | 230/383 [00:16<00:08, 18.44it/s] 61%|██████    | 233/383 [00:16<00:09, 16.27it/s] 61%|██████▏   | 235/383 [00:16<00:09, 15.02it/s]
 61%|██████▏   | 235/383 [00:16<00:09, 15.32it/s][A 61%|██████    | 232/383 [00:16<00:08, 16.80it/s] 61%|██████▏   | 235/383 [00:16<00:09, 14.82it/s] 62%|██████▏   | 237/383 [00:17<00:10, 14.22it/s]
 62%|██████▏   | 237/383 [00:17<00:10, 14.47it/s][A 61%|██████    | 234/383 [00:17<00:09, 15.77it/s] 62%|██████▏   | 237/383 [00:17<00:10, 14.22it/s] 62%|██████▏   | 239/383 [00:17<00:10, 13.69it/s]
 62%|██████▏   | 239/383 [00:17<00:10, 13.98it/s][A 62%|██████▏   | 236/383 [00:17<00:09, 14.99it/s] 62%|██████▏   | 239/383 [00:17<00:10, 13.82it/s] 63%|██████▎   | 241/383 [00:17<00:10, 13.31it/s]
 63%|██████▎   | 241/383 [00:17<00:10, 13.64it/s][A 62%|██████▏   | 238/383 [00:17<00:10, 14.38it/s] 63%|██████▎   | 241/383 [00:17<00:10, 13.34it/s] 63%|██████▎   | 243/383 [00:17<00:10, 13.17it/s]
 63%|██████▎   | 243/383 [00:17<00:10, 13.29it/s][A 63%|██████▎   | 240/383 [00:17<00:10, 14.06it/s] 63%|██████▎   | 243/383 [00:17<00:10, 13.20it/s] 64%|██████▍   | 245/383 [00:17<00:10, 12.96it/s]
 64%|██████▍   | 245/383 [00:17<00:10, 13.27it/s][A 63%|██████▎   | 242/383 [00:17<00:10, 13.70it/s] 64%|██████▍   | 245/383 [00:17<00:10, 13.06it/s] 64%|██████▍   | 247/383 [00:17<00:10, 13.08it/s]
 64%|██████▍   | 247/383 [00:17<00:10, 13.27it/s][A 64%|██████▎   | 244/383 [00:17<00:10, 13.61it/s] 64%|██████▍   | 247/383 [00:17<00:10, 12.86it/s] 65%|██████▌   | 249/383 [00:17<00:10, 13.01it/s]
 65%|██████▌   | 249/383 [00:17<00:10, 12.84it/s][A 64%|██████▍   | 246/383 [00:18<00:10, 13.29it/s] 65%|██████▌   | 249/383 [00:18<00:10, 12.86it/s] 66%|██████▌   | 251/383 [00:18<00:10, 12.64it/s]
 66%|██████▌   | 251/383 [00:18<00:10, 12.97it/s][A 65%|██████▍   | 248/383 [00:18<00:10, 13.15it/s] 66%|██████▌   | 251/383 [00:18<00:10, 12.96it/s] 66%|██████▌   | 253/383 [00:18<00:10, 12.69it/s]
 66%|██████▌   | 253/383 [00:18<00:10, 12.96it/s][A 65%|██████▌   | 250/383 [00:18<00:10, 13.04it/s] 66%|██████▌   | 253/383 [00:18<00:10, 12.67it/s] 67%|██████▋   | 255/383 [00:18<00:09, 12.91it/s]
 67%|██████▋   | 255/383 [00:18<00:09, 13.06it/s][A 66%|██████▌   | 252/383 [00:18<00:10, 12.88it/s] 67%|██████▋   | 255/383 [00:18<00:10, 12.64it/s] 67%|██████▋   | 257/383 [00:18<00:09, 13.10it/s]
 67%|██████▋   | 257/383 [00:18<00:09, 13.16it/s][A 66%|██████▋   | 254/383 [00:18<00:10, 12.77it/s] 67%|██████▋   | 257/383 [00:18<00:09, 12.94it/s] 68%|██████▊   | 259/383 [00:18<00:09, 13.36it/s]
 68%|██████▊   | 259/383 [00:18<00:09, 13.28it/s][A 67%|██████▋   | 256/383 [00:18<00:09, 12.88it/s] 68%|██████▊   | 259/383 [00:18<00:09, 13.06it/s] 68%|██████▊   | 261/383 [00:18<00:09, 13.23it/s]
 68%|██████▊   | 261/383 [00:18<00:09, 13.49it/s][A 67%|██████▋   | 258/383 [00:18<00:09, 12.98it/s] 68%|██████▊   | 261/383 [00:19<00:09, 13.09it/s] 69%|██████▊   | 263/383 [00:19<00:08, 13.43it/s]
 69%|██████▊   | 263/383 [00:19<00:08, 13.47it/s][A 68%|██████▊   | 260/383 [00:19<00:09, 13.15it/s] 69%|██████▉   | 266/383 [00:19<00:07, 16.40it/s] 69%|██████▊   | 263/383 [00:19<00:09, 13.17it/s]
 69%|██████▉   | 266/383 [00:19<00:07, 16.05it/s][A 68%|██████▊   | 262/383 [00:19<00:09, 13.29it/s] 70%|███████   | 269/383 [00:19<00:06, 18.77it/s]
 70%|███████   | 269/383 [00:19<00:06, 18.55it/s][A 69%|██████▉   | 266/383 [00:19<00:07, 15.69it/s] 69%|██████▉   | 264/383 [00:19<00:08, 14.40it/s] 71%|███████   | 272/383 [00:19<00:05, 19.01it/s]
 71%|███████   | 272/383 [00:19<00:05, 19.77it/s][A 70%|███████   | 269/383 [00:19<00:06, 18.04it/s] 70%|██████▉   | 267/383 [00:19<00:07, 16.47it/s] 72%|███████▏  | 274/383 [00:19<00:06, 17.69it/s] 71%|███████   | 272/383 [00:19<00:05, 19.09it/s]
 72%|███████▏  | 275/383 [00:19<00:05, 18.50it/s][A 70%|███████   | 270/383 [00:19<00:05, 18.85it/s] 72%|███████▏  | 274/383 [00:19<00:06, 17.84it/s] 72%|███████▏  | 276/383 [00:19<00:06, 16.51it/s]
 72%|███████▏  | 277/383 [00:19<00:05, 17.72it/s][A 71%|███████▏  | 273/383 [00:19<00:05, 19.44it/s] 72%|███████▏  | 276/383 [00:19<00:06, 17.41it/s] 73%|███████▎  | 278/383 [00:19<00:06, 16.33it/s]
 73%|███████▎  | 279/383 [00:19<00:06, 16.86it/s][A 72%|███████▏  | 275/383 [00:19<00:05, 18.48it/s] 73%|███████▎  | 280/383 [00:19<00:06, 16.17it/s] 73%|███████▎  | 278/383 [00:19<00:06, 16.56it/s]
 73%|███████▎  | 281/383 [00:20<00:06, 16.18it/s][A 72%|███████▏  | 277/383 [00:20<00:05, 17.77it/s] 73%|███████▎  | 280/383 [00:20<00:06, 16.27it/s] 74%|███████▎  | 282/383 [00:20<00:07, 13.86it/s] 73%|███████▎  | 279/383 [00:20<00:06, 16.44it/s]
 74%|███████▍  | 283/383 [00:20<00:07, 14.27it/s][A 74%|███████▎  | 282/383 [00:20<00:07, 14.11it/s] 74%|███████▍  | 284/383 [00:20<00:07, 12.93it/s] 73%|███████▎  | 281/383 [00:20<00:06, 14.69it/s]
 74%|███████▍  | 285/383 [00:20<00:07, 12.89it/s][A 74%|███████▍  | 284/383 [00:20<00:07, 12.76it/s] 75%|███████▍  | 286/383 [00:20<00:07, 12.21it/s] 74%|███████▍  | 283/383 [00:20<00:07, 13.06it/s]
 75%|███████▍  | 287/383 [00:20<00:07, 12.08it/s][A 75%|███████▍  | 286/383 [00:20<00:08, 12.11it/s] 75%|███████▌  | 288/383 [00:20<00:07, 11.90it/s] 74%|███████▍  | 285/383 [00:20<00:07, 12.68it/s] 75%|███████▌  | 288/383 [00:20<00:08, 11.37it/s] 75%|███████▍  | 287/383 [00:20<00:07, 12.34it/s]
 75%|███████▌  | 289/383 [00:20<00:10,  8.76it/s][A 76%|███████▌  | 290/383 [00:21<00:12,  7.74it/s] 75%|███████▌  | 289/383 [00:21<00:10,  9.27it/s] 76%|███████▌  | 290/383 [00:21<00:12,  7.31it/s] 76%|███████▌  | 291/383 [00:21<00:13,  6.62it/s]
 76%|███████▌  | 291/383 [00:21<00:14,  6.29it/s][A 76%|███████▌  | 291/383 [00:21<00:14,  6.22it/s] 76%|███████▌  | 292/383 [00:21<00:15,  5.83it/s]
 76%|███████▌  | 292/383 [00:21<00:16,  5.59it/s][A 76%|███████▌  | 291/383 [00:21<00:14,  6.39it/s] 76%|███████▌  | 292/383 [00:21<00:17,  5.33it/s] 77%|███████▋  | 293/383 [00:21<00:17,  5.12it/s] 76%|███████▌  | 292/383 [00:22<00:16,  5.62it/s]
 77%|███████▋  | 293/383 [00:22<00:18,  4.92it/s][A 77%|███████▋  | 293/383 [00:22<00:18,  4.91it/s] 77%|███████▋  | 294/383 [00:22<00:18,  4.79it/s] 77%|███████▋  | 293/383 [00:22<00:17,  5.10it/s]
 77%|███████▋  | 294/383 [00:22<00:19,  4.60it/s][A 77%|███████▋  | 295/383 [00:22<00:18,  4.71it/s] 77%|███████▋  | 294/383 [00:22<00:19,  4.49it/s] 77%|███████▋  | 294/383 [00:22<00:18,  4.73it/s]
 77%|███████▋  | 295/383 [00:22<00:19,  4.42it/s][A 77%|███████▋  | 296/383 [00:22<00:20,  4.27it/s] 77%|███████▋  | 295/383 [00:22<00:20,  4.20it/s] 77%|███████▋  | 295/383 [00:22<00:19,  4.54it/s]
 77%|███████▋  | 296/383 [00:22<00:20,  4.18it/s][A 78%|███████▊  | 297/383 [00:22<00:19,  4.31it/s] 77%|███████▋  | 296/383 [00:23<00:21,  4.08it/s] 77%|███████▋  | 296/383 [00:23<00:20,  4.27it/s]
 78%|███████▊  | 297/383 [00:23<00:21,  3.93it/s][A 78%|███████▊  | 298/383 [00:23<00:20,  4.10it/s] 78%|███████▊  | 297/383 [00:23<00:21,  4.08it/s] 78%|███████▊  | 297/383 [00:23<00:21,  4.06it/s]
 78%|███████▊  | 298/383 [00:23<00:21,  4.03it/s][A 78%|███████▊  | 298/383 [00:23<00:19,  4.28it/s] 78%|███████▊  | 299/383 [00:23<00:21,  3.95it/s]
 78%|███████▊  | 299/383 [00:23<00:20,  4.01it/s][A 78%|███████▊  | 298/383 [00:23<00:22,  3.80it/s] 78%|███████▊  | 300/383 [00:23<00:20,  4.06it/s] 78%|███████▊  | 299/383 [00:23<00:20,  4.06it/s]
 78%|███████▊  | 300/383 [00:23<00:21,  3.86it/s][A 78%|███████▊  | 299/383 [00:23<00:22,  3.78it/s] 79%|███████▊  | 301/383 [00:24<00:20,  3.92it/s] 78%|███████▊  | 300/383 [00:24<00:21,  3.89it/s]
 79%|███████▊  | 301/383 [00:24<00:21,  3.78it/s][A 78%|███████▊  | 300/383 [00:24<00:22,  3.71it/s] 79%|███████▊  | 301/383 [00:24<00:20,  4.03it/s] 79%|███████▉  | 302/383 [00:24<00:21,  3.85it/s]
 79%|███████▉  | 302/383 [00:24<00:21,  3.74it/s][A 79%|███████▊  | 301/383 [00:24<00:22,  3.65it/s] 79%|███████▉  | 303/383 [00:24<00:21,  3.80it/s] 79%|███████▉  | 302/383 [00:24<00:21,  3.82it/s]
 79%|███████▉  | 303/383 [00:24<00:20,  3.93it/s][A 79%|███████▉  | 303/383 [00:24<00:20,  3.97it/s] 79%|███████▉  | 302/383 [00:24<00:22,  3.62it/s] 79%|███████▉  | 304/383 [00:24<00:21,  3.68it/s]
 79%|███████▉  | 304/383 [00:24<00:21,  3.76it/s][A 79%|███████▉  | 304/383 [00:25<00:19,  4.00it/s] 80%|███████▉  | 305/383 [00:25<00:20,  3.83it/s] 79%|███████▉  | 303/383 [00:25<00:22,  3.63it/s] 80%|███████▉  | 306/383 [00:25<00:18,  4.09it/s]
 80%|███████▉  | 305/383 [00:25<00:21,  3.63it/s][A 80%|███████▉  | 305/383 [00:25<00:20,  3.85it/s] 79%|███████▉  | 304/383 [00:25<00:21,  3.62it/s] 80%|████████  | 307/383 [00:25<00:19,  3.95it/s]
 80%|███████▉  | 306/383 [00:25<00:20,  3.67it/s][A 80%|███████▉  | 306/383 [00:25<00:20,  3.81it/s] 80%|███████▉  | 305/383 [00:25<00:21,  3.63it/s] 80%|████████  | 308/383 [00:25<00:19,  3.81it/s]
 80%|████████  | 307/383 [00:25<00:20,  3.65it/s][A 80%|████████  | 307/383 [00:25<00:20,  3.73it/s] 80%|███████▉  | 306/383 [00:25<00:21,  3.62it/s] 81%|████████  | 309/383 [00:26<00:19,  3.85it/s] 80%|████████  | 308/383 [00:26<00:19,  3.87it/s]
 80%|████████  | 308/383 [00:26<00:20,  3.67it/s][A 80%|████████  | 307/383 [00:26<00:21,  3.59it/s]
 81%|████████  | 309/383 [00:26<00:19,  3.76it/s][A 81%|████████  | 309/383 [00:26<00:19,  3.79it/s] 81%|████████  | 310/383 [00:26<00:19,  3.70it/s] 80%|████████  | 308/383 [00:26<00:20,  3.57it/s]
 81%|████████  | 310/383 [00:26<00:18,  3.86it/s][A 81%|████████  | 310/383 [00:26<00:19,  3.71it/s] 81%|████████  | 311/383 [00:26<00:19,  3.64it/s] 81%|████████  | 309/383 [00:26<00:19,  3.80it/s]
 81%|████████  | 311/383 [00:26<00:18,  3.83it/s][A 81%|████████  | 311/383 [00:26<00:19,  3.69it/s] 81%|████████  | 310/383 [00:26<00:18,  3.90it/s] 81%|████████▏ | 312/383 [00:26<00:19,  3.63it/s]
 81%|████████▏ | 312/383 [00:27<00:18,  3.86it/s][A 81%|████████▏ | 312/383 [00:27<00:19,  3.64it/s] 82%|████████▏ | 313/383 [00:27<00:19,  3.64it/s] 81%|████████  | 311/383 [00:27<00:18,  3.81it/s]
 82%|████████▏ | 313/383 [00:27<00:19,  3.66it/s][A 81%|████████▏ | 312/383 [00:27<00:18,  3.88it/s] 82%|████████▏ | 313/383 [00:27<00:18,  3.69it/s] 82%|████████▏ | 314/383 [00:27<00:19,  3.61it/s]
 82%|████████▏ | 314/383 [00:27<00:18,  3.68it/s][A 82%|████████▏ | 313/383 [00:27<00:17,  3.92it/s] 82%|████████▏ | 314/383 [00:27<00:18,  3.79it/s] 82%|████████▏ | 315/383 [00:27<00:19,  3.51it/s] 82%|████████▏ | 314/383 [00:27<00:17,  3.97it/s]
 82%|████████▏ | 315/383 [00:27<00:18,  3.67it/s][A 82%|████████▏ | 315/383 [00:27<00:17,  3.84it/s] 83%|████████▎ | 316/383 [00:28<00:18,  3.64it/s] 82%|████████▏ | 315/383 [00:28<00:16,  4.09it/s]
 83%|████████▎ | 316/383 [00:28<00:18,  3.72it/s][A 83%|████████▎ | 316/383 [00:28<00:17,  3.81it/s] 83%|████████▎ | 317/383 [00:28<00:18,  3.64it/s] 83%|████████▎ | 316/383 [00:28<00:17,  3.80it/s]
 83%|████████▎ | 317/383 [00:28<00:17,  3.71it/s][A 83%|████████▎ | 317/383 [00:28<00:18,  3.64it/s] 83%|████████▎ | 318/383 [00:28<00:18,  3.51it/s] 83%|████████▎ | 317/383 [00:28<00:17,  3.76it/s]
 83%|████████▎ | 318/383 [00:28<00:18,  3.56it/s][A 83%|████████▎ | 318/383 [00:28<00:17,  3.61it/s] 83%|████████▎ | 319/383 [00:28<00:17,  3.70it/s] 83%|████████▎ | 318/383 [00:29<00:17,  3.71it/s]
 83%|████████▎ | 319/383 [00:29<00:17,  3.68it/s][A 83%|████████▎ | 319/383 [00:29<00:17,  3.67it/s] 84%|████████▎ | 320/383 [00:29<00:17,  3.69it/s]
 84%|████████▎ | 320/383 [00:29<00:16,  3.77it/s][A 83%|████████▎ | 319/383 [00:29<00:17,  3.71it/s] 84%|████████▎ | 320/383 [00:29<00:17,  3.69it/s] 84%|████████▍ | 321/383 [00:29<00:17,  3.60it/s]
 84%|████████▍ | 321/383 [00:29<00:16,  3.78it/s][A 84%|████████▎ | 320/383 [00:29<00:17,  3.68it/s] 84%|████████▍ | 321/383 [00:29<00:16,  3.73it/s] 84%|████████▍ | 322/383 [00:29<00:16,  3.63it/s]
 84%|████████▍ | 322/383 [00:29<00:15,  3.85it/s][A 84%|████████▍ | 321/383 [00:29<00:17,  3.59it/s] 84%|████████▍ | 322/383 [00:29<00:16,  3.59it/s] 84%|████████▍ | 323/383 [00:29<00:16,  3.75it/s]
 84%|████████▍ | 323/383 [00:30<00:15,  3.99it/s][A 84%|████████▍ | 322/383 [00:30<00:16,  3.78it/s] 84%|████████▍ | 323/383 [00:30<00:16,  3.73it/s] 85%|████████▍ | 324/383 [00:30<00:15,  3.71it/s]
 85%|████████▍ | 324/383 [00:30<00:15,  3.77it/s][A 84%|████████▍ | 323/383 [00:30<00:16,  3.72it/s] 85%|████████▍ | 324/383 [00:30<00:15,  3.72it/s] 85%|████████▍ | 325/383 [00:30<00:15,  3.80it/s]
 85%|████████▍ | 325/383 [00:30<00:15,  3.86it/s][A 85%|████████▍ | 324/383 [00:30<00:16,  3.68it/s] 85%|████████▍ | 325/383 [00:30<00:15,  3.82it/s] 85%|████████▌ | 326/383 [00:30<00:14,  3.96it/s]
 85%|████████▌ | 326/383 [00:30<00:15,  3.76it/s][A 85%|████████▌ | 326/383 [00:30<00:14,  3.85it/s] 85%|████████▍ | 325/383 [00:30<00:15,  3.67it/s] 85%|████████▌ | 327/383 [00:30<00:14,  3.89it/s]
 85%|████████▌ | 327/383 [00:31<00:14,  3.74it/s][A 85%|████████▌ | 326/383 [00:31<00:14,  3.82it/s] 85%|████████▌ | 327/383 [00:31<00:15,  3.67it/s] 86%|████████▌ | 328/383 [00:31<00:14,  3.79it/s]
 86%|████████▌ | 328/383 [00:31<00:14,  3.68it/s][A 85%|████████▌ | 327/383 [00:31<00:14,  3.89it/s] 86%|████████▌ | 328/383 [00:31<00:15,  3.64it/s] 86%|████████▌ | 329/383 [00:31<00:14,  3.80it/s]
 86%|████████▌ | 329/383 [00:31<00:14,  3.74it/s][A 86%|████████▌ | 328/383 [00:31<00:14,  3.88it/s] 86%|████████▌ | 329/383 [00:31<00:14,  3.76it/s] 86%|████████▌ | 330/383 [00:31<00:14,  3.74it/s] 86%|████████▌ | 329/383 [00:31<00:13,  3.89it/s]
 86%|████████▌ | 330/383 [00:31<00:14,  3.76it/s][A 86%|████████▋ | 331/383 [00:31<00:12,  4.25it/s] 86%|████████▌ | 330/383 [00:32<00:14,  3.64it/s] 87%|████████▋ | 332/383 [00:32<00:10,  4.84it/s] 86%|████████▌ | 330/383 [00:32<00:12,  4.13it/s] 86%|████████▋ | 331/383 [00:32<00:12,  4.27it/s]
 86%|████████▋ | 331/383 [00:32<00:13,  3.73it/s][A 87%|████████▋ | 333/383 [00:32<00:09,  5.33it/s] 87%|████████▋ | 332/383 [00:32<00:11,  4.60it/s]
 87%|████████▋ | 332/383 [00:32<00:12,  4.22it/s][A 87%|████████▋ | 334/383 [00:32<00:08,  5.62it/s] 86%|████████▋ | 331/383 [00:32<00:13,  3.99it/s]
 87%|████████▋ | 333/383 [00:32<00:10,  4.70it/s][A 87%|████████▋ | 335/383 [00:32<00:08,  5.81it/s] 87%|████████▋ | 333/383 [00:32<00:10,  4.85it/s] 87%|████████▋ | 332/383 [00:32<00:11,  4.37it/s]
 87%|████████▋ | 334/383 [00:32<00:09,  5.06it/s][A 88%|████████▊ | 336/383 [00:32<00:07,  5.94it/s] 87%|████████▋ | 334/383 [00:32<00:09,  5.09it/s] 87%|████████▋ | 333/383 [00:32<00:10,  4.96it/s] 87%|████████▋ | 335/383 [00:32<00:08,  5.63it/s] 88%|████████▊ | 337/383 [00:32<00:07,  6.11it/s]
 87%|████████▋ | 335/383 [00:32<00:09,  5.25it/s][A 87%|████████▋ | 334/383 [00:32<00:09,  5.44it/s] 88%|████████▊ | 336/383 [00:33<00:08,  5.73it/s] 88%|████████▊ | 338/383 [00:33<00:07,  5.92it/s] 87%|████████▋ | 335/383 [00:33<00:08,  5.49it/s]
 88%|████████▊ | 336/383 [00:33<00:08,  5.25it/s][A 88%|████████▊ | 337/383 [00:33<00:07,  5.89it/s] 88%|████████▊ | 336/383 [00:33<00:07,  5.90it/s] 89%|████████▉ | 340/383 [00:33<00:05,  7.69it/s]
 88%|████████▊ | 337/383 [00:33<00:08,  5.39it/s][A 88%|████████▊ | 338/383 [00:33<00:07,  6.01it/s] 88%|████████▊ | 337/383 [00:33<00:07,  6.11it/s] 89%|████████▉ | 342/383 [00:33<00:04,  9.47it/s]
 88%|████████▊ | 338/383 [00:33<00:08,  5.49it/s][A 89%|████████▉ | 340/383 [00:33<00:05,  8.40it/s] 90%|████████▉ | 344/383 [00:33<00:03, 10.79it/s] 88%|████████▊ | 338/383 [00:33<00:07,  6.18it/s]
 89%|████████▊ | 339/383 [00:33<00:07,  5.73it/s][A 89%|████████▉ | 342/383 [00:33<00:04,  9.83it/s] 90%|█████████ | 346/383 [00:33<00:03, 11.54it/s] 89%|████████▉ | 340/383 [00:33<00:05,  8.27it/s]
 89%|████████▉ | 341/383 [00:33<00:05,  7.90it/s][A 90%|████████▉ | 344/383 [00:33<00:03, 11.11it/s] 91%|█████████ | 348/383 [00:33<00:02, 12.47it/s] 89%|████████▉ | 342/383 [00:33<00:04,  9.97it/s]
 90%|████████▉ | 343/383 [00:33<00:04,  9.62it/s][A 90%|█████████ | 346/383 [00:33<00:03, 12.21it/s] 91%|█████████▏| 350/383 [00:33<00:02, 13.35it/s] 90%|████████▉ | 344/383 [00:33<00:03, 10.55it/s]
 90%|█████████ | 345/383 [00:33<00:03, 11.04it/s][A 91%|█████████ | 348/383 [00:34<00:02, 12.99it/s] 92%|█████████▏| 352/383 [00:34<00:02, 13.38it/s] 90%|█████████ | 346/383 [00:34<00:03, 11.92it/s]
 91%|█████████ | 347/383 [00:34<00:02, 12.18it/s][A 91%|█████████▏| 350/383 [00:34<00:02, 13.07it/s] 92%|█████████▏| 354/383 [00:34<00:02, 13.70it/s] 91%|█████████ | 348/383 [00:34<00:02, 12.87it/s]
 91%|█████████ | 349/383 [00:34<00:02, 12.89it/s][A 92%|█████████▏| 352/383 [00:34<00:02, 13.78it/s] 93%|█████████▎| 356/383 [00:34<00:01, 14.47it/s] 91%|█████████▏| 350/383 [00:34<00:02, 13.40it/s]
 92%|█████████▏| 351/383 [00:34<00:02, 13.29it/s][A 93%|█████████▎| 358/383 [00:34<00:01, 15.28it/s] 92%|█████████▏| 354/383 [00:34<00:02, 14.07it/s] 92%|█████████▏| 352/383 [00:34<00:02, 13.68it/s]
 92%|█████████▏| 353/383 [00:34<00:02, 13.27it/s][A 93%|█████████▎| 356/383 [00:34<00:01, 14.36it/s] 92%|█████████▏| 354/383 [00:34<00:02, 13.78it/s]
 93%|█████████▎| 355/383 [00:34<00:02, 13.58it/s][A 93%|█████████▎| 358/383 [00:34<00:01, 14.75it/s] 93%|█████████▎| 356/383 [00:34<00:01, 13.64it/s] 94%|█████████▍| 360/383 [00:34<00:02,  9.73it/s]
 93%|█████████▎| 357/383 [00:34<00:01, 14.40it/s][A 93%|█████████▎| 358/383 [00:34<00:01, 14.88it/s]
 94%|█████████▎| 359/383 [00:34<00:01, 15.45it/s][A 94%|█████████▍| 360/383 [00:35<00:02,  9.97it/s] 95%|█████████▍| 362/383 [00:35<00:02,  8.33it/s] 94%|█████████▍| 360/383 [00:35<00:01, 11.86it/s]
 94%|█████████▍| 361/383 [00:35<00:02, 10.16it/s][A 95%|█████████▍| 362/383 [00:35<00:02,  8.17it/s] 95%|█████████▍| 362/383 [00:35<00:02,  9.10it/s] 95%|█████████▌| 364/383 [00:35<00:02,  7.20it/s]
 95%|█████████▍| 363/383 [00:35<00:02,  8.47it/s][A 95%|█████████▌| 365/383 [00:35<00:02,  6.86it/s] 95%|█████████▌| 364/383 [00:35<00:02,  7.14it/s] 96%|█████████▌| 367/383 [00:35<00:01,  8.69it/s] 95%|█████████▌| 364/383 [00:35<00:02,  7.82it/s] 96%|█████████▋| 369/383 [00:35<00:01, 10.47it/s]
 95%|█████████▌| 365/383 [00:35<00:02,  7.44it/s][A 95%|█████████▌| 365/383 [00:35<00:02,  6.85it/s] 95%|█████████▌| 365/383 [00:35<00:02,  7.36it/s] 97%|█████████▋| 371/383 [00:35<00:00, 12.16it/s] 96%|█████████▌| 367/383 [00:36<00:01,  8.61it/s]
 96%|█████████▌| 366/383 [00:36<00:02,  7.38it/s][A 96%|█████████▌| 367/383 [00:36<00:01,  9.25it/s] 97%|█████████▋| 373/383 [00:36<00:00, 13.67it/s] 96%|█████████▋| 369/383 [00:36<00:01, 10.40it/s]
 96%|█████████▌| 368/383 [00:36<00:01,  9.20it/s][A 96%|█████████▋| 369/383 [00:36<00:01, 10.96it/s] 98%|█████████▊| 375/383 [00:36<00:00, 14.64it/s] 97%|█████████▋| 371/383 [00:36<00:00, 12.04it/s]
 97%|█████████▋| 370/383 [00:36<00:01, 10.89it/s][A 98%|█████████▊| 377/383 [00:36<00:00, 15.82it/s] 97%|█████████▋| 371/383 [00:36<00:00, 12.45it/s] 97%|█████████▋| 373/383 [00:36<00:00, 13.42it/s]
 97%|█████████▋| 372/383 [00:36<00:00, 12.34it/s][A 97%|█████████▋| 373/383 [00:36<00:00, 13.84it/s] 99%|█████████▉| 380/383 [00:36<00:00, 18.26it/s] 98%|█████████▊| 375/383 [00:36<00:00, 14.87it/s]
 98%|█████████▊| 374/383 [00:36<00:00, 13.79it/s][A 98%|█████████▊| 376/383 [00:36<00:00, 15.80it/s]100%|██████████| 383/383 [00:36<00:00, 19.14it/s]100%|██████████| 383/383 [00:36<00:00, 10.47it/s]
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
 99%|█████████▊| 378/383 [00:36<00:00, 16.92it/s]/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)

 98%|█████████▊| 377/383 [00:36<00:00, 15.74it/s][A/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/pytorch/torch/utils/checkpoint.py:426: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 99%|█████████▉| 379/383 [00:36<00:00, 18.39it/s] 99%|█████████▉| 381/383 [00:36<00:00, 19.50it/s]
 99%|█████████▉| 380/383 [00:36<00:00, 17.69it/s][A100%|█████████▉| 382/383 [00:36<00:00, 20.61it/s]100%|██████████| 383/383 [00:36<00:00, 10.39it/s]
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
100%|██████████| 383/383 [00:36<00:00, 10.38it/s]
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/pytorch/torch/utils/checkpoint.py:426: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

100%|██████████| 383/383 [00:36<00:00, 18.72it/s][A100%|██████████| 383/383 [00:36<00:00, 10.37it/s]
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
                                                      {'mmlu_loss': 3.0223227686421366, 'mmlu_eval_accuracy_conceptual_physics': 0.2692307692307692, 'mmlu_eval_accuracy_logical_fallacies': nan, 'mmlu_eval_accuracy_professional_psychology': nan, 'mmlu_eval_accuracy_public_relations': nan, 'mmlu_eval_accuracy_security_studies': nan, 'mmlu_eval_accuracy_high_school_physics': nan, 'mmlu_eval_accuracy_formal_logic': 0.21428571428571427, 'mmlu_eval_accuracy_anatomy': 0.21428571428571427, 'mmlu_eval_accuracy_moral_scenarios': nan, 'mmlu_eval_accuracy_moral_disputes': nan, 'mmlu_eval_accuracy_management': nan, 'mmlu_eval_accuracy_high_school_european_history': 0.3333333333333333, 'mmlu_eval_accuracy_professional_law': nan, 'mmlu_eval_accuracy_elementary_mathematics': 0.21951219512195122, 'mmlu_eval_accuracy_high_school_computer_science': 0.3333333333333333, 'mmlu_eval_accuracy_high_school_geography': 0.5, 'mmlu_eval_accuracy_global_facts': 0.3, 'mmlu_eval_accuracy_high_school_biology': 0.28125, 'mmlu_eval_accuracy_philosophy': nan, 'mmlu_eval_accuracy_astronomy': 0.1875, 'mmlu_eval_accuracy_nutrition': nan, 'mmlu_eval_accuracy_high_school_government_and_politics': nan, 'mmlu_eval_accuracy_medical_genetics': nan, 'mmlu_eval_accuracy_human_aging': nan, 'mmlu_eval_accuracy_human_sexuality': nan, 'mmlu_eval_accuracy_college_medicine': 0.22727272727272727, 'mmlu_eval_accuracy_abstract_algebra': 0.18181818181818182, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_us_foreign_policy': nan, 'mmlu_eval_accuracy_college_computer_science': 0.36363636363636365, 'mmlu_eval_accuracy_machine_learning': nan, 'mmlu_eval_accuracy_professional_accounting': nan, 'mmlu_eval_accuracy_world_religions': nan, 'mmlu_eval_accuracy_miscellaneous': nan, 'mmlu_eval_accuracy_jurisprudence': nan, 'mmlu_eval_accuracy_high_school_chemistry': 0.22727272727272727, 'mmlu_eval_accuracy_virology': nan, 'mmlu_eval_accuracy_high_school_mathematics': nan, 'mmlu_eval_accuracy_electrical_engineering': 0.375, 'mmlu_eval_accuracy_high_school_world_history': nan, 'mmlu_eval_accuracy_prehistory': nan, 'mmlu_eval_accuracy_high_school_statistics': nan, 'mmlu_eval_accuracy_high_school_psychology': nan, 'mmlu_eval_accuracy_high_school_microeconomics': nan, 'mmlu_eval_accuracy_high_school_us_history': nan, 'mmlu_eval_accuracy_college_biology': 0.4375, 'mmlu_eval_accuracy_high_school_macroeconomics': nan, 'mmlu_eval_accuracy_professional_medicine': nan, 'mmlu_eval_accuracy_clinical_knowledge': 0.20689655172413793, 'mmlu_eval_accuracy_business_ethics': 0.09090909090909091, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_econometrics': 0.5, 'mmlu_eval_accuracy_marketing': nan, 'mmlu_eval_accuracy_international_law': nan, 'mmlu_eval_accuracy_sociology': nan, 'mmlu_eval_accuracy_computer_security': 0.18181818181818182, 'mmlu_eval_accuracy_college_mathematics': 0.18181818181818182, 'mmlu_eval_accuracy': nan, 'epoch': 5.57}
 38%|███▊      | 768/2048 [1:09:04<1:42:02,  4.78s/it]/home/bagus/pytorch/torch/utils/checkpoint.py:426: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/pytorch/torch/utils/checkpoint.py:426: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 38%|███▊      | 769/2048 [1:09:09<7:15:09, 20.41s/it] 38%|███▊      | 770/2048 [1:09:14<5:34:36, 15.71s/it] 38%|███▊      | 771/2048 [1:09:18<4:22:01, 12.31s/it] 38%|███▊      | 772/2048 [1:09:23<3:31:09,  9.93s/it] 38%|███▊      | 773/2048 [1:09:27<2:54:02,  8.19s/it] 38%|███▊      | 774/2048 [1:09:30<2:25:55,  6.87s/it] 38%|███▊      | 775/2048 [1:09:35<2:09:09,  6.09s/it] 38%|███▊      | 776/2048 [1:09:39<1:54:47,  5.41s/it] 38%|███▊      | 777/2048 [1:09:42<1:41:36,  4.80s/it] 38%|███▊      | 778/2048 [1:09:47<1:43:55,  4.91s/it] 38%|███▊      | 779/2048 [1:09:52<1:45:28,  4.99s/it] 38%|███▊      | 780/2048 [1:09:57<1:46:23,  5.03s/it] 38%|███▊      | 781/2048 [1:10:03<1:46:31,  5.04s/it] 38%|███▊      | 782/2048 [1:10:07<1:44:42,  4.96s/it] 38%|███▊      | 783/2048 [1:10:12<1:41:22,  4.81s/it] 38%|███▊      | 784/2048 [1:10:16<1:37:02,  4.61s/it]                                                      {'loss': 0.269, 'learning_rate': 0.00014156766174583667, 'epoch': 5.69}
 38%|███▊      | 784/2048 [1:10:16<1:37:02,  4.61s/it] 38%|███▊      | 785/2048 [1:10:20<1:33:18,  4.43s/it] 38%|███▊      | 786/2048 [1:10:24<1:30:51,  4.32s/it] 38%|███▊      | 787/2048 [1:10:28<1:30:20,  4.30s/it] 38%|███▊      | 788/2048 [1:10:32<1:28:44,  4.23s/it] 39%|███▊      | 789/2048 [1:10:36<1:23:26,  3.98s/it] 39%|███▊      | 790/2048 [1:10:40<1:25:43,  4.09s/it] 39%|███▊      | 791/2048 [1:10:45<1:32:27,  4.41s/it] 39%|███▊      | 792/2048 [1:10:50<1:37:02,  4.64s/it] 39%|███▊      | 793/2048 [1:10:55<1:40:09,  4.79s/it] 39%|███▉      | 794/2048 [1:11:00<1:41:19,  4.85s/it] 39%|███▉      | 795/2048 [1:11:05<1:41:42,  4.87s/it] 39%|███▉      | 796/2048 [1:11:10<1:39:57,  4.79s/it] 39%|███▉      | 797/2048 [1:11:14<1:36:24,  4.62s/it] 39%|███▉      | 798/2048 [1:11:19<1:34:19,  4.53s/it] 39%|███▉      | 799/2048 [1:11:22<1:29:52,  4.32s/it] 39%|███▉      | 800/2048 [1:11:26<1:27:06,  4.19s/it]                                                      {'loss': 0.259, 'learning_rate': 0.00013925262631803723, 'epoch': 5.8}
 39%|███▉      | 800/2048 [1:11:26<1:27:06,  4.19s/it] 39%|███▉      | 801/2048 [1:11:30<1:23:30,  4.02s/it] 39%|███▉      | 802/2048 [1:11:33<1:19:16,  3.82s/it] 39%|███▉      | 803/2048 [1:11:38<1:27:38,  4.22s/it] 39%|███▉      | 804/2048 [1:11:44<1:33:26,  4.51s/it] 39%|███▉      | 805/2048 [1:11:49<1:37:19,  4.70s/it] 39%|███▉      | 806/2048 [1:11:54<1:39:43,  4.82s/it] 39%|███▉      | 807/2048 [1:11:59<1:40:15,  4.85s/it] 39%|███▉      | 808/2048 [1:12:03<1:38:54,  4.79s/it] 40%|███▉      | 809/2048 [1:12:08<1:36:51,  4.69s/it] 40%|███▉      | 810/2048 [1:12:12<1:32:02,  4.46s/it] 40%|███▉      | 811/2048 [1:12:16<1:28:07,  4.27s/it] 40%|███▉      | 812/2048 [1:12:20<1:26:27,  4.20s/it] 40%|███▉      | 813/2048 [1:12:23<1:24:27,  4.10s/it] 40%|███▉      | 814/2048 [1:12:27<1:19:26,  3.86s/it] 40%|███▉      | 815/2048 [1:12:31<1:22:28,  4.01s/it] 40%|███▉      | 816/2048 [1:12:36<1:29:31,  4.36s/it]                                                      {'loss': 0.2596, 'learning_rate': 0.00013691244733080485, 'epoch': 5.92}
 40%|███▉      | 816/2048 [1:12:36<1:29:31,  4.36s/it] 40%|███▉      | 817/2048 [1:12:41<1:34:24,  4.60s/it] 40%|███▉      | 818/2048 [1:12:47<1:37:43,  4.77s/it] 40%|███▉      | 819/2048 [1:12:52<1:39:05,  4.84s/it] 40%|████      | 820/2048 [1:12:56<1:39:04,  4.84s/it] 40%|████      | 821/2048 [1:13:01<1:37:00,  4.74s/it] 40%|████      | 822/2048 [1:13:05<1:33:48,  4.59s/it] 40%|████      | 823/2048 [1:13:09<1:31:00,  4.46s/it] 40%|████      | 824/2048 [1:13:13<1:27:20,  4.28s/it] 40%|████      | 825/2048 [1:13:17<1:24:38,  4.15s/it] 40%|████      | 826/2048 [1:13:20<1:19:22,  3.90s/it] 40%|████      | 827/2048 [1:13:24<1:15:40,  3.72s/it] 40%|████      | 828/2048 [1:13:29<1:23:39,  4.11s/it] 40%|████      | 829/2048 [1:13:34<1:30:02,  4.43s/it] 41%|████      | 830/2048 [1:13:39<1:35:32,  4.71s/it] 41%|████      | 831/2048 [1:13:44<1:37:32,  4.81s/it] 41%|████      | 832/2048 [1:13:49<1:37:45,  4.82s/it]                                                      {'loss': 0.2467, 'learning_rate': 0.00013454862380305794, 'epoch': 6.03}
 41%|████      | 832/2048 [1:13:49<1:37:45,  4.82s/it] 41%|████      | 833/2048 [1:13:54<1:36:41,  4.78s/it] 41%|████      | 834/2048 [1:13:58<1:34:13,  4.66s/it] 41%|████      | 835/2048 [1:14:02<1:30:42,  4.49s/it] 41%|████      | 836/2048 [1:14:06<1:27:08,  4.31s/it] 41%|████      | 837/2048 [1:14:10<1:25:14,  4.22s/it] 41%|████      | 838/2048 [1:14:14<1:23:28,  4.14s/it] 41%|████      | 839/2048 [1:14:17<1:17:45,  3.86s/it] 41%|████      | 840/2048 [1:14:21<1:15:50,  3.77s/it] 41%|████      | 841/2048 [1:14:26<1:24:15,  4.19s/it] 41%|████      | 842/2048 [1:14:31<1:30:05,  4.48s/it] 41%|████      | 843/2048 [1:14:36<1:34:03,  4.68s/it] 41%|████      | 844/2048 [1:14:41<1:36:16,  4.80s/it] 41%|████▏     | 845/2048 [1:14:46<1:36:48,  4.83s/it] 41%|████▏     | 846/2048 [1:14:51<1:35:01,  4.74s/it] 41%|████▏     | 847/2048 [1:14:55<1:33:01,  4.65s/it] 41%|████▏     | 848/2048 [1:14:59<1:29:17,  4.46s/it]                                                      {'loss': 0.206, 'learning_rate': 0.00013216266989939988, 'epoch': 6.15}
 41%|████▏     | 848/2048 [1:14:59<1:29:17,  4.46s/it] 41%|████▏     | 849/2048 [1:15:03<1:24:55,  4.25s/it] 42%|████▏     | 850/2048 [1:15:07<1:19:58,  4.01s/it] 42%|████▏     | 851/2048 [1:15:11<1:20:37,  4.04s/it] 42%|████▏     | 852/2048 [1:15:14<1:15:09,  3.77s/it] 42%|████▏     | 853/2048 [1:15:18<1:20:06,  4.02s/it] 42%|████▏     | 854/2048 [1:15:24<1:26:54,  4.37s/it] 42%|████▏     | 855/2048 [1:15:29<1:31:31,  4.60s/it] 42%|████▏     | 856/2048 [1:15:34<1:34:38,  4.76s/it] 42%|████▏     | 857/2048 [1:15:39<1:36:12,  4.85s/it] 42%|████▏     | 858/2048 [1:15:44<1:34:57,  4.79s/it] 42%|████▏     | 859/2048 [1:15:48<1:32:52,  4.69s/it] 42%|████▏     | 860/2048 [1:15:52<1:29:48,  4.54s/it] 42%|████▏     | 861/2048 [1:15:56<1:27:07,  4.40s/it] 42%|████▏     | 862/2048 [1:16:00<1:25:04,  4.30s/it] 42%|████▏     | 863/2048 [1:16:04<1:21:38,  4.13s/it] 42%|████▏     | 864/2048 [1:16:08<1:17:55,  3.95s/it]                                                      {'loss': 0.1867, 'learning_rate': 0.00012975611396020954, 'epoch': 6.27}
 42%|████▏     | 864/2048 [1:16:08<1:17:55,  3.95s/it] 42%|████▏     | 865/2048 [1:16:11<1:16:09,  3.86s/it] 42%|████▏     | 866/2048 [1:16:16<1:23:49,  4.26s/it] 42%|████▏     | 867/2048 [1:16:22<1:29:09,  4.53s/it] 42%|████▏     | 868/2048 [1:16:27<1:32:47,  4.72s/it] 42%|████▏     | 869/2048 [1:16:32<1:34:44,  4.82s/it] 42%|████▏     | 870/2048 [1:16:37<1:35:11,  4.85s/it] 43%|████▎     | 871/2048 [1:16:42<1:34:35,  4.82s/it] 43%|████▎     | 872/2048 [1:16:46<1:31:53,  4.69s/it] 43%|████▎     | 873/2048 [1:16:50<1:27:31,  4.47s/it] 43%|████▎     | 874/2048 [1:16:54<1:24:20,  4.31s/it] 43%|████▎     | 875/2048 [1:16:58<1:21:33,  4.17s/it] 43%|████▎     | 876/2048 [1:17:02<1:19:53,  4.09s/it] 43%|████▎     | 877/2048 [1:17:05<1:15:30,  3.87s/it] 43%|████▎     | 878/2048 [1:17:10<1:19:55,  4.10s/it] 43%|████▎     | 879/2048 [1:17:15<1:26:07,  4.42s/it] 43%|████▎     | 880/2048 [1:17:20<1:30:24,  4.64s/it]                                                      {'loss': 0.1765, 'learning_rate': 0.00012733049752265175, 'epoch': 6.38}
 43%|████▎     | 880/2048 [1:17:20<1:30:24,  4.64s/it] 43%|████▎     | 881/2048 [1:17:25<1:32:59,  4.78s/it] 43%|████▎     | 882/2048 [1:17:30<1:33:42,  4.82s/it] 43%|████▎     | 883/2048 [1:17:35<1:32:31,  4.77s/it] 43%|████▎     | 884/2048 [1:17:39<1:29:48,  4.63s/it] 43%|████▎     | 885/2048 [1:17:43<1:26:27,  4.46s/it] 43%|████▎     | 886/2048 [1:17:47<1:24:47,  4.38s/it] 43%|████▎     | 887/2048 [1:17:51<1:19:25,  4.11s/it] 43%|████▎     | 888/2048 [1:17:54<1:17:44,  4.02s/it] 43%|████▎     | 889/2048 [1:17:58<1:15:12,  3.89s/it] 43%|████▎     | 890/2048 [1:18:02<1:13:31,  3.81s/it] 44%|████▎     | 891/2048 [1:18:07<1:21:20,  4.22s/it] 44%|████▎     | 892/2048 [1:18:12<1:26:45,  4.50s/it] 44%|████▎     | 893/2048 [1:18:17<1:30:24,  4.70s/it] 44%|████▎     | 894/2048 [1:18:22<1:32:31,  4.81s/it] 44%|████▎     | 895/2048 [1:18:27<1:32:21,  4.81s/it] 44%|████▍     | 896/2048 [1:18:32<1:31:12,  4.75s/it]                                                      {'loss': 0.232, 'learning_rate': 0.00012488737433323426, 'epoch': 6.5}
 44%|████▍     | 896/2048 [1:18:32<1:31:12,  4.75s/it] 44%|████▍     | 897/2048 [1:18:36<1:28:21,  4.61s/it] 44%|████▍     | 898/2048 [1:18:40<1:23:45,  4.37s/it] 44%|████▍     | 899/2048 [1:18:44<1:22:08,  4.29s/it] 44%|████▍     | 900/2048 [1:18:48<1:20:14,  4.19s/it] 44%|████▍     | 901/2048 [1:18:52<1:18:40,  4.12s/it] 44%|████▍     | 902/2048 [1:18:55<1:14:03,  3.88s/it] 44%|████▍     | 903/2048 [1:19:00<1:18:18,  4.10s/it] 44%|████▍     | 904/2048 [1:19:05<1:24:21,  4.42s/it] 44%|████▍     | 905/2048 [1:19:10<1:28:25,  4.64s/it] 44%|████▍     | 906/2048 [1:19:15<1:31:02,  4.78s/it] 44%|████▍     | 907/2048 [1:19:20<1:31:51,  4.83s/it] 44%|████▍     | 908/2048 [1:19:25<1:31:10,  4.80s/it] 44%|████▍     | 909/2048 [1:19:29<1:29:19,  4.71s/it] 44%|████▍     | 910/2048 [1:19:34<1:27:22,  4.61s/it] 44%|████▍     | 911/2048 [1:19:38<1:23:26,  4.40s/it] 45%|████▍     | 912/2048 [1:19:42<1:22:09,  4.34s/it]                                                      {'loss': 0.2243, 'learning_rate': 0.0001224283093525438, 'epoch': 6.61}
 45%|████▍     | 912/2048 [1:19:42<1:22:09,  4.34s/it] 45%|████▍     | 913/2048 [1:19:46<1:19:16,  4.19s/it] 45%|████▍     | 914/2048 [1:19:49<1:17:21,  4.09s/it] 45%|████▍     | 915/2048 [1:19:53<1:14:28,  3.94s/it] 45%|████▍     | 916/2048 [1:19:58<1:21:21,  4.31s/it] 45%|████▍     | 917/2048 [1:20:03<1:26:07,  4.57s/it] 45%|████▍     | 918/2048 [1:20:09<1:29:19,  4.74s/it] 45%|████▍     | 919/2048 [1:20:14<1:31:03,  4.84s/it] 45%|████▍     | 920/2048 [1:20:18<1:31:15,  4.85s/it] 45%|████▍     | 921/2048 [1:20:23<1:29:35,  4.77s/it] 45%|████▌     | 922/2048 [1:20:27<1:26:24,  4.60s/it] 45%|████▌     | 923/2048 [1:20:31<1:23:43,  4.46s/it] 45%|████▌     | 924/2048 [1:20:35<1:20:34,  4.30s/it] 45%|████▌     | 925/2048 [1:20:39<1:18:56,  4.22s/it] 45%|████▌     | 926/2048 [1:20:43<1:17:21,  4.14s/it] 45%|████▌     | 927/2048 [1:20:46<1:11:29,  3.83s/it] 45%|████▌     | 928/2048 [1:20:51<1:15:49,  4.06s/it]                                                      {'loss': 0.2091, 'learning_rate': 0.00011995487775279917, 'epoch': 6.73}
 45%|████▌     | 928/2048 [1:20:51<1:15:49,  4.06s/it] 45%|████▌     | 929/2048 [1:20:56<1:21:57,  4.39s/it] 45%|████▌     | 930/2048 [1:21:01<1:26:09,  4.62s/it] 45%|████▌     | 931/2048 [1:21:06<1:28:58,  4.78s/it] 46%|████▌     | 932/2048 [1:21:11<1:29:53,  4.83s/it] 46%|████▌     | 933/2048 [1:21:16<1:29:58,  4.84s/it] 46%|████▌     | 934/2048 [1:21:21<1:27:30,  4.71s/it] 46%|████▌     | 935/2048 [1:21:25<1:24:44,  4.57s/it] 46%|████▌     | 936/2048 [1:21:29<1:21:46,  4.41s/it] 46%|████▌     | 937/2048 [1:21:33<1:19:24,  4.29s/it] 46%|████▌     | 938/2048 [1:21:37<1:16:10,  4.12s/it] 46%|████▌     | 939/2048 [1:21:40<1:12:06,  3.90s/it] 46%|████▌     | 940/2048 [1:21:44<1:10:33,  3.82s/it] 46%|████▌     | 941/2048 [1:21:49<1:17:57,  4.23s/it] 46%|████▌     | 942/2048 [1:21:54<1:23:06,  4.51s/it] 46%|████▌     | 943/2048 [1:21:59<1:26:26,  4.69s/it] 46%|████▌     | 944/2048 [1:22:04<1:28:03,  4.79s/it]                                                      {'loss': 0.2004, 'learning_rate': 0.00011746866390886305, 'epoch': 6.85}
 46%|████▌     | 944/2048 [1:22:04<1:28:03,  4.79s/it] 46%|████▌     | 945/2048 [1:22:09<1:28:19,  4.80s/it] 46%|████▌     | 946/2048 [1:22:14<1:26:37,  4.72s/it] 46%|████▌     | 947/2048 [1:22:18<1:24:15,  4.59s/it] 46%|████▋     | 948/2048 [1:22:22<1:20:45,  4.40s/it] 46%|████▋     | 949/2048 [1:22:26<1:20:01,  4.37s/it] 46%|████▋     | 950/2048 [1:22:30<1:16:58,  4.21s/it] 46%|████▋     | 951/2048 [1:22:34<1:13:53,  4.04s/it] 46%|████▋     | 952/2048 [1:22:37<1:08:45,  3.76s/it] 47%|████▋     | 953/2048 [1:22:41<1:13:08,  4.01s/it] 47%|████▋     | 954/2048 [1:22:46<1:19:26,  4.36s/it] 47%|████▋     | 955/2048 [1:22:52<1:23:43,  4.60s/it] 47%|████▋     | 956/2048 [1:22:57<1:26:25,  4.75s/it] 47%|████▋     | 957/2048 [1:23:02<1:26:37,  4.76s/it] 47%|████▋     | 958/2048 [1:23:06<1:26:15,  4.75s/it] 47%|████▋     | 959/2048 [1:23:11<1:24:11,  4.64s/it] 47%|████▋     | 960/2048 [1:23:15<1:22:00,  4.52s/it]                                                      {'loss': 0.2238, 'learning_rate': 0.0001149712603833589, 'epoch': 6.96}
 47%|████▋     | 960/2048 [1:23:15<1:22:00,  4.52s/it] 47%|████▋     | 961/2048 [1:23:19<1:19:10,  4.37s/it] 47%|████▋     | 962/2048 [1:23:23<1:15:58,  4.20s/it] 47%|████▋     | 963/2048 [1:23:27<1:14:58,  4.15s/it] 47%|████▋     | 964/2048 [1:23:30<1:11:45,  3.97s/it] 47%|████▋     | 965/2048 [1:23:34<1:09:19,  3.84s/it] 47%|████▋     | 966/2048 [1:23:39<1:16:50,  4.26s/it] 47%|████▋     | 967/2048 [1:23:44<1:21:41,  4.53s/it] 47%|████▋     | 968/2048 [1:23:49<1:24:58,  4.72s/it] 47%|████▋     | 969/2048 [1:23:54<1:26:48,  4.83s/it] 47%|████▋     | 970/2048 [1:23:59<1:26:22,  4.81s/it] 47%|████▋     | 971/2048 [1:24:04<1:25:07,  4.74s/it] 47%|████▋     | 972/2048 [1:24:08<1:23:18,  4.65s/it] 48%|████▊     | 973/2048 [1:24:12<1:20:43,  4.51s/it] 48%|████▊     | 974/2048 [1:24:16<1:16:24,  4.27s/it] 48%|████▊     | 975/2048 [1:24:20<1:13:26,  4.11s/it] 48%|████▊     | 976/2048 [1:24:24<1:12:07,  4.04s/it]                                                      {'loss': 0.1942, 'learning_rate': 0.00011246426690654335, 'epoch': 7.08}
 48%|████▊     | 976/2048 [1:24:24<1:12:07,  4.04s/it] 48%|████▊     | 977/2048 [1:24:27<1:09:28,  3.89s/it] 48%|████▊     | 978/2048 [1:24:31<1:09:05,  3.87s/it] 48%|████▊     | 979/2048 [1:24:36<1:15:57,  4.26s/it] 48%|████▊     | 980/2048 [1:24:41<1:20:41,  4.53s/it] 48%|████▊     | 981/2048 [1:24:47<1:23:43,  4.71s/it] 48%|████▊     | 982/2048 [1:24:52<1:25:14,  4.80s/it] 48%|████▊     | 983/2048 [1:24:56<1:25:37,  4.82s/it] 48%|████▊     | 984/2048 [1:25:01<1:24:46,  4.78s/it] 48%|████▊     | 985/2048 [1:25:05<1:22:02,  4.63s/it] 48%|████▊     | 986/2048 [1:25:10<1:19:30,  4.49s/it] 48%|████▊     | 987/2048 [1:25:13<1:16:07,  4.31s/it] 48%|████▊     | 988/2048 [1:25:18<1:14:47,  4.23s/it] 48%|████▊     | 989/2048 [1:25:21<1:11:38,  4.06s/it] 48%|████▊     | 990/2048 [1:25:24<1:07:08,  3.81s/it] 48%|████▊     | 991/2048 [1:25:29<1:12:47,  4.13s/it] 48%|████▊     | 992/2048 [1:25:34<1:18:12,  4.44s/it]                                                      {'loss': 0.1598, 'learning_rate': 0.00010994928935158702, 'epoch': 7.19}
 48%|████▊     | 992/2048 [1:25:34<1:18:12,  4.44s/it] 48%|████▊     | 993/2048 [1:25:40<1:21:56,  4.66s/it] 49%|████▊     | 994/2048 [1:25:45<1:24:19,  4.80s/it] 49%|████▊     | 995/2048 [1:25:50<1:24:51,  4.83s/it] 49%|████▊     | 996/2048 [1:25:54<1:24:13,  4.80s/it] 49%|████▊     | 997/2048 [1:25:59<1:21:14,  4.64s/it] 49%|████▊     | 998/2048 [1:26:03<1:18:20,  4.48s/it] 49%|████▉     | 999/2048 [1:26:07<1:15:27,  4.32s/it] 49%|████▉     | 1000/2048 [1:26:11<1:16:00,  4.35s/it] 49%|████▉     | 1001/2048 [1:26:15<1:12:52,  4.18s/it] 49%|████▉     | 1002/2048 [1:26:18<1:08:13,  3.91s/it] 49%|████▉     | 1003/2048 [1:26:22<1:07:45,  3.89s/it] 49%|████▉     | 1004/2048 [1:26:27<1:14:23,  4.28s/it] 49%|████▉     | 1005/2048 [1:26:32<1:18:56,  4.54s/it] 49%|████▉     | 1006/2048 [1:26:37<1:22:02,  4.72s/it] 49%|████▉     | 1007/2048 [1:26:42<1:23:19,  4.80s/it] 49%|████▉     | 1008/2048 [1:26:47<1:23:12,  4.80s/it]                                                       {'loss': 0.152, 'learning_rate': 0.0001074279387059208, 'epoch': 7.31}
 49%|████▉     | 1008/2048 [1:26:47<1:23:12,  4.80s/it] 49%|████▉     | 1009/2048 [1:26:52<1:22:58,  4.79s/it] 49%|████▉     | 1010/2048 [1:26:56<1:19:58,  4.62s/it] 49%|████▉     | 1011/2048 [1:27:00<1:16:22,  4.42s/it] 49%|████▉     | 1012/2048 [1:27:04<1:13:52,  4.28s/it] 49%|████▉     | 1013/2048 [1:27:08<1:12:03,  4.18s/it] 50%|████▉     | 1014/2048 [1:27:12<1:10:03,  4.07s/it] 50%|████▉     | 1015/2048 [1:27:15<1:05:13,  3.79s/it] 50%|████▉     | 1016/2048 [1:27:20<1:10:49,  4.12s/it] 50%|████▉     | 1017/2048 [1:27:25<1:16:11,  4.43s/it] 50%|████▉     | 1018/2048 [1:27:30<1:19:48,  4.65s/it] 50%|████▉     | 1019/2048 [1:27:35<1:21:42,  4.76s/it] 50%|████▉     | 1020/2048 [1:27:40<1:22:32,  4.82s/it] 50%|████▉     | 1021/2048 [1:27:45<1:21:32,  4.76s/it] 50%|████▉     | 1022/2048 [1:27:49<1:19:15,  4.63s/it] 50%|████▉     | 1023/2048 [1:27:53<1:17:07,  4.51s/it] 50%|█████     | 1024/2048 [1:27:58<1:15:06,  4.40s/it]                                                       {'loss': 0.1763, 'learning_rate': 0.00010490183003930578, 'epoch': 7.43}
 50%|█████     | 1024/2048 [1:27:58<1:15:06,  4.40s/it]
  0%|          | 0/256 [00:00<?, ?it/s][A
  2%|▏         | 4/256 [00:00<00:09, 27.41it/s][A
  3%|▎         | 7/256 [00:00<00:11, 21.14it/s][A
  4%|▍         | 10/256 [00:00<00:12, 19.36it/s][A
  5%|▍         | 12/256 [00:00<00:13, 18.42it/s][A
  5%|▌         | 14/256 [00:00<00:13, 17.87it/s][A
  6%|▋         | 16/256 [00:00<00:13, 18.21it/s][A
  7%|▋         | 18/256 [00:00<00:13, 17.81it/s][A
  8%|▊         | 21/256 [00:01<00:12, 18.53it/s][A
  9%|▉         | 23/256 [00:01<00:12, 18.04it/s][A
 10%|▉         | 25/256 [00:01<00:13, 17.68it/s][A
 11%|█         | 27/256 [00:01<00:12, 17.76it/s][A
 11%|█▏        | 29/256 [00:01<00:13, 17.46it/s][A
 12%|█▏        | 31/256 [00:01<00:13, 17.26it/s][A
 13%|█▎        | 34/256 [00:01<00:11, 18.72it/s][A
 14%|█▍        | 36/256 [00:01<00:12, 18.15it/s][A
 15%|█▍        | 38/256 [00:02<00:12, 17.63it/s][A
 16%|█▌        | 40/256 [00:02<00:12, 17.37it/s][A
 16%|█▋        | 42/256 [00:02<00:12, 17.74it/s][A
 17%|█▋        | 44/256 [00:02<00:12, 17.48it/s][A
 18%|█▊        | 46/256 [00:02<00:12, 17.28it/s][A
 19%|█▉        | 48/256 [00:02<00:11, 17.39it/s][A
 20%|█▉        | 50/256 [00:02<00:12, 17.09it/s][A
 20%|██        | 52/256 [00:02<00:12, 16.99it/s][A
 21%|██        | 54/256 [00:02<00:11, 17.77it/s][A
 22%|██▏       | 56/256 [00:03<00:11, 17.59it/s][A
 23%|██▎       | 58/256 [00:03<00:11, 17.52it/s][A
 23%|██▎       | 60/256 [00:03<00:11, 17.28it/s][A
 24%|██▍       | 62/256 [00:03<00:11, 16.95it/s][A
 25%|██▌       | 64/256 [00:03<00:11, 17.24it/s][A
 26%|██▌       | 66/256 [00:03<00:11, 17.09it/s][A
 27%|██▋       | 68/256 [00:03<00:11, 17.00it/s][A
 27%|██▋       | 70/256 [00:03<00:10, 17.14it/s][A
 28%|██▊       | 72/256 [00:04<00:10, 17.02it/s][A
 29%|██▉       | 74/256 [00:04<00:10, 16.98it/s][A
 30%|██▉       | 76/256 [00:04<00:10, 16.99it/s][A
 30%|███       | 78/256 [00:04<00:10, 16.97it/s][A
 31%|███▏      | 80/256 [00:04<00:10, 16.92it/s][A
 32%|███▏      | 82/256 [00:04<00:10, 16.99it/s][A
 33%|███▎      | 84/256 [00:04<00:10, 17.16it/s][A
 34%|███▎      | 86/256 [00:04<00:09, 17.06it/s][A
 34%|███▍      | 88/256 [00:04<00:09, 17.29it/s][A
 35%|███▌      | 90/256 [00:05<00:09, 17.02it/s][A
 36%|███▌      | 92/256 [00:05<00:09, 16.92it/s][A
 37%|███▋      | 94/256 [00:05<00:09, 17.00it/s][A
 38%|███▊      | 96/256 [00:05<00:09, 16.91it/s][A
 39%|███▊      | 99/256 [00:05<00:08, 17.74it/s][A
 39%|███▉      | 101/256 [00:05<00:08, 17.38it/s][A
 40%|████      | 103/256 [00:05<00:08, 17.48it/s][A
 41%|████▏     | 106/256 [00:06<00:08, 18.35it/s][A
 42%|████▏     | 108/256 [00:06<00:08, 17.96it/s][A
 43%|████▎     | 110/256 [00:06<00:08, 17.73it/s][A
 44%|████▍     | 112/256 [00:06<00:08, 17.45it/s][A
 45%|████▍     | 114/256 [00:06<00:08, 17.58it/s][A
 46%|████▌     | 117/256 [00:06<00:07, 18.03it/s][A
 46%|████▋     | 119/256 [00:06<00:07, 17.83it/s][A
 47%|████▋     | 121/256 [00:06<00:07, 17.46it/s][A
 48%|████▊     | 123/256 [00:06<00:07, 17.32it/s][A
 49%|████▉     | 125/256 [00:07<00:07, 17.15it/s][A
 50%|█████     | 128/256 [00:07<00:06, 18.66it/s][A
 51%|█████     | 130/256 [00:07<00:06, 18.23it/s][A
 52%|█████▏    | 132/256 [00:07<00:07, 17.68it/s][A
 52%|█████▏    | 134/256 [00:07<00:06, 17.54it/s][A
 53%|█████▎    | 136/256 [00:07<00:06, 17.40it/s][A
 54%|█████▍    | 138/256 [00:07<00:06, 17.20it/s][A
 55%|█████▍    | 140/256 [00:07<00:06, 17.07it/s][A
 56%|█████▌    | 143/256 [00:08<00:06, 18.20it/s][A
 57%|█████▋    | 145/256 [00:08<00:06, 17.63it/s][A
 57%|█████▋    | 147/256 [00:08<00:06, 17.42it/s][A
 58%|█████▊    | 149/256 [00:08<00:06, 17.23it/s][A
 59%|█████▉    | 151/256 [00:08<00:06, 17.10it/s][A
 60%|██████    | 154/256 [00:08<00:05, 17.72it/s][A
 61%|██████    | 156/256 [00:08<00:05, 17.59it/s][A
 62%|██████▏   | 158/256 [00:08<00:05, 17.23it/s][A
 63%|██████▎   | 161/256 [00:09<00:05, 17.97it/s][A
 64%|██████▎   | 163/256 [00:09<00:05, 17.52it/s][A
 64%|██████▍   | 165/256 [00:09<00:05, 17.44it/s][A
 65%|██████▌   | 167/256 [00:09<00:05, 17.07it/s][A
 66%|██████▌   | 169/256 [00:09<00:05, 17.32it/s][A
 67%|██████▋   | 171/256 [00:09<00:04, 17.08it/s][A
 68%|██████▊   | 173/256 [00:09<00:04, 16.94it/s][A
 68%|██████▊   | 175/256 [00:09<00:04, 17.00it/s][A
 69%|██████▉   | 177/256 [00:10<00:04, 16.83it/s][A
 70%|██████▉   | 179/256 [00:10<00:04, 16.92it/s][A
 71%|███████   | 182/256 [00:10<00:03, 19.22it/s][A
 72%|███████▏  | 184/256 [00:10<00:03, 18.60it/s][A
 73%|███████▎  | 186/256 [00:10<00:03, 17.93it/s][A
 74%|███████▍  | 189/256 [00:10<00:03, 18.42it/s][A
 75%|███████▍  | 191/256 [00:10<00:03, 17.83it/s][A
 75%|███████▌  | 193/256 [00:10<00:03, 17.67it/s][A
 76%|███████▌  | 195/256 [00:11<00:03, 17.27it/s][A
 77%|███████▋  | 197/256 [00:11<00:03, 17.57it/s][A
 78%|███████▊  | 199/256 [00:11<00:03, 18.18it/s][A
 79%|███████▊  | 201/256 [00:11<00:03, 17.87it/s][A
 79%|███████▉  | 203/256 [00:11<00:03, 17.41it/s][A
 80%|████████  | 205/256 [00:11<00:02, 17.36it/s][A
 81%|████████  | 207/256 [00:11<00:02, 17.05it/s][A
 82%|████████▏ | 209/256 [00:11<00:02, 16.95it/s][A
 82%|████████▏ | 211/256 [00:12<00:02, 16.92it/s][A
 83%|████████▎ | 213/256 [00:12<00:02, 16.88it/s][A
 84%|████████▍ | 215/256 [00:12<00:02, 16.84it/s][A
 85%|████████▍ | 217/256 [00:12<00:02, 16.84it/s][A
 86%|████████▌ | 219/256 [00:12<00:02, 16.94it/s][A
 86%|████████▋ | 221/256 [00:12<00:02, 16.77it/s][A
 87%|████████▋ | 223/256 [00:12<00:01, 16.78it/s][A
 88%|████████▊ | 225/256 [00:12<00:01, 17.20it/s][A
 89%|████████▊ | 227/256 [00:12<00:01, 16.94it/s][A
 89%|████████▉ | 229/256 [00:13<00:01, 16.85it/s][A
 90%|█████████ | 231/256 [00:13<00:01, 17.09it/s][A
 91%|█████████ | 233/256 [00:13<00:01, 17.05it/s][A
 92%|█████████▏| 235/256 [00:13<00:01, 17.02it/s][A
 93%|█████████▎| 238/256 [00:13<00:01, 17.95it/s][A
 94%|█████████▍| 240/256 [00:13<00:00, 17.73it/s][A
 95%|█████████▍| 242/256 [00:13<00:00, 17.45it/s][A
 95%|█████████▌| 244/256 [00:13<00:00, 17.77it/s][A
 96%|█████████▋| 247/256 [00:14<00:00, 18.26it/s][A
 97%|█████████▋| 249/256 [00:14<00:00, 17.95it/s][A
 98%|█████████▊| 252/256 [00:14<00:00, 19.02it/s][A
 99%|█████████▉| 254/256 [00:14<00:00, 18.54it/s][A
100%|██████████| 256/256 [00:14<00:00, 17.00it/s][A                                                       
                                                 [A{'eval_loss': 2.003866195678711, 'eval_runtime': 14.7527, 'eval_samples_per_second': 69.411, 'eval_steps_per_second': 17.353, 'epoch': 7.43}
 50%|█████     | 1024/2048 [1:28:12<1:15:06,  4.40s/it]
100%|██████████| 256/256 [00:14<00:00, 17.00it/s][A
                                                 [A  0%|          | 0/383 [00:00<?, ?it/s]  0%|          | 0/383 [00:00<?, ?it/s]  0%|          | 0/383 [00:00<?, ?it/s]
  0%|          | 0/383 [00:00<?, ?it/s][A  0%|          | 1/383 [00:00<00:40,  9.35it/s]  1%|          | 2/383 [00:00<00:26, 14.64it/s]
  0%|          | 1/383 [00:00<00:52,  7.33it/s][A  1%|          | 2/383 [00:00<00:27, 14.08it/s]  1%|          | 3/383 [00:00<00:24, 15.29it/s]
  1%|          | 4/383 [00:00<00:23, 16.27it/s][A  1%|▏         | 5/383 [00:00<00:19, 19.38it/s]  1%|▏         | 5/383 [00:00<00:20, 18.42it/s]  1%|▏         | 5/383 [00:00<00:21, 17.31it/s]  2%|▏         | 7/383 [00:00<00:20, 17.92it/s]  2%|▏         | 7/383 [00:00<00:21, 17.19it/s]
  2%|▏         | 7/383 [00:00<00:20, 18.64it/s][A  2%|▏         | 7/383 [00:00<00:22, 16.74it/s]  2%|▏         | 9/383 [00:00<00:23, 16.09it/s]  2%|▏         | 9/383 [00:00<00:24, 15.46it/s]
  2%|▏         | 9/383 [00:00<00:23, 16.20it/s][A  2%|▏         | 9/383 [00:00<00:23, 16.00it/s]  3%|▎         | 11/383 [00:00<00:23, 15.54it/s]
  3%|▎         | 11/383 [00:00<00:24, 15.46it/s][A  3%|▎         | 11/383 [00:00<00:25, 14.65it/s]  3%|▎         | 11/383 [00:00<00:24, 15.03it/s]  3%|▎         | 13/383 [00:00<00:24, 15.07it/s]  3%|▎         | 13/383 [00:00<00:24, 15.33it/s]
  3%|▎         | 13/383 [00:00<00:24, 14.88it/s][A  3%|▎         | 13/383 [00:00<00:25, 14.44it/s]  4%|▍         | 15/383 [00:00<00:22, 16.13it/s]  4%|▍         | 15/383 [00:00<00:22, 16.25it/s]
  4%|▍         | 15/383 [00:00<00:23, 15.91it/s][A  4%|▍         | 15/383 [00:00<00:23, 15.46it/s]  4%|▍         | 17/383 [00:01<00:21, 16.77it/s]  4%|▍         | 17/383 [00:01<00:21, 16.99it/s]
  4%|▍         | 17/383 [00:01<00:21, 16.81it/s][A  4%|▍         | 17/383 [00:01<00:22, 16.25it/s]  5%|▍         | 19/383 [00:01<00:20, 17.35it/s]  5%|▍         | 19/383 [00:01<00:20, 17.53it/s]
  5%|▍         | 19/383 [00:01<00:20, 17.43it/s][A  5%|▍         | 19/383 [00:01<00:22, 16.52it/s]  5%|▌         | 21/383 [00:01<00:20, 17.36it/s]  5%|▌         | 21/383 [00:01<00:20, 17.69it/s]
  5%|▌         | 21/383 [00:01<00:20, 17.72it/s][A  5%|▌         | 21/383 [00:01<00:21, 16.81it/s]  6%|▌         | 23/383 [00:01<00:20, 17.23it/s]
  6%|▌         | 23/383 [00:01<00:20, 17.62it/s][A  6%|▌         | 23/383 [00:01<00:20, 17.36it/s]  6%|▌         | 23/383 [00:01<00:21, 17.01it/s]  7%|▋         | 25/383 [00:01<00:21, 16.74it/s]
  7%|▋         | 25/383 [00:01<00:20, 17.39it/s][A  7%|▋         | 25/383 [00:01<00:21, 17.01it/s]  7%|▋         | 25/383 [00:01<00:21, 16.72it/s]
  7%|▋         | 27/383 [00:01<00:20, 17.12it/s][A  7%|▋         | 27/383 [00:01<00:24, 14.62it/s]  7%|▋         | 27/383 [00:01<00:24, 14.69it/s]  7%|▋         | 27/383 [00:01<00:24, 14.42it/s]
  8%|▊         | 29/383 [00:01<00:25, 13.69it/s][A  8%|▊         | 29/383 [00:01<00:27, 12.83it/s]  8%|▊         | 29/383 [00:01<00:27, 12.83it/s]  8%|▊         | 29/383 [00:01<00:28, 12.45it/s]
  8%|▊         | 31/383 [00:01<00:25, 13.59it/s][A  8%|▊         | 31/383 [00:02<00:27, 13.00it/s]  8%|▊         | 31/383 [00:02<00:26, 13.09it/s]  8%|▊         | 31/383 [00:02<00:26, 13.18it/s]
  9%|▊         | 33/383 [00:02<00:25, 13.79it/s][A  9%|▊         | 33/383 [00:02<00:25, 13.69it/s]  9%|▊         | 33/383 [00:02<00:25, 13.80it/s]  9%|▊         | 33/383 [00:02<00:24, 14.05it/s]
  9%|▉         | 35/383 [00:02<00:23, 14.51it/s][A  9%|▉         | 35/383 [00:02<00:24, 14.12it/s]  9%|▉         | 35/383 [00:02<00:24, 14.37it/s]  9%|▉         | 35/383 [00:02<00:23, 14.64it/s]
 10%|▉         | 37/383 [00:02<00:23, 14.83it/s][A 10%|▉         | 37/383 [00:02<00:23, 14.92it/s] 10%|▉         | 37/383 [00:02<00:23, 14.97it/s] 10%|▉         | 37/383 [00:02<00:22, 15.23it/s]
 10%|█         | 39/383 [00:02<00:22, 15.35it/s][A 10%|█         | 39/383 [00:02<00:22, 15.48it/s] 10%|█         | 39/383 [00:02<00:22, 15.56it/s] 10%|█         | 39/383 [00:02<00:21, 15.72it/s]
 11%|█         | 41/383 [00:02<00:21, 15.96it/s][A 11%|█         | 41/383 [00:02<00:21, 15.91it/s] 11%|█         | 41/383 [00:02<00:21, 16.04it/s] 11%|█         | 41/383 [00:02<00:21, 15.87it/s] 11%|█         | 43/383 [00:02<00:20, 16.92it/s]
 11%|█▏        | 44/383 [00:02<00:18, 17.84it/s][A 11%|█▏        | 44/383 [00:02<00:18, 18.35it/s] 11%|█         | 43/383 [00:02<00:20, 16.51it/s] 12%|█▏        | 46/383 [00:02<00:17, 19.55it/s]
 12%|█▏        | 47/383 [00:02<00:16, 20.03it/s][A 12%|█▏        | 47/383 [00:02<00:16, 20.62it/s] 12%|█▏        | 46/383 [00:02<00:17, 19.16it/s] 13%|█▎        | 49/383 [00:02<00:15, 21.46it/s]
 13%|█▎        | 50/383 [00:02<00:15, 21.59it/s][A 13%|█▎        | 49/383 [00:03<00:15, 21.20it/s] 13%|█▎        | 50/383 [00:03<00:16, 20.80it/s]
 14%|█▍        | 53/383 [00:03<00:17, 18.41it/s][A 14%|█▍        | 53/383 [00:03<00:17, 18.46it/s] 14%|█▎        | 52/383 [00:03<00:18, 17.43it/s] 14%|█▎        | 52/383 [00:03<00:18, 18.13it/s]
 14%|█▍        | 55/383 [00:03<00:17, 18.32it/s][A 14%|█▍        | 54/383 [00:03<00:18, 17.76it/s] 14%|█▍        | 55/383 [00:03<00:17, 18.38it/s] 14%|█▍        | 54/383 [00:03<00:18, 17.88it/s]
 15%|█▍        | 57/383 [00:03<00:18, 18.02it/s][A 15%|█▍        | 56/383 [00:03<00:18, 17.99it/s] 15%|█▍        | 57/383 [00:03<00:17, 18.23it/s] 15%|█▍        | 56/383 [00:03<00:18, 17.97it/s] 15%|█▌        | 58/383 [00:03<00:18, 17.40it/s]
 15%|█▌        | 59/383 [00:03<00:18, 17.16it/s][A 15%|█▌        | 59/383 [00:03<00:18, 17.18it/s] 15%|█▌        | 58/383 [00:03<00:18, 17.21it/s] 16%|█▌        | 60/383 [00:03<00:19, 16.83it/s]
 16%|█▌        | 61/383 [00:03<00:19, 16.77it/s] 16%|█▌        | 61/383 [00:03<00:19, 16.20it/s][A 16%|█▌        | 60/383 [00:03<00:19, 16.84it/s] 16%|█▌        | 62/383 [00:03<00:19, 16.09it/s]
 16%|█▋        | 63/383 [00:03<00:19, 16.15it/s][A 16%|█▋        | 63/383 [00:03<00:19, 16.40it/s] 16%|█▌        | 62/383 [00:03<00:19, 16.23it/s] 17%|█▋        | 64/383 [00:03<00:19, 15.98it/s]
 17%|█▋        | 65/383 [00:03<00:19, 16.14it/s][A 17%|█▋        | 64/383 [00:03<00:19, 16.25it/s] 17%|█▋        | 65/383 [00:03<00:19, 15.91it/s] 17%|█▋        | 66/383 [00:04<00:20, 15.73it/s]
 17%|█▋        | 67/383 [00:04<00:20, 15.74it/s][A 17%|█▋        | 66/383 [00:04<00:19, 16.24it/s] 17%|█▋        | 67/383 [00:04<00:20, 15.68it/s] 18%|█▊        | 68/383 [00:04<00:20, 15.64it/s] 18%|█▊        | 68/383 [00:04<00:20, 15.04it/s]
 18%|█▊        | 69/383 [00:04<00:21, 14.94it/s][A 18%|█▊        | 69/383 [00:04<00:20, 14.96it/s]
 19%|█▊        | 71/383 [00:04<00:20, 15.10it/s][A 18%|█▊        | 70/383 [00:04<00:20, 15.05it/s] 18%|█▊        | 70/383 [00:04<00:21, 14.63it/s] 19%|█▊        | 71/383 [00:04<00:20, 15.11it/s]
 19%|█▉        | 73/383 [00:04<00:19, 16.06it/s][A 19%|█▉        | 72/383 [00:04<00:20, 15.53it/s] 19%|█▉        | 72/383 [00:04<00:19, 15.71it/s] 19%|█▉        | 73/383 [00:04<00:19, 15.71it/s]
 20%|█▉        | 75/383 [00:04<00:19, 15.93it/s][A 19%|█▉        | 74/383 [00:04<00:19, 16.21it/s] 20%|█▉        | 75/383 [00:04<00:19, 15.70it/s] 19%|█▉        | 74/383 [00:04<00:20, 15.29it/s] 20%|█▉        | 76/383 [00:04<00:18, 16.26it/s]
 20%|██        | 77/383 [00:04<00:19, 15.95it/s][A 20%|█▉        | 76/383 [00:04<00:19, 15.65it/s] 20%|██        | 77/383 [00:04<00:19, 15.82it/s] 20%|██        | 78/383 [00:04<00:18, 16.17it/s]
 21%|██        | 79/383 [00:04<00:19, 15.94it/s][A 20%|██        | 78/383 [00:04<00:19, 15.82it/s] 21%|██        | 79/383 [00:04<00:19, 15.85it/s] 21%|██        | 80/383 [00:04<00:18, 16.09it/s]
 21%|██        | 81/383 [00:04<00:19, 15.74it/s][A 21%|██        | 80/383 [00:04<00:19, 15.76it/s] 21%|██        | 81/383 [00:05<00:19, 15.60it/s] 21%|██▏       | 82/383 [00:05<00:18, 15.95it/s]
 22%|██▏       | 83/383 [00:05<00:18, 15.92it/s][A 21%|██▏       | 82/383 [00:05<00:18, 15.96it/s] 22%|██▏       | 83/383 [00:05<00:19, 15.75it/s] 22%|██▏       | 84/383 [00:05<00:18, 15.99it/s]
 22%|██▏       | 85/383 [00:05<00:18, 15.72it/s][A 22%|██▏       | 84/383 [00:05<00:18, 16.11it/s] 22%|██▏       | 85/383 [00:05<00:18, 16.06it/s] 22%|██▏       | 86/383 [00:05<00:18, 16.23it/s] 22%|██▏       | 86/383 [00:05<00:18, 16.08it/s]
 23%|██▎       | 87/383 [00:05<00:20, 14.25it/s][A 23%|██▎       | 87/383 [00:05<00:20, 14.18it/s] 23%|██▎       | 88/383 [00:05<00:22, 13.00it/s] 23%|██▎       | 88/383 [00:05<00:23, 12.53it/s]
 23%|██▎       | 89/383 [00:05<00:24, 11.83it/s][A 23%|██▎       | 89/383 [00:05<00:32,  9.03it/s] 23%|██▎       | 90/383 [00:05<00:34,  8.60it/s]
 24%|██▍       | 91/383 [00:06<00:39,  7.40it/s][A 23%|██▎       | 90/383 [00:06<00:42,  6.90it/s] 24%|██▍       | 91/383 [00:06<00:48,  6.02it/s] 24%|██▍       | 92/383 [00:06<00:48,  5.99it/s] 24%|██▍       | 92/383 [00:06<00:47,  6.09it/s] 24%|██▍       | 92/383 [00:06<00:52,  5.56it/s]
 24%|██▍       | 93/383 [00:06<00:50,  5.71it/s][A 25%|██▍       | 94/383 [00:06<00:39,  7.30it/s]
 25%|██▌       | 96/383 [00:06<00:36,  7.92it/s][A 24%|██▍       | 93/383 [00:06<00:55,  5.26it/s] 25%|██▌       | 96/383 [00:06<00:31,  9.18it/s] 24%|██▍       | 93/383 [00:06<00:53,  5.44it/s]
 26%|██▌       | 98/383 [00:06<00:30,  9.45it/s][A 25%|██▍       | 95/383 [00:06<00:41,  6.93it/s] 26%|██▌       | 98/383 [00:06<00:25, 11.06it/s] 25%|██▍       | 95/383 [00:06<00:40,  7.17it/s]
 26%|██▌       | 100/383 [00:07<00:26, 10.88it/s][A 25%|██▌       | 97/383 [00:07<00:32,  8.78it/s] 26%|██▌       | 100/383 [00:07<00:22, 12.40it/s] 25%|██▌       | 97/383 [00:07<00:31,  9.01it/s]
 27%|██▋       | 102/383 [00:07<00:22, 12.23it/s][A 26%|██▌       | 99/383 [00:07<00:27, 10.48it/s] 27%|██▋       | 102/383 [00:07<00:20, 13.77it/s] 26%|██▌       | 99/383 [00:07<00:26, 10.70it/s]
 27%|██▋       | 104/383 [00:07<00:20, 13.36it/s][A 26%|██▋       | 101/383 [00:07<00:23, 11.80it/s] 27%|██▋       | 104/383 [00:07<00:18, 15.03it/s] 26%|██▋       | 101/383 [00:07<00:23, 12.11it/s]
 28%|██▊       | 106/383 [00:07<00:18, 14.74it/s][A 28%|██▊       | 106/383 [00:07<00:17, 16.15it/s] 27%|██▋       | 103/383 [00:07<00:21, 13.08it/s] 27%|██▋       | 103/383 [00:07<00:20, 13.49it/s]
 28%|██▊       | 108/383 [00:07<00:17, 15.91it/s][A 28%|██▊       | 108/383 [00:07<00:16, 16.87it/s] 27%|██▋       | 105/383 [00:07<00:19, 14.33it/s] 27%|██▋       | 105/383 [00:07<00:18, 14.82it/s]
 29%|██▊       | 110/383 [00:07<00:16, 16.79it/s][A 29%|██▊       | 110/383 [00:07<00:15, 17.56it/s] 28%|██▊       | 107/383 [00:07<00:17, 15.45it/s] 28%|██▊       | 107/383 [00:07<00:17, 15.99it/s]
 29%|██▉       | 112/383 [00:07<00:15, 17.40it/s][A 29%|██▉       | 112/383 [00:07<00:15, 18.05it/s] 28%|██▊       | 109/383 [00:07<00:16, 16.40it/s] 28%|██▊       | 109/383 [00:07<00:16, 16.88it/s]
 30%|██▉       | 114/383 [00:07<00:15, 17.91it/s][A 30%|██▉       | 114/383 [00:07<00:14, 18.44it/s] 29%|██▉       | 111/383 [00:07<00:15, 17.14it/s] 29%|██▉       | 111/383 [00:07<00:15, 17.51it/s]
 30%|███       | 116/383 [00:07<00:14, 17.93it/s][A 30%|██▉       | 113/383 [00:07<00:15, 17.83it/s] 30%|███       | 116/383 [00:07<00:15, 17.76it/s] 30%|██▉       | 113/383 [00:07<00:15, 17.90it/s]
 31%|███       | 118/383 [00:08<00:15, 17.51it/s][A 30%|███       | 115/383 [00:08<00:15, 17.79it/s] 30%|███       | 115/383 [00:08<00:15, 17.85it/s] 31%|███       | 118/383 [00:08<00:15, 17.21it/s]
 31%|███▏      | 120/383 [00:08<00:15, 17.06it/s][A 31%|███       | 117/383 [00:08<00:15, 16.99it/s] 31%|███▏      | 120/383 [00:08<00:15, 17.17it/s] 31%|███       | 117/383 [00:08<00:15, 17.39it/s]
 32%|███▏      | 122/383 [00:08<00:15, 16.88it/s][A 32%|███▏      | 122/383 [00:08<00:15, 17.39it/s] 31%|███       | 119/383 [00:08<00:15, 16.84it/s] 31%|███       | 119/383 [00:08<00:15, 16.93it/s]
 32%|███▏      | 124/383 [00:08<00:14, 17.46it/s][A 32%|███▏      | 124/383 [00:08<00:14, 18.08it/s] 32%|███▏      | 121/383 [00:08<00:15, 16.61it/s] 32%|███▏      | 121/383 [00:08<00:15, 16.57it/s]
 33%|███▎      | 126/383 [00:08<00:14, 18.03it/s][A 33%|███▎      | 126/383 [00:08<00:14, 18.35it/s] 32%|███▏      | 123/383 [00:08<00:15, 16.92it/s] 32%|███▏      | 123/383 [00:08<00:14, 17.39it/s]
 33%|███▎      | 128/383 [00:08<00:13, 18.56it/s][A 33%|███▎      | 128/383 [00:08<00:13, 18.70it/s] 33%|███▎      | 125/383 [00:08<00:14, 17.42it/s] 33%|███▎      | 125/383 [00:08<00:14, 17.87it/s]
 34%|███▍      | 130/383 [00:08<00:14, 17.62it/s][A 34%|███▍      | 130/383 [00:08<00:14, 17.09it/s] 33%|███▎      | 127/383 [00:08<00:14, 17.85it/s] 33%|███▎      | 127/383 [00:08<00:13, 18.30it/s]
 34%|███▍      | 132/383 [00:08<00:14, 16.76it/s][A 34%|███▎      | 129/383 [00:08<00:13, 18.21it/s] 34%|███▎      | 129/383 [00:08<00:14, 17.33it/s] 34%|███▍      | 132/383 [00:08<00:15, 16.54it/s]
 35%|███▍      | 134/383 [00:08<00:15, 16.43it/s][A 34%|███▍      | 131/383 [00:08<00:14, 17.61it/s] 34%|███▍      | 131/383 [00:08<00:14, 16.89it/s] 35%|███▍      | 134/383 [00:09<00:15, 16.48it/s]
 36%|███▌      | 136/383 [00:09<00:15, 16.42it/s][A 35%|███▍      | 133/383 [00:09<00:14, 16.89it/s] 36%|███▌      | 136/383 [00:09<00:15, 16.32it/s] 35%|███▍      | 133/383 [00:09<00:15, 16.56it/s]
 36%|███▌      | 138/383 [00:09<00:14, 16.44it/s][A 35%|███▌      | 135/383 [00:09<00:14, 16.67it/s] 36%|███▌      | 138/383 [00:09<00:14, 16.42it/s] 35%|███▌      | 135/383 [00:09<00:15, 16.31it/s]
 37%|███▋      | 140/383 [00:09<00:14, 16.34it/s][A 36%|███▌      | 137/383 [00:09<00:14, 16.48it/s] 37%|███▋      | 140/383 [00:09<00:14, 16.42it/s] 36%|███▌      | 137/383 [00:09<00:15, 16.03it/s]
 37%|███▋      | 142/383 [00:09<00:14, 16.27it/s][A 36%|███▋      | 139/383 [00:09<00:14, 16.57it/s] 37%|███▋      | 142/383 [00:09<00:14, 16.38it/s] 36%|███▋      | 139/383 [00:09<00:15, 15.98it/s]
 38%|███▊      | 144/383 [00:09<00:14, 16.21it/s][A 38%|███▊      | 144/383 [00:09<00:14, 16.44it/s] 37%|███▋      | 141/383 [00:09<00:14, 16.17it/s] 37%|███▋      | 141/383 [00:09<00:14, 16.16it/s]
 38%|███▊      | 146/383 [00:09<00:14, 16.13it/s][A 38%|███▊      | 146/383 [00:09<00:14, 16.40it/s] 37%|███▋      | 143/383 [00:09<00:14, 16.21it/s] 37%|███▋      | 143/383 [00:09<00:14, 16.23it/s]
 39%|███▊      | 148/383 [00:09<00:14, 16.07it/s][A 38%|███▊      | 145/383 [00:09<00:14, 16.18it/s] 38%|███▊      | 145/383 [00:09<00:14, 16.07it/s] 39%|███▊      | 148/383 [00:09<00:15, 14.92it/s] 38%|███▊      | 147/383 [00:09<00:14, 16.19it/s] 38%|███▊      | 147/383 [00:10<00:14, 16.18it/s]
 39%|███▉      | 150/383 [00:10<00:17, 13.13it/s][A 39%|███▉      | 150/383 [00:10<00:18, 12.59it/s] 39%|███▉      | 149/383 [00:10<00:17, 13.41it/s] 39%|███▉      | 149/383 [00:10<00:17, 13.35it/s]
 40%|███▉      | 152/383 [00:10<00:19, 12.01it/s][A 40%|███▉      | 152/383 [00:10<00:19, 11.75it/s] 39%|███▉      | 151/383 [00:10<00:19, 12.02it/s] 39%|███▉      | 151/383 [00:10<00:19, 11.83it/s] 40%|███▉      | 153/383 [00:10<00:19, 11.50it/s]
 40%|████      | 154/383 [00:10<00:25,  9.16it/s][A 40%|███▉      | 153/383 [00:10<00:21, 10.88it/s] 40%|████      | 154/383 [00:10<00:26,  8.60it/s] 40%|████      | 155/383 [00:11<00:30,  7.41it/s]
 41%|████      | 156/383 [00:11<00:35,  6.34it/s][A 40%|████      | 155/383 [00:11<00:32,  7.11it/s] 41%|████      | 156/383 [00:11<00:36,  6.24it/s] 41%|████      | 156/383 [00:11<00:36,  6.30it/s] 41%|████      | 156/383 [00:11<00:36,  6.18it/s]
 41%|████      | 157/383 [00:11<00:41,  5.45it/s][A 41%|████      | 157/383 [00:11<00:39,  5.71it/s] 41%|████      | 157/383 [00:11<00:39,  5.73it/s] 41%|████      | 157/383 [00:11<00:41,  5.44it/s] 41%|████▏     | 158/383 [00:11<00:43,  5.19it/s]
 41%|████▏     | 158/383 [00:11<00:46,  4.80it/s][A 41%|████▏     | 158/383 [00:11<00:43,  5.11it/s] 42%|████▏     | 159/383 [00:11<00:42,  5.28it/s] 41%|████▏     | 158/383 [00:11<00:45,  4.98it/s]
 42%|████▏     | 159/383 [00:12<00:49,  4.54it/s][A 42%|████▏     | 159/383 [00:12<00:42,  5.24it/s] 42%|████▏     | 160/383 [00:12<00:45,  4.89it/s] 42%|████▏     | 159/383 [00:12<00:48,  4.61it/s] 42%|████▏     | 160/383 [00:12<00:45,  4.95it/s]
 42%|████▏     | 160/383 [00:12<00:51,  4.32it/s][A 42%|████▏     | 161/383 [00:12<00:44,  5.00it/s] 42%|████▏     | 160/383 [00:12<00:47,  4.71it/s]
 42%|████▏     | 161/383 [00:12<00:48,  4.60it/s][A 42%|████▏     | 161/383 [00:12<00:46,  4.79it/s] 42%|████▏     | 162/383 [00:12<00:42,  5.21it/s] 42%|████▏     | 161/383 [00:12<00:49,  4.50it/s]
 42%|████▏     | 162/383 [00:12<00:49,  4.49it/s][A 42%|████▏     | 162/383 [00:12<00:47,  4.62it/s] 43%|████▎     | 163/383 [00:12<00:46,  4.73it/s] 42%|████▏     | 162/383 [00:12<00:46,  4.71it/s]
 43%|████▎     | 163/383 [00:12<00:46,  4.70it/s][A 43%|████▎     | 163/383 [00:12<00:47,  4.67it/s] 43%|████▎     | 164/383 [00:13<00:50,  4.38it/s] 43%|████▎     | 163/383 [00:13<00:46,  4.78it/s]
 43%|████▎     | 164/383 [00:13<00:50,  4.38it/s][A 43%|████▎     | 164/383 [00:13<00:49,  4.41it/s] 43%|████▎     | 165/383 [00:13<00:46,  4.68it/s] 43%|████▎     | 164/383 [00:13<00:48,  4.50it/s] 44%|████▍     | 168/383 [00:13<00:25,  8.53it/s]
 43%|████▎     | 165/383 [00:13<00:48,  4.52it/s][A 43%|████▎     | 165/383 [00:13<00:47,  4.57it/s]
 44%|████▍     | 168/383 [00:13<00:25,  8.30it/s][A 45%|████▍     | 171/383 [00:13<00:17, 11.83it/s] 44%|████▍     | 168/383 [00:13<00:25,  8.39it/s] 45%|████▌     | 173/383 [00:13<00:15, 13.42it/s] 43%|████▎     | 165/383 [00:13<00:51,  4.21it/s]
 45%|████▍     | 171/383 [00:13<00:17, 11.83it/s][A 45%|████▍     | 171/383 [00:13<00:17, 11.94it/s] 44%|████▍     | 168/383 [00:13<00:27,  7.88it/s]
 45%|████▌     | 174/383 [00:13<00:14, 14.53it/s][A 46%|████▌     | 175/383 [00:13<00:15, 13.39it/s] 45%|████▌     | 174/383 [00:13<00:14, 14.69it/s] 45%|████▍     | 171/383 [00:13<00:18, 11.31it/s]
 46%|████▌     | 176/383 [00:13<00:14, 14.23it/s][A 46%|████▌     | 177/383 [00:13<00:15, 13.19it/s] 46%|████▌     | 176/383 [00:13<00:14, 14.34it/s] 45%|████▌     | 174/383 [00:13<00:15, 13.61it/s] 47%|████▋     | 179/383 [00:13<00:14, 14.54it/s]
 46%|████▋     | 178/383 [00:13<00:14, 14.56it/s][A 46%|████▋     | 178/383 [00:14<00:13, 14.68it/s] 47%|████▋     | 181/383 [00:14<00:13, 15.35it/s]
 47%|████▋     | 180/383 [00:14<00:13, 15.58it/s][A 46%|████▌     | 176/383 [00:14<00:15, 13.52it/s] 47%|████▋     | 180/383 [00:14<00:12, 15.71it/s] 48%|████▊     | 183/383 [00:14<00:12, 16.20it/s]
 48%|████▊     | 182/383 [00:14<00:12, 16.06it/s][A 46%|████▋     | 178/383 [00:14<00:14, 13.98it/s] 48%|████▊     | 182/383 [00:14<00:12, 16.32it/s]
 48%|████▊     | 184/383 [00:14<00:11, 16.70it/s][A 48%|████▊     | 185/383 [00:14<00:12, 15.98it/s] 47%|████▋     | 180/383 [00:14<00:13, 15.08it/s] 48%|████▊     | 184/383 [00:14<00:11, 16.86it/s]
 49%|████▊     | 186/383 [00:14<00:12, 16.05it/s][A 48%|████▊     | 182/383 [00:14<00:12, 15.75it/s] 49%|████▉     | 187/383 [00:14<00:12, 15.15it/s] 49%|████▊     | 186/383 [00:14<00:12, 15.58it/s]
 49%|████▉     | 188/383 [00:14<00:11, 16.58it/s][A 48%|████▊     | 184/383 [00:14<00:12, 16.56it/s] 50%|████▉     | 190/383 [00:14<00:10, 17.59it/s] 49%|████▉     | 188/383 [00:14<00:12, 16.12it/s]
 50%|████▉     | 191/383 [00:14<00:10, 18.60it/s][A 50%|█████     | 192/383 [00:14<00:10, 17.92it/s] 49%|████▊     | 186/383 [00:14<00:12, 15.41it/s] 50%|████▉     | 191/383 [00:14<00:10, 18.33it/s]
 50%|█████     | 193/383 [00:14<00:10, 18.52it/s][A 51%|█████     | 194/383 [00:14<00:10, 18.13it/s] 49%|████▉     | 188/383 [00:14<00:12, 15.38it/s] 50%|█████     | 193/383 [00:14<00:10, 18.47it/s]
 51%|█████     | 195/383 [00:14<00:10, 18.43it/s][A 51%|█████     | 196/383 [00:14<00:10, 18.44it/s] 51%|█████     | 195/383 [00:14<00:10, 18.54it/s] 50%|████▉     | 191/383 [00:14<00:10, 17.79it/s]
 51%|█████▏    | 197/383 [00:15<00:09, 18.69it/s][A 52%|█████▏    | 199/383 [00:15<00:09, 20.03it/s] 51%|█████▏    | 197/383 [00:15<00:09, 18.85it/s] 50%|█████     | 193/383 [00:15<00:10, 18.09it/s]
 52%|█████▏    | 199/383 [00:15<00:09, 19.04it/s][A 53%|█████▎    | 202/383 [00:15<00:08, 21.82it/s] 52%|█████▏    | 199/383 [00:15<00:09, 19.11it/s] 51%|█████     | 195/383 [00:15<00:10, 18.32it/s]
 53%|█████▎    | 202/383 [00:15<00:08, 21.13it/s][A 54%|█████▎    | 205/383 [00:15<00:07, 23.05it/s] 53%|█████▎    | 202/383 [00:15<00:08, 21.21it/s] 51%|█████▏    | 197/383 [00:15<00:10, 18.47it/s]
 54%|█████▎    | 205/383 [00:15<00:07, 22.77it/s][A 54%|█████▍    | 208/383 [00:15<00:07, 23.89it/s] 54%|█████▎    | 205/383 [00:15<00:07, 22.50it/s] 52%|█████▏    | 200/383 [00:15<00:09, 19.61it/s]
 54%|█████▍    | 208/383 [00:15<00:07, 23.62it/s][A 55%|█████▌    | 211/383 [00:15<00:07, 24.54it/s] 54%|█████▍    | 208/383 [00:15<00:07, 23.48it/s] 53%|█████▎    | 203/383 [00:15<00:08, 21.41it/s]
 55%|█████▌    | 211/383 [00:15<00:07, 24.30it/s][A 56%|█████▌    | 214/383 [00:15<00:06, 24.31it/s] 55%|█████▌    | 211/383 [00:15<00:07, 24.23it/s] 54%|█████▍    | 206/383 [00:15<00:07, 22.75it/s]
 56%|█████▌    | 214/383 [00:15<00:06, 24.72it/s][A 57%|█████▋    | 217/383 [00:15<00:06, 24.80it/s] 56%|█████▌    | 214/383 [00:15<00:06, 24.67it/s] 55%|█████▍    | 209/383 [00:15<00:07, 23.56it/s]
 57%|█████▋    | 217/383 [00:15<00:06, 24.31it/s][A 57%|█████▋    | 220/383 [00:15<00:06, 25.00it/s] 57%|█████▋    | 217/383 [00:15<00:06, 24.86it/s] 55%|█████▌    | 212/383 [00:15<00:07, 24.07it/s]
 57%|█████▋    | 220/383 [00:15<00:06, 24.81it/s][A 57%|█████▋    | 220/383 [00:15<00:06, 25.15it/s] 56%|█████▌    | 215/383 [00:16<00:06, 24.56it/s] 58%|█████▊    | 223/383 [00:16<00:07, 22.17it/s]
 58%|█████▊    | 223/383 [00:16<00:07, 22.78it/s][A 57%|█████▋    | 218/383 [00:16<00:06, 24.91it/s] 58%|█████▊    | 223/383 [00:16<00:07, 22.16it/s] 59%|█████▉    | 226/383 [00:16<00:07, 20.43it/s] 58%|█████▊    | 221/383 [00:16<00:06, 23.75it/s]
 59%|█████▉    | 226/383 [00:16<00:07, 20.63it/s][A 59%|█████▉    | 226/383 [00:16<00:07, 20.15it/s] 60%|█████▉    | 229/383 [00:16<00:07, 19.26it/s] 58%|█████▊    | 224/383 [00:16<00:07, 20.88it/s]
 60%|█████▉    | 229/383 [00:16<00:08, 19.09it/s][A 60%|█████▉    | 229/383 [00:16<00:08, 18.93it/s] 60%|██████    | 231/383 [00:16<00:08, 17.31it/s]
 60%|██████    | 231/383 [00:16<00:08, 17.83it/s][A 59%|█████▉    | 227/383 [00:16<00:08, 19.21it/s] 60%|██████    | 231/383 [00:16<00:08, 18.03it/s] 61%|██████    | 233/383 [00:16<00:09, 15.97it/s]
 61%|██████    | 233/383 [00:16<00:09, 16.35it/s][A 61%|██████    | 233/383 [00:16<00:09, 16.27it/s] 60%|██████    | 230/383 [00:16<00:08, 18.42it/s] 61%|██████▏   | 235/383 [00:16<00:09, 15.02it/s]
 61%|██████▏   | 235/383 [00:16<00:09, 15.31it/s][A 61%|██████    | 232/383 [00:16<00:09, 16.77it/s] 61%|██████▏   | 235/383 [00:16<00:09, 14.83it/s] 62%|██████▏   | 237/383 [00:17<00:10, 14.22it/s]
 62%|██████▏   | 237/383 [00:17<00:10, 14.46it/s][A 61%|██████    | 234/383 [00:17<00:09, 15.75it/s] 62%|██████▏   | 237/383 [00:17<00:10, 14.22it/s] 62%|██████▏   | 239/383 [00:17<00:10, 13.69it/s]
 62%|██████▏   | 239/383 [00:17<00:10, 13.97it/s][A 62%|██████▏   | 236/383 [00:17<00:09, 14.97it/s] 62%|██████▏   | 239/383 [00:17<00:10, 13.82it/s] 63%|██████▎   | 241/383 [00:17<00:10, 13.32it/s]
 63%|██████▎   | 241/383 [00:17<00:10, 13.64it/s][A 62%|██████▏   | 238/383 [00:17<00:10, 14.36it/s] 63%|██████▎   | 241/383 [00:17<00:10, 13.34it/s] 63%|██████▎   | 243/383 [00:17<00:10, 13.18it/s]
 63%|██████▎   | 243/383 [00:17<00:10, 13.29it/s][A 63%|██████▎   | 240/383 [00:17<00:10, 14.05it/s] 63%|██████▎   | 243/383 [00:17<00:10, 13.20it/s] 64%|██████▍   | 245/383 [00:17<00:10, 12.96it/s]
 64%|██████▍   | 245/383 [00:17<00:10, 13.27it/s][A 63%|██████▎   | 242/383 [00:17<00:10, 13.68it/s] 64%|██████▍   | 245/383 [00:17<00:10, 13.06it/s] 64%|██████▍   | 247/383 [00:17<00:10, 13.08it/s]
 64%|██████▍   | 247/383 [00:17<00:10, 13.27it/s][A 64%|██████▎   | 244/383 [00:17<00:10, 13.59it/s] 64%|██████▍   | 247/383 [00:17<00:10, 12.86it/s] 65%|██████▌   | 249/383 [00:17<00:10, 13.01it/s]
 65%|██████▌   | 249/383 [00:18<00:10, 12.83it/s][A 64%|██████▍   | 246/383 [00:18<00:10, 13.28it/s] 65%|██████▌   | 249/383 [00:18<00:10, 12.86it/s] 66%|██████▌   | 251/383 [00:18<00:10, 12.64it/s]
 66%|██████▌   | 251/383 [00:18<00:10, 12.97it/s][A 65%|██████▍   | 248/383 [00:18<00:10, 13.14it/s] 66%|██████▌   | 251/383 [00:18<00:10, 12.96it/s] 66%|██████▌   | 253/383 [00:18<00:10, 12.69it/s]
 66%|██████▌   | 253/383 [00:18<00:10, 12.96it/s][A 65%|██████▌   | 250/383 [00:18<00:10, 13.03it/s] 66%|██████▌   | 253/383 [00:18<00:10, 12.67it/s] 67%|██████▋   | 255/383 [00:18<00:09, 12.91it/s]
 67%|██████▋   | 255/383 [00:18<00:09, 13.06it/s][A 66%|██████▌   | 252/383 [00:18<00:10, 12.87it/s] 67%|██████▋   | 255/383 [00:18<00:10, 12.64it/s] 67%|██████▋   | 257/383 [00:18<00:09, 13.10it/s]
 67%|██████▋   | 257/383 [00:18<00:09, 13.16it/s][A 66%|██████▋   | 254/383 [00:18<00:10, 12.76it/s] 67%|██████▋   | 257/383 [00:18<00:09, 12.94it/s] 68%|██████▊   | 259/383 [00:18<00:09, 13.36it/s]
 68%|██████▊   | 259/383 [00:18<00:09, 13.28it/s][A 67%|██████▋   | 256/383 [00:18<00:09, 12.87it/s] 68%|██████▊   | 259/383 [00:18<00:09, 13.06it/s] 68%|██████▊   | 261/383 [00:18<00:09, 13.23it/s]
 68%|██████▊   | 261/383 [00:18<00:09, 13.49it/s][A 67%|██████▋   | 258/383 [00:18<00:09, 12.97it/s] 68%|██████▊   | 261/383 [00:19<00:09, 13.09it/s] 69%|██████▊   | 263/383 [00:19<00:08, 13.43it/s]
 69%|██████▊   | 263/383 [00:19<00:08, 13.47it/s][A 68%|██████▊   | 260/383 [00:19<00:09, 13.14it/s] 69%|██████▉   | 266/383 [00:19<00:07, 16.40it/s] 69%|██████▊   | 263/383 [00:19<00:09, 13.17it/s]
 69%|██████▉   | 266/383 [00:19<00:07, 16.05it/s][A 70%|███████   | 269/383 [00:19<00:06, 18.77it/s] 68%|██████▊   | 262/383 [00:19<00:09, 13.28it/s]
 70%|███████   | 269/383 [00:19<00:06, 18.56it/s][A 69%|██████▉   | 266/383 [00:19<00:07, 15.69it/s] 69%|██████▉   | 264/383 [00:19<00:08, 14.39it/s] 71%|███████   | 272/383 [00:19<00:05, 19.01it/s] 70%|███████   | 269/383 [00:19<00:06, 18.04it/s]
 71%|███████   | 272/383 [00:19<00:05, 19.77it/s][A 70%|██████▉   | 267/383 [00:19<00:07, 16.45it/s] 72%|███████▏  | 274/383 [00:19<00:06, 17.69it/s] 71%|███████   | 272/383 [00:19<00:05, 19.10it/s]
 72%|███████▏  | 275/383 [00:19<00:05, 18.50it/s][A 70%|███████   | 270/383 [00:19<00:06, 18.83it/s] 72%|███████▏  | 274/383 [00:19<00:06, 17.84it/s] 72%|███████▏  | 276/383 [00:19<00:06, 16.51it/s]
 72%|███████▏  | 277/383 [00:19<00:05, 17.72it/s][A 71%|███████▏  | 273/383 [00:19<00:05, 19.41it/s] 72%|███████▏  | 276/383 [00:19<00:06, 17.41it/s] 73%|███████▎  | 278/383 [00:19<00:06, 16.33it/s]
 73%|███████▎  | 279/383 [00:19<00:06, 16.87it/s][A 72%|███████▏  | 275/383 [00:19<00:05, 18.46it/s] 73%|███████▎  | 280/383 [00:19<00:06, 16.17it/s] 73%|███████▎  | 278/383 [00:19<00:06, 16.56it/s]
 73%|███████▎  | 281/383 [00:20<00:06, 16.18it/s][A 72%|███████▏  | 277/383 [00:20<00:05, 17.75it/s] 73%|███████▎  | 280/383 [00:20<00:06, 16.27it/s] 74%|███████▎  | 282/383 [00:20<00:07, 13.85it/s] 73%|███████▎  | 279/383 [00:20<00:06, 16.43it/s]
 74%|███████▍  | 283/383 [00:20<00:07, 14.27it/s][A 74%|███████▎  | 282/383 [00:20<00:07, 14.12it/s] 74%|███████▍  | 284/383 [00:20<00:07, 12.93it/s] 73%|███████▎  | 281/383 [00:20<00:06, 14.67it/s]
 74%|███████▍  | 285/383 [00:20<00:07, 12.90it/s][A 74%|███████▍  | 284/383 [00:20<00:07, 12.77it/s] 75%|███████▍  | 286/383 [00:20<00:07, 12.21it/s] 74%|███████▍  | 283/383 [00:20<00:07, 13.05it/s]
 75%|███████▍  | 287/383 [00:20<00:07, 12.08it/s][A 75%|███████▍  | 286/383 [00:20<00:08, 12.12it/s] 75%|███████▌  | 288/383 [00:20<00:07, 11.90it/s] 74%|███████▍  | 285/383 [00:20<00:07, 12.67it/s] 75%|███████▌  | 288/383 [00:20<00:08, 11.38it/s] 75%|███████▍  | 287/383 [00:20<00:07, 12.33it/s]
 75%|███████▌  | 289/383 [00:20<00:10,  8.76it/s][A 76%|███████▌  | 290/383 [00:21<00:12,  7.74it/s] 75%|███████▌  | 289/383 [00:21<00:10,  9.27it/s] 76%|███████▌  | 290/383 [00:21<00:12,  7.31it/s] 76%|███████▌  | 291/383 [00:21<00:13,  6.62it/s]
 76%|███████▌  | 291/383 [00:21<00:14,  6.29it/s][A 76%|███████▌  | 291/383 [00:21<00:14,  6.22it/s] 76%|███████▌  | 292/383 [00:21<00:15,  5.83it/s]
 76%|███████▌  | 292/383 [00:21<00:16,  5.59it/s][A 76%|███████▌  | 291/383 [00:21<00:14,  6.39it/s] 76%|███████▌  | 292/383 [00:21<00:17,  5.33it/s] 77%|███████▋  | 293/383 [00:21<00:17,  5.12it/s]
 77%|███████▋  | 293/383 [00:22<00:18,  4.92it/s][A 76%|███████▌  | 292/383 [00:22<00:16,  5.62it/s] 77%|███████▋  | 293/383 [00:22<00:18,  4.91it/s] 77%|███████▋  | 294/383 [00:22<00:18,  4.79it/s]
 77%|███████▋  | 294/383 [00:22<00:19,  4.60it/s][A 77%|███████▋  | 293/383 [00:22<00:17,  5.10it/s] 77%|███████▋  | 295/383 [00:22<00:18,  4.71it/s] 77%|███████▋  | 294/383 [00:22<00:19,  4.50it/s]
 77%|███████▋  | 295/383 [00:22<00:19,  4.42it/s][A 77%|███████▋  | 294/383 [00:22<00:18,  4.73it/s] 77%|███████▋  | 296/383 [00:22<00:20,  4.26it/s] 77%|███████▋  | 295/383 [00:22<00:20,  4.20it/s] 77%|███████▋  | 295/383 [00:22<00:19,  4.54it/s]
 77%|███████▋  | 296/383 [00:22<00:20,  4.19it/s][A 78%|███████▊  | 297/383 [00:22<00:19,  4.31it/s] 77%|███████▋  | 296/383 [00:23<00:21,  4.08it/s] 77%|███████▋  | 296/383 [00:23<00:20,  4.26it/s]
 78%|███████▊  | 297/383 [00:23<00:21,  3.93it/s][A 78%|███████▊  | 298/383 [00:23<00:20,  4.10it/s] 78%|███████▊  | 297/383 [00:23<00:21,  4.08it/s]
 78%|███████▊  | 298/383 [00:23<00:21,  4.04it/s][A 78%|███████▊  | 297/383 [00:23<00:21,  4.06it/s] 78%|███████▊  | 298/383 [00:23<00:19,  4.28it/s] 78%|███████▊  | 299/383 [00:23<00:21,  3.95it/s]
 78%|███████▊  | 299/383 [00:23<00:20,  4.01it/s][A 78%|███████▊  | 298/383 [00:23<00:22,  3.80it/s] 78%|███████▊  | 300/383 [00:23<00:20,  4.06it/s] 78%|███████▊  | 299/383 [00:23<00:20,  4.06it/s]
 78%|███████▊  | 300/383 [00:23<00:21,  3.86it/s][A 78%|███████▊  | 299/383 [00:23<00:22,  3.78it/s] 79%|███████▊  | 301/383 [00:24<00:20,  3.92it/s] 78%|███████▊  | 300/383 [00:24<00:21,  3.89it/s]
 79%|███████▊  | 301/383 [00:24<00:21,  3.78it/s][A 79%|███████▊  | 301/383 [00:24<00:20,  4.03it/s] 78%|███████▊  | 300/383 [00:24<00:22,  3.71it/s] 79%|███████▉  | 302/383 [00:24<00:21,  3.85it/s]
 79%|███████▉  | 302/383 [00:24<00:21,  3.75it/s][A 79%|███████▉  | 303/383 [00:24<00:21,  3.80it/s] 79%|███████▉  | 302/383 [00:24<00:21,  3.82it/s] 79%|███████▊  | 301/383 [00:24<00:22,  3.65it/s]
 79%|███████▉  | 303/383 [00:24<00:20,  3.93it/s][A 79%|███████▉  | 303/383 [00:24<00:20,  3.97it/s] 79%|███████▉  | 302/383 [00:24<00:22,  3.62it/s] 79%|███████▉  | 304/383 [00:24<00:21,  3.68it/s]
 79%|███████▉  | 304/383 [00:24<00:21,  3.76it/s][A 79%|███████▉  | 304/383 [00:25<00:19,  4.00it/s] 80%|███████▉  | 305/383 [00:25<00:20,  3.83it/s] 79%|███████▉  | 303/383 [00:25<00:22,  3.63it/s] 80%|███████▉  | 306/383 [00:25<00:18,  4.09it/s]
 80%|███████▉  | 305/383 [00:25<00:21,  3.63it/s][A 80%|███████▉  | 305/383 [00:25<00:20,  3.85it/s] 79%|███████▉  | 304/383 [00:25<00:21,  3.62it/s] 80%|████████  | 307/383 [00:25<00:19,  3.95it/s]
 80%|███████▉  | 306/383 [00:25<00:20,  3.67it/s][A 80%|███████▉  | 306/383 [00:25<00:20,  3.81it/s] 80%|███████▉  | 305/383 [00:25<00:21,  3.63it/s] 80%|████████  | 308/383 [00:25<00:19,  3.81it/s]
 80%|████████  | 307/383 [00:25<00:20,  3.65it/s][A 80%|████████  | 307/383 [00:25<00:20,  3.73it/s] 80%|███████▉  | 306/383 [00:25<00:21,  3.62it/s] 81%|████████  | 309/383 [00:26<00:19,  3.85it/s] 80%|████████  | 308/383 [00:26<00:19,  3.87it/s]
 80%|████████  | 308/383 [00:26<00:20,  3.67it/s][A 80%|████████  | 307/383 [00:26<00:21,  3.59it/s]
 81%|████████  | 309/383 [00:26<00:19,  3.76it/s][A 81%|████████  | 309/383 [00:26<00:19,  3.79it/s] 81%|████████  | 310/383 [00:26<00:19,  3.70it/s] 80%|████████  | 308/383 [00:26<00:20,  3.57it/s]
 81%|████████  | 310/383 [00:26<00:18,  3.86it/s][A 81%|████████  | 310/383 [00:26<00:19,  3.71it/s] 81%|████████  | 311/383 [00:26<00:19,  3.64it/s] 81%|████████  | 309/383 [00:26<00:19,  3.80it/s]
 81%|████████  | 311/383 [00:26<00:18,  3.83it/s][A 81%|████████  | 311/383 [00:26<00:19,  3.69it/s] 81%|████████▏ | 312/383 [00:26<00:19,  3.64it/s] 81%|████████  | 310/383 [00:26<00:18,  3.90it/s]
 81%|████████▏ | 312/383 [00:27<00:18,  3.86it/s][A 81%|████████▏ | 312/383 [00:27<00:19,  3.64it/s] 82%|████████▏ | 313/383 [00:27<00:19,  3.64it/s] 81%|████████  | 311/383 [00:27<00:18,  3.81it/s]
 82%|████████▏ | 313/383 [00:27<00:19,  3.66it/s][A 82%|████████▏ | 313/383 [00:27<00:18,  3.69it/s] 81%|████████▏ | 312/383 [00:27<00:18,  3.88it/s] 82%|████████▏ | 314/383 [00:27<00:19,  3.61it/s]
 82%|████████▏ | 314/383 [00:27<00:18,  3.68it/s][A 82%|████████▏ | 314/383 [00:27<00:18,  3.79it/s] 82%|████████▏ | 313/383 [00:27<00:17,  3.91it/s] 82%|████████▏ | 315/383 [00:27<00:19,  3.51it/s] 82%|████████▏ | 315/383 [00:27<00:17,  3.84it/s]
 82%|████████▏ | 315/383 [00:27<00:18,  3.67it/s][A 82%|████████▏ | 314/383 [00:27<00:17,  3.96it/s] 83%|████████▎ | 316/383 [00:28<00:18,  3.64it/s] 82%|████████▏ | 315/383 [00:28<00:16,  4.09it/s]
 83%|████████▎ | 316/383 [00:28<00:18,  3.72it/s][A 83%|████████▎ | 316/383 [00:28<00:17,  3.81it/s] 83%|████████▎ | 317/383 [00:28<00:18,  3.64it/s]
 83%|████████▎ | 317/383 [00:28<00:17,  3.71it/s][A 83%|████████▎ | 316/383 [00:28<00:17,  3.80it/s] 83%|████████▎ | 317/383 [00:28<00:18,  3.64it/s] 83%|████████▎ | 318/383 [00:28<00:18,  3.51it/s] 83%|████████▎ | 317/383 [00:28<00:17,  3.75it/s]
 83%|████████▎ | 318/383 [00:28<00:18,  3.56it/s][A 83%|████████▎ | 318/383 [00:28<00:17,  3.61it/s] 83%|████████▎ | 319/383 [00:28<00:17,  3.70it/s]
 83%|████████▎ | 319/383 [00:29<00:17,  3.68it/s][A 83%|████████▎ | 318/383 [00:29<00:17,  3.71it/s] 83%|████████▎ | 319/383 [00:29<00:17,  3.67it/s] 84%|████████▎ | 320/383 [00:29<00:17,  3.69it/s]
 84%|████████▎ | 320/383 [00:29<00:16,  3.76it/s][A 83%|████████▎ | 319/383 [00:29<00:17,  3.71it/s] 84%|████████▎ | 320/383 [00:29<00:17,  3.69it/s] 84%|████████▍ | 321/383 [00:29<00:17,  3.60it/s]
 84%|████████▍ | 321/383 [00:29<00:16,  3.78it/s][A 84%|████████▍ | 321/383 [00:29<00:16,  3.73it/s] 84%|████████▎ | 320/383 [00:29<00:17,  3.68it/s] 84%|████████▍ | 322/383 [00:29<00:16,  3.63it/s]
 84%|████████▍ | 322/383 [00:29<00:15,  3.85it/s][A 84%|████████▍ | 321/383 [00:29<00:17,  3.59it/s] 84%|████████▍ | 322/383 [00:29<00:16,  3.59it/s] 84%|████████▍ | 323/383 [00:29<00:16,  3.75it/s]
 84%|████████▍ | 323/383 [00:30<00:15,  3.98it/s][A 84%|████████▍ | 322/383 [00:30<00:16,  3.78it/s] 84%|████████▍ | 323/383 [00:30<00:16,  3.73it/s] 85%|████████▍ | 324/383 [00:30<00:15,  3.71it/s]
 85%|████████▍ | 324/383 [00:30<00:15,  3.77it/s][A 84%|████████▍ | 323/383 [00:30<00:16,  3.72it/s] 85%|████████▍ | 324/383 [00:30<00:15,  3.72it/s] 85%|████████▍ | 325/383 [00:30<00:15,  3.80it/s]
 85%|████████▍ | 325/383 [00:30<00:15,  3.86it/s][A 85%|████████▍ | 325/383 [00:30<00:15,  3.82it/s] 85%|████████▍ | 324/383 [00:30<00:16,  3.68it/s] 85%|████████▌ | 326/383 [00:30<00:14,  3.96it/s]
 85%|████████▌ | 326/383 [00:30<00:15,  3.75it/s][A 85%|████████▌ | 326/383 [00:30<00:14,  3.85it/s] 85%|████████▌ | 327/383 [00:30<00:14,  3.89it/s] 85%|████████▍ | 325/383 [00:30<00:15,  3.67it/s]
 85%|████████▌ | 327/383 [00:31<00:14,  3.74it/s][A 85%|████████▌ | 326/383 [00:31<00:14,  3.82it/s] 85%|████████▌ | 327/383 [00:31<00:15,  3.67it/s] 86%|████████▌ | 328/383 [00:31<00:14,  3.79it/s]
 86%|████████▌ | 328/383 [00:31<00:14,  3.68it/s][A 85%|████████▌ | 327/383 [00:31<00:14,  3.89it/s] 86%|████████▌ | 328/383 [00:31<00:15,  3.64it/s] 86%|████████▌ | 329/383 [00:31<00:14,  3.80it/s]
 86%|████████▌ | 329/383 [00:31<00:14,  3.74it/s][A 86%|████████▌ | 328/383 [00:31<00:14,  3.88it/s] 86%|████████▌ | 329/383 [00:31<00:14,  3.76it/s] 86%|████████▌ | 330/383 [00:31<00:14,  3.74it/s] 86%|████████▋ | 331/383 [00:31<00:12,  4.25it/s]
 86%|████████▌ | 330/383 [00:31<00:14,  3.76it/s][A 86%|████████▌ | 329/383 [00:31<00:13,  3.89it/s] 86%|████████▌ | 330/383 [00:32<00:14,  3.64it/s] 87%|████████▋ | 332/383 [00:32<00:10,  4.84it/s] 86%|████████▌ | 330/383 [00:32<00:12,  4.13it/s] 86%|████████▋ | 331/383 [00:32<00:12,  4.27it/s]
 86%|████████▋ | 331/383 [00:32<00:13,  3.73it/s][A 87%|████████▋ | 333/383 [00:32<00:09,  5.33it/s] 87%|████████▋ | 332/383 [00:32<00:11,  4.60it/s]
 87%|████████▋ | 332/383 [00:32<00:12,  4.22it/s][A 87%|████████▋ | 334/383 [00:32<00:08,  5.62it/s] 86%|████████▋ | 331/383 [00:32<00:13,  3.99it/s]
 87%|████████▋ | 333/383 [00:32<00:10,  4.70it/s][A 87%|████████▋ | 333/383 [00:32<00:10,  4.85it/s] 87%|████████▋ | 335/383 [00:32<00:08,  5.81it/s] 87%|████████▋ | 332/383 [00:32<00:11,  4.37it/s]
 87%|████████▋ | 334/383 [00:32<00:09,  5.06it/s][A 88%|████████▊ | 336/383 [00:32<00:07,  5.94it/s] 87%|████████▋ | 334/383 [00:32<00:09,  5.09it/s] 87%|████████▋ | 333/383 [00:32<00:10,  4.96it/s] 87%|████████▋ | 335/383 [00:32<00:08,  5.63it/s] 88%|████████▊ | 337/383 [00:32<00:07,  6.11it/s]
 87%|████████▋ | 335/383 [00:32<00:09,  5.25it/s][A 87%|████████▋ | 334/383 [00:32<00:09,  5.43it/s] 88%|████████▊ | 336/383 [00:33<00:08,  5.73it/s] 88%|████████▊ | 338/383 [00:33<00:07,  5.92it/s]
 88%|████████▊ | 336/383 [00:33<00:08,  5.25it/s][A 87%|████████▋ | 335/383 [00:33<00:08,  5.49it/s] 88%|████████▊ | 337/383 [00:33<00:07,  5.89it/s] 89%|████████▉ | 340/383 [00:33<00:05,  7.69it/s] 88%|████████▊ | 336/383 [00:33<00:07,  5.89it/s]
 88%|████████▊ | 337/383 [00:33<00:08,  5.39it/s][A 88%|████████▊ | 338/383 [00:33<00:07,  6.01it/s] 89%|████████▉ | 342/383 [00:33<00:04,  9.47it/s] 88%|████████▊ | 337/383 [00:33<00:07,  6.11it/s]
 88%|████████▊ | 338/383 [00:33<00:08,  5.49it/s][A 89%|████████▉ | 340/383 [00:33<00:05,  8.40it/s] 90%|████████▉ | 344/383 [00:33<00:03, 10.79it/s] 88%|████████▊ | 338/383 [00:33<00:07,  6.17it/s]
 89%|████████▊ | 339/383 [00:33<00:07,  5.73it/s][A 89%|████████▉ | 342/383 [00:33<00:04,  9.83it/s] 90%|█████████ | 346/383 [00:33<00:03, 11.54it/s] 89%|████████▉ | 340/383 [00:33<00:05,  8.26it/s]
 89%|████████▉ | 341/383 [00:33<00:05,  7.89it/s][A 90%|████████▉ | 344/383 [00:33<00:03, 11.11it/s] 91%|█████████ | 348/383 [00:33<00:02, 12.47it/s] 89%|████████▉ | 342/383 [00:33<00:04,  9.96it/s]
 90%|████████▉ | 343/383 [00:33<00:04,  9.61it/s][A 90%|█████████ | 346/383 [00:33<00:03, 12.21it/s] 91%|█████████▏| 350/383 [00:33<00:02, 13.35it/s] 90%|████████▉ | 344/383 [00:33<00:03, 10.54it/s]
 90%|█████████ | 345/383 [00:33<00:03, 11.04it/s][A 91%|█████████ | 348/383 [00:34<00:02, 12.99it/s] 92%|█████████▏| 352/383 [00:34<00:02, 13.38it/s] 90%|█████████ | 346/383 [00:34<00:03, 11.91it/s]
 91%|█████████ | 347/383 [00:34<00:02, 12.18it/s][A 91%|█████████▏| 350/383 [00:34<00:02, 13.07it/s] 92%|█████████▏| 354/383 [00:34<00:02, 13.70it/s] 91%|█████████ | 348/383 [00:34<00:02, 12.86it/s]
 91%|█████████ | 349/383 [00:34<00:02, 12.89it/s][A 92%|█████████▏| 352/383 [00:34<00:02, 13.79it/s] 93%|█████████▎| 356/383 [00:34<00:01, 14.47it/s] 91%|█████████▏| 350/383 [00:34<00:02, 13.39it/s]
 92%|█████████▏| 351/383 [00:34<00:02, 13.29it/s][A 93%|█████████▎| 358/383 [00:34<00:01, 15.28it/s] 92%|█████████▏| 354/383 [00:34<00:02, 14.08it/s] 92%|█████████▏| 352/383 [00:34<00:02, 13.67it/s]
 92%|█████████▏| 353/383 [00:34<00:02, 13.27it/s][A 93%|█████████▎| 356/383 [00:34<00:01, 14.37it/s] 92%|█████████▏| 354/383 [00:34<00:02, 13.76it/s]
 93%|█████████▎| 355/383 [00:34<00:02, 13.58it/s][A 93%|█████████▎| 358/383 [00:34<00:01, 14.75it/s] 94%|█████████▍| 360/383 [00:34<00:02,  9.73it/s]
 93%|█████████▎| 357/383 [00:34<00:01, 14.40it/s][A 93%|█████████▎| 356/383 [00:34<00:01, 13.63it/s]
 94%|█████████▎| 359/383 [00:34<00:01, 15.44it/s][A 93%|█████████▎| 358/383 [00:34<00:01, 14.87it/s] 94%|█████████▍| 360/383 [00:35<00:02,  9.97it/s] 95%|█████████▍| 362/383 [00:35<00:02,  8.33it/s] 94%|█████████▍| 360/383 [00:35<00:01, 11.85it/s]
 94%|█████████▍| 361/383 [00:35<00:02, 10.16it/s][A 95%|█████████▍| 362/383 [00:35<00:02,  8.17it/s] 95%|█████████▌| 364/383 [00:35<00:02,  7.20it/s] 95%|█████████▍| 362/383 [00:35<00:02,  9.10it/s]
 95%|█████████▍| 363/383 [00:35<00:02,  8.47it/s][A 95%|█████████▌| 365/383 [00:35<00:02,  6.86it/s] 95%|█████████▌| 364/383 [00:35<00:02,  7.14it/s] 96%|█████████▌| 367/383 [00:35<00:01,  8.69it/s] 95%|█████████▌| 364/383 [00:35<00:02,  7.81it/s] 96%|█████████▋| 369/383 [00:35<00:01, 10.47it/s] 95%|█████████▌| 365/383 [00:35<00:02,  6.85it/s]
 95%|█████████▌| 365/383 [00:35<00:02,  7.44it/s][A 97%|█████████▋| 371/383 [00:35<00:00, 12.16it/s] 95%|█████████▌| 365/383 [00:36<00:02,  7.36it/s] 96%|█████████▌| 367/383 [00:36<00:01,  8.61it/s]
 96%|█████████▌| 366/383 [00:36<00:02,  7.38it/s][A 97%|█████████▋| 373/383 [00:36<00:00, 13.67it/s] 96%|█████████▌| 367/383 [00:36<00:01,  9.24it/s] 96%|█████████▋| 369/383 [00:36<00:01, 10.40it/s]
 96%|█████████▌| 368/383 [00:36<00:01,  9.20it/s][A 98%|█████████▊| 375/383 [00:36<00:00, 14.64it/s] 96%|█████████▋| 369/383 [00:36<00:01, 10.96it/s] 97%|█████████▋| 371/383 [00:36<00:00, 12.04it/s]
 97%|█████████▋| 370/383 [00:36<00:01, 10.89it/s][A 98%|█████████▊| 377/383 [00:36<00:00, 15.82it/s] 97%|█████████▋| 371/383 [00:36<00:00, 12.44it/s] 97%|█████████▋| 373/383 [00:36<00:00, 13.42it/s]
 97%|█████████▋| 372/383 [00:36<00:00, 12.34it/s][A 99%|█████████▉| 380/383 [00:36<00:00, 18.26it/s] 97%|█████████▋| 373/383 [00:36<00:00, 13.83it/s] 98%|█████████▊| 375/383 [00:36<00:00, 14.87it/s]
 98%|█████████▊| 374/383 [00:36<00:00, 13.79it/s][A100%|██████████| 383/383 [00:36<00:00, 19.37it/s]100%|██████████| 383/383 [00:36<00:00, 10.47it/s]
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
 98%|█████████▊| 376/383 [00:36<00:00, 15.78it/s]/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
 99%|█████████▊| 378/383 [00:36<00:00, 16.91it/s]/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/pytorch/torch/utils/checkpoint.py:426: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 98%|█████████▊| 377/383 [00:36<00:00, 15.74it/s][A 99%|█████████▉| 379/383 [00:36<00:00, 18.37it/s] 99%|█████████▉| 381/383 [00:36<00:00, 19.49it/s]
 99%|█████████▉| 380/383 [00:36<00:00, 17.69it/s][A100%|██████████| 383/383 [00:36<00:00, 10.40it/s]
100%|█████████▉| 382/383 [00:36<00:00, 20.59it/s]/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
100%|██████████| 383/383 [00:36<00:00, 10.38it/s]
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/pytorch/torch/utils/checkpoint.py:426: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

100%|██████████| 383/383 [00:36<00:00, 18.63it/s][A100%|██████████| 383/383 [00:36<00:00, 10.36it/s]
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
                                                       {'mmlu_loss': 4.51468842583599, 'mmlu_eval_accuracy_conceptual_physics': 0.34615384615384615, 'mmlu_eval_accuracy_logical_fallacies': nan, 'mmlu_eval_accuracy_professional_psychology': nan, 'mmlu_eval_accuracy_public_relations': nan, 'mmlu_eval_accuracy_security_studies': nan, 'mmlu_eval_accuracy_high_school_physics': nan, 'mmlu_eval_accuracy_formal_logic': 0.2857142857142857, 'mmlu_eval_accuracy_anatomy': 0.07142857142857142, 'mmlu_eval_accuracy_moral_scenarios': nan, 'mmlu_eval_accuracy_moral_disputes': nan, 'mmlu_eval_accuracy_management': nan, 'mmlu_eval_accuracy_high_school_european_history': 0.3333333333333333, 'mmlu_eval_accuracy_professional_law': nan, 'mmlu_eval_accuracy_elementary_mathematics': 0.2682926829268293, 'mmlu_eval_accuracy_high_school_computer_science': 0.3333333333333333, 'mmlu_eval_accuracy_high_school_geography': 0.4166666666666667, 'mmlu_eval_accuracy_global_facts': 0.2, 'mmlu_eval_accuracy_high_school_biology': 0.21875, 'mmlu_eval_accuracy_philosophy': nan, 'mmlu_eval_accuracy_astronomy': 0.1875, 'mmlu_eval_accuracy_nutrition': nan, 'mmlu_eval_accuracy_high_school_government_and_politics': nan, 'mmlu_eval_accuracy_medical_genetics': nan, 'mmlu_eval_accuracy_human_aging': nan, 'mmlu_eval_accuracy_human_sexuality': nan, 'mmlu_eval_accuracy_college_medicine': 0.18181818181818182, 'mmlu_eval_accuracy_abstract_algebra': 0.0, 'mmlu_eval_accuracy_college_physics': 0.2727272727272727, 'mmlu_eval_accuracy_us_foreign_policy': nan, 'mmlu_eval_accuracy_college_computer_science': 0.18181818181818182, 'mmlu_eval_accuracy_machine_learning': nan, 'mmlu_eval_accuracy_professional_accounting': nan, 'mmlu_eval_accuracy_world_religions': nan, 'mmlu_eval_accuracy_miscellaneous': nan, 'mmlu_eval_accuracy_jurisprudence': nan, 'mmlu_eval_accuracy_high_school_chemistry': 0.18181818181818182, 'mmlu_eval_accuracy_virology': nan, 'mmlu_eval_accuracy_high_school_mathematics': nan, 'mmlu_eval_accuracy_electrical_engineering': 0.3125, 'mmlu_eval_accuracy_high_school_world_history': nan, 'mmlu_eval_accuracy_prehistory': nan, 'mmlu_eval_accuracy_high_school_statistics': nan, 'mmlu_eval_accuracy_high_school_psychology': nan, 'mmlu_eval_accuracy_high_school_microeconomics': nan, 'mmlu_eval_accuracy_high_school_us_history': nan, 'mmlu_eval_accuracy_college_biology': 0.3125, 'mmlu_eval_accuracy_high_school_macroeconomics': nan, 'mmlu_eval_accuracy_professional_medicine': nan, 'mmlu_eval_accuracy_clinical_knowledge': 0.20689655172413793, 'mmlu_eval_accuracy_business_ethics': 0.45454545454545453, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_econometrics': 0.25, 'mmlu_eval_accuracy_marketing': nan, 'mmlu_eval_accuracy_international_law': nan, 'mmlu_eval_accuracy_sociology': nan, 'mmlu_eval_accuracy_computer_security': 0.09090909090909091, 'mmlu_eval_accuracy_college_mathematics': 0.36363636363636365, 'mmlu_eval_accuracy': nan, 'epoch': 7.43}
 50%|█████     | 1024/2048 [1:28:49<1:15:06,  4.40s/it]/home/bagus/pytorch/torch/utils/checkpoint.py:426: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/pytorch/torch/utils/checkpoint.py:426: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 50%|█████     | 1025/2048 [1:28:54<5:38:57, 19.88s/it] 50%|█████     | 1026/2048 [1:28:58<4:17:48, 15.14s/it] 50%|█████     | 1027/2048 [1:29:01<3:17:55, 11.63s/it] 50%|█████     | 1028/2048 [1:29:05<2:37:41,  9.28s/it] 50%|█████     | 1029/2048 [1:29:10<2:16:37,  8.04s/it] 50%|█████     | 1030/2048 [1:29:15<2:01:49,  7.18s/it] 50%|█████     | 1031/2048 [1:29:20<1:51:19,  6.57s/it] 50%|█████     | 1032/2048 [1:29:25<1:43:45,  6.13s/it] 50%|█████     | 1033/2048 [1:29:30<1:37:07,  5.74s/it] 50%|█████     | 1034/2048 [1:29:35<1:31:08,  5.39s/it] 51%|█████     | 1035/2048 [1:29:39<1:25:57,  5.09s/it] 51%|█████     | 1036/2048 [1:29:43<1:21:07,  4.81s/it] 51%|█████     | 1037/2048 [1:29:48<1:17:41,  4.61s/it] 51%|█████     | 1038/2048 [1:29:51<1:14:11,  4.41s/it] 51%|█████     | 1039/2048 [1:29:56<1:12:31,  4.31s/it] 51%|█████     | 1040/2048 [1:29:59<1:06:52,  3.98s/it]                                                       {'loss': 0.1619, 'learning_rate': 0.00010237258146928848, 'epoch': 7.54}
 51%|█████     | 1040/2048 [1:29:59<1:06:52,  3.98s/it] 51%|█████     | 1041/2048 [1:30:04<1:11:20,  4.25s/it] 51%|█████     | 1042/2048 [1:30:09<1:15:53,  4.53s/it] 51%|█████     | 1043/2048 [1:30:14<1:18:54,  4.71s/it] 51%|█████     | 1044/2048 [1:30:19<1:20:34,  4.81s/it] 51%|█████     | 1045/2048 [1:30:24<1:20:46,  4.83s/it] 51%|█████     | 1046/2048 [1:30:29<1:19:50,  4.78s/it] 51%|█████     | 1047/2048 [1:30:33<1:17:41,  4.66s/it] 51%|█████     | 1048/2048 [1:30:37<1:14:07,  4.45s/it] 51%|█████     | 1049/2048 [1:30:41<1:12:14,  4.34s/it] 51%|█████▏    | 1050/2048 [1:30:45<1:09:15,  4.16s/it] 51%|█████▏    | 1051/2048 [1:30:48<1:06:42,  4.01s/it] 51%|█████▏    | 1052/2048 [1:30:52<1:03:35,  3.83s/it] 51%|█████▏    | 1053/2048 [1:30:56<1:03:39,  3.84s/it] 51%|█████▏    | 1054/2048 [1:31:01<1:10:12,  4.24s/it] 52%|█████▏    | 1055/2048 [1:31:06<1:14:43,  4.52s/it] 52%|█████▏    | 1056/2048 [1:31:11<1:17:48,  4.71s/it]                                                       {'loss': 0.1389, 'learning_rate': 9.984181312470378e-05, 'epoch': 7.66}
 52%|█████▏    | 1056/2048 [1:31:11<1:17:48,  4.71s/it] 52%|█████▏    | 1057/2048 [1:31:16<1:19:07,  4.79s/it] 52%|█████▏    | 1058/2048 [1:31:21<1:19:08,  4.80s/it] 52%|█████▏    | 1059/2048 [1:31:25<1:17:30,  4.70s/it] 52%|█████▏    | 1060/2048 [1:31:30<1:15:40,  4.60s/it] 52%|█████▏    | 1061/2048 [1:31:34<1:14:19,  4.52s/it] 52%|█████▏    | 1062/2048 [1:31:38<1:11:13,  4.33s/it] 52%|█████▏    | 1063/2048 [1:31:42<1:09:18,  4.22s/it] 52%|█████▏    | 1064/2048 [1:31:46<1:07:01,  4.09s/it] 52%|█████▏    | 1065/2048 [1:31:49<1:03:01,  3.85s/it] 52%|█████▏    | 1066/2048 [1:31:54<1:08:02,  4.16s/it] 52%|█████▏    | 1067/2048 [1:31:59<1:12:55,  4.46s/it] 52%|█████▏    | 1068/2048 [1:32:04<1:16:12,  4.67s/it] 52%|█████▏    | 1069/2048 [1:32:09<1:17:57,  4.78s/it] 52%|█████▏    | 1070/2048 [1:32:14<1:18:38,  4.83s/it] 52%|█████▏    | 1071/2048 [1:32:19<1:17:40,  4.77s/it] 52%|█████▏    | 1072/2048 [1:32:23<1:15:34,  4.65s/it]                                                       {'loss': 0.1607, 'learning_rate': 9.73111461078893e-05, 'epoch': 7.78}
 52%|█████▏    | 1072/2048 [1:32:23<1:15:34,  4.65s/it] 52%|█████▏    | 1073/2048 [1:32:27<1:13:43,  4.54s/it] 52%|█████▏    | 1074/2048 [1:32:31<1:10:50,  4.36s/it] 52%|█████▏    | 1075/2048 [1:32:35<1:07:22,  4.15s/it] 53%|█████▎    | 1076/2048 [1:32:39<1:05:04,  4.02s/it] 53%|█████▎    | 1077/2048 [1:32:42<1:03:11,  3.90s/it] 53%|█████▎    | 1078/2048 [1:32:46<1:02:53,  3.89s/it] 53%|█████▎    | 1079/2048 [1:32:51<1:09:01,  4.27s/it] 53%|█████▎    | 1080/2048 [1:32:57<1:13:17,  4.54s/it] 53%|█████▎    | 1081/2048 [1:33:02<1:16:10,  4.73s/it] 53%|█████▎    | 1082/2048 [1:33:07<1:17:56,  4.84s/it] 53%|█████▎    | 1083/2048 [1:33:12<1:18:12,  4.86s/it] 53%|█████▎    | 1084/2048 [1:33:16<1:16:01,  4.73s/it] 53%|█████▎    | 1085/2048 [1:33:21<1:13:37,  4.59s/it] 53%|█████▎    | 1086/2048 [1:33:24<1:10:38,  4.41s/it] 53%|█████▎    | 1087/2048 [1:33:28<1:08:12,  4.26s/it] 53%|█████▎    | 1088/2048 [1:33:32<1:05:59,  4.12s/it]                                                       {'loss': 0.1615, 'learning_rate': 9.478220145627645e-05, 'epoch': 7.89}
 53%|█████▎    | 1088/2048 [1:33:32<1:05:59,  4.12s/it] 53%|█████▎    | 1089/2048 [1:33:36<1:05:24,  4.09s/it] 53%|█████▎    | 1090/2048 [1:33:39<1:01:10,  3.83s/it] 53%|█████▎    | 1091/2048 [1:33:44<1:06:07,  4.15s/it] 53%|█████▎    | 1092/2048 [1:33:49<1:10:56,  4.45s/it] 53%|█████▎    | 1093/2048 [1:33:55<1:14:15,  4.67s/it] 53%|█████▎    | 1094/2048 [1:34:00<1:16:28,  4.81s/it] 53%|█████▎    | 1095/2048 [1:34:05<1:17:29,  4.88s/it] 54%|█████▎    | 1096/2048 [1:34:10<1:16:25,  4.82s/it] 54%|█████▎    | 1097/2048 [1:34:14<1:14:56,  4.73s/it] 54%|█████▎    | 1098/2048 [1:34:18<1:11:40,  4.53s/it] 54%|█████▎    | 1099/2048 [1:34:22<1:07:56,  4.30s/it] 54%|█████▎    | 1100/2048 [1:34:26<1:06:35,  4.21s/it] 54%|█████▍    | 1101/2048 [1:34:30<1:04:41,  4.10s/it] 54%|█████▍    | 1102/2048 [1:34:33<1:02:47,  3.98s/it] 54%|█████▍    | 1103/2048 [1:34:37<1:00:37,  3.85s/it] 54%|█████▍    | 1104/2048 [1:34:42<1:07:08,  4.27s/it]                                                       {'loss': 0.1404, 'learning_rate': 9.225659910402291e-05, 'epoch': 8.01}
 54%|█████▍    | 1104/2048 [1:34:42<1:07:08,  4.27s/it] 54%|█████▍    | 1105/2048 [1:34:47<1:11:20,  4.54s/it] 54%|█████▍    | 1106/2048 [1:34:53<1:14:10,  4.72s/it] 54%|█████▍    | 1107/2048 [1:34:58<1:15:50,  4.84s/it] 54%|█████▍    | 1108/2048 [1:35:02<1:15:54,  4.84s/it] 54%|█████▍    | 1109/2048 [1:35:07<1:14:30,  4.76s/it] 54%|█████▍    | 1110/2048 [1:35:11<1:11:30,  4.57s/it] 54%|█████▍    | 1111/2048 [1:35:16<1:11:04,  4.55s/it] 54%|█████▍    | 1112/2048 [1:35:20<1:07:44,  4.34s/it] 54%|█████▍    | 1113/2048 [1:35:23<1:05:36,  4.21s/it] 54%|█████▍    | 1114/2048 [1:35:28<1:05:15,  4.19s/it] 54%|█████▍    | 1115/2048 [1:35:31<1:01:42,  3.97s/it] 54%|█████▍    | 1116/2048 [1:35:35<1:01:56,  3.99s/it] 55%|█████▍    | 1117/2048 [1:35:40<1:07:21,  4.34s/it] 55%|█████▍    | 1118/2048 [1:35:45<1:11:03,  4.58s/it] 55%|█████▍    | 1119/2048 [1:35:51<1:13:32,  4.75s/it] 55%|█████▍    | 1120/2048 [1:35:56<1:14:34,  4.82s/it]                                                       {'loss': 0.1304, 'learning_rate': 8.973595684435179e-05, 'epoch': 8.12}
 55%|█████▍    | 1120/2048 [1:35:56<1:14:34,  4.82s/it] 55%|█████▍    | 1121/2048 [1:36:00<1:14:10,  4.80s/it] 55%|█████▍    | 1122/2048 [1:36:05<1:12:45,  4.71s/it] 55%|█████▍    | 1123/2048 [1:36:09<1:10:02,  4.54s/it] 55%|█████▍    | 1124/2048 [1:36:13<1:06:36,  4.32s/it] 55%|█████▍    | 1125/2048 [1:36:17<1:04:45,  4.21s/it] 55%|█████▍    | 1126/2048 [1:36:20<1:02:43,  4.08s/it] 55%|█████▌    | 1127/2048 [1:36:25<1:03:08,  4.11s/it] 55%|█████▌    | 1128/2048 [1:36:28<58:31,  3.82s/it]   55%|█████▌    | 1129/2048 [1:36:33<1:04:40,  4.22s/it] 55%|█████▌    | 1130/2048 [1:36:38<1:08:56,  4.51s/it] 55%|█████▌    | 1131/2048 [1:36:43<1:11:49,  4.70s/it] 55%|█████▌    | 1132/2048 [1:36:48<1:13:42,  4.83s/it] 55%|█████▌    | 1133/2048 [1:36:53<1:13:35,  4.83s/it] 55%|█████▌    | 1134/2048 [1:36:58<1:12:49,  4.78s/it] 55%|█████▌    | 1135/2048 [1:37:02<1:10:26,  4.63s/it] 55%|█████▌    | 1136/2048 [1:37:06<1:08:04,  4.48s/it]                                                       {'loss': 0.1586, 'learning_rate': 8.722188929326236e-05, 'epoch': 8.24}
 55%|█████▌    | 1136/2048 [1:37:06<1:08:04,  4.48s/it] 56%|█████▌    | 1137/2048 [1:37:10<1:05:33,  4.32s/it] 56%|█████▌    | 1138/2048 [1:37:14<1:03:38,  4.20s/it] 56%|█████▌    | 1139/2048 [1:37:18<1:01:29,  4.06s/it] 56%|█████▌    | 1140/2048 [1:37:21<57:51,  3.82s/it]   56%|█████▌    | 1141/2048 [1:37:25<59:00,  3.90s/it] 56%|█████▌    | 1142/2048 [1:37:30<1:04:40,  4.28s/it] 56%|█████▌    | 1143/2048 [1:37:36<1:08:34,  4.55s/it] 56%|█████▌    | 1144/2048 [1:37:41<1:11:12,  4.73s/it] 56%|█████▌    | 1145/2048 [1:37:46<1:12:29,  4.82s/it] 56%|█████▌    | 1146/2048 [1:37:51<1:12:05,  4.80s/it] 56%|█████▌    | 1147/2048 [1:37:55<1:10:10,  4.67s/it] 56%|█████▌    | 1148/2048 [1:37:59<1:08:31,  4.57s/it] 56%|█████▌    | 1149/2048 [1:38:03<1:06:04,  4.41s/it] 56%|█████▌    | 1150/2048 [1:38:07<1:03:11,  4.22s/it] 56%|█████▌    | 1151/2048 [1:38:11<1:00:36,  4.05s/it] 56%|█████▋    | 1152/2048 [1:38:14<58:45,  3.93s/it]                                                       {'loss': 0.1276, 'learning_rate': 8.471600685527577e-05, 'epoch': 8.36}
 56%|█████▋    | 1152/2048 [1:38:14<58:45,  3.93s/it] 56%|█████▋    | 1153/2048 [1:38:17<54:52,  3.68s/it] 56%|█████▋    | 1154/2048 [1:38:23<1:01:28,  4.13s/it] 56%|█████▋    | 1155/2048 [1:38:28<1:06:03,  4.44s/it] 56%|█████▋    | 1156/2048 [1:38:33<1:09:09,  4.65s/it] 56%|█████▋    | 1157/2048 [1:38:38<1:10:59,  4.78s/it] 57%|█████▋    | 1158/2048 [1:38:43<1:10:49,  4.78s/it] 57%|█████▋    | 1159/2048 [1:38:48<1:10:49,  4.78s/it] 57%|█████▋    | 1160/2048 [1:38:52<1:08:08,  4.60s/it] 57%|█████▋    | 1161/2048 [1:38:56<1:05:15,  4.41s/it] 57%|█████▋    | 1162/2048 [1:39:00<1:04:27,  4.36s/it] 57%|█████▋    | 1163/2048 [1:39:04<1:02:55,  4.27s/it] 57%|█████▋    | 1164/2048 [1:39:08<1:01:09,  4.15s/it] 57%|█████▋    | 1165/2048 [1:39:11<57:27,  3.90s/it]   57%|█████▋    | 1166/2048 [1:39:15<58:13,  3.96s/it] 57%|█████▋    | 1167/2048 [1:39:20<1:03:28,  4.32s/it] 57%|█████▋    | 1168/2048 [1:39:26<1:07:05,  4.57s/it]                                                       {'loss': 0.1038, 'learning_rate': 8.221991469187851e-05, 'epoch': 8.47}
 57%|█████▋    | 1168/2048 [1:39:26<1:07:05,  4.57s/it] 57%|█████▋    | 1169/2048 [1:39:31<1:09:20,  4.73s/it] 57%|█████▋    | 1170/2048 [1:39:36<1:09:56,  4.78s/it] 57%|█████▋    | 1171/2048 [1:39:40<1:09:51,  4.78s/it] 57%|█████▋    | 1172/2048 [1:39:45<1:08:18,  4.68s/it] 57%|█████▋    | 1173/2048 [1:39:49<1:06:22,  4.55s/it] 57%|█████▋    | 1174/2048 [1:39:53<1:03:38,  4.37s/it] 57%|█████▋    | 1175/2048 [1:39:57<1:03:23,  4.36s/it] 57%|█████▋    | 1176/2048 [1:40:01<1:01:53,  4.26s/it] 57%|█████▋    | 1177/2048 [1:40:05<58:38,  4.04s/it]   58%|█████▊    | 1178/2048 [1:40:08<54:43,  3.77s/it] 58%|█████▊    | 1179/2048 [1:40:13<1:00:43,  4.19s/it] 58%|█████▊    | 1180/2048 [1:40:18<1:04:53,  4.49s/it] 58%|█████▊    | 1181/2048 [1:40:24<1:07:40,  4.68s/it] 58%|█████▊    | 1182/2048 [1:40:29<1:09:16,  4.80s/it] 58%|█████▊    | 1183/2048 [1:40:34<1:09:30,  4.82s/it] 58%|█████▊    | 1184/2048 [1:40:38<1:08:41,  4.77s/it]                                                       {'loss': 0.1249, 'learning_rate': 7.973521169332451e-05, 'epoch': 8.59}
 58%|█████▊    | 1184/2048 [1:40:38<1:08:41,  4.77s/it] 58%|█████▊    | 1185/2048 [1:40:43<1:07:26,  4.69s/it] 58%|█████▊    | 1186/2048 [1:40:47<1:04:02,  4.46s/it] 58%|█████▊    | 1187/2048 [1:40:50<1:01:31,  4.29s/it] 58%|█████▊    | 1188/2048 [1:40:54<59:15,  4.13s/it]   58%|█████▊    | 1189/2048 [1:40:58<58:33,  4.09s/it] 58%|█████▊    | 1190/2048 [1:41:02<55:36,  3.89s/it] 58%|█████▊    | 1191/2048 [1:41:06<56:21,  3.95s/it] 58%|█████▊    | 1192/2048 [1:41:11<1:01:31,  4.31s/it] 58%|█████▊    | 1193/2048 [1:41:16<1:05:06,  4.57s/it] 58%|█████▊    | 1194/2048 [1:41:21<1:07:30,  4.74s/it] 58%|█████▊    | 1195/2048 [1:41:26<1:08:46,  4.84s/it] 58%|█████▊    | 1196/2048 [1:41:31<1:08:24,  4.82s/it] 58%|█████▊    | 1197/2048 [1:41:36<1:07:03,  4.73s/it] 58%|█████▊    | 1198/2048 [1:41:40<1:05:42,  4.64s/it] 59%|█████▊    | 1199/2048 [1:41:44<1:02:51,  4.44s/it] 59%|█████▊    | 1200/2048 [1:41:48<1:00:40,  4.29s/it]                                                       {'loss': 0.1368, 'learning_rate': 7.726348945445416e-05, 'epoch': 8.7}
 59%|█████▊    | 1200/2048 [1:41:48<1:00:40,  4.29s/it] 59%|█████▊    | 1201/2048 [1:41:52<59:19,  4.20s/it]   59%|█████▊    | 1202/2048 [1:41:55<55:56,  3.97s/it] 59%|█████▊    | 1203/2048 [1:41:59<52:28,  3.73s/it] 59%|█████▉    | 1204/2048 [1:42:04<58:30,  4.16s/it] 59%|█████▉    | 1205/2048 [1:42:09<1:02:40,  4.46s/it] 59%|█████▉    | 1206/2048 [1:42:14<1:05:26,  4.66s/it] 59%|█████▉    | 1207/2048 [1:42:19<1:07:16,  4.80s/it] 59%|█████▉    | 1208/2048 [1:42:24<1:07:55,  4.85s/it] 59%|█████▉    | 1209/2048 [1:42:29<1:07:40,  4.84s/it] 59%|█████▉    | 1210/2048 [1:42:33<1:05:27,  4.69s/it] 59%|█████▉    | 1211/2048 [1:42:37<1:03:24,  4.55s/it] 59%|█████▉    | 1212/2048 [1:42:41<1:00:25,  4.34s/it] 59%|█████▉    | 1213/2048 [1:42:45<58:39,  4.21s/it]   59%|█████▉    | 1214/2048 [1:42:49<58:30,  4.21s/it] 59%|█████▉    | 1215/2048 [1:42:53<54:51,  3.95s/it] 59%|█████▉    | 1216/2048 [1:42:57<55:22,  3.99s/it]                                                     {'loss': 0.1405, 'learning_rate': 7.480633125518636e-05, 'epoch': 8.82}
 59%|█████▉    | 1216/2048 [1:42:57<55:22,  3.99s/it] 59%|█████▉    | 1217/2048 [1:43:02<1:00:12,  4.35s/it] 59%|█████▉    | 1218/2048 [1:43:07<1:03:32,  4.59s/it] 60%|█████▉    | 1219/2048 [1:43:12<1:05:42,  4.76s/it] 60%|█████▉    | 1220/2048 [1:43:17<1:06:43,  4.84s/it] 60%|█████▉    | 1221/2048 [1:43:22<1:06:30,  4.82s/it] 60%|█████▉    | 1222/2048 [1:43:27<1:04:54,  4.72s/it] 60%|█████▉    | 1223/2048 [1:43:31<1:02:36,  4.55s/it] 60%|█████▉    | 1224/2048 [1:43:35<1:00:19,  4.39s/it] 60%|█████▉    | 1225/2048 [1:43:39<58:49,  4.29s/it]   60%|█████▉    | 1226/2048 [1:43:42<55:43,  4.07s/it] 60%|█████▉    | 1227/2048 [1:43:46<53:36,  3.92s/it] 60%|█████▉    | 1228/2048 [1:43:49<50:22,  3.69s/it] 60%|██████    | 1229/2048 [1:43:54<56:23,  4.13s/it] 60%|██████    | 1230/2048 [1:43:59<1:00:34,  4.44s/it] 60%|██████    | 1231/2048 [1:44:05<1:03:23,  4.66s/it] 60%|██████    | 1232/2048 [1:44:10<1:05:12,  4.79s/it]                                                       {'loss': 0.1218, 'learning_rate': 7.236531104633698e-05, 'epoch': 8.94}
 60%|██████    | 1232/2048 [1:44:10<1:05:12,  4.79s/it] 60%|██████    | 1233/2048 [1:44:15<1:05:34,  4.83s/it] 60%|██████    | 1234/2048 [1:44:19<1:04:42,  4.77s/it] 60%|██████    | 1235/2048 [1:44:24<1:03:43,  4.70s/it] 60%|██████    | 1236/2048 [1:44:28<1:01:35,  4.55s/it] 60%|██████    | 1237/2048 [1:44:32<58:44,  4.35s/it]   60%|██████    | 1238/2048 [1:44:36<57:02,  4.23s/it] 60%|██████    | 1239/2048 [1:44:40<57:09,  4.24s/it] 61%|██████    | 1240/2048 [1:44:44<54:15,  4.03s/it] 61%|██████    | 1241/2048 [1:44:48<53:56,  4.01s/it] 61%|██████    | 1242/2048 [1:44:53<58:32,  4.36s/it] 61%|██████    | 1243/2048 [1:44:58<1:01:41,  4.60s/it] 61%|██████    | 1244/2048 [1:45:03<1:03:45,  4.76s/it] 61%|██████    | 1245/2048 [1:45:08<1:04:07,  4.79s/it] 61%|██████    | 1246/2048 [1:45:13<1:03:57,  4.79s/it] 61%|██████    | 1247/2048 [1:45:17<1:02:23,  4.67s/it] 61%|██████    | 1248/2048 [1:45:21<1:00:20,  4.53s/it]                                                       {'loss': 0.1472, 'learning_rate': 6.994199244141313e-05, 'epoch': 9.05}
 61%|██████    | 1248/2048 [1:45:21<1:00:20,  4.53s/it] 61%|██████    | 1249/2048 [1:45:25<57:59,  4.35s/it]   61%|██████    | 1250/2048 [1:45:29<56:35,  4.25s/it] 61%|██████    | 1251/2048 [1:45:33<55:58,  4.21s/it] 61%|██████    | 1252/2048 [1:45:37<55:33,  4.19s/it] 61%|██████    | 1253/2048 [1:45:41<51:42,  3.90s/it] 61%|██████    | 1254/2048 [1:45:45<53:18,  4.03s/it] 61%|██████▏   | 1255/2048 [1:45:50<57:46,  4.37s/it] 61%|██████▏   | 1256/2048 [1:45:55<1:01:07,  4.63s/it] 61%|██████▏   | 1257/2048 [1:46:01<1:03:03,  4.78s/it] 61%|██████▏   | 1258/2048 [1:46:06<1:04:13,  4.88s/it] 61%|██████▏   | 1259/2048 [1:46:11<1:04:19,  4.89s/it] 62%|██████▏   | 1260/2048 [1:46:15<1:02:59,  4.80s/it] 62%|██████▏   | 1261/2048 [1:46:20<1:01:15,  4.67s/it] 62%|██████▏   | 1262/2048 [1:46:24<58:52,  4.49s/it]   62%|██████▏   | 1263/2048 [1:46:28<57:20,  4.38s/it] 62%|██████▏   | 1264/2048 [1:46:32<56:55,  4.36s/it]                                                     {'loss': 0.1113, 'learning_rate': 6.753792771502882e-05, 'epoch': 9.17}
 62%|██████▏   | 1264/2048 [1:46:32<56:55,  4.36s/it] 62%|██████▏   | 1265/2048 [1:46:35<52:59,  4.06s/it] 62%|██████▏   | 1266/2048 [1:46:39<49:53,  3.83s/it] 62%|██████▏   | 1267/2048 [1:46:44<55:22,  4.25s/it] 62%|██████▏   | 1268/2048 [1:46:49<58:51,  4.53s/it] 62%|██████▏   | 1269/2048 [1:46:54<1:01:10,  4.71s/it] 62%|██████▏   | 1270/2048 [1:46:59<1:02:15,  4.80s/it] 62%|██████▏   | 1271/2048 [1:47:04<1:02:38,  4.84s/it] 62%|██████▏   | 1272/2048 [1:47:09<1:01:18,  4.74s/it] 62%|██████▏   | 1273/2048 [1:47:13<58:49,  4.55s/it]   62%|██████▏   | 1274/2048 [1:47:17<56:56,  4.41s/it] 62%|██████▏   | 1275/2048 [1:47:21<54:32,  4.23s/it] 62%|██████▏   | 1276/2048 [1:47:25<53:10,  4.13s/it] 62%|██████▏   | 1277/2048 [1:47:28<50:39,  3.94s/it] 62%|██████▏   | 1278/2048 [1:47:31<47:28,  3.70s/it] 62%|██████▏   | 1279/2048 [1:47:36<49:53,  3.89s/it] 62%|██████▎   | 1280/2048 [1:47:41<54:43,  4.28s/it]                                                     {'loss': 0.1149, 'learning_rate': 6.515465680858412e-05, 'epoch': 9.28}
 62%|██████▎   | 1280/2048 [1:47:41<54:43,  4.28s/it]
  0%|          | 0/256 [00:00<?, ?it/s][A
  2%|▏         | 4/256 [00:00<00:09, 27.45it/s][A
  3%|▎         | 7/256 [00:00<00:11, 21.15it/s][A
  4%|▍         | 10/256 [00:00<00:12, 19.37it/s][A
  5%|▍         | 12/256 [00:00<00:13, 18.42it/s][A
  5%|▌         | 14/256 [00:00<00:13, 17.87it/s][A
  6%|▋         | 16/256 [00:00<00:13, 18.21it/s][A
  7%|▋         | 18/256 [00:00<00:13, 17.81it/s][A
  8%|▊         | 21/256 [00:01<00:12, 18.55it/s][A
  9%|▉         | 23/256 [00:01<00:12, 18.05it/s][A
 10%|▉         | 25/256 [00:01<00:13, 17.69it/s][A
 11%|█         | 27/256 [00:01<00:12, 17.77it/s][A
 11%|█▏        | 29/256 [00:01<00:12, 17.47it/s][A
 12%|█▏        | 31/256 [00:01<00:13, 17.28it/s][A
 13%|█▎        | 34/256 [00:01<00:11, 18.73it/s][A
 14%|█▍        | 36/256 [00:01<00:12, 18.16it/s][A
 15%|█▍        | 38/256 [00:02<00:12, 17.64it/s][A
 16%|█▌        | 40/256 [00:02<00:12, 17.37it/s][A
 16%|█▋        | 42/256 [00:02<00:12, 17.74it/s][A
 17%|█▋        | 44/256 [00:02<00:12, 17.49it/s][A
 18%|█▊        | 46/256 [00:02<00:12, 17.28it/s][A
 19%|█▉        | 48/256 [00:02<00:11, 17.40it/s][A
 20%|█▉        | 50/256 [00:02<00:12, 17.10it/s][A
 20%|██        | 52/256 [00:02<00:12, 17.00it/s][A
 21%|██        | 54/256 [00:02<00:11, 17.78it/s][A
 22%|██▏       | 56/256 [00:03<00:11, 17.60it/s][A
 23%|██▎       | 58/256 [00:03<00:11, 17.52it/s][A
 23%|██▎       | 60/256 [00:03<00:11, 17.29it/s][A
 24%|██▍       | 62/256 [00:03<00:11, 16.96it/s][A
 25%|██▌       | 64/256 [00:03<00:11, 17.25it/s][A
 26%|██▌       | 66/256 [00:03<00:11, 17.10it/s][A
 27%|██▋       | 68/256 [00:03<00:11, 17.00it/s][A
 27%|██▋       | 70/256 [00:03<00:10, 17.14it/s][A
 28%|██▊       | 72/256 [00:04<00:10, 17.02it/s][A
 29%|██▉       | 74/256 [00:04<00:10, 16.99it/s][A
 30%|██▉       | 76/256 [00:04<00:10, 17.00it/s][A
 30%|███       | 78/256 [00:04<00:10, 16.98it/s][A
 31%|███▏      | 80/256 [00:04<00:10, 16.93it/s][A
 32%|███▏      | 82/256 [00:04<00:10, 16.98it/s][A
 33%|███▎      | 84/256 [00:04<00:10, 17.15it/s][A
 34%|███▎      | 86/256 [00:04<00:09, 17.06it/s][A
 34%|███▍      | 88/256 [00:04<00:09, 17.26it/s][A
 35%|███▌      | 90/256 [00:05<00:09, 17.01it/s][A
 36%|███▌      | 92/256 [00:05<00:09, 16.92it/s][A
 37%|███▋      | 94/256 [00:05<00:09, 17.00it/s][A
 38%|███▊      | 96/256 [00:05<00:09, 16.90it/s][A
 39%|███▊      | 99/256 [00:05<00:08, 17.75it/s][A
 39%|███▉      | 101/256 [00:05<00:08, 17.39it/s][A
 40%|████      | 103/256 [00:05<00:08, 17.50it/s][A
 41%|████▏     | 106/256 [00:06<00:08, 18.35it/s][A
 42%|████▏     | 108/256 [00:06<00:08, 17.97it/s][A
 43%|████▎     | 110/256 [00:06<00:08, 17.74it/s][A
 44%|████▍     | 112/256 [00:06<00:08, 17.46it/s][A
 45%|████▍     | 114/256 [00:06<00:08, 17.59it/s][A
 46%|████▌     | 117/256 [00:06<00:07, 18.03it/s][A
 46%|████▋     | 119/256 [00:06<00:07, 17.83it/s][A
 47%|████▋     | 121/256 [00:06<00:07, 17.46it/s][A
 48%|████▊     | 123/256 [00:06<00:07, 17.32it/s][A
 49%|████▉     | 125/256 [00:07<00:07, 17.15it/s][A
 50%|█████     | 128/256 [00:07<00:06, 18.66it/s][A
 51%|█████     | 130/256 [00:07<00:06, 18.23it/s][A
 52%|█████▏    | 132/256 [00:07<00:07, 17.68it/s][A
 52%|█████▏    | 134/256 [00:07<00:06, 17.52it/s][A
 53%|█████▎    | 136/256 [00:07<00:06, 17.38it/s][A
 54%|█████▍    | 138/256 [00:07<00:06, 17.19it/s][A
 55%|█████▍    | 140/256 [00:07<00:06, 17.06it/s][A
 56%|█████▌    | 143/256 [00:08<00:06, 18.20it/s][A
 57%|█████▋    | 145/256 [00:08<00:06, 17.64it/s][A
 57%|█████▋    | 147/256 [00:08<00:06, 17.43it/s][A
 58%|█████▊    | 149/256 [00:08<00:06, 17.24it/s][A
 59%|█████▉    | 151/256 [00:08<00:06, 17.11it/s][A
 60%|██████    | 154/256 [00:08<00:05, 17.74it/s][A
 61%|██████    | 156/256 [00:08<00:05, 17.60it/s][A
 62%|██████▏   | 158/256 [00:08<00:05, 17.24it/s][A
 63%|██████▎   | 161/256 [00:09<00:05, 17.98it/s][A
 64%|██████▎   | 163/256 [00:09<00:05, 17.54it/s][A
 64%|██████▍   | 165/256 [00:09<00:05, 17.44it/s][A
 65%|██████▌   | 167/256 [00:09<00:05, 17.07it/s][A
 66%|██████▌   | 169/256 [00:09<00:05, 17.32it/s][A
 67%|██████▋   | 171/256 [00:09<00:04, 17.08it/s][A
 68%|██████▊   | 173/256 [00:09<00:04, 16.94it/s][A
 68%|██████▊   | 175/256 [00:09<00:04, 17.00it/s][A
 69%|██████▉   | 177/256 [00:10<00:04, 16.83it/s][A
 70%|██████▉   | 179/256 [00:10<00:04, 16.92it/s][A
 71%|███████   | 182/256 [00:10<00:03, 19.23it/s][A
 72%|███████▏  | 184/256 [00:10<00:03, 18.62it/s][A
 73%|███████▎  | 186/256 [00:10<00:03, 17.95it/s][A
 74%|███████▍  | 189/256 [00:10<00:03, 18.43it/s][A
 75%|███████▍  | 191/256 [00:10<00:03, 17.85it/s][A
 75%|███████▌  | 193/256 [00:10<00:03, 17.68it/s][A
 76%|███████▌  | 195/256 [00:11<00:03, 17.29it/s][A
 77%|███████▋  | 197/256 [00:11<00:03, 17.59it/s][A
 78%|███████▊  | 199/256 [00:11<00:03, 18.19it/s][A
 79%|███████▊  | 201/256 [00:11<00:03, 17.88it/s][A
 79%|███████▉  | 203/256 [00:11<00:03, 17.42it/s][A
 80%|████████  | 205/256 [00:11<00:02, 17.35it/s][A
 81%|████████  | 207/256 [00:11<00:02, 17.05it/s][A
 82%|████████▏ | 209/256 [00:11<00:02, 16.95it/s][A
 82%|████████▏ | 211/256 [00:11<00:02, 16.92it/s][A
 83%|████████▎ | 213/256 [00:12<00:02, 16.88it/s][A
 84%|████████▍ | 215/256 [00:12<00:02, 16.84it/s][A
 85%|████████▍ | 217/256 [00:12<00:02, 16.84it/s][A
 86%|████████▌ | 219/256 [00:12<00:02, 16.93it/s][A
 86%|████████▋ | 221/256 [00:12<00:02, 16.78it/s][A
 87%|████████▋ | 223/256 [00:12<00:01, 16.79it/s][A
 88%|████████▊ | 225/256 [00:12<00:01, 17.21it/s][A
 89%|████████▊ | 227/256 [00:12<00:01, 16.95it/s][A
 89%|████████▉ | 229/256 [00:13<00:01, 16.86it/s][A
 90%|█████████ | 231/256 [00:13<00:01, 17.09it/s][A
 91%|█████████ | 233/256 [00:13<00:01, 17.06it/s][A
 92%|█████████▏| 235/256 [00:13<00:01, 17.03it/s][A
 93%|█████████▎| 238/256 [00:13<00:01, 17.95it/s][A
 94%|█████████▍| 240/256 [00:13<00:00, 17.74it/s][A
 95%|█████████▍| 242/256 [00:13<00:00, 17.46it/s][A
 95%|█████████▌| 244/256 [00:13<00:00, 17.78it/s][A
 96%|█████████▋| 247/256 [00:14<00:00, 18.28it/s][A
 97%|█████████▋| 249/256 [00:14<00:00, 17.96it/s][A
 98%|█████████▊| 252/256 [00:14<00:00, 19.04it/s][A
 99%|█████████▉| 254/256 [00:14<00:00, 18.55it/s][A
100%|██████████| 256/256 [00:14<00:00, 16.93it/s][A                                                     
                                                 [A{'eval_loss': 2.100444793701172, 'eval_runtime': 14.7483, 'eval_samples_per_second': 69.432, 'eval_steps_per_second': 17.358, 'epoch': 9.28}
 62%|██████▎   | 1280/2048 [1:47:56<54:43,  4.28s/it]
100%|██████████| 256/256 [00:14<00:00, 16.93it/s][A
                                                 [A  0%|          | 0/383 [00:00<?, ?it/s]  0%|          | 0/383 [00:00<?, ?it/s]  0%|          | 0/383 [00:00<?, ?it/s]
  0%|          | 0/383 [00:00<?, ?it/s][A  0%|          | 1/383 [00:00<00:40,  9.37it/s]  1%|          | 2/383 [00:00<00:25, 14.88it/s]  1%|          | 2/383 [00:00<00:26, 14.19it/s]
  0%|          | 1/383 [00:00<00:57,  6.68it/s][A  1%|          | 3/383 [00:00<00:24, 15.32it/s]  1%|▏         | 5/383 [00:00<00:19, 19.45it/s]  1%|▏         | 5/383 [00:00<00:20, 18.55it/s]
  1%|          | 4/383 [00:00<00:24, 15.62it/s][A  1%|▏         | 5/383 [00:00<00:21, 17.33it/s]  2%|▏         | 7/383 [00:00<00:20, 17.94it/s]  2%|▏         | 7/383 [00:00<00:21, 17.27it/s]
  2%|▏         | 7/383 [00:00<00:20, 18.24it/s][A  2%|▏         | 7/383 [00:00<00:22, 16.75it/s]  2%|▏         | 9/383 [00:00<00:23, 16.10it/s]  2%|▏         | 9/383 [00:00<00:24, 15.50it/s]  2%|▏         | 9/383 [00:00<00:23, 16.00it/s]
  2%|▏         | 9/383 [00:00<00:23, 16.00it/s][A  3%|▎         | 11/383 [00:00<00:23, 15.54it/s]  3%|▎         | 11/383 [00:00<00:25, 14.68it/s]  3%|▎         | 11/383 [00:00<00:24, 15.03it/s]
  3%|▎         | 11/383 [00:00<00:24, 15.34it/s][A  3%|▎         | 13/383 [00:00<00:24, 15.07it/s]  3%|▎         | 13/383 [00:00<00:24, 15.33it/s]  3%|▎         | 13/383 [00:00<00:25, 14.47it/s]
  3%|▎         | 13/383 [00:00<00:24, 14.80it/s][A  4%|▍         | 15/383 [00:00<00:22, 16.13it/s]  4%|▍         | 15/383 [00:00<00:22, 16.25it/s]  4%|▍         | 15/383 [00:00<00:23, 15.49it/s]
  4%|▍         | 15/383 [00:00<00:23, 15.85it/s][A  4%|▍         | 17/383 [00:01<00:21, 16.78it/s]  4%|▍         | 17/383 [00:01<00:21, 16.99it/s]  4%|▍         | 17/383 [00:01<00:22, 16.28it/s]
  4%|▍         | 17/383 [00:01<00:21, 16.76it/s][A  5%|▍         | 19/383 [00:01<00:20, 17.36it/s]  5%|▍         | 19/383 [00:01<00:20, 17.53it/s]
  5%|▍         | 19/383 [00:01<00:20, 17.39it/s][A  5%|▍         | 19/383 [00:01<00:21, 16.55it/s]  5%|▌         | 21/383 [00:01<00:20, 17.36it/s]  5%|▌         | 21/383 [00:01<00:20, 17.69it/s]
  5%|▌         | 21/383 [00:01<00:20, 17.69it/s][A  5%|▌         | 21/383 [00:01<00:21, 16.84it/s]  6%|▌         | 23/383 [00:01<00:20, 17.23it/s]  6%|▌         | 23/383 [00:01<00:20, 17.36it/s]
  6%|▌         | 23/383 [00:01<00:20, 17.59it/s][A  6%|▌         | 23/383 [00:01<00:21, 17.03it/s]  7%|▋         | 25/383 [00:01<00:21, 16.74it/s]  7%|▋         | 25/383 [00:01<00:21, 17.01it/s]
  7%|▋         | 25/383 [00:01<00:20, 17.38it/s][A  7%|▋         | 25/383 [00:01<00:21, 16.75it/s]
  7%|▋         | 27/383 [00:01<00:20, 17.10it/s][A  7%|▋         | 27/383 [00:01<00:24, 14.62it/s]  7%|▋         | 27/383 [00:01<00:24, 14.69it/s]  7%|▋         | 27/383 [00:01<00:24, 14.44it/s]
  8%|▊         | 29/383 [00:01<00:25, 13.68it/s][A  8%|▊         | 29/383 [00:01<00:27, 12.83it/s]  8%|▊         | 29/383 [00:01<00:27, 12.80it/s]  8%|▊         | 29/383 [00:01<00:28, 12.46it/s]
  8%|▊         | 31/383 [00:02<00:25, 13.59it/s][A  8%|▊         | 31/383 [00:02<00:27, 13.00it/s]  8%|▊         | 31/383 [00:02<00:26, 13.07it/s]  8%|▊         | 31/383 [00:02<00:26, 13.19it/s]
  9%|▊         | 33/383 [00:02<00:25, 13.79it/s][A  9%|▊         | 33/383 [00:02<00:25, 13.69it/s]  9%|▊         | 33/383 [00:02<00:25, 13.78it/s]  9%|▊         | 33/383 [00:02<00:24, 14.07it/s]
  9%|▉         | 35/383 [00:02<00:23, 14.51it/s][A  9%|▉         | 35/383 [00:02<00:24, 14.12it/s]  9%|▉         | 35/383 [00:02<00:24, 14.36it/s]  9%|▉         | 35/383 [00:02<00:23, 14.66it/s]
 10%|▉         | 37/383 [00:02<00:23, 14.83it/s][A 10%|▉         | 37/383 [00:02<00:23, 14.92it/s] 10%|▉         | 37/383 [00:02<00:23, 14.96it/s] 10%|▉         | 37/383 [00:02<00:22, 15.25it/s]
 10%|█         | 39/383 [00:02<00:22, 15.35it/s][A 10%|█         | 39/383 [00:02<00:22, 15.48it/s] 10%|█         | 39/383 [00:02<00:22, 15.55it/s] 10%|█         | 39/383 [00:02<00:21, 15.74it/s]
 11%|█         | 41/383 [00:02<00:21, 15.98it/s][A 11%|█         | 41/383 [00:02<00:21, 15.91it/s] 11%|█         | 41/383 [00:02<00:21, 16.04it/s] 11%|█         | 41/383 [00:02<00:21, 15.89it/s] 11%|█         | 43/383 [00:02<00:20, 16.91it/s] 11%|█▏        | 44/383 [00:02<00:18, 18.33it/s]
 11%|█▏        | 44/383 [00:02<00:18, 17.86it/s][A 11%|█         | 43/383 [00:02<00:20, 16.53it/s] 12%|█▏        | 46/383 [00:02<00:17, 19.54it/s] 12%|█▏        | 47/383 [00:02<00:16, 20.61it/s]
 12%|█▏        | 47/383 [00:02<00:16, 20.04it/s][A 12%|█▏        | 46/383 [00:02<00:17, 19.19it/s] 13%|█▎        | 49/383 [00:02<00:15, 21.45it/s]
 13%|█▎        | 50/383 [00:03<00:15, 21.59it/s][A 13%|█▎        | 49/383 [00:03<00:15, 21.23it/s] 13%|█▎        | 50/383 [00:03<00:16, 20.79it/s]
 14%|█▍        | 53/383 [00:03<00:17, 18.41it/s][A 14%|█▍        | 53/383 [00:03<00:17, 18.45it/s] 14%|█▎        | 52/383 [00:03<00:18, 17.43it/s] 14%|█▎        | 52/383 [00:03<00:18, 18.15it/s] 14%|█▍        | 54/383 [00:03<00:18, 17.76it/s]
 14%|█▍        | 55/383 [00:03<00:17, 18.32it/s][A 14%|█▍        | 55/383 [00:03<00:17, 18.37it/s] 14%|█▍        | 54/383 [00:03<00:18, 17.90it/s] 15%|█▍        | 56/383 [00:03<00:18, 17.98it/s] 15%|█▍        | 57/383 [00:03<00:17, 18.23it/s]
 15%|█▍        | 57/383 [00:03<00:18, 18.02it/s][A 15%|█▍        | 56/383 [00:03<00:18, 17.99it/s] 15%|█▌        | 58/383 [00:03<00:18, 17.39it/s]
 15%|█▌        | 59/383 [00:03<00:18, 17.16it/s][A 15%|█▌        | 59/383 [00:03<00:18, 17.18it/s] 15%|█▌        | 58/383 [00:03<00:18, 17.23it/s] 16%|█▌        | 60/383 [00:03<00:19, 16.82it/s] 16%|█▌        | 60/383 [00:03<00:19, 16.86it/s] 16%|█▌        | 61/383 [00:03<00:19, 16.77it/s]
 16%|█▌        | 61/383 [00:03<00:19, 16.19it/s][A 16%|█▌        | 62/383 [00:03<00:19, 16.09it/s] 16%|█▋        | 63/383 [00:03<00:19, 16.40it/s] 16%|█▌        | 62/383 [00:03<00:19, 16.25it/s]
 16%|█▋        | 63/383 [00:03<00:19, 16.15it/s][A 17%|█▋        | 64/383 [00:03<00:19, 15.98it/s] 17%|█▋        | 64/383 [00:03<00:19, 16.27it/s]
 17%|█▋        | 65/383 [00:03<00:19, 16.13it/s][A 17%|█▋        | 65/383 [00:03<00:19, 15.91it/s] 17%|█▋        | 66/383 [00:04<00:19, 16.26it/s] 17%|█▋        | 66/383 [00:04<00:20, 15.73it/s] 17%|█▋        | 67/383 [00:04<00:20, 15.68it/s]
 17%|█▋        | 67/383 [00:04<00:20, 15.74it/s][A 18%|█▊        | 68/383 [00:04<00:20, 15.66it/s] 18%|█▊        | 68/383 [00:04<00:20, 15.03it/s] 18%|█▊        | 69/383 [00:04<00:20, 14.96it/s]
 18%|█▊        | 69/383 [00:04<00:21, 14.94it/s][A 18%|█▊        | 70/383 [00:04<00:20, 15.06it/s] 19%|█▊        | 71/383 [00:04<00:20, 15.11it/s] 18%|█▊        | 70/383 [00:04<00:21, 14.63it/s]
 19%|█▊        | 71/383 [00:04<00:20, 15.10it/s][A 19%|█▉        | 72/383 [00:04<00:19, 15.73it/s]
 19%|█▉        | 73/383 [00:04<00:19, 16.06it/s][A 19%|█▉        | 72/383 [00:04<00:20, 15.53it/s] 19%|█▉        | 73/383 [00:04<00:19, 15.71it/s] 19%|█▉        | 74/383 [00:04<00:19, 16.23it/s]
 20%|█▉        | 75/383 [00:04<00:19, 15.93it/s][A 20%|█▉        | 75/383 [00:04<00:19, 15.70it/s] 19%|█▉        | 74/383 [00:04<00:20, 15.29it/s] 20%|█▉        | 76/383 [00:04<00:18, 16.18it/s]
 20%|██        | 77/383 [00:04<00:19, 15.95it/s][A 20%|██        | 77/383 [00:04<00:19, 15.82it/s] 20%|█▉        | 76/383 [00:04<00:19, 15.65it/s] 20%|██        | 78/383 [00:04<00:18, 16.11it/s]
 21%|██        | 79/383 [00:04<00:19, 15.94it/s][A 20%|██        | 78/383 [00:04<00:19, 15.82it/s] 21%|██        | 79/383 [00:04<00:19, 15.85it/s] 21%|██        | 80/383 [00:04<00:18, 16.06it/s] 21%|██        | 80/383 [00:04<00:19, 15.76it/s]
 21%|██        | 81/383 [00:04<00:19, 15.74it/s][A 21%|██        | 81/383 [00:05<00:19, 15.60it/s] 21%|██▏       | 82/383 [00:05<00:18, 15.94it/s] 21%|██▏       | 82/383 [00:05<00:18, 15.96it/s]
 22%|██▏       | 83/383 [00:05<00:18, 15.91it/s][A 22%|██▏       | 83/383 [00:05<00:19, 15.75it/s] 22%|██▏       | 84/383 [00:05<00:18, 15.98it/s] 22%|██▏       | 84/383 [00:05<00:18, 16.11it/s] 22%|██▏       | 85/383 [00:05<00:18, 16.06it/s]
 22%|██▏       | 85/383 [00:05<00:18, 15.71it/s][A 22%|██▏       | 86/383 [00:05<00:18, 16.23it/s] 22%|██▏       | 86/383 [00:05<00:18, 16.08it/s]
 23%|██▎       | 87/383 [00:05<00:20, 14.24it/s][A 23%|██▎       | 87/383 [00:05<00:20, 14.18it/s] 23%|██▎       | 88/383 [00:05<00:22, 13.01it/s] 23%|██▎       | 88/383 [00:05<00:23, 12.53it/s]
 23%|██▎       | 89/383 [00:05<00:24, 11.83it/s][A 23%|██▎       | 89/383 [00:05<00:32,  9.03it/s] 23%|██▎       | 90/383 [00:05<00:34,  8.61it/s]
 24%|██▍       | 91/383 [00:06<00:39,  7.40it/s][A 23%|██▎       | 90/383 [00:06<00:42,  6.90it/s] 24%|██▍       | 91/383 [00:06<00:48,  6.02it/s] 24%|██▍       | 92/383 [00:06<00:48,  5.99it/s] 24%|██▍       | 92/383 [00:06<00:47,  6.09it/s] 24%|██▍       | 92/383 [00:06<00:52,  5.56it/s]
 24%|██▍       | 93/383 [00:06<00:50,  5.71it/s][A 25%|██▍       | 94/383 [00:06<00:39,  7.30it/s] 24%|██▍       | 93/383 [00:06<00:55,  5.26it/s]
 25%|██▌       | 96/383 [00:06<00:36,  7.92it/s][A 25%|██▌       | 96/383 [00:06<00:31,  9.18it/s] 24%|██▍       | 93/383 [00:06<00:53,  5.43it/s]
 26%|██▌       | 98/383 [00:06<00:30,  9.45it/s][A 25%|██▍       | 95/383 [00:06<00:41,  6.94it/s] 26%|██▌       | 98/383 [00:06<00:25, 11.06it/s] 25%|██▍       | 95/383 [00:06<00:40,  7.17it/s] 25%|██▌       | 97/383 [00:07<00:32,  8.79it/s]
 26%|██▌       | 100/383 [00:07<00:26, 10.88it/s][A 26%|██▌       | 100/383 [00:07<00:22, 12.40it/s] 25%|██▌       | 97/383 [00:07<00:31,  9.01it/s] 26%|██▌       | 99/383 [00:07<00:27, 10.49it/s]
 27%|██▋       | 102/383 [00:07<00:22, 12.23it/s][A 27%|██▋       | 102/383 [00:07<00:20, 13.77it/s] 26%|██▌       | 99/383 [00:07<00:26, 10.69it/s] 26%|██▋       | 101/383 [00:07<00:23, 11.80it/s]
 27%|██▋       | 104/383 [00:07<00:20, 13.36it/s][A 27%|██▋       | 104/383 [00:07<00:18, 15.02it/s] 26%|██▋       | 101/383 [00:07<00:23, 12.11it/s]
 28%|██▊       | 106/383 [00:07<00:18, 14.74it/s][A 27%|██▋       | 103/383 [00:07<00:21, 13.09it/s] 28%|██▊       | 106/383 [00:07<00:17, 16.14it/s] 27%|██▋       | 103/383 [00:07<00:20, 13.49it/s]
 28%|██▊       | 108/383 [00:07<00:17, 15.91it/s][A 27%|██▋       | 105/383 [00:07<00:19, 14.34it/s] 28%|██▊       | 108/383 [00:07<00:16, 16.86it/s] 27%|██▋       | 105/383 [00:07<00:18, 14.81it/s]
 29%|██▊       | 110/383 [00:07<00:16, 16.79it/s][A 29%|██▊       | 110/383 [00:07<00:15, 17.55it/s] 28%|██▊       | 107/383 [00:07<00:17, 15.47it/s] 28%|██▊       | 107/383 [00:07<00:17, 15.99it/s]
 29%|██▉       | 112/383 [00:07<00:15, 17.41it/s][A 29%|██▉       | 112/383 [00:07<00:15, 18.05it/s] 28%|██▊       | 109/383 [00:07<00:16, 16.41it/s] 28%|██▊       | 109/383 [00:07<00:16, 16.88it/s]
 30%|██▉       | 114/383 [00:07<00:15, 17.92it/s][A 30%|██▉       | 114/383 [00:07<00:14, 18.44it/s] 29%|██▉       | 111/383 [00:07<00:15, 17.16it/s] 29%|██▉       | 111/383 [00:07<00:15, 17.51it/s]
 30%|███       | 116/383 [00:07<00:14, 17.93it/s][A 30%|██▉       | 113/383 [00:07<00:15, 17.85it/s] 30%|███       | 116/383 [00:07<00:15, 17.75it/s] 30%|██▉       | 113/383 [00:07<00:15, 17.89it/s]
 31%|███       | 118/383 [00:08<00:15, 17.51it/s][A 30%|███       | 115/383 [00:08<00:15, 17.80it/s] 30%|███       | 115/383 [00:08<00:15, 17.85it/s] 31%|███       | 118/383 [00:08<00:15, 17.21it/s]
 31%|███▏      | 120/383 [00:08<00:15, 17.06it/s][A 31%|███       | 117/383 [00:08<00:15, 17.01it/s] 31%|███▏      | 120/383 [00:08<00:15, 17.17it/s] 31%|███       | 117/383 [00:08<00:15, 17.39it/s]
 32%|███▏      | 122/383 [00:08<00:15, 16.87it/s][A 31%|███       | 119/383 [00:08<00:15, 16.86it/s] 32%|███▏      | 122/383 [00:08<00:15, 17.39it/s] 31%|███       | 119/383 [00:08<00:15, 16.93it/s]
 32%|███▏      | 124/383 [00:08<00:14, 17.45it/s][A 32%|███▏      | 124/383 [00:08<00:14, 18.08it/s] 32%|███▏      | 121/383 [00:08<00:15, 16.63it/s] 32%|███▏      | 121/383 [00:08<00:15, 16.57it/s]
 33%|███▎      | 126/383 [00:08<00:14, 18.03it/s][A 33%|███▎      | 126/383 [00:08<00:14, 18.35it/s] 32%|███▏      | 123/383 [00:08<00:15, 16.93it/s] 32%|███▏      | 123/383 [00:08<00:14, 17.39it/s]
 33%|███▎      | 128/383 [00:08<00:13, 18.55it/s][A 33%|███▎      | 128/383 [00:08<00:13, 18.70it/s] 33%|███▎      | 125/383 [00:08<00:14, 17.41it/s] 33%|███▎      | 125/383 [00:08<00:14, 17.87it/s]
 34%|███▍      | 130/383 [00:08<00:14, 17.62it/s][A 33%|███▎      | 127/383 [00:08<00:14, 17.83it/s] 34%|███▍      | 130/383 [00:08<00:14, 17.08it/s] 33%|███▎      | 127/383 [00:08<00:13, 18.30it/s]
 34%|███▍      | 132/383 [00:08<00:14, 16.76it/s][A 34%|███▎      | 129/383 [00:08<00:13, 18.21it/s] 34%|███▎      | 129/383 [00:08<00:14, 17.32it/s] 34%|███▍      | 132/383 [00:08<00:15, 16.54it/s]
 35%|███▍      | 134/383 [00:08<00:15, 16.42it/s][A 34%|███▍      | 131/383 [00:08<00:14, 17.61it/s] 34%|███▍      | 131/383 [00:08<00:14, 16.89it/s] 35%|███▍      | 134/383 [00:09<00:15, 16.48it/s]
 36%|███▌      | 136/383 [00:09<00:15, 16.42it/s][A 35%|███▍      | 133/383 [00:09<00:14, 16.88it/s] 35%|███▍      | 133/383 [00:09<00:15, 16.57it/s] 36%|███▌      | 136/383 [00:09<00:15, 16.31it/s]
 36%|███▌      | 138/383 [00:09<00:14, 16.44it/s][A 35%|███▌      | 135/383 [00:09<00:14, 16.66it/s] 35%|███▌      | 135/383 [00:09<00:15, 16.31it/s] 36%|███▌      | 138/383 [00:09<00:14, 16.42it/s]
 37%|███▋      | 140/383 [00:09<00:14, 16.33it/s][A 36%|███▌      | 137/383 [00:09<00:14, 16.47it/s] 37%|███▋      | 140/383 [00:09<00:14, 16.42it/s] 36%|███▌      | 137/383 [00:09<00:15, 16.03it/s]
 37%|███▋      | 142/383 [00:09<00:14, 16.26it/s][A 36%|███▋      | 139/383 [00:09<00:14, 16.56it/s] 37%|███▋      | 142/383 [00:09<00:14, 16.37it/s] 36%|███▋      | 139/383 [00:09<00:15, 15.99it/s]
 38%|███▊      | 144/383 [00:09<00:14, 16.21it/s][A 38%|███▊      | 144/383 [00:09<00:14, 16.42it/s] 37%|███▋      | 141/383 [00:09<00:14, 16.17it/s] 37%|███▋      | 141/383 [00:09<00:14, 16.17it/s]
 38%|███▊      | 146/383 [00:09<00:14, 16.12it/s][A 38%|███▊      | 146/383 [00:09<00:14, 16.38it/s] 37%|███▋      | 143/383 [00:09<00:14, 16.21it/s] 37%|███▋      | 143/383 [00:09<00:14, 16.23it/s]
 39%|███▊      | 148/383 [00:09<00:14, 16.06it/s][A 38%|███▊      | 145/383 [00:09<00:14, 16.17it/s] 38%|███▊      | 145/383 [00:09<00:14, 16.08it/s] 39%|███▊      | 148/383 [00:09<00:15, 14.91it/s] 38%|███▊      | 147/383 [00:09<00:14, 16.19it/s] 38%|███▊      | 147/383 [00:09<00:14, 16.19it/s]
 39%|███▉      | 150/383 [00:10<00:17, 13.13it/s][A 39%|███▉      | 150/383 [00:10<00:18, 12.58it/s] 39%|███▉      | 149/383 [00:10<00:17, 13.40it/s] 39%|███▉      | 149/383 [00:10<00:17, 13.36it/s]
 40%|███▉      | 152/383 [00:10<00:19, 12.01it/s][A 40%|███▉      | 152/383 [00:10<00:19, 11.75it/s] 39%|███▉      | 151/383 [00:10<00:19, 12.02it/s] 39%|███▉      | 151/383 [00:10<00:19, 11.84it/s] 40%|███▉      | 153/383 [00:10<00:19, 11.50it/s]
 40%|████      | 154/383 [00:10<00:25,  9.16it/s][A 40%|███▉      | 153/383 [00:10<00:21, 10.89it/s] 40%|████      | 154/383 [00:10<00:26,  8.59it/s] 40%|████      | 155/383 [00:11<00:30,  7.41it/s] 40%|████      | 155/383 [00:11<00:32,  7.11it/s]
 41%|████      | 156/383 [00:11<00:35,  6.34it/s][A 41%|████      | 156/383 [00:11<00:36,  6.24it/s] 41%|████      | 156/383 [00:11<00:36,  6.30it/s] 41%|████      | 156/383 [00:11<00:36,  6.18it/s]
 41%|████      | 157/383 [00:11<00:41,  5.45it/s][A 41%|████      | 157/383 [00:11<00:39,  5.71it/s] 41%|████      | 157/383 [00:11<00:39,  5.73it/s] 41%|████      | 157/383 [00:11<00:41,  5.44it/s] 41%|████▏     | 158/383 [00:11<00:43,  5.19it/s]
 41%|████▏     | 158/383 [00:11<00:46,  4.80it/s][A 41%|████▏     | 158/383 [00:11<00:44,  5.11it/s] 42%|████▏     | 159/383 [00:11<00:42,  5.28it/s] 41%|████▏     | 158/383 [00:11<00:45,  4.98it/s]
 42%|████▏     | 159/383 [00:12<00:49,  4.54it/s][A 42%|████▏     | 159/383 [00:12<00:42,  5.24it/s] 42%|████▏     | 160/383 [00:12<00:45,  4.89it/s] 42%|████▏     | 159/383 [00:12<00:48,  4.61it/s] 42%|████▏     | 160/383 [00:12<00:45,  4.95it/s]
 42%|████▏     | 160/383 [00:12<00:51,  4.32it/s][A 42%|████▏     | 161/383 [00:12<00:44,  5.00it/s] 42%|████▏     | 160/383 [00:12<00:47,  4.72it/s]
 42%|████▏     | 161/383 [00:12<00:48,  4.60it/s][A 42%|████▏     | 161/383 [00:12<00:46,  4.79it/s] 42%|████▏     | 162/383 [00:12<00:42,  5.21it/s] 42%|████▏     | 161/383 [00:12<00:49,  4.51it/s]
 42%|████▏     | 162/383 [00:12<00:49,  4.49it/s][A 42%|████▏     | 162/383 [00:12<00:47,  4.62it/s] 43%|████▎     | 163/383 [00:12<00:46,  4.73it/s] 42%|████▏     | 162/383 [00:12<00:46,  4.71it/s]
 43%|████▎     | 163/383 [00:12<00:46,  4.70it/s][A 43%|████▎     | 163/383 [00:12<00:47,  4.67it/s] 43%|████▎     | 163/383 [00:13<00:46,  4.78it/s] 43%|████▎     | 164/383 [00:13<00:50,  4.38it/s]
 43%|████▎     | 164/383 [00:13<00:50,  4.38it/s][A 43%|████▎     | 164/383 [00:13<00:49,  4.41it/s] 43%|████▎     | 165/383 [00:13<00:46,  4.68it/s] 43%|████▎     | 164/383 [00:13<00:48,  4.50it/s] 44%|████▍     | 168/383 [00:13<00:25,  8.53it/s]
 43%|████▎     | 165/383 [00:13<00:48,  4.52it/s][A 43%|████▎     | 165/383 [00:13<00:47,  4.57it/s] 45%|████▍     | 171/383 [00:13<00:17, 11.83it/s]
 44%|████▍     | 168/383 [00:13<00:25,  8.30it/s][A 44%|████▍     | 168/383 [00:13<00:25,  8.39it/s] 43%|████▎     | 165/383 [00:13<00:51,  4.22it/s] 45%|████▌     | 173/383 [00:13<00:15, 13.42it/s]
 45%|████▍     | 171/383 [00:13<00:17, 11.83it/s][A 45%|████▍     | 171/383 [00:13<00:17, 11.94it/s] 44%|████▍     | 168/383 [00:13<00:27,  7.88it/s] 46%|████▌     | 175/383 [00:13<00:15, 13.40it/s]
 45%|████▌     | 174/383 [00:13<00:14, 14.53it/s][A 45%|████▌     | 174/383 [00:13<00:14, 14.69it/s] 45%|████▍     | 171/383 [00:13<00:18, 11.32it/s]
 46%|████▌     | 176/383 [00:13<00:14, 14.23it/s][A 46%|████▌     | 177/383 [00:13<00:15, 13.19it/s] 46%|████▌     | 176/383 [00:13<00:14, 14.34it/s] 45%|████▌     | 174/383 [00:13<00:15, 13.63it/s] 47%|████▋     | 179/383 [00:13<00:14, 14.54it/s]
 46%|████▋     | 178/383 [00:13<00:14, 14.56it/s][A 46%|████▋     | 178/383 [00:14<00:13, 14.68it/s] 47%|████▋     | 181/383 [00:14<00:13, 15.35it/s] 46%|████▌     | 176/383 [00:14<00:15, 13.53it/s]
 47%|████▋     | 180/383 [00:14<00:13, 15.57it/s][A 47%|████▋     | 180/383 [00:14<00:12, 15.71it/s] 48%|████▊     | 183/383 [00:14<00:12, 16.21it/s]
 48%|████▊     | 182/383 [00:14<00:12, 16.06it/s][A 46%|████▋     | 178/383 [00:14<00:14, 13.99it/s] 48%|████▊     | 182/383 [00:14<00:12, 16.32it/s] 48%|████▊     | 185/383 [00:14<00:12, 15.98it/s]
 48%|████▊     | 184/383 [00:14<00:11, 16.70it/s][A 47%|████▋     | 180/383 [00:14<00:13, 15.09it/s] 48%|████▊     | 184/383 [00:14<00:11, 16.86it/s] 48%|████▊     | 182/383 [00:14<00:12, 15.77it/s]
 49%|████▊     | 186/383 [00:14<00:12, 16.05it/s][A 49%|████▉     | 187/383 [00:14<00:12, 15.15it/s] 49%|████▊     | 186/383 [00:14<00:12, 15.58it/s] 48%|████▊     | 184/383 [00:14<00:12, 16.58it/s]
 49%|████▉     | 188/383 [00:14<00:11, 16.58it/s][A 50%|████▉     | 190/383 [00:14<00:10, 17.59it/s] 49%|████▉     | 188/383 [00:14<00:12, 16.12it/s] 49%|████▊     | 186/383 [00:14<00:12, 15.43it/s]
 50%|████▉     | 191/383 [00:14<00:10, 18.60it/s][A 50%|█████     | 192/383 [00:14<00:10, 17.92it/s] 50%|████▉     | 191/383 [00:14<00:10, 18.33it/s]
 50%|█████     | 193/383 [00:14<00:10, 18.52it/s][A 51%|█████     | 194/383 [00:14<00:10, 18.13it/s] 49%|████▉     | 188/383 [00:14<00:12, 15.39it/s] 50%|█████     | 193/383 [00:14<00:10, 18.47it/s] 51%|█████     | 196/383 [00:14<00:10, 18.44it/s]
 51%|█████     | 195/383 [00:14<00:10, 18.43it/s][A 50%|████▉     | 191/383 [00:14<00:10, 17.80it/s] 51%|█████     | 195/383 [00:14<00:10, 18.53it/s]
 51%|█████▏    | 197/383 [00:15<00:09, 18.69it/s][A 52%|█████▏    | 199/383 [00:15<00:09, 20.04it/s] 50%|█████     | 193/383 [00:15<00:10, 18.11it/s] 51%|█████▏    | 197/383 [00:15<00:09, 18.84it/s]
 52%|█████▏    | 199/383 [00:15<00:09, 19.04it/s][A 53%|█████▎    | 202/383 [00:15<00:08, 21.83it/s] 51%|█████     | 195/383 [00:15<00:10, 18.33it/s] 52%|█████▏    | 199/383 [00:15<00:09, 19.11it/s]
 53%|█████▎    | 202/383 [00:15<00:08, 21.13it/s][A 54%|█████▎    | 205/383 [00:15<00:07, 23.05it/s] 51%|█████▏    | 197/383 [00:15<00:10, 18.49it/s] 53%|█████▎    | 202/383 [00:15<00:08, 21.21it/s]
 54%|█████▎    | 205/383 [00:15<00:07, 22.76it/s][A 54%|█████▍    | 208/383 [00:15<00:07, 23.89it/s] 54%|█████▎    | 205/383 [00:15<00:07, 22.50it/s] 52%|█████▏    | 200/383 [00:15<00:09, 19.64it/s]
 54%|█████▍    | 208/383 [00:15<00:07, 23.61it/s][A 55%|█████▌    | 211/383 [00:15<00:07, 24.54it/s] 54%|█████▍    | 208/383 [00:15<00:07, 23.47it/s] 53%|█████▎    | 203/383 [00:15<00:08, 21.44it/s]
 55%|█████▌    | 211/383 [00:15<00:07, 24.29it/s][A 56%|█████▌    | 214/383 [00:15<00:06, 24.32it/s] 55%|█████▌    | 211/383 [00:15<00:07, 24.22it/s] 54%|█████▍    | 206/383 [00:15<00:07, 22.78it/s]
 56%|█████▌    | 214/383 [00:15<00:06, 24.71it/s][A 57%|█████▋    | 217/383 [00:15<00:06, 24.81it/s] 56%|█████▌    | 214/383 [00:15<00:06, 24.66it/s] 55%|█████▍    | 209/383 [00:15<00:07, 23.59it/s]
 57%|█████▋    | 217/383 [00:15<00:06, 24.30it/s][A 57%|█████▋    | 220/383 [00:15<00:06, 25.00it/s] 57%|█████▋    | 217/383 [00:15<00:06, 24.85it/s] 55%|█████▌    | 212/383 [00:15<00:07, 24.10it/s]
 57%|█████▋    | 220/383 [00:15<00:06, 24.80it/s][A 57%|█████▋    | 220/383 [00:15<00:06, 25.14it/s] 56%|█████▌    | 215/383 [00:16<00:06, 24.59it/s] 58%|█████▊    | 223/383 [00:16<00:07, 22.18it/s]
 58%|█████▊    | 223/383 [00:16<00:07, 22.78it/s][A 57%|█████▋    | 218/383 [00:16<00:06, 24.94it/s] 58%|█████▊    | 223/383 [00:16<00:07, 22.14it/s] 59%|█████▉    | 226/383 [00:16<00:07, 20.43it/s] 58%|█████▊    | 221/383 [00:16<00:06, 23.79it/s]
 59%|█████▉    | 226/383 [00:16<00:07, 20.63it/s][A 59%|█████▉    | 226/383 [00:16<00:07, 20.14it/s] 60%|█████▉    | 229/383 [00:16<00:07, 19.26it/s] 58%|█████▊    | 224/383 [00:16<00:07, 20.91it/s]
 60%|█████▉    | 229/383 [00:16<00:08, 19.09it/s][A 60%|█████▉    | 229/383 [00:16<00:08, 18.92it/s] 60%|██████    | 231/383 [00:16<00:08, 17.31it/s]
 60%|██████    | 231/383 [00:16<00:08, 17.83it/s][A 59%|█████▉    | 227/383 [00:16<00:08, 19.24it/s] 60%|██████    | 231/383 [00:16<00:08, 18.02it/s] 61%|██████    | 233/383 [00:16<00:09, 15.98it/s]
 61%|██████    | 233/383 [00:16<00:09, 16.35it/s][A 60%|██████    | 230/383 [00:16<00:08, 18.44it/s] 61%|██████    | 233/383 [00:16<00:09, 16.26it/s] 61%|██████▏   | 235/383 [00:16<00:09, 15.02it/s]
 61%|██████▏   | 235/383 [00:16<00:09, 15.31it/s][A 61%|██████    | 232/383 [00:16<00:08, 16.80it/s] 61%|██████▏   | 235/383 [00:16<00:09, 14.82it/s] 62%|██████▏   | 237/383 [00:17<00:10, 14.22it/s]
 62%|██████▏   | 237/383 [00:17<00:10, 14.46it/s][A 61%|██████    | 234/383 [00:17<00:09, 15.77it/s] 62%|██████▏   | 237/383 [00:17<00:10, 14.22it/s] 62%|██████▏   | 239/383 [00:17<00:10, 13.69it/s]
 62%|██████▏   | 239/383 [00:17<00:10, 13.97it/s][A 62%|██████▏   | 236/383 [00:17<00:09, 14.99it/s] 62%|██████▏   | 239/383 [00:17<00:10, 13.81it/s] 63%|██████▎   | 241/383 [00:17<00:10, 13.32it/s]
 63%|██████▎   | 241/383 [00:17<00:10, 13.63it/s][A 62%|██████▏   | 238/383 [00:17<00:10, 14.38it/s] 63%|██████▎   | 241/383 [00:17<00:10, 13.34it/s] 63%|██████▎   | 243/383 [00:17<00:10, 13.18it/s]
 63%|██████▎   | 243/383 [00:17<00:10, 13.29it/s][A 63%|██████▎   | 240/383 [00:17<00:10, 14.06it/s] 63%|██████▎   | 243/383 [00:17<00:10, 13.20it/s] 64%|██████▍   | 245/383 [00:17<00:10, 12.96it/s]
 64%|██████▍   | 245/383 [00:17<00:10, 13.27it/s][A 63%|██████▎   | 242/383 [00:17<00:10, 13.70it/s] 64%|██████▍   | 245/383 [00:17<00:10, 13.06it/s] 64%|██████▍   | 247/383 [00:17<00:10, 13.08it/s]
 64%|██████▍   | 247/383 [00:17<00:10, 13.27it/s][A 64%|██████▎   | 244/383 [00:17<00:10, 13.61it/s] 64%|██████▍   | 247/383 [00:17<00:10, 12.86it/s] 65%|██████▌   | 249/383 [00:17<00:10, 13.01it/s]
 65%|██████▌   | 249/383 [00:18<00:10, 12.83it/s][A 64%|██████▍   | 246/383 [00:18<00:10, 13.29it/s] 65%|██████▌   | 249/383 [00:18<00:10, 12.86it/s] 66%|██████▌   | 251/383 [00:18<00:10, 12.64it/s]
 66%|██████▌   | 251/383 [00:18<00:10, 12.97it/s][A 65%|██████▍   | 248/383 [00:18<00:10, 13.15it/s] 66%|██████▌   | 251/383 [00:18<00:10, 12.96it/s] 66%|██████▌   | 253/383 [00:18<00:10, 12.69it/s]
 66%|██████▌   | 253/383 [00:18<00:10, 12.96it/s][A 65%|██████▌   | 250/383 [00:18<00:10, 13.04it/s] 66%|██████▌   | 253/383 [00:18<00:10, 12.67it/s] 67%|██████▋   | 255/383 [00:18<00:09, 12.91it/s]
 67%|██████▋   | 255/383 [00:18<00:09, 13.06it/s][A 66%|██████▌   | 252/383 [00:18<00:10, 12.88it/s] 67%|██████▋   | 255/383 [00:18<00:10, 12.64it/s] 67%|██████▋   | 257/383 [00:18<00:09, 13.10it/s]
 67%|██████▋   | 257/383 [00:18<00:09, 13.16it/s][A 66%|██████▋   | 254/383 [00:18<00:10, 12.77it/s] 67%|██████▋   | 257/383 [00:18<00:09, 12.94it/s] 68%|██████▊   | 259/383 [00:18<00:09, 13.37it/s]
 68%|██████▊   | 259/383 [00:18<00:09, 13.28it/s][A 67%|██████▋   | 256/383 [00:18<00:09, 12.88it/s] 68%|██████▊   | 259/383 [00:18<00:09, 13.06it/s] 68%|██████▊   | 261/383 [00:18<00:09, 13.23it/s]
 68%|██████▊   | 261/383 [00:18<00:09, 13.49it/s][A 67%|██████▋   | 258/383 [00:18<00:09, 12.98it/s] 68%|██████▊   | 261/383 [00:19<00:09, 13.09it/s] 69%|██████▊   | 263/383 [00:19<00:08, 13.43it/s]
 69%|██████▊   | 263/383 [00:19<00:08, 13.47it/s][A 68%|██████▊   | 260/383 [00:19<00:09, 13.15it/s] 69%|██████▉   | 266/383 [00:19<00:07, 16.41it/s] 69%|██████▊   | 263/383 [00:19<00:09, 13.17it/s]
 69%|██████▉   | 266/383 [00:19<00:07, 16.05it/s][A 70%|███████   | 269/383 [00:19<00:06, 18.77it/s] 68%|██████▊   | 262/383 [00:19<00:09, 13.29it/s] 69%|██████▉   | 266/383 [00:19<00:07, 15.69it/s]
 70%|███████   | 269/383 [00:19<00:06, 18.55it/s][A 69%|██████▉   | 264/383 [00:19<00:08, 14.40it/s] 71%|███████   | 272/383 [00:19<00:05, 19.01it/s] 70%|███████   | 269/383 [00:19<00:06, 18.04it/s]
 71%|███████   | 272/383 [00:19<00:05, 19.76it/s][A 70%|██████▉   | 267/383 [00:19<00:07, 16.47it/s] 72%|███████▏  | 274/383 [00:19<00:06, 17.69it/s] 71%|███████   | 272/383 [00:19<00:05, 19.09it/s]
 72%|███████▏  | 275/383 [00:19<00:05, 18.50it/s][A 70%|███████   | 270/383 [00:19<00:05, 18.85it/s] 72%|███████▏  | 274/383 [00:19<00:06, 17.83it/s] 72%|███████▏  | 276/383 [00:19<00:06, 16.51it/s]
 72%|███████▏  | 277/383 [00:19<00:05, 17.71it/s][A 71%|███████▏  | 273/383 [00:19<00:05, 19.44it/s] 72%|███████▏  | 276/383 [00:19<00:06, 17.41it/s] 73%|███████▎  | 278/383 [00:19<00:06, 16.33it/s]
 73%|███████▎  | 279/383 [00:19<00:06, 16.86it/s][A 72%|███████▏  | 275/383 [00:19<00:05, 18.48it/s] 73%|███████▎  | 280/383 [00:19<00:06, 16.17it/s] 73%|███████▎  | 278/383 [00:19<00:06, 16.55it/s]
 73%|███████▎  | 281/383 [00:20<00:06, 16.17it/s][A 72%|███████▏  | 277/383 [00:20<00:05, 17.77it/s] 73%|███████▎  | 280/383 [00:20<00:06, 16.27it/s] 74%|███████▎  | 282/383 [00:20<00:07, 13.86it/s] 73%|███████▎  | 279/383 [00:20<00:06, 16.45it/s]
 74%|███████▍  | 283/383 [00:20<00:07, 14.27it/s][A 74%|███████▎  | 282/383 [00:20<00:07, 14.12it/s] 74%|███████▍  | 284/383 [00:20<00:07, 12.94it/s] 73%|███████▎  | 281/383 [00:20<00:06, 14.69it/s]
 74%|███████▍  | 285/383 [00:20<00:07, 12.89it/s][A 74%|███████▍  | 284/383 [00:20<00:07, 12.76it/s] 75%|███████▍  | 286/383 [00:20<00:07, 12.21it/s] 74%|███████▍  | 283/383 [00:20<00:07, 13.06it/s]
 75%|███████▍  | 287/383 [00:20<00:07, 12.08it/s][A 75%|███████▍  | 286/383 [00:20<00:08, 12.12it/s] 75%|███████▌  | 288/383 [00:20<00:07, 11.90it/s] 74%|███████▍  | 285/383 [00:20<00:07, 12.68it/s] 75%|███████▌  | 288/383 [00:20<00:08, 11.37it/s] 75%|███████▍  | 287/383 [00:20<00:07, 12.34it/s]
 75%|███████▌  | 289/383 [00:20<00:10,  8.76it/s][A 76%|███████▌  | 290/383 [00:21<00:12,  7.74it/s] 75%|███████▌  | 289/383 [00:21<00:10,  9.27it/s] 76%|███████▌  | 290/383 [00:21<00:12,  7.31it/s] 76%|███████▌  | 291/383 [00:21<00:13,  6.62it/s]
 76%|███████▌  | 291/383 [00:21<00:14,  6.29it/s][A 76%|███████▌  | 291/383 [00:21<00:14,  6.22it/s] 76%|███████▌  | 292/383 [00:21<00:15,  5.83it/s] 76%|███████▌  | 291/383 [00:21<00:14,  6.39it/s]
 76%|███████▌  | 292/383 [00:21<00:16,  5.59it/s][A 76%|███████▌  | 292/383 [00:21<00:17,  5.33it/s] 77%|███████▋  | 293/383 [00:21<00:17,  5.12it/s] 76%|███████▌  | 292/383 [00:22<00:16,  5.62it/s]
 77%|███████▋  | 293/383 [00:22<00:18,  4.92it/s][A 77%|███████▋  | 293/383 [00:22<00:18,  4.91it/s] 77%|███████▋  | 294/383 [00:22<00:18,  4.79it/s] 77%|███████▋  | 293/383 [00:22<00:17,  5.10it/s]
 77%|███████▋  | 294/383 [00:22<00:19,  4.60it/s][A 77%|███████▋  | 295/383 [00:22<00:18,  4.71it/s] 77%|███████▋  | 294/383 [00:22<00:19,  4.49it/s] 77%|███████▋  | 294/383 [00:22<00:18,  4.73it/s]
 77%|███████▋  | 295/383 [00:22<00:19,  4.42it/s][A 77%|███████▋  | 296/383 [00:22<00:20,  4.26it/s] 77%|███████▋  | 295/383 [00:22<00:20,  4.20it/s] 77%|███████▋  | 295/383 [00:22<00:19,  4.54it/s]
 77%|███████▋  | 296/383 [00:22<00:20,  4.19it/s][A 78%|███████▊  | 297/383 [00:22<00:19,  4.31it/s] 77%|███████▋  | 296/383 [00:23<00:21,  4.07it/s] 77%|███████▋  | 296/383 [00:23<00:20,  4.27it/s]
 78%|███████▊  | 297/383 [00:23<00:21,  3.93it/s][A 78%|███████▊  | 298/383 [00:23<00:20,  4.10it/s] 78%|███████▊  | 297/383 [00:23<00:21,  4.08it/s] 78%|███████▊  | 297/383 [00:23<00:21,  4.06it/s]
 78%|███████▊  | 298/383 [00:23<00:21,  4.04it/s][A 78%|███████▊  | 298/383 [00:23<00:19,  4.28it/s] 78%|███████▊  | 299/383 [00:23<00:21,  3.95it/s]
 78%|███████▊  | 299/383 [00:23<00:20,  4.01it/s][A 78%|███████▊  | 298/383 [00:23<00:22,  3.80it/s] 78%|███████▊  | 300/383 [00:23<00:20,  4.06it/s] 78%|███████▊  | 299/383 [00:23<00:20,  4.06it/s]
 78%|███████▊  | 300/383 [00:23<00:21,  3.86it/s][A 78%|███████▊  | 299/383 [00:23<00:22,  3.78it/s] 79%|███████▊  | 301/383 [00:24<00:20,  3.92it/s] 78%|███████▊  | 300/383 [00:24<00:21,  3.89it/s]
 79%|███████▊  | 301/383 [00:24<00:21,  3.78it/s][A 78%|███████▊  | 300/383 [00:24<00:22,  3.71it/s] 79%|███████▊  | 301/383 [00:24<00:20,  4.03it/s] 79%|███████▉  | 302/383 [00:24<00:21,  3.85it/s]
 79%|███████▉  | 302/383 [00:24<00:21,  3.75it/s][A 79%|███████▊  | 301/383 [00:24<00:22,  3.65it/s] 79%|███████▉  | 303/383 [00:24<00:21,  3.80it/s] 79%|███████▉  | 302/383 [00:24<00:21,  3.81it/s]
 79%|███████▉  | 303/383 [00:24<00:20,  3.93it/s][A 79%|███████▉  | 303/383 [00:24<00:20,  3.97it/s] 79%|███████▉  | 302/383 [00:24<00:22,  3.62it/s] 79%|███████▉  | 304/383 [00:24<00:21,  3.68it/s]
 79%|███████▉  | 304/383 [00:25<00:21,  3.76it/s][A 79%|███████▉  | 304/383 [00:25<00:19,  4.00it/s] 80%|███████▉  | 305/383 [00:25<00:20,  3.83it/s] 79%|███████▉  | 303/383 [00:25<00:22,  3.63it/s] 80%|███████▉  | 306/383 [00:25<00:18,  4.09it/s] 80%|███████▉  | 305/383 [00:25<00:20,  3.85it/s]
 80%|███████▉  | 305/383 [00:25<00:21,  3.63it/s][A 79%|███████▉  | 304/383 [00:25<00:21,  3.62it/s] 80%|████████  | 307/383 [00:25<00:19,  3.95it/s]
 80%|███████▉  | 306/383 [00:25<00:20,  3.67it/s][A 80%|███████▉  | 306/383 [00:25<00:20,  3.81it/s] 80%|███████▉  | 305/383 [00:25<00:21,  3.63it/s] 80%|████████  | 308/383 [00:25<00:19,  3.81it/s]
 80%|████████  | 307/383 [00:25<00:20,  3.65it/s][A 80%|████████  | 307/383 [00:25<00:20,  3.73it/s] 80%|███████▉  | 306/383 [00:25<00:21,  3.62it/s] 81%|████████  | 309/383 [00:26<00:19,  3.85it/s] 80%|████████  | 308/383 [00:26<00:19,  3.87it/s]
 80%|████████  | 308/383 [00:26<00:20,  3.67it/s][A 80%|████████  | 307/383 [00:26<00:21,  3.59it/s] 81%|████████  | 309/383 [00:26<00:19,  3.79it/s]
 81%|████████  | 309/383 [00:26<00:19,  3.76it/s][A 81%|████████  | 310/383 [00:26<00:19,  3.70it/s] 80%|████████  | 308/383 [00:26<00:20,  3.57it/s]
 81%|████████  | 310/383 [00:26<00:18,  3.86it/s][A 81%|████████  | 310/383 [00:26<00:19,  3.71it/s] 81%|████████  | 311/383 [00:26<00:19,  3.64it/s] 81%|████████  | 309/383 [00:26<00:19,  3.80it/s]
 81%|████████  | 311/383 [00:26<00:18,  3.83it/s][A 81%|████████  | 311/383 [00:26<00:19,  3.69it/s] 81%|████████▏ | 312/383 [00:26<00:19,  3.64it/s] 81%|████████  | 310/383 [00:26<00:18,  3.90it/s]
 81%|████████▏ | 312/383 [00:27<00:18,  3.86it/s][A 81%|████████▏ | 312/383 [00:27<00:19,  3.64it/s] 82%|████████▏ | 313/383 [00:27<00:19,  3.64it/s] 81%|████████  | 311/383 [00:27<00:18,  3.81it/s]
 82%|████████▏ | 313/383 [00:27<00:19,  3.66it/s][A 81%|████████▏ | 312/383 [00:27<00:18,  3.88it/s] 82%|████████▏ | 313/383 [00:27<00:18,  3.69it/s] 82%|████████▏ | 314/383 [00:27<00:19,  3.61it/s]
 82%|████████▏ | 314/383 [00:27<00:18,  3.68it/s][A 82%|████████▏ | 314/383 [00:27<00:18,  3.79it/s] 82%|████████▏ | 313/383 [00:27<00:17,  3.91it/s] 82%|████████▏ | 315/383 [00:27<00:19,  3.51it/s] 82%|████████▏ | 314/383 [00:27<00:17,  3.97it/s] 82%|████████▏ | 315/383 [00:27<00:17,  3.84it/s]
 82%|████████▏ | 315/383 [00:27<00:18,  3.67it/s][A 83%|████████▎ | 316/383 [00:28<00:18,  3.64it/s] 82%|████████▏ | 315/383 [00:28<00:16,  4.09it/s] 83%|████████▎ | 316/383 [00:28<00:17,  3.81it/s]
 83%|████████▎ | 316/383 [00:28<00:18,  3.72it/s][A 83%|████████▎ | 317/383 [00:28<00:18,  3.64it/s] 83%|████████▎ | 316/383 [00:28<00:17,  3.80it/s]
 83%|████████▎ | 317/383 [00:28<00:17,  3.71it/s][A 83%|████████▎ | 317/383 [00:28<00:18,  3.63it/s] 83%|████████▎ | 318/383 [00:28<00:18,  3.51it/s] 83%|████████▎ | 317/383 [00:28<00:17,  3.76it/s] 83%|████████▎ | 318/383 [00:28<00:17,  3.61it/s]
 83%|████████▎ | 318/383 [00:28<00:18,  3.56it/s][A 83%|████████▎ | 319/383 [00:28<00:17,  3.70it/s] 83%|████████▎ | 318/383 [00:29<00:17,  3.71it/s]
 83%|████████▎ | 319/383 [00:29<00:17,  3.68it/s][A 83%|████████▎ | 319/383 [00:29<00:17,  3.67it/s] 84%|████████▎ | 320/383 [00:29<00:17,  3.69it/s] 83%|████████▎ | 319/383 [00:29<00:17,  3.71it/s]
 84%|████████▎ | 320/383 [00:29<00:16,  3.77it/s][A 84%|████████▎ | 320/383 [00:29<00:17,  3.69it/s] 84%|████████▍ | 321/383 [00:29<00:17,  3.60it/s]
 84%|████████▍ | 321/383 [00:29<00:16,  3.78it/s][A 84%|████████▎ | 320/383 [00:29<00:17,  3.68it/s] 84%|████████▍ | 321/383 [00:29<00:16,  3.73it/s] 84%|████████▍ | 322/383 [00:29<00:16,  3.63it/s]
 84%|████████▍ | 322/383 [00:29<00:15,  3.85it/s][A 84%|████████▍ | 321/383 [00:29<00:17,  3.59it/s] 84%|████████▍ | 322/383 [00:29<00:16,  3.59it/s] 84%|████████▍ | 323/383 [00:29<00:15,  3.75it/s]
 84%|████████▍ | 323/383 [00:30<00:15,  3.99it/s][A 84%|████████▍ | 322/383 [00:30<00:16,  3.78it/s] 84%|████████▍ | 323/383 [00:30<00:16,  3.73it/s] 85%|████████▍ | 324/383 [00:30<00:15,  3.71it/s]
 85%|████████▍ | 324/383 [00:30<00:15,  3.77it/s][A 84%|████████▍ | 323/383 [00:30<00:16,  3.72it/s] 85%|████████▍ | 324/383 [00:30<00:15,  3.72it/s] 85%|████████▍ | 325/383 [00:30<00:15,  3.80it/s]
 85%|████████▍ | 325/383 [00:30<00:15,  3.86it/s][A 85%|████████▍ | 324/383 [00:30<00:16,  3.68it/s] 85%|████████▍ | 325/383 [00:30<00:15,  3.82it/s] 85%|████████▌ | 326/383 [00:30<00:14,  3.96it/s]
 85%|████████▌ | 326/383 [00:30<00:15,  3.76it/s][A 85%|████████▌ | 326/383 [00:30<00:14,  3.85it/s] 85%|████████▍ | 325/383 [00:30<00:15,  3.67it/s] 85%|████████▌ | 327/383 [00:30<00:14,  3.89it/s]
 85%|████████▌ | 327/383 [00:31<00:14,  3.74it/s][A 85%|████████▌ | 326/383 [00:31<00:14,  3.82it/s] 85%|████████▌ | 327/383 [00:31<00:15,  3.67it/s] 86%|████████▌ | 328/383 [00:31<00:14,  3.79it/s] 85%|████████▌ | 327/383 [00:31<00:14,  3.89it/s]
 86%|████████▌ | 328/383 [00:31<00:14,  3.68it/s][A 86%|████████▌ | 329/383 [00:31<00:14,  3.80it/s] 86%|████████▌ | 328/383 [00:31<00:15,  3.64it/s] 86%|████████▌ | 328/383 [00:31<00:14,  3.88it/s]
 86%|████████▌ | 329/383 [00:31<00:14,  3.74it/s][A 86%|████████▌ | 329/383 [00:31<00:14,  3.75it/s] 86%|████████▌ | 330/383 [00:31<00:14,  3.74it/s] 86%|████████▌ | 329/383 [00:31<00:13,  3.89it/s] 86%|████████▋ | 331/383 [00:31<00:12,  4.25it/s]
 86%|████████▌ | 330/383 [00:31<00:14,  3.76it/s][A 86%|████████▌ | 330/383 [00:32<00:14,  3.64it/s] 87%|████████▋ | 332/383 [00:32<00:10,  4.84it/s] 86%|████████▌ | 330/383 [00:32<00:12,  4.13it/s] 86%|████████▋ | 331/383 [00:32<00:12,  4.27it/s] 87%|████████▋ | 333/383 [00:32<00:09,  5.34it/s]
 86%|████████▋ | 331/383 [00:32<00:13,  3.73it/s][A 87%|████████▋ | 332/383 [00:32<00:11,  4.60it/s] 87%|████████▋ | 334/383 [00:32<00:08,  5.62it/s]
 87%|████████▋ | 332/383 [00:32<00:12,  4.22it/s][A 86%|████████▋ | 331/383 [00:32<00:13,  3.99it/s] 87%|████████▋ | 335/383 [00:32<00:08,  5.81it/s] 87%|████████▋ | 333/383 [00:32<00:10,  4.85it/s]
 87%|████████▋ | 333/383 [00:32<00:10,  4.70it/s][A 87%|████████▋ | 332/383 [00:32<00:11,  4.37it/s] 88%|████████▊ | 336/383 [00:32<00:07,  5.94it/s]
 87%|████████▋ | 334/383 [00:32<00:09,  5.06it/s][A 87%|████████▋ | 334/383 [00:32<00:09,  5.09it/s] 87%|████████▋ | 333/383 [00:32<00:10,  4.96it/s] 87%|████████▋ | 335/383 [00:32<00:08,  5.63it/s] 88%|████████▊ | 337/383 [00:32<00:07,  6.11it/s] 87%|████████▋ | 334/383 [00:32<00:09,  5.44it/s]
 87%|████████▋ | 335/383 [00:32<00:09,  5.25it/s][A 88%|████████▊ | 336/383 [00:33<00:08,  5.73it/s] 88%|████████▊ | 338/383 [00:33<00:07,  5.92it/s] 87%|████████▋ | 335/383 [00:33<00:08,  5.49it/s]
 88%|████████▊ | 336/383 [00:33<00:08,  5.25it/s][A 88%|████████▊ | 337/383 [00:33<00:07,  5.89it/s] 88%|████████▊ | 336/383 [00:33<00:07,  5.90it/s] 89%|████████▉ | 340/383 [00:33<00:05,  7.69it/s]
 88%|████████▊ | 337/383 [00:33<00:08,  5.39it/s][A 88%|████████▊ | 338/383 [00:33<00:07,  6.01it/s] 89%|████████▉ | 342/383 [00:33<00:04,  9.47it/s] 88%|████████▊ | 337/383 [00:33<00:07,  6.11it/s]
 88%|████████▊ | 338/383 [00:33<00:08,  5.49it/s][A 89%|████████▉ | 340/383 [00:33<00:05,  8.40it/s] 90%|████████▉ | 344/383 [00:33<00:03, 10.79it/s] 88%|████████▊ | 338/383 [00:33<00:07,  6.18it/s]
 89%|████████▊ | 339/383 [00:33<00:07,  5.73it/s][A 89%|████████▉ | 342/383 [00:33<00:04,  9.82it/s] 90%|█████████ | 346/383 [00:33<00:03, 11.54it/s] 89%|████████▉ | 340/383 [00:33<00:05,  8.27it/s]
 89%|████████▉ | 341/383 [00:33<00:05,  7.90it/s][A 90%|████████▉ | 344/383 [00:33<00:03, 11.11it/s] 91%|█████████ | 348/383 [00:33<00:02, 12.47it/s] 89%|████████▉ | 342/383 [00:33<00:04,  9.97it/s]
 90%|████████▉ | 343/383 [00:33<00:04,  9.62it/s][A 90%|█████████ | 346/383 [00:33<00:03, 12.21it/s] 91%|█████████▏| 350/383 [00:33<00:02, 13.35it/s] 90%|████████▉ | 344/383 [00:33<00:03, 10.54it/s]
 90%|█████████ | 345/383 [00:34<00:03, 11.04it/s][A 91%|█████████ | 348/383 [00:34<00:02, 12.99it/s] 92%|█████████▏| 352/383 [00:34<00:02, 13.38it/s] 90%|█████████ | 346/383 [00:34<00:03, 11.92it/s]
 91%|█████████ | 347/383 [00:34<00:02, 12.18it/s][A 91%|█████████▏| 350/383 [00:34<00:02, 13.07it/s] 92%|█████████▏| 354/383 [00:34<00:02, 13.70it/s] 91%|█████████ | 348/383 [00:34<00:02, 12.87it/s]
 91%|█████████ | 349/383 [00:34<00:02, 12.89it/s][A 93%|█████████▎| 356/383 [00:34<00:01, 14.47it/s] 92%|█████████▏| 352/383 [00:34<00:02, 13.79it/s] 91%|█████████▏| 350/383 [00:34<00:02, 13.40it/s]
 92%|█████████▏| 351/383 [00:34<00:02, 13.29it/s][A 93%|█████████▎| 358/383 [00:34<00:01, 15.28it/s] 92%|█████████▏| 354/383 [00:34<00:02, 14.07it/s] 92%|█████████▏| 352/383 [00:34<00:02, 13.68it/s]
 92%|█████████▏| 353/383 [00:34<00:02, 13.26it/s][A 93%|█████████▎| 356/383 [00:34<00:01, 14.37it/s] 92%|█████████▏| 354/383 [00:34<00:02, 13.77it/s]
 93%|█████████▎| 355/383 [00:34<00:02, 13.58it/s][A 93%|█████████▎| 358/383 [00:34<00:01, 14.75it/s] 93%|█████████▎| 356/383 [00:34<00:01, 13.64it/s] 94%|█████████▍| 360/383 [00:34<00:02,  9.73it/s]
 93%|█████████▎| 357/383 [00:34<00:01, 14.39it/s][A 93%|█████████▎| 358/383 [00:34<00:01, 14.88it/s]
 94%|█████████▎| 359/383 [00:34<00:01, 15.44it/s][A 94%|█████████▍| 360/383 [00:35<00:02,  9.97it/s] 95%|█████████▍| 362/383 [00:35<00:02,  8.33it/s] 94%|█████████▍| 360/383 [00:35<00:01, 11.86it/s]
 94%|█████████▍| 361/383 [00:35<00:02, 10.16it/s][A 95%|█████████▍| 362/383 [00:35<00:02,  8.17it/s] 95%|█████████▍| 362/383 [00:35<00:02,  9.10it/s] 95%|█████████▌| 364/383 [00:35<00:02,  7.20it/s]
 95%|█████████▍| 363/383 [00:35<00:02,  8.47it/s][A 95%|█████████▌| 365/383 [00:35<00:02,  6.86it/s] 95%|█████████▌| 364/383 [00:35<00:02,  7.13it/s] 96%|█████████▌| 367/383 [00:35<00:01,  8.69it/s] 95%|█████████▌| 364/383 [00:35<00:02,  7.82it/s] 96%|█████████▋| 369/383 [00:35<00:01, 10.47it/s] 95%|█████████▌| 365/383 [00:35<00:02,  6.85it/s]
 95%|█████████▌| 365/383 [00:35<00:02,  7.44it/s][A 97%|█████████▋| 371/383 [00:35<00:00, 12.16it/s] 95%|█████████▌| 365/383 [00:35<00:02,  7.36it/s] 96%|█████████▌| 367/383 [00:36<00:01,  8.61it/s] 97%|█████████▋| 373/383 [00:36<00:00, 13.67it/s]
 96%|█████████▌| 366/383 [00:36<00:02,  7.38it/s][A 96%|█████████▌| 367/383 [00:36<00:01,  9.25it/s] 96%|█████████▋| 369/383 [00:36<00:01, 10.40it/s]
 96%|█████████▌| 368/383 [00:36<00:01,  9.20it/s][A 98%|█████████▊| 375/383 [00:36<00:00, 14.64it/s] 96%|█████████▋| 369/383 [00:36<00:01, 10.96it/s] 97%|█████████▋| 371/383 [00:36<00:00, 12.04it/s] 98%|█████████▊| 377/383 [00:36<00:00, 15.82it/s]
 97%|█████████▋| 370/383 [00:36<00:01, 10.89it/s][A 97%|█████████▋| 371/383 [00:36<00:00, 12.45it/s] 97%|█████████▋| 373/383 [00:36<00:00, 13.42it/s] 97%|█████████▋| 373/383 [00:36<00:00, 13.84it/s]
 97%|█████████▋| 372/383 [00:36<00:00, 12.34it/s][A 99%|█████████▉| 380/383 [00:36<00:00, 18.26it/s] 98%|█████████▊| 375/383 [00:36<00:00, 14.87it/s]
 98%|█████████▊| 374/383 [00:36<00:00, 13.79it/s][A100%|██████████| 383/383 [00:36<00:00, 19.38it/s]100%|██████████| 383/383 [00:36<00:00, 10.47it/s]
 98%|█████████▊| 376/383 [00:36<00:00, 15.80it/s]/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
 99%|█████████▊| 378/383 [00:36<00:00, 16.91it/s]/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/pytorch/torch/utils/checkpoint.py:426: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 98%|█████████▊| 377/383 [00:36<00:00, 15.74it/s][A 99%|█████████▉| 379/383 [00:36<00:00, 18.39it/s] 99%|█████████▉| 381/383 [00:36<00:00, 19.48it/s]100%|█████████▉| 382/383 [00:36<00:00, 20.61it/s]
 99%|█████████▉| 380/383 [00:36<00:00, 17.68it/s][A100%|██████████| 383/383 [00:36<00:00, 10.40it/s]
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
100%|██████████| 383/383 [00:36<00:00, 10.38it/s]
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/pytorch/torch/utils/checkpoint.py:426: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

100%|██████████| 383/383 [00:36<00:00, 18.78it/s][A100%|██████████| 383/383 [00:36<00:00, 10.36it/s]
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
                                                     {'mmlu_loss': 3.20650198627701, 'mmlu_eval_accuracy_conceptual_physics': 0.3076923076923077, 'mmlu_eval_accuracy_logical_fallacies': nan, 'mmlu_eval_accuracy_professional_psychology': nan, 'mmlu_eval_accuracy_public_relations': nan, 'mmlu_eval_accuracy_security_studies': nan, 'mmlu_eval_accuracy_high_school_physics': nan, 'mmlu_eval_accuracy_formal_logic': 0.21428571428571427, 'mmlu_eval_accuracy_anatomy': 0.14285714285714285, 'mmlu_eval_accuracy_moral_scenarios': nan, 'mmlu_eval_accuracy_moral_disputes': nan, 'mmlu_eval_accuracy_management': nan, 'mmlu_eval_accuracy_high_school_european_history': 0.3333333333333333, 'mmlu_eval_accuracy_professional_law': nan, 'mmlu_eval_accuracy_elementary_mathematics': 0.1951219512195122, 'mmlu_eval_accuracy_high_school_computer_science': 0.3333333333333333, 'mmlu_eval_accuracy_high_school_geography': 0.4166666666666667, 'mmlu_eval_accuracy_global_facts': 0.3, 'mmlu_eval_accuracy_high_school_biology': 0.25, 'mmlu_eval_accuracy_philosophy': nan, 'mmlu_eval_accuracy_astronomy': 0.1875, 'mmlu_eval_accuracy_nutrition': nan, 'mmlu_eval_accuracy_high_school_government_and_politics': nan, 'mmlu_eval_accuracy_medical_genetics': nan, 'mmlu_eval_accuracy_human_aging': nan, 'mmlu_eval_accuracy_human_sexuality': nan, 'mmlu_eval_accuracy_college_medicine': 0.18181818181818182, 'mmlu_eval_accuracy_abstract_algebra': 0.18181818181818182, 'mmlu_eval_accuracy_college_physics': 0.2727272727272727, 'mmlu_eval_accuracy_us_foreign_policy': nan, 'mmlu_eval_accuracy_college_computer_science': 0.09090909090909091, 'mmlu_eval_accuracy_machine_learning': nan, 'mmlu_eval_accuracy_professional_accounting': nan, 'mmlu_eval_accuracy_world_religions': nan, 'mmlu_eval_accuracy_miscellaneous': nan, 'mmlu_eval_accuracy_jurisprudence': nan, 'mmlu_eval_accuracy_high_school_chemistry': 0.2727272727272727, 'mmlu_eval_accuracy_virology': nan, 'mmlu_eval_accuracy_high_school_mathematics': nan, 'mmlu_eval_accuracy_electrical_engineering': 0.3125, 'mmlu_eval_accuracy_high_school_world_history': nan, 'mmlu_eval_accuracy_prehistory': nan, 'mmlu_eval_accuracy_high_school_statistics': nan, 'mmlu_eval_accuracy_high_school_psychology': nan, 'mmlu_eval_accuracy_high_school_microeconomics': nan, 'mmlu_eval_accuracy_high_school_us_history': nan, 'mmlu_eval_accuracy_college_biology': 0.125, 'mmlu_eval_accuracy_high_school_macroeconomics': nan, 'mmlu_eval_accuracy_professional_medicine': nan, 'mmlu_eval_accuracy_clinical_knowledge': 0.2413793103448276, 'mmlu_eval_accuracy_business_ethics': 0.0, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_econometrics': 0.4166666666666667, 'mmlu_eval_accuracy_marketing': nan, 'mmlu_eval_accuracy_international_law': nan, 'mmlu_eval_accuracy_sociology': nan, 'mmlu_eval_accuracy_computer_security': 0.18181818181818182, 'mmlu_eval_accuracy_college_mathematics': 0.09090909090909091, 'mmlu_eval_accuracy': nan, 'epoch': 9.28}
 62%|██████▎   | 1280/2048 [1:48:33<54:43,  4.28s/it]/home/bagus/pytorch/torch/utils/checkpoint.py:426: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/pytorch/torch/utils/checkpoint.py:426: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 63%|██████▎   | 1281/2048 [1:48:38<4:17:13, 20.12s/it] 63%|██████▎   | 1282/2048 [1:48:43<3:19:29, 15.63s/it] 63%|██████▎   | 1283/2048 [1:48:48<2:38:20, 12.42s/it] 63%|██████▎   | 1284/2048 [1:48:53<2:09:05, 10.14s/it] 63%|██████▎   | 1285/2048 [1:48:57<1:46:58,  8.41s/it] 63%|██████▎   | 1286/2048 [1:49:01<1:31:07,  7.18s/it] 63%|██████▎   | 1287/2048 [1:49:05<1:18:44,  6.21s/it] 63%|██████▎   | 1288/2048 [1:49:09<1:10:28,  5.56s/it] 63%|██████▎   | 1289/2048 [1:49:14<1:04:42,  5.12s/it] 63%|██████▎   | 1290/2048 [1:49:18<1:00:24,  4.78s/it] 63%|██████▎   | 1291/2048 [1:49:21<54:57,  4.36s/it]   63%|██████▎   | 1292/2048 [1:49:26<57:56,  4.60s/it] 63%|██████▎   | 1293/2048 [1:49:31<1:00:00,  4.77s/it] 63%|██████▎   | 1294/2048 [1:49:36<1:01:13,  4.87s/it] 63%|██████▎   | 1295/2048 [1:49:41<1:01:52,  4.93s/it] 63%|██████▎   | 1296/2048 [1:49:46<1:00:52,  4.86s/it]                                                       {'loss': 0.1112, 'learning_rate': 6.279370634384429e-05, 'epoch': 9.4}
 63%|██████▎   | 1296/2048 [1:49:46<1:00:52,  4.86s/it] 63%|██████▎   | 1297/2048 [1:49:51<59:46,  4.77s/it]   63%|██████▎   | 1298/2048 [1:49:55<58:02,  4.64s/it] 63%|██████▎   | 1299/2048 [1:49:59<56:17,  4.51s/it] 63%|██████▎   | 1300/2048 [1:50:03<54:52,  4.40s/it] 64%|██████▎   | 1301/2048 [1:50:07<53:14,  4.28s/it] 64%|██████▎   | 1302/2048 [1:50:11<52:26,  4.22s/it] 64%|██████▎   | 1303/2048 [1:50:15<49:04,  3.95s/it] 64%|██████▎   | 1304/2048 [1:50:19<50:29,  4.07s/it] 64%|██████▎   | 1305/2048 [1:50:24<54:29,  4.40s/it] 64%|██████▍   | 1306/2048 [1:50:29<57:12,  4.63s/it] 64%|██████▍   | 1307/2048 [1:50:35<58:56,  4.77s/it] 64%|██████▍   | 1308/2048 [1:50:40<59:37,  4.83s/it] 64%|██████▍   | 1309/2048 [1:50:44<58:51,  4.78s/it] 64%|██████▍   | 1310/2048 [1:50:49<57:48,  4.70s/it] 64%|██████▍   | 1311/2048 [1:50:53<56:11,  4.58s/it] 64%|██████▍   | 1312/2048 [1:50:57<53:39,  4.37s/it]                                                     {'loss': 0.1202, 'learning_rate': 6.045658864505097e-05, 'epoch': 9.52}
 64%|██████▍   | 1312/2048 [1:50:57<53:39,  4.37s/it] 64%|██████▍   | 1313/2048 [1:51:01<52:55,  4.32s/it] 64%|██████▍   | 1314/2048 [1:51:05<51:48,  4.23s/it] 64%|██████▍   | 1315/2048 [1:51:09<50:46,  4.16s/it] 64%|██████▍   | 1316/2048 [1:51:13<48:23,  3.97s/it] 64%|██████▍   | 1317/2048 [1:51:18<52:42,  4.33s/it] 64%|██████▍   | 1318/2048 [1:51:23<55:42,  4.58s/it] 64%|██████▍   | 1319/2048 [1:51:28<57:44,  4.75s/it] 64%|██████▍   | 1320/2048 [1:51:33<58:58,  4.86s/it] 65%|██████▍   | 1321/2048 [1:51:38<58:59,  4.87s/it] 65%|██████▍   | 1322/2048 [1:51:43<58:21,  4.82s/it] 65%|██████▍   | 1323/2048 [1:51:47<56:45,  4.70s/it] 65%|██████▍   | 1324/2048 [1:51:52<55:12,  4.58s/it] 65%|██████▍   | 1325/2048 [1:51:56<53:22,  4.43s/it] 65%|██████▍   | 1326/2048 [1:52:00<51:42,  4.30s/it] 65%|██████▍   | 1327/2048 [1:52:03<49:28,  4.12s/it] 65%|██████▍   | 1328/2048 [1:52:07<46:16,  3.86s/it]                                                     {'loss': 0.1028, 'learning_rate': 5.814480077019173e-05, 'epoch': 9.63}
 65%|██████▍   | 1328/2048 [1:52:07<46:16,  3.86s/it] 65%|██████▍   | 1329/2048 [1:52:11<48:01,  4.01s/it] 65%|██████▍   | 1330/2048 [1:52:16<52:08,  4.36s/it] 65%|██████▍   | 1331/2048 [1:52:21<54:56,  4.60s/it] 65%|██████▌   | 1332/2048 [1:52:26<56:49,  4.76s/it] 65%|██████▌   | 1333/2048 [1:52:31<57:11,  4.80s/it] 65%|██████▌   | 1334/2048 [1:52:36<57:05,  4.80s/it] 65%|██████▌   | 1335/2048 [1:52:41<56:00,  4.71s/it] 65%|██████▌   | 1336/2048 [1:52:45<53:46,  4.53s/it] 65%|██████▌   | 1337/2048 [1:52:48<50:52,  4.29s/it] 65%|██████▌   | 1338/2048 [1:52:52<48:46,  4.12s/it] 65%|██████▌   | 1339/2048 [1:52:56<48:19,  4.09s/it] 65%|██████▌   | 1340/2048 [1:53:00<46:59,  3.98s/it] 65%|██████▌   | 1341/2048 [1:53:03<44:39,  3.79s/it] 66%|██████▌   | 1342/2048 [1:53:08<49:27,  4.20s/it] 66%|██████▌   | 1343/2048 [1:53:14<52:47,  4.49s/it] 66%|██████▌   | 1344/2048 [1:53:19<55:02,  4.69s/it]                                                     {'loss': 0.1006, 'learning_rate': 5.585982355204844e-05, 'epoch': 9.75}
 66%|██████▌   | 1344/2048 [1:53:19<55:02,  4.69s/it] 66%|██████▌   | 1345/2048 [1:53:24<56:29,  4.82s/it] 66%|██████▌   | 1346/2048 [1:53:29<56:38,  4.84s/it] 66%|██████▌   | 1347/2048 [1:53:33<55:39,  4.76s/it] 66%|██████▌   | 1348/2048 [1:53:38<54:02,  4.63s/it] 66%|██████▌   | 1349/2048 [1:53:42<52:00,  4.46s/it] 66%|██████▌   | 1350/2048 [1:53:46<49:50,  4.28s/it] 66%|██████▌   | 1351/2048 [1:53:50<48:50,  4.20s/it] 66%|██████▌   | 1352/2048 [1:53:54<48:03,  4.14s/it] 66%|██████▌   | 1353/2048 [1:53:57<45:19,  3.91s/it] 66%|██████▌   | 1354/2048 [1:54:01<46:42,  4.04s/it] 66%|██████▌   | 1355/2048 [1:54:06<50:33,  4.38s/it] 66%|██████▌   | 1356/2048 [1:54:12<53:12,  4.61s/it] 66%|██████▋   | 1357/2048 [1:54:17<54:59,  4.78s/it] 66%|██████▋   | 1358/2048 [1:54:22<55:37,  4.84s/it] 66%|██████▋   | 1359/2048 [1:54:27<55:37,  4.84s/it] 66%|██████▋   | 1360/2048 [1:54:31<54:15,  4.73s/it]                                                     {'loss': 0.1274, 'learning_rate': 5.360312064963904e-05, 'epoch': 9.86}
 66%|██████▋   | 1360/2048 [1:54:31<54:15,  4.73s/it] 66%|██████▋   | 1361/2048 [1:54:35<52:35,  4.59s/it] 67%|██████▋   | 1362/2048 [1:54:39<50:33,  4.42s/it] 67%|██████▋   | 1363/2048 [1:54:43<47:59,  4.20s/it] 67%|██████▋   | 1364/2048 [1:54:47<46:08,  4.05s/it] 67%|██████▋   | 1365/2048 [1:54:51<45:23,  3.99s/it] 67%|██████▋   | 1366/2048 [1:54:54<43:31,  3.83s/it] 67%|██████▋   | 1367/2048 [1:54:59<48:01,  4.23s/it] 67%|██████▋   | 1368/2048 [1:55:04<51:07,  4.51s/it] 67%|██████▋   | 1369/2048 [1:55:10<53:16,  4.71s/it] 67%|██████▋   | 1370/2048 [1:55:15<54:27,  4.82s/it] 67%|██████▋   | 1371/2048 [1:55:20<54:52,  4.86s/it] 67%|██████▋   | 1372/2048 [1:55:24<53:40,  4.76s/it] 67%|██████▋   | 1373/2048 [1:55:29<52:37,  4.68s/it] 67%|██████▋   | 1374/2048 [1:55:33<50:37,  4.51s/it] 67%|██████▋   | 1375/2048 [1:55:37<48:17,  4.31s/it] 67%|██████▋   | 1376/2048 [1:55:40<46:53,  4.19s/it]                                                     {'loss': 0.1083, 'learning_rate': 5.137613761065982e-05, 'epoch': 9.98}
 67%|██████▋   | 1376/2048 [1:55:40<46:53,  4.19s/it] 67%|██████▋   | 1377/2048 [1:55:44<45:19,  4.05s/it] 67%|██████▋   | 1378/2048 [1:55:47<42:21,  3.79s/it] 67%|██████▋   | 1379/2048 [1:55:52<43:43,  3.92s/it] 67%|██████▋   | 1380/2048 [1:55:57<47:49,  4.30s/it] 67%|██████▋   | 1381/2048 [1:56:02<50:39,  4.56s/it] 67%|██████▋   | 1382/2048 [1:56:07<52:29,  4.73s/it] 68%|██████▊   | 1383/2048 [1:56:12<53:36,  4.84s/it] 68%|██████▊   | 1384/2048 [1:56:17<53:23,  4.82s/it] 68%|██████▊   | 1385/2048 [1:56:22<52:38,  4.76s/it] 68%|██████▊   | 1386/2048 [1:56:26<51:15,  4.65s/it] 68%|██████▊   | 1387/2048 [1:56:30<49:29,  4.49s/it] 68%|██████▊   | 1388/2048 [1:56:34<48:01,  4.37s/it] 68%|██████▊   | 1389/2048 [1:56:38<46:37,  4.25s/it] 68%|██████▊   | 1390/2048 [1:56:42<45:22,  4.14s/it] 68%|██████▊   | 1391/2048 [1:56:45<42:27,  3.88s/it] 68%|██████▊   | 1392/2048 [1:56:50<44:42,  4.09s/it]                                                     {'loss': 0.0985, 'learning_rate': 4.918030094552939e-05, 'epoch': 10.1}
 68%|██████▊   | 1392/2048 [1:56:50<44:42,  4.09s/it] 68%|██████▊   | 1393/2048 [1:56:55<48:10,  4.41s/it] 68%|██████▊   | 1394/2048 [1:57:00<50:33,  4.64s/it] 68%|██████▊   | 1395/2048 [1:57:05<52:08,  4.79s/it] 68%|██████▊   | 1396/2048 [1:57:10<52:41,  4.85s/it] 68%|██████▊   | 1397/2048 [1:57:15<52:14,  4.81s/it] 68%|██████▊   | 1398/2048 [1:57:20<51:20,  4.74s/it] 68%|██████▊   | 1399/2048 [1:57:24<49:22,  4.57s/it] 68%|██████▊   | 1400/2048 [1:57:28<47:18,  4.38s/it] 68%|██████▊   | 1401/2048 [1:57:32<45:32,  4.22s/it] 68%|██████▊   | 1402/2048 [1:57:35<43:50,  4.07s/it] 69%|██████▊   | 1403/2048 [1:57:39<42:26,  3.95s/it] 69%|██████▊   | 1404/2048 [1:57:43<41:17,  3.85s/it] 69%|██████▊   | 1405/2048 [1:57:48<45:28,  4.24s/it] 69%|██████▊   | 1406/2048 [1:57:53<48:21,  4.52s/it] 69%|██████▊   | 1407/2048 [1:57:58<50:17,  4.71s/it] 69%|██████▉   | 1408/2048 [1:58:03<51:23,  4.82s/it]                                                     {'loss': 0.0941, 'learning_rate': 4.7017017213626636e-05, 'epoch': 10.21}
 69%|██████▉   | 1408/2048 [1:58:03<51:23,  4.82s/it] 69%|██████▉   | 1409/2048 [1:58:08<51:35,  4.84s/it] 69%|██████▉   | 1410/2048 [1:58:13<50:38,  4.76s/it] 69%|██████▉   | 1411/2048 [1:58:17<49:12,  4.63s/it] 69%|██████▉   | 1412/2048 [1:58:21<47:16,  4.46s/it] 69%|██████▉   | 1413/2048 [1:58:25<45:32,  4.30s/it] 69%|██████▉   | 1414/2048 [1:58:29<45:18,  4.29s/it] 69%|██████▉   | 1415/2048 [1:58:33<43:58,  4.17s/it] 69%|██████▉   | 1416/2048 [1:58:36<40:51,  3.88s/it] 69%|██████▉   | 1417/2048 [1:58:41<43:07,  4.10s/it] 69%|██████▉   | 1418/2048 [1:58:46<46:25,  4.42s/it] 69%|██████▉   | 1419/2048 [1:58:51<48:37,  4.64s/it] 69%|██████▉   | 1420/2048 [1:58:56<49:56,  4.77s/it] 69%|██████▉   | 1421/2048 [1:59:01<50:15,  4.81s/it] 69%|██████▉   | 1422/2048 [1:59:06<49:43,  4.77s/it] 69%|██████▉   | 1423/2048 [1:59:10<48:25,  4.65s/it] 70%|██████▉   | 1424/2048 [1:59:14<46:19,  4.45s/it]                                                     {'loss': 0.1181, 'learning_rate': 4.488767212230883e-05, 'epoch': 10.33}
 70%|██████▉   | 1424/2048 [1:59:14<46:19,  4.45s/it] 70%|██████▉   | 1425/2048 [1:59:18<44:04,  4.24s/it] 70%|██████▉   | 1426/2048 [1:59:22<41:54,  4.04s/it] 70%|██████▉   | 1427/2048 [1:59:26<41:56,  4.05s/it] 70%|██████▉   | 1428/2048 [1:59:29<40:16,  3.90s/it] 70%|██████▉   | 1429/2048 [1:59:33<39:20,  3.81s/it] 70%|██████▉   | 1430/2048 [1:59:38<43:28,  4.22s/it] 70%|██████▉   | 1431/2048 [1:59:43<46:19,  4.51s/it] 70%|██████▉   | 1432/2048 [1:59:48<48:14,  4.70s/it] 70%|██████▉   | 1433/2048 [1:59:53<49:15,  4.81s/it] 70%|███████   | 1434/2048 [1:59:58<49:27,  4.83s/it] 70%|███████   | 1435/2048 [2:00:03<48:28,  4.74s/it] 70%|███████   | 1436/2048 [2:00:07<46:45,  4.58s/it] 70%|███████   | 1437/2048 [2:00:11<45:27,  4.46s/it] 70%|███████   | 1438/2048 [2:00:15<43:37,  4.29s/it] 70%|███████   | 1439/2048 [2:00:19<41:30,  4.09s/it] 70%|███████   | 1440/2048 [2:00:23<41:03,  4.05s/it]                                                     {'loss': 0.0953, 'learning_rate': 4.279362963928665e-05, 'epoch': 10.44}
 70%|███████   | 1440/2048 [2:00:23<41:03,  4.05s/it] 70%|███████   | 1441/2048 [2:00:26<38:24,  3.80s/it] 70%|███████   | 1442/2048 [2:00:30<40:47,  4.04s/it] 70%|███████   | 1443/2048 [2:00:36<44:08,  4.38s/it] 71%|███████   | 1444/2048 [2:00:41<46:25,  4.61s/it] 71%|███████   | 1445/2048 [2:00:46<47:50,  4.76s/it] 71%|███████   | 1446/2048 [2:00:51<48:22,  4.82s/it] 71%|███████   | 1447/2048 [2:00:56<47:55,  4.78s/it] 71%|███████   | 1448/2048 [2:01:00<46:32,  4.65s/it] 71%|███████   | 1449/2048 [2:01:04<44:37,  4.47s/it] 71%|███████   | 1450/2048 [2:01:08<42:53,  4.30s/it] 71%|███████   | 1451/2048 [2:01:12<42:02,  4.23s/it] 71%|███████   | 1452/2048 [2:01:16<40:19,  4.06s/it] 71%|███████   | 1453/2048 [2:01:19<38:53,  3.92s/it] 71%|███████   | 1454/2048 [2:01:23<37:52,  3.83s/it] 71%|███████   | 1455/2048 [2:01:28<41:48,  4.23s/it] 71%|███████   | 1456/2048 [2:01:33<44:30,  4.51s/it]                                                     {'loss': 0.092, 'learning_rate': 4.073623111892458e-05, 'epoch': 10.56}
 71%|███████   | 1456/2048 [2:01:33<44:30,  4.51s/it] 71%|███████   | 1457/2048 [2:01:38<46:20,  4.70s/it] 71%|███████   | 1458/2048 [2:01:43<47:20,  4.81s/it] 71%|███████   | 1459/2048 [2:01:48<47:30,  4.84s/it] 71%|███████▏  | 1460/2048 [2:01:53<46:37,  4.76s/it] 71%|███████▏  | 1461/2048 [2:01:57<45:34,  4.66s/it] 71%|███████▏  | 1462/2048 [2:02:01<43:52,  4.49s/it] 71%|███████▏  | 1463/2048 [2:02:05<41:50,  4.29s/it] 71%|███████▏  | 1464/2048 [2:02:09<40:37,  4.17s/it] 72%|███████▏  | 1465/2048 [2:02:13<40:11,  4.14s/it] 72%|███████▏  | 1466/2048 [2:02:16<37:51,  3.90s/it] 72%|███████▏  | 1467/2048 [2:02:21<39:52,  4.12s/it] 72%|███████▏  | 1468/2048 [2:02:26<42:50,  4.43s/it] 72%|███████▏  | 1469/2048 [2:02:31<44:51,  4.65s/it] 72%|███████▏  | 1470/2048 [2:02:36<46:02,  4.78s/it] 72%|███████▏  | 1471/2048 [2:02:41<46:28,  4.83s/it] 72%|███████▏  | 1472/2048 [2:02:46<46:16,  4.82s/it]                                                     {'loss': 0.1082, 'learning_rate': 3.871679444302635e-05, 'epoch': 10.68}
 72%|███████▏  | 1472/2048 [2:02:46<46:16,  4.82s/it] 72%|███████▏  | 1473/2048 [2:02:51<45:37,  4.76s/it] 72%|███████▏  | 1474/2048 [2:02:55<44:29,  4.65s/it] 72%|███████▏  | 1475/2048 [2:02:59<42:25,  4.44s/it] 72%|███████▏  | 1476/2048 [2:03:03<41:34,  4.36s/it] 72%|███████▏  | 1477/2048 [2:03:07<39:56,  4.20s/it] 72%|███████▏  | 1478/2048 [2:03:11<37:56,  3.99s/it] 72%|███████▏  | 1479/2048 [2:03:14<36:45,  3.88s/it] 72%|███████▏  | 1480/2048 [2:03:19<40:21,  4.26s/it] 72%|███████▏  | 1481/2048 [2:03:25<42:51,  4.54s/it] 72%|███████▏  | 1482/2048 [2:03:30<44:31,  4.72s/it] 72%|███████▏  | 1483/2048 [2:03:35<45:11,  4.80s/it] 72%|███████▏  | 1484/2048 [2:03:40<45:05,  4.80s/it] 73%|███████▎  | 1485/2048 [2:03:44<44:24,  4.73s/it] 73%|███████▎  | 1486/2048 [2:03:48<43:04,  4.60s/it] 73%|███████▎  | 1487/2048 [2:03:52<41:22,  4.43s/it] 73%|███████▎  | 1488/2048 [2:03:57<40:16,  4.32s/it]                                                     {'loss': 0.1238, 'learning_rate': 3.673661317665612e-05, 'epoch': 10.79}
 73%|███████▎  | 1488/2048 [2:03:57<40:16,  4.32s/it] 73%|███████▎  | 1489/2048 [2:04:01<39:34,  4.25s/it] 73%|███████▎  | 1490/2048 [2:04:05<38:40,  4.16s/it] 73%|███████▎  | 1491/2048 [2:04:08<36:19,  3.91s/it] 73%|███████▎  | 1492/2048 [2:04:13<38:11,  4.12s/it] 73%|███████▎  | 1493/2048 [2:04:18<41:01,  4.44s/it] 73%|███████▎  | 1494/2048 [2:04:23<42:55,  4.65s/it] 73%|███████▎  | 1495/2048 [2:04:28<44:07,  4.79s/it] 73%|███████▎  | 1496/2048 [2:04:33<44:13,  4.81s/it] 73%|███████▎  | 1497/2048 [2:04:37<43:31,  4.74s/it] 73%|███████▎  | 1498/2048 [2:04:42<43:06,  4.70s/it] 73%|███████▎  | 1499/2048 [2:04:46<41:19,  4.52s/it] 73%|███████▎  | 1500/2048 [2:04:50<39:49,  4.36s/it] 73%|███████▎  | 1501/2048 [2:04:54<38:11,  4.19s/it] 73%|███████▎  | 1502/2048 [2:04:58<37:44,  4.15s/it] 73%|███████▎  | 1503/2048 [2:05:01<35:52,  3.95s/it] 73%|███████▎  | 1504/2048 [2:05:05<34:43,  3.83s/it]                                                     {'loss': 0.0983, 'learning_rate': 3.4796955739535795e-05, 'epoch': 10.91}
 73%|███████▎  | 1504/2048 [2:05:05<34:43,  3.83s/it] 73%|███████▎  | 1505/2048 [2:05:10<38:17,  4.23s/it] 74%|███████▎  | 1506/2048 [2:05:15<40:45,  4.51s/it] 74%|███████▎  | 1507/2048 [2:05:20<42:22,  4.70s/it] 74%|███████▎  | 1508/2048 [2:05:25<43:10,  4.80s/it] 74%|███████▎  | 1509/2048 [2:05:30<42:50,  4.77s/it] 74%|███████▎  | 1510/2048 [2:05:35<42:08,  4.70s/it] 74%|███████▍  | 1511/2048 [2:05:39<41:01,  4.58s/it] 74%|███████▍  | 1512/2048 [2:05:43<39:17,  4.40s/it] 74%|███████▍  | 1513/2048 [2:05:47<38:38,  4.33s/it] 74%|███████▍  | 1514/2048 [2:05:51<37:32,  4.22s/it] 74%|███████▍  | 1515/2048 [2:05:55<35:36,  4.01s/it] 74%|███████▍  | 1516/2048 [2:05:58<33:27,  3.77s/it] 74%|███████▍  | 1517/2048 [2:06:02<35:26,  4.00s/it] 74%|███████▍  | 1518/2048 [2:06:08<38:27,  4.35s/it] 74%|███████▍  | 1519/2048 [2:06:13<40:31,  4.60s/it] 74%|███████▍  | 1520/2048 [2:06:18<41:53,  4.76s/it]                                                     {'loss': 0.0848, 'learning_rate': 3.289906459354948e-05, 'epoch': 11.02}
 74%|███████▍  | 1520/2048 [2:06:18<41:53,  4.76s/it] 74%|███████▍  | 1521/2048 [2:06:23<42:36,  4.85s/it] 74%|███████▍  | 1522/2048 [2:06:28<42:13,  4.82s/it] 74%|███████▍  | 1523/2048 [2:06:32<41:24,  4.73s/it] 74%|███████▍  | 1524/2048 [2:06:36<40:08,  4.60s/it] 74%|███████▍  | 1525/2048 [2:06:41<39:06,  4.49s/it] 75%|███████▍  | 1526/2048 [2:06:44<37:12,  4.28s/it] 75%|███████▍  | 1527/2048 [2:06:48<36:24,  4.19s/it] 75%|███████▍  | 1528/2048 [2:06:52<34:36,  3.99s/it] 75%|███████▍  | 1529/2048 [2:06:55<32:28,  3.75s/it] 75%|███████▍  | 1530/2048 [2:07:00<35:17,  4.09s/it] 75%|███████▍  | 1531/2048 [2:07:05<38:00,  4.41s/it] 75%|███████▍  | 1532/2048 [2:07:10<39:50,  4.63s/it] 75%|███████▍  | 1533/2048 [2:07:15<40:52,  4.76s/it] 75%|███████▍  | 1534/2048 [2:07:20<41:24,  4.83s/it] 75%|███████▍  | 1535/2048 [2:07:25<40:57,  4.79s/it] 75%|███████▌  | 1536/2048 [2:07:30<39:48,  4.66s/it]                                                     {'loss': 0.1032, 'learning_rate': 3.104415544687519e-05, 'epoch': 11.14}
 75%|███████▌  | 1536/2048 [2:07:30<39:48,  4.66s/it]
  0%|          | 0/256 [00:00<?, ?it/s][A
  2%|▏         | 4/256 [00:00<00:09, 27.48it/s][A
  3%|▎         | 7/256 [00:00<00:11, 21.17it/s][A
  4%|▍         | 10/256 [00:00<00:12, 19.38it/s][A
  5%|▍         | 12/256 [00:00<00:13, 18.43it/s][A
  5%|▌         | 14/256 [00:00<00:13, 17.87it/s][A
  6%|▋         | 16/256 [00:00<00:13, 18.22it/s][A
  7%|▋         | 18/256 [00:00<00:13, 17.82it/s][A
  8%|▊         | 21/256 [00:01<00:12, 18.55it/s][A
  9%|▉         | 23/256 [00:01<00:12, 18.04it/s][A
 10%|▉         | 25/256 [00:01<00:13, 17.69it/s][A
 11%|█         | 27/256 [00:01<00:12, 17.77it/s][A
 11%|█▏        | 29/256 [00:01<00:12, 17.47it/s][A
 12%|█▏        | 31/256 [00:01<00:13, 17.28it/s][A
 13%|█▎        | 34/256 [00:01<00:11, 18.73it/s][A
 14%|█▍        | 36/256 [00:01<00:12, 18.17it/s][A
 15%|█▍        | 38/256 [00:02<00:12, 17.64it/s][A
 16%|█▌        | 40/256 [00:02<00:12, 17.38it/s][A
 16%|█▋        | 42/256 [00:02<00:12, 17.75it/s][A
 17%|█▋        | 44/256 [00:02<00:12, 17.50it/s][A
 18%|█▊        | 46/256 [00:02<00:12, 17.29it/s][A
 19%|█▉        | 48/256 [00:02<00:11, 17.39it/s][A
 20%|█▉        | 50/256 [00:02<00:12, 17.09it/s][A
 20%|██        | 52/256 [00:02<00:12, 17.00it/s][A
 21%|██        | 54/256 [00:02<00:11, 17.78it/s][A
 22%|██▏       | 56/256 [00:03<00:11, 17.60it/s][A
 23%|██▎       | 58/256 [00:03<00:11, 17.52it/s][A
 23%|██▎       | 60/256 [00:03<00:11, 17.29it/s][A
 24%|██▍       | 62/256 [00:03<00:11, 16.96it/s][A
 25%|██▌       | 64/256 [00:03<00:11, 17.25it/s][A
 26%|██▌       | 66/256 [00:03<00:11, 17.10it/s][A
 27%|██▋       | 68/256 [00:03<00:11, 17.01it/s][A
 27%|██▋       | 70/256 [00:03<00:10, 17.15it/s][A
 28%|██▊       | 72/256 [00:04<00:10, 17.03it/s][A
 29%|██▉       | 74/256 [00:04<00:10, 16.99it/s][A
 30%|██▉       | 76/256 [00:04<00:10, 17.01it/s][A
 30%|███       | 78/256 [00:04<00:10, 16.98it/s][A
 31%|███▏      | 80/256 [00:04<00:10, 16.93it/s][A
 32%|███▏      | 82/256 [00:04<00:10, 17.00it/s][A
 33%|███▎      | 84/256 [00:04<00:10, 17.16it/s][A
 34%|███▎      | 86/256 [00:04<00:09, 17.06it/s][A
 34%|███▍      | 88/256 [00:04<00:09, 17.29it/s][A
 35%|███▌      | 90/256 [00:05<00:09, 17.03it/s][A
 36%|███▌      | 92/256 [00:05<00:09, 16.93it/s][A
 37%|███▋      | 94/256 [00:05<00:09, 17.01it/s][A
 38%|███▊      | 96/256 [00:05<00:09, 16.92it/s][A
 39%|███▊      | 99/256 [00:05<00:08, 17.75it/s][A
 39%|███▉      | 101/256 [00:05<00:08, 17.38it/s][A
 40%|████      | 103/256 [00:05<00:08, 17.49it/s][A
 41%|████▏     | 106/256 [00:06<00:08, 18.35it/s][A
 42%|████▏     | 108/256 [00:06<00:08, 17.97it/s][A
 43%|████▎     | 110/256 [00:06<00:08, 17.75it/s][A
 44%|████▍     | 112/256 [00:06<00:08, 17.46it/s][A
 45%|████▍     | 114/256 [00:06<00:08, 17.59it/s][A
 46%|████▌     | 117/256 [00:06<00:07, 18.04it/s][A
 46%|████▋     | 119/256 [00:06<00:07, 17.83it/s][A
 47%|████▋     | 121/256 [00:06<00:07, 17.46it/s][A
 48%|████▊     | 123/256 [00:06<00:07, 17.33it/s][A
 49%|████▉     | 125/256 [00:07<00:07, 17.16it/s][A
 50%|█████     | 128/256 [00:07<00:06, 18.67it/s][A
 51%|█████     | 130/256 [00:07<00:06, 18.24it/s][A
 52%|█████▏    | 132/256 [00:07<00:07, 17.69it/s][A
 52%|█████▏    | 134/256 [00:07<00:06, 17.55it/s][A
 53%|█████▎    | 136/256 [00:07<00:06, 17.39it/s][A
 54%|█████▍    | 138/256 [00:07<00:06, 17.20it/s][A
 55%|█████▍    | 140/256 [00:07<00:06, 17.07it/s][A
 56%|█████▌    | 143/256 [00:08<00:06, 18.21it/s][A
 57%|█████▋    | 145/256 [00:08<00:06, 17.65it/s][A
 57%|█████▋    | 147/256 [00:08<00:06, 17.44it/s][A
 58%|█████▊    | 149/256 [00:08<00:06, 17.25it/s][A
 59%|█████▉    | 151/256 [00:08<00:06, 17.11it/s][A
 60%|██████    | 154/256 [00:08<00:05, 17.73it/s][A
 61%|██████    | 156/256 [00:08<00:05, 17.59it/s][A
 62%|██████▏   | 158/256 [00:08<00:05, 17.23it/s][A
 63%|██████▎   | 161/256 [00:09<00:05, 17.98it/s][A
 64%|██████▎   | 163/256 [00:09<00:05, 17.53it/s][A
 64%|██████▍   | 165/256 [00:09<00:05, 17.44it/s][A
 65%|██████▌   | 167/256 [00:09<00:05, 17.08it/s][A
 66%|██████▌   | 169/256 [00:09<00:05, 17.33it/s][A
 67%|██████▋   | 171/256 [00:09<00:04, 17.08it/s][A
 68%|██████▊   | 173/256 [00:09<00:04, 16.96it/s][A
 68%|██████▊   | 175/256 [00:09<00:04, 17.01it/s][A
 69%|██████▉   | 177/256 [00:10<00:04, 16.84it/s][A
 70%|██████▉   | 179/256 [00:10<00:04, 16.92it/s][A
 71%|███████   | 182/256 [00:10<00:03, 19.23it/s][A
 72%|███████▏  | 184/256 [00:10<00:03, 18.61it/s][A
 73%|███████▎  | 186/256 [00:10<00:03, 17.95it/s][A
 74%|███████▍  | 189/256 [00:10<00:03, 18.44it/s][A
 75%|███████▍  | 191/256 [00:10<00:03, 17.85it/s][A
 75%|███████▌  | 193/256 [00:10<00:03, 17.68it/s][A
 76%|███████▌  | 195/256 [00:11<00:03, 17.28it/s][A
 77%|███████▋  | 197/256 [00:11<00:03, 17.58it/s][A
 78%|███████▊  | 199/256 [00:11<00:03, 18.18it/s][A
 79%|███████▊  | 201/256 [00:11<00:03, 17.87it/s][A
 79%|███████▉  | 203/256 [00:11<00:03, 17.41it/s][A
 80%|████████  | 205/256 [00:11<00:02, 17.35it/s][A
 81%|████████  | 207/256 [00:11<00:02, 17.05it/s][A
 82%|████████▏ | 209/256 [00:11<00:02, 16.95it/s][A
 82%|████████▏ | 211/256 [00:11<00:02, 16.92it/s][A
 83%|████████▎ | 213/256 [00:12<00:02, 16.88it/s][A
 84%|████████▍ | 215/256 [00:12<00:02, 16.85it/s][A
 85%|████████▍ | 217/256 [00:12<00:02, 16.84it/s][A
 86%|████████▌ | 219/256 [00:12<00:02, 16.94it/s][A
 86%|████████▋ | 221/256 [00:12<00:02, 16.78it/s][A
 87%|████████▋ | 223/256 [00:12<00:01, 16.78it/s][A
 88%|████████▊ | 225/256 [00:12<00:01, 17.21it/s][A
 89%|████████▊ | 227/256 [00:12<00:01, 16.95it/s][A
 89%|████████▉ | 229/256 [00:13<00:01, 16.86it/s][A
 90%|█████████ | 231/256 [00:13<00:01, 17.09it/s][A
 91%|█████████ | 233/256 [00:13<00:01, 17.06it/s][A
 92%|█████████▏| 235/256 [00:13<00:01, 17.03it/s][A
 93%|█████████▎| 238/256 [00:13<00:01, 17.95it/s][A
 94%|█████████▍| 240/256 [00:13<00:00, 17.74it/s][A
 95%|█████████▍| 242/256 [00:13<00:00, 17.46it/s][A
 95%|█████████▌| 244/256 [00:13<00:00, 17.78it/s][A
 96%|█████████▋| 247/256 [00:14<00:00, 18.28it/s][A
 97%|█████████▋| 249/256 [00:14<00:00, 17.96it/s][A
 98%|█████████▊| 252/256 [00:14<00:00, 19.04it/s][A
 99%|█████████▉| 254/256 [00:14<00:00, 18.55it/s][A
100%|██████████| 256/256 [00:14<00:00, 17.02it/s][A                                                     
                                                 [A{'eval_loss': 2.1634905338287354, 'eval_runtime': 14.7444, 'eval_samples_per_second': 69.45, 'eval_steps_per_second': 17.362, 'epoch': 11.14}
 75%|███████▌  | 1536/2048 [2:07:44<39:48,  4.66s/it]
100%|██████████| 256/256 [00:14<00:00, 17.02it/s][A
                                                 [A  0%|          | 0/383 [00:00<?, ?it/s]  0%|          | 0/383 [00:00<?, ?it/s]  0%|          | 0/383 [00:00<?, ?it/s]
  0%|          | 0/383 [00:00<?, ?it/s][A  0%|          | 1/383 [00:00<00:41,  9.24it/s]
  1%|          | 2/383 [00:00<00:25, 14.83it/s]  0%|          | 1/383 [00:00<00:51,  7.44it/s][A  1%|          | 2/383 [00:00<00:26, 14.21it/s]  1%|          | 3/383 [00:00<00:24, 15.25it/s]
  1%|          | 4/383 [00:00<00:23, 16.37it/s][A  1%|▏         | 5/383 [00:00<00:19, 19.47it/s]  1%|▏         | 5/383 [00:00<00:20, 18.50it/s]  1%|▏         | 5/383 [00:00<00:21, 17.29it/s]  2%|▏         | 7/383 [00:00<00:20, 17.96it/s]  2%|▏         | 7/383 [00:00<00:21, 17.23it/s]
  2%|▏         | 7/383 [00:00<00:20, 18.70it/s][A  2%|▏         | 7/383 [00:00<00:22, 16.72it/s]  2%|▏         | 9/383 [00:00<00:23, 16.12it/s]  2%|▏         | 9/383 [00:00<00:24, 15.49it/s]
  2%|▏         | 9/383 [00:00<00:23, 16.23it/s][A  2%|▏         | 9/383 [00:00<00:23, 15.99it/s]  3%|▎         | 11/383 [00:00<00:23, 15.55it/s]
  3%|▎         | 11/383 [00:00<00:24, 15.48it/s][A  3%|▎         | 11/383 [00:00<00:25, 14.67it/s]  3%|▎         | 11/383 [00:00<00:24, 15.02it/s]  3%|▎         | 13/383 [00:00<00:24, 15.08it/s]  3%|▎         | 13/383 [00:00<00:24, 15.33it/s]
  3%|▎         | 13/383 [00:00<00:24, 14.88it/s][A  3%|▎         | 13/383 [00:00<00:25, 14.46it/s]  4%|▍         | 15/383 [00:00<00:22, 16.14it/s]  4%|▍         | 15/383 [00:00<00:22, 16.25it/s]
  4%|▍         | 15/383 [00:00<00:23, 15.92it/s][A  4%|▍         | 15/383 [00:00<00:23, 15.48it/s]  4%|▍         | 17/383 [00:01<00:21, 16.78it/s]  4%|▍         | 17/383 [00:01<00:21, 16.99it/s]
  4%|▍         | 17/383 [00:01<00:21, 16.82it/s][A  4%|▍         | 17/383 [00:01<00:22, 16.27it/s]  5%|▍         | 19/383 [00:01<00:20, 17.36it/s]  5%|▍         | 19/383 [00:01<00:20, 17.53it/s]
  5%|▍         | 19/383 [00:01<00:20, 17.44it/s][A  5%|▍         | 19/383 [00:01<00:22, 16.54it/s]  5%|▌         | 21/383 [00:01<00:20, 17.37it/s]
  5%|▌         | 21/383 [00:01<00:20, 17.73it/s][A  5%|▌         | 21/383 [00:01<00:20, 17.69it/s]  5%|▌         | 21/383 [00:01<00:21, 16.83it/s]  6%|▌         | 23/383 [00:01<00:20, 17.23it/s]
  6%|▌         | 23/383 [00:01<00:20, 17.62it/s][A  6%|▌         | 23/383 [00:01<00:20, 17.36it/s]  6%|▌         | 23/383 [00:01<00:21, 17.03it/s]  7%|▋         | 25/383 [00:01<00:21, 16.75it/s]
  7%|▋         | 25/383 [00:01<00:20, 17.40it/s][A  7%|▋         | 25/383 [00:01<00:21, 17.01it/s]  7%|▋         | 25/383 [00:01<00:21, 16.74it/s]
  7%|▋         | 27/383 [00:01<00:20, 17.12it/s][A  7%|▋         | 27/383 [00:01<00:24, 14.62it/s]  7%|▋         | 27/383 [00:01<00:24, 14.69it/s]  7%|▋         | 27/383 [00:01<00:24, 14.44it/s]
  8%|▊         | 29/383 [00:01<00:25, 13.70it/s][A  8%|▊         | 29/383 [00:01<00:27, 12.84it/s]  8%|▊         | 29/383 [00:01<00:27, 12.83it/s]  8%|▊         | 29/383 [00:01<00:28, 12.46it/s]
  8%|▊         | 31/383 [00:01<00:25, 13.60it/s][A  8%|▊         | 31/383 [00:02<00:27, 13.00it/s]  8%|▊         | 31/383 [00:02<00:26, 13.09it/s]  8%|▊         | 31/383 [00:02<00:26, 13.19it/s]
  9%|▊         | 33/383 [00:02<00:25, 13.79it/s][A  9%|▊         | 33/383 [00:02<00:25, 13.69it/s]  9%|▊         | 33/383 [00:02<00:25, 13.80it/s]  9%|▊         | 33/383 [00:02<00:24, 14.06it/s]
  9%|▉         | 35/383 [00:02<00:23, 14.52it/s][A  9%|▉         | 35/383 [00:02<00:24, 14.12it/s]  9%|▉         | 35/383 [00:02<00:24, 14.37it/s]  9%|▉         | 35/383 [00:02<00:23, 14.66it/s]
 10%|▉         | 37/383 [00:02<00:23, 14.83it/s][A 10%|▉         | 37/383 [00:02<00:23, 14.93it/s] 10%|▉         | 37/383 [00:02<00:23, 14.97it/s] 10%|▉         | 37/383 [00:02<00:22, 15.24it/s]
 10%|█         | 39/383 [00:02<00:22, 15.35it/s][A 10%|█         | 39/383 [00:02<00:22, 15.48it/s] 10%|█         | 39/383 [00:02<00:22, 15.56it/s] 10%|█         | 39/383 [00:02<00:21, 15.73it/s]
 11%|█         | 41/383 [00:02<00:21, 15.97it/s][A 11%|█         | 41/383 [00:02<00:21, 15.91it/s] 11%|█         | 41/383 [00:02<00:21, 16.04it/s] 11%|█         | 41/383 [00:02<00:21, 15.89it/s]
 11%|█▏        | 44/383 [00:02<00:18, 17.86it/s][A 11%|█         | 43/383 [00:02<00:20, 16.91it/s] 11%|█▏        | 44/383 [00:02<00:18, 18.35it/s] 11%|█         | 43/383 [00:02<00:20, 16.53it/s]
 12%|█▏        | 47/383 [00:02<00:16, 20.05it/s][A 12%|█▏        | 46/383 [00:02<00:17, 19.55it/s] 12%|█▏        | 47/383 [00:02<00:16, 20.62it/s] 12%|█▏        | 46/383 [00:02<00:17, 19.18it/s] 13%|█▎        | 49/383 [00:02<00:15, 21.46it/s]
 13%|█▎        | 50/383 [00:02<00:15, 21.61it/s][A 13%|█▎        | 49/383 [00:03<00:15, 21.22it/s] 13%|█▎        | 50/383 [00:03<00:16, 20.80it/s]
 14%|█▍        | 53/383 [00:03<00:17, 18.42it/s][A 14%|█▍        | 53/383 [00:03<00:17, 18.46it/s] 14%|█▎        | 52/383 [00:03<00:18, 17.44it/s] 14%|█▎        | 52/383 [00:03<00:18, 18.15it/s]
 14%|█▍        | 55/383 [00:03<00:17, 18.33it/s][A 14%|█▍        | 54/383 [00:03<00:18, 17.76it/s] 14%|█▍        | 55/383 [00:03<00:17, 18.37it/s] 14%|█▍        | 54/383 [00:03<00:18, 17.90it/s]
 15%|█▍        | 57/383 [00:03<00:18, 18.03it/s][A 15%|█▍        | 56/383 [00:03<00:18, 17.99it/s] 15%|█▍        | 57/383 [00:03<00:17, 18.23it/s] 15%|█▍        | 56/383 [00:03<00:18, 17.98it/s]
 15%|█▌        | 59/383 [00:03<00:18, 17.17it/s][A 15%|█▌        | 58/383 [00:03<00:18, 17.40it/s] 15%|█▌        | 59/383 [00:03<00:18, 17.18it/s] 15%|█▌        | 58/383 [00:03<00:18, 17.23it/s] 16%|█▌        | 60/383 [00:03<00:19, 16.83it/s]
 16%|█▌        | 61/383 [00:03<00:19, 16.20it/s][A 16%|█▌        | 61/383 [00:03<00:19, 16.76it/s] 16%|█▌        | 60/383 [00:03<00:19, 16.86it/s]
 16%|█▋        | 63/383 [00:03<00:19, 16.16it/s][A 16%|█▌        | 62/383 [00:03<00:19, 16.09it/s] 16%|█▋        | 63/383 [00:03<00:19, 16.39it/s] 16%|█▌        | 62/383 [00:03<00:19, 16.25it/s]
 17%|█▋        | 65/383 [00:03<00:19, 16.14it/s][A 17%|█▋        | 64/383 [00:03<00:19, 15.98it/s] 17%|█▋        | 64/383 [00:03<00:19, 16.27it/s] 17%|█▋        | 65/383 [00:03<00:19, 15.91it/s] 17%|█▋        | 66/383 [00:04<00:19, 16.26it/s]
 17%|█▋        | 67/383 [00:04<00:20, 15.75it/s][A 17%|█▋        | 66/383 [00:04<00:20, 15.73it/s] 17%|█▋        | 67/383 [00:04<00:20, 15.68it/s] 18%|█▊        | 68/383 [00:04<00:20, 15.66it/s] 18%|█▊        | 68/383 [00:04<00:20, 15.04it/s]
 18%|█▊        | 69/383 [00:04<00:21, 14.95it/s][A 18%|█▊        | 69/383 [00:04<00:20, 14.95it/s]
 19%|█▊        | 71/383 [00:04<00:20, 15.11it/s][A 18%|█▊        | 70/383 [00:04<00:20, 15.06it/s] 19%|█▊        | 71/383 [00:04<00:20, 15.11it/s] 18%|█▊        | 70/383 [00:04<00:21, 14.63it/s]
 19%|█▉        | 73/383 [00:04<00:19, 16.07it/s][A 19%|█▉        | 72/383 [00:04<00:19, 15.73it/s] 19%|█▉        | 72/383 [00:04<00:20, 15.53it/s] 19%|█▉        | 73/383 [00:04<00:19, 15.71it/s] 19%|█▉        | 74/383 [00:04<00:19, 16.23it/s]
 20%|█▉        | 75/383 [00:04<00:19, 15.94it/s][A 20%|█▉        | 75/383 [00:04<00:19, 15.70it/s] 19%|█▉        | 74/383 [00:04<00:20, 15.29it/s] 20%|█▉        | 76/383 [00:04<00:18, 16.28it/s]
 20%|██        | 77/383 [00:04<00:19, 15.96it/s][A 20%|██        | 77/383 [00:04<00:19, 15.82it/s] 20%|█▉        | 76/383 [00:04<00:19, 15.65it/s] 20%|██        | 78/383 [00:04<00:18, 16.18it/s]
 21%|██        | 79/383 [00:04<00:19, 15.95it/s][A 20%|██        | 78/383 [00:04<00:19, 15.82it/s] 21%|██        | 79/383 [00:04<00:19, 15.84it/s] 21%|██        | 80/383 [00:04<00:18, 16.11it/s]
 21%|██        | 81/383 [00:04<00:19, 15.75it/s][A 21%|██        | 80/383 [00:04<00:19, 15.75it/s] 21%|██        | 81/383 [00:04<00:19, 15.60it/s] 21%|██▏       | 82/383 [00:05<00:18, 15.97it/s]
 22%|██▏       | 83/383 [00:05<00:18, 15.93it/s][A 21%|██▏       | 82/383 [00:05<00:18, 15.96it/s] 22%|██▏       | 83/383 [00:05<00:19, 15.75it/s] 22%|██▏       | 84/383 [00:05<00:18, 16.00it/s]
 22%|██▏       | 85/383 [00:05<00:18, 15.72it/s][A 22%|██▏       | 84/383 [00:05<00:18, 16.11it/s] 22%|██▏       | 85/383 [00:05<00:18, 16.06it/s] 22%|██▏       | 86/383 [00:05<00:18, 16.25it/s] 22%|██▏       | 86/383 [00:05<00:18, 16.08it/s]
 23%|██▎       | 87/383 [00:05<00:20, 14.25it/s][A 23%|██▎       | 87/383 [00:05<00:20, 14.18it/s] 23%|██▎       | 88/383 [00:05<00:22, 13.02it/s] 23%|██▎       | 88/383 [00:05<00:23, 12.53it/s]
 23%|██▎       | 89/383 [00:05<00:24, 11.83it/s][A 23%|██▎       | 89/383 [00:05<00:32,  9.02it/s] 23%|██▎       | 90/383 [00:05<00:34,  8.61it/s]
 24%|██▍       | 91/383 [00:06<00:39,  7.40it/s][A 23%|██▎       | 90/383 [00:06<00:42,  6.90it/s] 24%|██▍       | 91/383 [00:06<00:48,  6.02it/s] 24%|██▍       | 92/383 [00:06<00:48,  5.99it/s] 24%|██▍       | 92/383 [00:06<00:47,  6.09it/s] 24%|██▍       | 92/383 [00:06<00:52,  5.56it/s]
 24%|██▍       | 93/383 [00:06<00:50,  5.71it/s][A 25%|██▍       | 94/383 [00:06<00:39,  7.30it/s]
 25%|██▍       | 95/383 [00:06<00:39,  7.27it/s][A 24%|██▍       | 93/383 [00:06<00:55,  5.26it/s] 25%|██▌       | 96/383 [00:06<00:31,  9.18it/s]
 25%|██▌       | 97/383 [00:06<00:31,  8.96it/s][A 24%|██▍       | 93/383 [00:06<00:53,  5.43it/s] 25%|██▍       | 95/383 [00:06<00:41,  6.94it/s] 26%|██▌       | 98/383 [00:06<00:25, 11.06it/s]
 26%|██▌       | 99/383 [00:06<00:26, 10.73it/s][A 25%|██▍       | 95/383 [00:06<00:40,  7.17it/s] 25%|██▌       | 97/383 [00:07<00:32,  8.79it/s] 26%|██▌       | 100/383 [00:07<00:22, 12.40it/s] 25%|██▌       | 97/383 [00:07<00:31,  9.01it/s]
 26%|██▋       | 101/383 [00:07<00:23, 11.98it/s][A 26%|██▌       | 99/383 [00:07<00:27, 10.49it/s] 27%|██▋       | 102/383 [00:07<00:20, 13.77it/s] 26%|██▌       | 99/383 [00:07<00:26, 10.70it/s]
 27%|██▋       | 103/383 [00:07<00:21, 13.25it/s][A 26%|██▋       | 101/383 [00:07<00:23, 11.81it/s] 27%|██▋       | 104/383 [00:07<00:18, 15.03it/s]
 27%|██▋       | 105/383 [00:07<00:19, 14.54it/s][A 26%|██▋       | 101/383 [00:07<00:23, 12.11it/s] 27%|██▋       | 103/383 [00:07<00:21, 13.09it/s] 28%|██▊       | 106/383 [00:07<00:17, 16.15it/s]
 28%|██▊       | 107/383 [00:07<00:17, 15.78it/s][A 27%|██▋       | 103/383 [00:07<00:20, 13.49it/s] 27%|██▋       | 105/383 [00:07<00:19, 14.34it/s] 28%|██▊       | 108/383 [00:07<00:16, 16.87it/s]
 28%|██▊       | 109/383 [00:07<00:16, 16.83it/s][A 27%|██▋       | 105/383 [00:07<00:18, 14.82it/s] 29%|██▊       | 110/383 [00:07<00:15, 17.55it/s] 28%|██▊       | 107/383 [00:07<00:17, 15.47it/s]
 29%|██▉       | 111/383 [00:07<00:15, 17.39it/s][A 28%|██▊       | 107/383 [00:07<00:17, 15.99it/s] 29%|██▉       | 112/383 [00:07<00:15, 18.05it/s] 28%|██▊       | 109/383 [00:07<00:16, 16.42it/s]
 30%|██▉       | 113/383 [00:07<00:15, 17.88it/s][A 28%|██▊       | 109/383 [00:07<00:16, 16.88it/s] 30%|██▉       | 114/383 [00:07<00:14, 18.44it/s] 29%|██▉       | 111/383 [00:07<00:15, 17.16it/s]
 30%|███       | 115/383 [00:07<00:14, 18.33it/s][A 29%|██▉       | 111/383 [00:07<00:15, 17.51it/s] 30%|██▉       | 113/383 [00:07<00:15, 17.86it/s] 30%|███       | 116/383 [00:07<00:15, 17.75it/s] 30%|██▉       | 113/383 [00:07<00:15, 17.90it/s]
 31%|███       | 117/383 [00:07<00:15, 17.70it/s][A 30%|███       | 115/383 [00:08<00:15, 17.81it/s] 30%|███       | 115/383 [00:08<00:15, 17.85it/s] 31%|███       | 118/383 [00:08<00:15, 17.21it/s]
 31%|███       | 119/383 [00:08<00:15, 17.24it/s][A 31%|███       | 117/383 [00:08<00:15, 17.01it/s] 31%|███▏      | 120/383 [00:08<00:15, 17.17it/s] 31%|███       | 117/383 [00:08<00:15, 17.39it/s]
 32%|███▏      | 121/383 [00:08<00:15, 17.06it/s][A 31%|███       | 119/383 [00:08<00:15, 16.87it/s] 32%|███▏      | 122/383 [00:08<00:15, 17.39it/s] 31%|███       | 119/383 [00:08<00:15, 16.93it/s]
 32%|███▏      | 123/383 [00:08<00:15, 17.12it/s][A 32%|███▏      | 124/383 [00:08<00:14, 18.08it/s] 32%|███▏      | 121/383 [00:08<00:15, 16.63it/s]
 33%|███▎      | 125/383 [00:08<00:14, 17.78it/s][A 32%|███▏      | 121/383 [00:08<00:15, 16.58it/s] 33%|███▎      | 126/383 [00:08<00:14, 18.35it/s]
 33%|███▎      | 127/383 [00:08<00:13, 18.39it/s][A 32%|███▏      | 123/383 [00:08<00:15, 16.93it/s] 32%|███▏      | 123/383 [00:08<00:14, 17.39it/s] 33%|███▎      | 128/383 [00:08<00:13, 18.70it/s] 33%|███▎      | 125/383 [00:08<00:14, 17.44it/s]
 34%|███▎      | 129/383 [00:08<00:14, 18.14it/s][A 33%|███▎      | 125/383 [00:08<00:14, 17.87it/s] 33%|███▎      | 127/383 [00:08<00:14, 17.87it/s] 34%|███▍      | 130/383 [00:08<00:14, 17.08it/s] 33%|███▎      | 127/383 [00:08<00:13, 18.30it/s]
 34%|███▍      | 131/383 [00:08<00:14, 16.98it/s][A 34%|███▎      | 129/383 [00:08<00:13, 18.21it/s] 34%|███▎      | 129/383 [00:08<00:14, 17.35it/s] 34%|███▍      | 132/383 [00:08<00:15, 16.53it/s]
 35%|███▍      | 133/383 [00:08<00:15, 16.56it/s][A 34%|███▍      | 131/383 [00:08<00:14, 17.61it/s] 34%|███▍      | 131/383 [00:08<00:14, 16.90it/s] 35%|███▍      | 134/383 [00:08<00:15, 16.48it/s]
 35%|███▌      | 135/383 [00:09<00:15, 16.46it/s][A 35%|███▍      | 133/383 [00:09<00:15, 16.58it/s] 35%|███▍      | 133/383 [00:09<00:14, 16.89it/s] 36%|███▌      | 136/383 [00:09<00:15, 16.31it/s]
 36%|███▌      | 137/383 [00:09<00:14, 16.45it/s][A 35%|███▌      | 135/383 [00:09<00:14, 16.67it/s] 35%|███▌      | 135/383 [00:09<00:15, 16.32it/s] 36%|███▌      | 138/383 [00:09<00:14, 16.42it/s]
 36%|███▋      | 139/383 [00:09<00:14, 16.36it/s][A 36%|███▌      | 137/383 [00:09<00:14, 16.48it/s] 37%|███▋      | 140/383 [00:09<00:14, 16.42it/s] 36%|███▌      | 137/383 [00:09<00:15, 16.04it/s]
 37%|███▋      | 141/383 [00:09<00:14, 16.37it/s][A 36%|███▋      | 139/383 [00:09<00:14, 16.57it/s] 37%|███▋      | 142/383 [00:09<00:14, 16.38it/s] 36%|███▋      | 139/383 [00:09<00:15, 15.99it/s]
 37%|███▋      | 143/383 [00:09<00:14, 16.23it/s][A 38%|███▊      | 144/383 [00:09<00:14, 16.44it/s] 37%|███▋      | 141/383 [00:09<00:14, 16.17it/s] 37%|███▋      | 141/383 [00:09<00:14, 16.18it/s]
 38%|███▊      | 145/383 [00:09<00:14, 16.16it/s][A 38%|███▊      | 146/383 [00:09<00:14, 16.40it/s] 37%|███▋      | 143/383 [00:09<00:14, 16.20it/s] 37%|███▋      | 143/383 [00:09<00:14, 16.24it/s]
 38%|███▊      | 147/383 [00:09<00:14, 16.27it/s][A 38%|███▊      | 145/383 [00:09<00:14, 16.16it/s] 38%|███▊      | 145/383 [00:09<00:14, 16.08it/s] 39%|███▊      | 148/383 [00:09<00:15, 14.92it/s]
 39%|███▉      | 149/383 [00:09<00:15, 14.63it/s][A 38%|███▊      | 147/383 [00:09<00:14, 16.18it/s] 38%|███▊      | 147/383 [00:09<00:14, 16.19it/s] 39%|███▉      | 150/383 [00:10<00:18, 12.59it/s]
 39%|███▉      | 151/383 [00:10<00:18, 12.41it/s][A 39%|███▉      | 149/383 [00:10<00:17, 13.40it/s] 39%|███▉      | 149/383 [00:10<00:17, 13.37it/s] 40%|███▉      | 152/383 [00:10<00:19, 11.75it/s]
 40%|███▉      | 153/383 [00:10<00:20, 11.42it/s][A 39%|███▉      | 151/383 [00:10<00:19, 12.01it/s] 39%|███▉      | 151/383 [00:10<00:19, 11.84it/s] 40%|███▉      | 153/383 [00:10<00:19, 11.50it/s] 40%|███▉      | 153/383 [00:10<00:21, 10.89it/s] 40%|████      | 154/383 [00:10<00:26,  8.60it/s]
 40%|████      | 155/383 [00:10<00:29,  7.66it/s][A 40%|████      | 155/383 [00:11<00:30,  7.41it/s]
 41%|████      | 156/383 [00:11<00:36,  6.18it/s][A 40%|████      | 155/383 [00:11<00:32,  7.11it/s] 41%|████      | 156/383 [00:11<00:36,  6.24it/s] 41%|████      | 156/383 [00:11<00:36,  6.30it/s] 41%|████      | 156/383 [00:11<00:36,  6.18it/s]
 41%|████      | 157/383 [00:11<00:42,  5.26it/s][A 41%|████      | 157/383 [00:11<00:39,  5.71it/s] 41%|████      | 157/383 [00:11<00:39,  5.73it/s] 41%|████      | 157/383 [00:11<00:41,  5.44it/s] 41%|████▏     | 158/383 [00:11<00:43,  5.19it/s]
 41%|████▏     | 158/383 [00:11<00:48,  4.62it/s][A 41%|████▏     | 158/383 [00:11<00:43,  5.11it/s] 42%|████▏     | 159/383 [00:11<00:42,  5.28it/s] 41%|████▏     | 158/383 [00:11<00:45,  4.98it/s]
 42%|████▏     | 159/383 [00:11<00:50,  4.40it/s][A 42%|████▏     | 159/383 [00:12<00:42,  5.24it/s] 42%|████▏     | 160/383 [00:12<00:45,  4.89it/s] 42%|████▏     | 159/383 [00:12<00:48,  4.61it/s]
 42%|████▏     | 160/383 [00:12<00:52,  4.21it/s][A 42%|████▏     | 160/383 [00:12<00:45,  4.95it/s] 42%|████▏     | 161/383 [00:12<00:44,  5.00it/s] 42%|████▏     | 160/383 [00:12<00:47,  4.72it/s]
 42%|████▏     | 161/383 [00:12<00:49,  4.52it/s][A 42%|████▏     | 161/383 [00:12<00:46,  4.79it/s] 42%|████▏     | 162/383 [00:12<00:42,  5.21it/s] 42%|████▏     | 161/383 [00:12<00:49,  4.51it/s]
 42%|████▏     | 162/383 [00:12<00:49,  4.43it/s][A 42%|████▏     | 162/383 [00:12<00:47,  4.62it/s] 43%|████▎     | 163/383 [00:12<00:46,  4.73it/s] 42%|████▏     | 162/383 [00:12<00:46,  4.71it/s]
 43%|████▎     | 163/383 [00:12<00:47,  4.66it/s][A 43%|████▎     | 163/383 [00:12<00:47,  4.67it/s] 43%|████▎     | 163/383 [00:13<00:46,  4.78it/s] 43%|████▎     | 164/383 [00:13<00:50,  4.38it/s]
 43%|████▎     | 164/383 [00:13<00:50,  4.35it/s][A 43%|████▎     | 164/383 [00:13<00:49,  4.41it/s] 43%|████▎     | 165/383 [00:13<00:46,  4.68it/s] 43%|████▎     | 164/383 [00:13<00:48,  4.50it/s] 44%|████▍     | 168/383 [00:13<00:25,  8.53it/s]
 43%|████▎     | 165/383 [00:13<00:48,  4.50it/s][A 43%|████▎     | 165/383 [00:13<00:47,  4.57it/s]
 44%|████▍     | 168/383 [00:13<00:25,  8.30it/s][A 45%|████▍     | 171/383 [00:13<00:17, 11.83it/s] 44%|████▍     | 168/383 [00:13<00:25,  8.39it/s] 43%|████▎     | 165/383 [00:13<00:51,  4.22it/s] 45%|████▌     | 173/383 [00:13<00:15, 13.42it/s]
 45%|████▍     | 171/383 [00:13<00:17, 11.86it/s][A 45%|████▍     | 171/383 [00:13<00:17, 11.94it/s] 44%|████▍     | 168/383 [00:13<00:27,  7.88it/s]
 45%|████▌     | 174/383 [00:13<00:14, 14.56it/s][A 46%|████▌     | 175/383 [00:13<00:15, 13.40it/s] 45%|████▌     | 174/383 [00:13<00:14, 14.69it/s] 45%|████▍     | 171/383 [00:13<00:18, 11.32it/s]
 46%|████▌     | 176/383 [00:13<00:14, 14.26it/s][A 46%|████▌     | 177/383 [00:13<00:15, 13.19it/s] 46%|████▌     | 176/383 [00:13<00:14, 14.34it/s] 45%|████▌     | 174/383 [00:13<00:15, 13.63it/s] 47%|████▋     | 179/383 [00:13<00:14, 14.54it/s]
 46%|████▋     | 178/383 [00:13<00:14, 14.58it/s][A 46%|████▋     | 178/383 [00:14<00:13, 14.68it/s]
 47%|████▋     | 180/383 [00:14<00:13, 15.60it/s][A 47%|████▋     | 181/383 [00:14<00:13, 15.35it/s] 46%|████▌     | 176/383 [00:14<00:15, 13.54it/s] 47%|████▋     | 180/383 [00:14<00:12, 15.71it/s] 48%|████▊     | 183/383 [00:14<00:12, 16.20it/s]
 48%|████▊     | 182/383 [00:14<00:12, 16.08it/s][A 46%|████▋     | 178/383 [00:14<00:14, 14.00it/s] 48%|████▊     | 182/383 [00:14<00:12, 16.32it/s]
 48%|████▊     | 184/383 [00:14<00:11, 16.71it/s][A 48%|████▊     | 185/383 [00:14<00:12, 15.98it/s] 47%|████▋     | 180/383 [00:14<00:13, 15.10it/s] 48%|████▊     | 184/383 [00:14<00:11, 16.86it/s]
 49%|████▊     | 186/383 [00:14<00:12, 16.06it/s][A 48%|████▊     | 182/383 [00:14<00:12, 15.77it/s] 49%|████▉     | 187/383 [00:14<00:12, 15.15it/s] 49%|████▊     | 186/383 [00:14<00:12, 15.57it/s] 48%|████▊     | 184/383 [00:14<00:12, 16.58it/s]
 49%|████▉     | 188/383 [00:14<00:11, 16.60it/s][A 50%|████▉     | 190/383 [00:14<00:10, 17.59it/s] 49%|████▉     | 188/383 [00:14<00:12, 16.12it/s]
 50%|████▉     | 191/383 [00:14<00:10, 18.62it/s][A 49%|████▊     | 186/383 [00:14<00:12, 15.43it/s] 50%|█████     | 192/383 [00:14<00:10, 17.92it/s] 50%|████▉     | 191/383 [00:14<00:10, 18.33it/s]
 50%|█████     | 193/383 [00:14<00:10, 18.53it/s][A 51%|█████     | 194/383 [00:14<00:10, 18.13it/s] 49%|████▉     | 188/383 [00:14<00:12, 15.40it/s] 50%|█████     | 193/383 [00:14<00:10, 18.47it/s]
 51%|█████     | 195/383 [00:14<00:10, 18.45it/s][A 51%|█████     | 196/383 [00:14<00:10, 18.44it/s] 50%|████▉     | 191/383 [00:14<00:10, 17.81it/s] 51%|█████     | 195/383 [00:14<00:10, 18.54it/s]
 51%|█████▏    | 197/383 [00:15<00:09, 18.71it/s][A 52%|█████▏    | 199/383 [00:15<00:09, 20.04it/s] 50%|█████     | 193/383 [00:15<00:10, 18.12it/s] 51%|█████▏    | 197/383 [00:15<00:09, 18.85it/s]
 52%|█████▏    | 199/383 [00:15<00:09, 19.06it/s][A 53%|█████▎    | 202/383 [00:15<00:08, 21.83it/s] 51%|█████     | 195/383 [00:15<00:10, 18.34it/s] 52%|█████▏    | 199/383 [00:15<00:09, 19.11it/s]
 53%|█████▎    | 202/383 [00:15<00:08, 21.14it/s][A 51%|█████▏    | 197/383 [00:15<00:10, 18.49it/s] 54%|█████▎    | 205/383 [00:15<00:07, 23.05it/s] 53%|█████▎    | 202/383 [00:15<00:08, 21.22it/s]
 54%|█████▎    | 205/383 [00:15<00:07, 22.79it/s][A 54%|█████▍    | 208/383 [00:15<00:07, 23.89it/s] 54%|█████▎    | 205/383 [00:15<00:07, 22.50it/s] 52%|█████▏    | 200/383 [00:15<00:09, 19.64it/s]
 54%|█████▍    | 208/383 [00:15<00:07, 23.64it/s][A 55%|█████▌    | 211/383 [00:15<00:07, 24.53it/s] 54%|█████▍    | 208/383 [00:15<00:07, 23.48it/s] 53%|█████▎    | 203/383 [00:15<00:08, 21.44it/s]
 55%|█████▌    | 211/383 [00:15<00:07, 24.32it/s][A 56%|█████▌    | 214/383 [00:15<00:06, 24.31it/s] 55%|█████▌    | 211/383 [00:15<00:07, 24.22it/s] 54%|█████▍    | 206/383 [00:15<00:07, 22.78it/s]
 56%|█████▌    | 214/383 [00:15<00:06, 24.74it/s][A 57%|█████▋    | 217/383 [00:15<00:06, 24.80it/s] 56%|█████▌    | 214/383 [00:15<00:06, 24.66it/s] 55%|█████▍    | 209/383 [00:15<00:07, 23.59it/s]
 57%|█████▋    | 217/383 [00:15<00:06, 24.33it/s][A 57%|█████▋    | 220/383 [00:15<00:06, 25.00it/s] 57%|█████▋    | 217/383 [00:15<00:06, 24.85it/s] 55%|█████▌    | 212/383 [00:15<00:07, 24.10it/s]
 57%|█████▋    | 220/383 [00:15<00:06, 24.83it/s][A 57%|█████▋    | 220/383 [00:15<00:06, 25.14it/s] 56%|█████▌    | 215/383 [00:15<00:06, 24.59it/s] 58%|█████▊    | 223/383 [00:16<00:07, 22.17it/s]
 58%|█████▊    | 223/383 [00:16<00:07, 22.80it/s][A 57%|█████▋    | 218/383 [00:16<00:06, 24.95it/s] 58%|█████▊    | 223/383 [00:16<00:07, 22.16it/s] 59%|█████▉    | 226/383 [00:16<00:07, 20.43it/s] 58%|█████▊    | 221/383 [00:16<00:06, 23.79it/s]
 59%|█████▉    | 226/383 [00:16<00:07, 20.64it/s][A 59%|█████▉    | 226/383 [00:16<00:07, 20.15it/s] 60%|█████▉    | 229/383 [00:16<00:07, 19.25it/s] 58%|█████▊    | 224/383 [00:16<00:07, 20.90it/s]
 60%|█████▉    | 229/383 [00:16<00:08, 19.09it/s][A 60%|█████▉    | 229/383 [00:16<00:08, 18.93it/s] 60%|██████    | 231/383 [00:16<00:08, 17.31it/s]
 60%|██████    | 231/383 [00:16<00:08, 17.83it/s][A 59%|█████▉    | 227/383 [00:16<00:08, 19.24it/s] 60%|██████    | 231/383 [00:16<00:08, 18.03it/s] 61%|██████    | 233/383 [00:16<00:09, 15.98it/s]
 61%|██████    | 233/383 [00:16<00:09, 16.35it/s][A 60%|██████    | 230/383 [00:16<00:08, 18.44it/s] 61%|██████    | 233/383 [00:16<00:09, 16.27it/s] 61%|██████▏   | 235/383 [00:16<00:09, 15.02it/s]
 61%|██████▏   | 235/383 [00:16<00:09, 15.32it/s][A 61%|██████    | 232/383 [00:16<00:08, 16.80it/s] 61%|██████▏   | 235/383 [00:16<00:09, 14.82it/s] 62%|██████▏   | 237/383 [00:17<00:10, 14.22it/s]
 62%|██████▏   | 237/383 [00:17<00:10, 14.47it/s][A 61%|██████    | 234/383 [00:17<00:09, 15.77it/s] 62%|██████▏   | 237/383 [00:17<00:10, 14.22it/s] 62%|██████▏   | 239/383 [00:17<00:10, 13.69it/s]
 62%|██████▏   | 239/383 [00:17<00:10, 13.98it/s][A 62%|██████▏   | 236/383 [00:17<00:09, 14.99it/s] 62%|██████▏   | 239/383 [00:17<00:10, 13.81it/s] 63%|██████▎   | 241/383 [00:17<00:10, 13.32it/s]
 63%|██████▎   | 241/383 [00:17<00:10, 13.64it/s][A 62%|██████▏   | 238/383 [00:17<00:10, 14.38it/s] 63%|██████▎   | 241/383 [00:17<00:10, 13.34it/s] 63%|██████▎   | 243/383 [00:17<00:10, 13.18it/s]
 63%|██████▎   | 243/383 [00:17<00:10, 13.30it/s][A 63%|██████▎   | 240/383 [00:17<00:10, 14.06it/s] 63%|██████▎   | 243/383 [00:17<00:10, 13.20it/s] 64%|██████▍   | 245/383 [00:17<00:10, 12.96it/s]
 64%|██████▍   | 245/383 [00:17<00:10, 13.27it/s][A 63%|██████▎   | 242/383 [00:17<00:10, 13.70it/s] 64%|██████▍   | 245/383 [00:17<00:10, 13.06it/s] 64%|██████▍   | 247/383 [00:17<00:10, 13.08it/s]
 64%|██████▍   | 247/383 [00:17<00:10, 13.28it/s][A 64%|██████▎   | 244/383 [00:17<00:10, 13.61it/s] 64%|██████▍   | 247/383 [00:17<00:10, 12.86it/s] 65%|██████▌   | 249/383 [00:17<00:10, 13.01it/s]
 65%|██████▌   | 249/383 [00:17<00:10, 12.84it/s][A 64%|██████▍   | 246/383 [00:18<00:10, 13.29it/s] 65%|██████▌   | 249/383 [00:18<00:10, 12.86it/s] 66%|██████▌   | 251/383 [00:18<00:10, 12.64it/s]
 66%|██████▌   | 251/383 [00:18<00:10, 12.98it/s][A 65%|██████▍   | 248/383 [00:18<00:10, 13.15it/s] 66%|██████▌   | 251/383 [00:18<00:10, 12.96it/s] 66%|██████▌   | 253/383 [00:18<00:10, 12.69it/s]
 66%|██████▌   | 253/383 [00:18<00:10, 12.96it/s][A 65%|██████▌   | 250/383 [00:18<00:10, 13.04it/s] 66%|██████▌   | 253/383 [00:18<00:10, 12.67it/s] 67%|██████▋   | 255/383 [00:18<00:09, 12.91it/s]
 67%|██████▋   | 255/383 [00:18<00:09, 13.07it/s][A 66%|██████▌   | 252/383 [00:18<00:10, 12.88it/s] 67%|██████▋   | 255/383 [00:18<00:10, 12.64it/s] 67%|██████▋   | 257/383 [00:18<00:09, 13.10it/s]
 67%|██████▋   | 257/383 [00:18<00:09, 13.17it/s][A 66%|██████▋   | 254/383 [00:18<00:10, 12.77it/s] 67%|██████▋   | 257/383 [00:18<00:09, 12.94it/s] 68%|██████▊   | 259/383 [00:18<00:09, 13.37it/s]
 68%|██████▊   | 259/383 [00:18<00:09, 13.29it/s][A 67%|██████▋   | 256/383 [00:18<00:09, 12.89it/s] 68%|██████▊   | 259/383 [00:18<00:09, 13.06it/s] 68%|██████▊   | 261/383 [00:18<00:09, 13.23it/s]
 68%|██████▊   | 261/383 [00:18<00:09, 13.49it/s][A 67%|██████▋   | 258/383 [00:18<00:09, 12.99it/s] 68%|██████▊   | 261/383 [00:19<00:09, 13.09it/s] 69%|██████▊   | 263/383 [00:19<00:08, 13.43it/s]
 69%|██████▊   | 263/383 [00:19<00:08, 13.48it/s][A 68%|██████▊   | 260/383 [00:19<00:09, 13.16it/s] 69%|██████▉   | 266/383 [00:19<00:07, 16.41it/s] 69%|██████▊   | 263/383 [00:19<00:09, 13.17it/s]
 69%|██████▉   | 266/383 [00:19<00:07, 16.06it/s][A 68%|██████▊   | 262/383 [00:19<00:09, 13.29it/s] 70%|███████   | 269/383 [00:19<00:06, 18.77it/s]
 70%|███████   | 269/383 [00:19<00:06, 18.57it/s][A 69%|██████▉   | 266/383 [00:19<00:07, 15.69it/s] 69%|██████▉   | 264/383 [00:19<00:08, 14.40it/s] 71%|███████   | 272/383 [00:19<00:05, 19.01it/s]
 71%|███████   | 272/383 [00:19<00:05, 19.78it/s][A 70%|███████   | 269/383 [00:19<00:06, 18.04it/s] 70%|██████▉   | 267/383 [00:19<00:07, 16.47it/s] 72%|███████▏  | 274/383 [00:19<00:06, 17.69it/s] 71%|███████   | 272/383 [00:19<00:05, 19.09it/s]
 72%|███████▏  | 275/383 [00:19<00:05, 18.52it/s][A 70%|███████   | 270/383 [00:19<00:05, 18.86it/s] 72%|███████▏  | 276/383 [00:19<00:06, 16.51it/s] 72%|███████▏  | 274/383 [00:19<00:06, 17.83it/s]
 72%|███████▏  | 277/383 [00:19<00:05, 17.73it/s][A 71%|███████▏  | 273/383 [00:19<00:05, 19.44it/s] 72%|███████▏  | 276/383 [00:19<00:06, 17.41it/s] 73%|███████▎  | 278/383 [00:19<00:06, 16.33it/s]
 73%|███████▎  | 279/383 [00:19<00:06, 16.87it/s][A 72%|███████▏  | 275/383 [00:19<00:05, 18.49it/s] 73%|███████▎  | 280/383 [00:19<00:06, 16.17it/s] 73%|███████▎  | 278/383 [00:19<00:06, 16.55it/s]
 73%|███████▎  | 281/383 [00:20<00:06, 16.18it/s][A 72%|███████▏  | 277/383 [00:20<00:05, 17.77it/s] 73%|███████▎  | 280/383 [00:20<00:06, 16.27it/s] 74%|███████▎  | 282/383 [00:20<00:07, 13.86it/s] 73%|███████▎  | 279/383 [00:20<00:06, 16.45it/s]
 74%|███████▍  | 283/383 [00:20<00:07, 14.28it/s][A 74%|███████▎  | 282/383 [00:20<00:07, 14.12it/s] 74%|███████▍  | 284/383 [00:20<00:07, 12.93it/s] 73%|███████▎  | 281/383 [00:20<00:06, 14.69it/s]
 74%|███████▍  | 285/383 [00:20<00:07, 12.90it/s][A 74%|███████▍  | 284/383 [00:20<00:07, 12.77it/s] 75%|███████▍  | 286/383 [00:20<00:07, 12.21it/s] 74%|███████▍  | 283/383 [00:20<00:07, 13.07it/s]
 75%|███████▍  | 287/383 [00:20<00:07, 12.08it/s][A 75%|███████▍  | 286/383 [00:20<00:08, 12.12it/s] 75%|███████▌  | 288/383 [00:20<00:07, 11.90it/s] 74%|███████▍  | 285/383 [00:20<00:07, 12.68it/s] 75%|███████▌  | 288/383 [00:20<00:08, 11.38it/s] 75%|███████▍  | 287/383 [00:20<00:07, 12.35it/s]
 75%|███████▌  | 289/383 [00:20<00:10,  8.77it/s][A 76%|███████▌  | 290/383 [00:21<00:12,  7.74it/s] 75%|███████▌  | 289/383 [00:21<00:10,  9.28it/s] 76%|███████▌  | 290/383 [00:21<00:12,  7.31it/s] 76%|███████▌  | 291/383 [00:21<00:13,  6.62it/s]
 76%|███████▌  | 291/383 [00:21<00:14,  6.29it/s][A 76%|███████▌  | 291/383 [00:21<00:14,  6.22it/s] 76%|███████▌  | 292/383 [00:21<00:15,  5.84it/s]
 76%|███████▌  | 292/383 [00:21<00:16,  5.59it/s][A 76%|███████▌  | 291/383 [00:21<00:14,  6.39it/s] 76%|███████▌  | 292/383 [00:21<00:17,  5.33it/s] 77%|███████▋  | 293/383 [00:21<00:17,  5.12it/s] 76%|███████▌  | 292/383 [00:22<00:16,  5.62it/s]
 77%|███████▋  | 293/383 [00:22<00:18,  4.92it/s][A 77%|███████▋  | 293/383 [00:22<00:18,  4.91it/s] 77%|███████▋  | 294/383 [00:22<00:18,  4.79it/s] 77%|███████▋  | 293/383 [00:22<00:17,  5.10it/s]
 77%|███████▋  | 294/383 [00:22<00:19,  4.60it/s][A 77%|███████▋  | 295/383 [00:22<00:18,  4.71it/s] 77%|███████▋  | 294/383 [00:22<00:19,  4.49it/s]
 77%|███████▋  | 295/383 [00:22<00:19,  4.42it/s][A 77%|███████▋  | 294/383 [00:22<00:18,  4.73it/s] 77%|███████▋  | 296/383 [00:22<00:20,  4.27it/s] 77%|███████▋  | 295/383 [00:22<00:20,  4.20it/s] 77%|███████▋  | 295/383 [00:22<00:19,  4.54it/s]
 77%|███████▋  | 296/383 [00:22<00:20,  4.18it/s][A 78%|███████▊  | 297/383 [00:22<00:19,  4.31it/s] 77%|███████▋  | 296/383 [00:23<00:21,  4.07it/s] 77%|███████▋  | 296/383 [00:23<00:20,  4.27it/s]
 78%|███████▊  | 297/383 [00:23<00:21,  3.93it/s][A 78%|███████▊  | 298/383 [00:23<00:20,  4.10it/s] 78%|███████▊  | 297/383 [00:23<00:21,  4.08it/s] 78%|███████▊  | 297/383 [00:23<00:21,  4.06it/s]
 78%|███████▊  | 298/383 [00:23<00:21,  4.04it/s][A 78%|███████▊  | 298/383 [00:23<00:19,  4.28it/s] 78%|███████▊  | 299/383 [00:23<00:21,  3.95it/s]
 78%|███████▊  | 299/383 [00:23<00:20,  4.01it/s][A 78%|███████▊  | 298/383 [00:23<00:22,  3.80it/s] 78%|███████▊  | 300/383 [00:23<00:20,  4.06it/s] 78%|███████▊  | 299/383 [00:23<00:20,  4.06it/s]
 78%|███████▊  | 300/383 [00:23<00:21,  3.86it/s][A 78%|███████▊  | 299/383 [00:23<00:22,  3.78it/s] 79%|███████▊  | 301/383 [00:23<00:20,  3.92it/s] 78%|███████▊  | 300/383 [00:24<00:21,  3.89it/s]
 79%|███████▊  | 301/383 [00:24<00:21,  3.78it/s][A 78%|███████▊  | 300/383 [00:24<00:22,  3.71it/s] 79%|███████▊  | 301/383 [00:24<00:20,  4.03it/s] 79%|███████▉  | 302/383 [00:24<00:21,  3.85it/s]
 79%|███████▉  | 302/383 [00:24<00:21,  3.74it/s][A 79%|███████▊  | 301/383 [00:24<00:22,  3.65it/s] 79%|███████▉  | 303/383 [00:24<00:21,  3.80it/s] 79%|███████▉  | 302/383 [00:24<00:21,  3.81it/s]
 79%|███████▉  | 303/383 [00:24<00:20,  3.93it/s][A 79%|███████▉  | 303/383 [00:24<00:20,  3.97it/s] 79%|███████▉  | 302/383 [00:24<00:22,  3.62it/s] 79%|███████▉  | 304/383 [00:24<00:21,  3.68it/s]
 79%|███████▉  | 304/383 [00:24<00:21,  3.75it/s][A 79%|███████▉  | 304/383 [00:25<00:19,  4.00it/s] 80%|███████▉  | 305/383 [00:25<00:20,  3.83it/s] 79%|███████▉  | 303/383 [00:25<00:22,  3.63it/s] 80%|███████▉  | 306/383 [00:25<00:18,  4.09it/s]
 80%|███████▉  | 305/383 [00:25<00:21,  3.63it/s][A 80%|███████▉  | 305/383 [00:25<00:20,  3.85it/s] 79%|███████▉  | 304/383 [00:25<00:21,  3.62it/s] 80%|████████  | 307/383 [00:25<00:19,  3.96it/s]
 80%|███████▉  | 306/383 [00:25<00:20,  3.67it/s][A 80%|███████▉  | 306/383 [00:25<00:20,  3.81it/s] 80%|███████▉  | 305/383 [00:25<00:21,  3.63it/s] 80%|████████  | 308/383 [00:25<00:19,  3.81it/s]
 80%|████████  | 307/383 [00:25<00:20,  3.65it/s][A 80%|████████  | 307/383 [00:25<00:20,  3.73it/s] 80%|███████▉  | 306/383 [00:25<00:21,  3.62it/s] 81%|████████  | 309/383 [00:26<00:19,  3.85it/s] 80%|████████  | 308/383 [00:26<00:19,  3.87it/s]
 80%|████████  | 308/383 [00:26<00:20,  3.67it/s][A 80%|████████  | 307/383 [00:26<00:21,  3.59it/s]
 81%|████████  | 309/383 [00:26<00:19,  3.76it/s][A 81%|████████  | 309/383 [00:26<00:19,  3.79it/s] 81%|████████  | 310/383 [00:26<00:19,  3.70it/s] 80%|████████  | 308/383 [00:26<00:20,  3.57it/s]
 81%|████████  | 310/383 [00:26<00:18,  3.86it/s][A 81%|████████  | 310/383 [00:26<00:19,  3.71it/s] 81%|████████  | 311/383 [00:26<00:19,  3.64it/s] 81%|████████  | 309/383 [00:26<00:19,  3.80it/s]
 81%|████████  | 311/383 [00:26<00:18,  3.83it/s][A 81%|████████  | 311/383 [00:26<00:19,  3.69it/s] 81%|████████▏ | 312/383 [00:26<00:19,  3.64it/s] 81%|████████  | 310/383 [00:26<00:18,  3.90it/s]
 81%|████████▏ | 312/383 [00:27<00:18,  3.86it/s][A 81%|████████▏ | 312/383 [00:27<00:19,  3.64it/s] 82%|████████▏ | 313/383 [00:27<00:19,  3.64it/s] 81%|████████  | 311/383 [00:27<00:18,  3.81it/s]
 82%|████████▏ | 313/383 [00:27<00:19,  3.66it/s][A 81%|████████▏ | 312/383 [00:27<00:18,  3.88it/s] 82%|████████▏ | 313/383 [00:27<00:18,  3.69it/s] 82%|████████▏ | 314/383 [00:27<00:19,  3.61it/s]
 82%|████████▏ | 314/383 [00:27<00:18,  3.68it/s][A 82%|████████▏ | 313/383 [00:27<00:17,  3.91it/s] 82%|████████▏ | 314/383 [00:27<00:18,  3.79it/s] 82%|████████▏ | 315/383 [00:27<00:19,  3.51it/s] 82%|████████▏ | 314/383 [00:27<00:17,  3.97it/s]
 82%|████████▏ | 315/383 [00:27<00:18,  3.67it/s][A 82%|████████▏ | 315/383 [00:27<00:17,  3.84it/s] 83%|████████▎ | 316/383 [00:28<00:18,  3.64it/s] 82%|████████▏ | 315/383 [00:28<00:16,  4.09it/s]
 83%|████████▎ | 316/383 [00:28<00:18,  3.72it/s][A 83%|████████▎ | 316/383 [00:28<00:17,  3.81it/s] 83%|████████▎ | 317/383 [00:28<00:18,  3.64it/s] 83%|████████▎ | 316/383 [00:28<00:17,  3.80it/s]
 83%|████████▎ | 317/383 [00:28<00:17,  3.71it/s][A 83%|████████▎ | 317/383 [00:28<00:18,  3.63it/s] 83%|████████▎ | 318/383 [00:28<00:18,  3.51it/s] 83%|████████▎ | 317/383 [00:28<00:17,  3.76it/s]
 83%|████████▎ | 318/383 [00:28<00:18,  3.56it/s][A 83%|████████▎ | 318/383 [00:28<00:17,  3.61it/s] 83%|████████▎ | 319/383 [00:28<00:17,  3.70it/s] 83%|████████▎ | 318/383 [00:29<00:17,  3.71it/s]
 83%|████████▎ | 319/383 [00:29<00:17,  3.68it/s][A 83%|████████▎ | 319/383 [00:29<00:17,  3.67it/s] 84%|████████▎ | 320/383 [00:29<00:17,  3.69it/s]
 84%|████████▎ | 320/383 [00:29<00:16,  3.77it/s][A 83%|████████▎ | 319/383 [00:29<00:17,  3.71it/s] 84%|████████▎ | 320/383 [00:29<00:17,  3.69it/s] 84%|████████▍ | 321/383 [00:29<00:17,  3.60it/s]
 84%|████████▍ | 321/383 [00:29<00:16,  3.78it/s][A 84%|████████▎ | 320/383 [00:29<00:17,  3.68it/s] 84%|████████▍ | 321/383 [00:29<00:16,  3.73it/s] 84%|████████▍ | 322/383 [00:29<00:16,  3.62it/s]
 84%|████████▍ | 322/383 [00:29<00:15,  3.85it/s][A 84%|████████▍ | 321/383 [00:29<00:17,  3.59it/s] 84%|████████▍ | 322/383 [00:29<00:16,  3.59it/s] 84%|████████▍ | 323/383 [00:29<00:16,  3.75it/s]
 84%|████████▍ | 323/383 [00:30<00:15,  3.99it/s][A 84%|████████▍ | 322/383 [00:30<00:16,  3.78it/s] 84%|████████▍ | 323/383 [00:30<00:16,  3.73it/s] 85%|████████▍ | 324/383 [00:30<00:15,  3.71it/s]
 85%|████████▍ | 324/383 [00:30<00:15,  3.77it/s][A 84%|████████▍ | 323/383 [00:30<00:16,  3.72it/s] 85%|████████▍ | 324/383 [00:30<00:15,  3.72it/s] 85%|████████▍ | 325/383 [00:30<00:15,  3.80it/s]
 85%|████████▍ | 325/383 [00:30<00:15,  3.86it/s][A 85%|████████▍ | 324/383 [00:30<00:16,  3.68it/s] 85%|████████▍ | 325/383 [00:30<00:15,  3.82it/s] 85%|████████▌ | 326/383 [00:30<00:14,  3.96it/s]
 85%|████████▌ | 326/383 [00:30<00:15,  3.76it/s][A 85%|████████▌ | 326/383 [00:30<00:14,  3.85it/s] 85%|████████▍ | 325/383 [00:30<00:15,  3.67it/s] 85%|████████▌ | 327/383 [00:30<00:14,  3.89it/s]
 85%|████████▌ | 327/383 [00:31<00:14,  3.75it/s][A 85%|████████▌ | 326/383 [00:31<00:14,  3.82it/s] 85%|████████▌ | 327/383 [00:31<00:15,  3.67it/s] 86%|████████▌ | 328/383 [00:31<00:14,  3.79it/s]
 86%|████████▌ | 328/383 [00:31<00:14,  3.68it/s][A 85%|████████▌ | 327/383 [00:31<00:14,  3.89it/s] 86%|████████▌ | 329/383 [00:31<00:14,  3.80it/s] 86%|████████▌ | 328/383 [00:31<00:15,  3.64it/s]
 86%|████████▌ | 329/383 [00:31<00:14,  3.74it/s][A 86%|████████▌ | 328/383 [00:31<00:14,  3.88it/s] 86%|████████▌ | 329/383 [00:31<00:14,  3.76it/s] 86%|████████▌ | 330/383 [00:31<00:14,  3.74it/s]
 86%|████████▌ | 330/383 [00:31<00:14,  3.76it/s][A 86%|████████▌ | 329/383 [00:31<00:13,  3.89it/s] 86%|████████▋ | 331/383 [00:31<00:12,  4.25it/s] 86%|████████▌ | 330/383 [00:32<00:14,  3.64it/s] 87%|████████▋ | 332/383 [00:32<00:10,  4.84it/s] 86%|████████▌ | 330/383 [00:32<00:12,  4.13it/s] 86%|████████▋ | 331/383 [00:32<00:12,  4.27it/s]
 86%|████████▋ | 331/383 [00:32<00:13,  3.73it/s][A 87%|████████▋ | 333/383 [00:32<00:09,  5.34it/s] 87%|████████▋ | 332/383 [00:32<00:11,  4.60it/s]
 87%|████████▋ | 332/383 [00:32<00:12,  4.22it/s][A 87%|████████▋ | 334/383 [00:32<00:08,  5.62it/s] 86%|████████▋ | 331/383 [00:32<00:13,  3.99it/s]
 87%|████████▋ | 333/383 [00:32<00:10,  4.70it/s][A 87%|████████▋ | 335/383 [00:32<00:08,  5.81it/s] 87%|████████▋ | 333/383 [00:32<00:10,  4.85it/s] 87%|████████▋ | 332/383 [00:32<00:11,  4.37it/s]
 87%|████████▋ | 334/383 [00:32<00:09,  5.06it/s][A 88%|████████▊ | 336/383 [00:32<00:07,  5.94it/s] 87%|████████▋ | 334/383 [00:32<00:09,  5.09it/s] 87%|████████▋ | 333/383 [00:32<00:10,  4.96it/s] 87%|████████▋ | 335/383 [00:32<00:08,  5.63it/s] 88%|████████▊ | 337/383 [00:32<00:07,  6.11it/s]
 87%|████████▋ | 335/383 [00:32<00:09,  5.25it/s][A 87%|████████▋ | 334/383 [00:32<00:09,  5.44it/s] 88%|████████▊ | 336/383 [00:33<00:08,  5.73it/s] 88%|████████▊ | 338/383 [00:33<00:07,  5.92it/s] 87%|████████▋ | 335/383 [00:33<00:08,  5.49it/s]
 88%|████████▊ | 336/383 [00:33<00:08,  5.25it/s][A 88%|████████▊ | 337/383 [00:33<00:07,  5.89it/s] 88%|████████▊ | 336/383 [00:33<00:07,  5.90it/s] 89%|████████▉ | 340/383 [00:33<00:05,  7.69it/s]
 88%|████████▊ | 337/383 [00:33<00:08,  5.39it/s][A 88%|████████▊ | 338/383 [00:33<00:07,  6.01it/s] 89%|████████▉ | 342/383 [00:33<00:04,  9.47it/s] 88%|████████▊ | 337/383 [00:33<00:07,  6.11it/s]
 88%|████████▊ | 338/383 [00:33<00:08,  5.49it/s][A 89%|████████▉ | 340/383 [00:33<00:05,  8.40it/s] 90%|████████▉ | 344/383 [00:33<00:03, 10.79it/s] 88%|████████▊ | 338/383 [00:33<00:07,  6.18it/s]
 89%|████████▊ | 339/383 [00:33<00:07,  5.73it/s][A 89%|████████▉ | 342/383 [00:33<00:04,  9.83it/s] 90%|█████████ | 346/383 [00:33<00:03, 11.54it/s] 89%|████████▉ | 340/383 [00:33<00:05,  8.27it/s]
 89%|████████▉ | 341/383 [00:33<00:05,  7.90it/s][A 90%|████████▉ | 344/383 [00:33<00:03, 11.11it/s] 91%|█████████ | 348/383 [00:33<00:02, 12.47it/s] 89%|████████▉ | 342/383 [00:33<00:04,  9.97it/s]
 90%|████████▉ | 343/383 [00:33<00:04,  9.62it/s][A 90%|█████████ | 346/383 [00:33<00:03, 12.21it/s] 91%|█████████▏| 350/383 [00:33<00:02, 13.35it/s] 90%|████████▉ | 344/383 [00:33<00:03, 10.55it/s]
 90%|█████████ | 345/383 [00:33<00:03, 11.04it/s][A 91%|█████████ | 348/383 [00:34<00:02, 12.99it/s] 92%|█████████▏| 352/383 [00:34<00:02, 13.38it/s] 90%|█████████ | 346/383 [00:34<00:03, 11.92it/s]
 91%|█████████ | 347/383 [00:34<00:02, 12.18it/s][A 91%|█████████▏| 350/383 [00:34<00:02, 13.07it/s] 92%|█████████▏| 354/383 [00:34<00:02, 13.70it/s] 91%|█████████ | 348/383 [00:34<00:02, 12.87it/s]
 91%|█████████ | 349/383 [00:34<00:02, 12.89it/s][A 93%|█████████▎| 356/383 [00:34<00:01, 14.48it/s] 92%|█████████▏| 352/383 [00:34<00:02, 13.79it/s] 91%|█████████▏| 350/383 [00:34<00:02, 13.40it/s]
 92%|█████████▏| 351/383 [00:34<00:02, 13.29it/s][A 93%|█████████▎| 358/383 [00:34<00:01, 15.29it/s] 92%|█████████▏| 354/383 [00:34<00:02, 14.08it/s] 92%|█████████▏| 352/383 [00:34<00:02, 13.68it/s]
 92%|█████████▏| 353/383 [00:34<00:02, 13.27it/s][A 93%|█████████▎| 356/383 [00:34<00:01, 14.37it/s] 92%|█████████▏| 354/383 [00:34<00:02, 13.77it/s]
 93%|█████████▎| 355/383 [00:34<00:02, 13.58it/s][A 93%|█████████▎| 358/383 [00:34<00:01, 14.75it/s] 93%|█████████▎| 356/383 [00:34<00:01, 13.64it/s] 94%|█████████▍| 360/383 [00:34<00:02,  9.74it/s]
 93%|█████████▎| 357/383 [00:34<00:01, 14.40it/s][A 93%|█████████▎| 358/383 [00:34<00:01, 14.88it/s]
 94%|█████████▎| 359/383 [00:34<00:01, 15.45it/s][A 94%|█████████▍| 360/383 [00:35<00:02,  9.97it/s] 95%|█████████▍| 362/383 [00:35<00:02,  8.33it/s] 94%|█████████▍| 360/383 [00:35<00:01, 11.86it/s]
 94%|█████████▍| 361/383 [00:35<00:02, 10.16it/s][A 95%|█████████▍| 362/383 [00:35<00:02,  8.17it/s] 95%|█████████▍| 362/383 [00:35<00:02,  9.10it/s] 95%|█████████▌| 364/383 [00:35<00:02,  7.20it/s]
 95%|█████████▍| 363/383 [00:35<00:02,  8.47it/s][A 95%|█████████▌| 365/383 [00:35<00:02,  6.86it/s] 95%|█████████▌| 364/383 [00:35<00:02,  7.14it/s] 96%|█████████▌| 367/383 [00:35<00:01,  8.69it/s] 95%|█████████▌| 364/383 [00:35<00:02,  7.82it/s] 96%|█████████▋| 369/383 [00:35<00:01, 10.48it/s]
 95%|█████████▌| 365/383 [00:35<00:02,  7.44it/s][A 95%|█████████▌| 365/383 [00:35<00:02,  6.85it/s] 97%|█████████▋| 371/383 [00:35<00:00, 12.16it/s] 95%|█████████▌| 365/383 [00:35<00:02,  7.36it/s] 96%|█████████▌| 367/383 [00:36<00:01,  8.61it/s]
 96%|█████████▌| 366/383 [00:36<00:02,  7.38it/s][A 97%|█████████▋| 373/383 [00:36<00:00, 13.67it/s] 96%|█████████▌| 367/383 [00:36<00:01,  9.25it/s] 96%|█████████▋| 369/383 [00:36<00:01, 10.40it/s]
 96%|█████████▌| 368/383 [00:36<00:01,  9.20it/s][A 98%|█████████▊| 375/383 [00:36<00:00, 14.64it/s] 96%|█████████▋| 369/383 [00:36<00:01, 10.96it/s] 97%|█████████▋| 371/383 [00:36<00:00, 12.04it/s]
 97%|█████████▋| 370/383 [00:36<00:01, 10.89it/s][A 98%|█████████▊| 377/383 [00:36<00:00, 15.82it/s] 97%|█████████▋| 371/383 [00:36<00:00, 12.45it/s] 97%|█████████▋| 373/383 [00:36<00:00, 13.42it/s]
 97%|█████████▋| 372/383 [00:36<00:00, 12.34it/s][A 97%|█████████▋| 373/383 [00:36<00:00, 13.84it/s] 99%|█████████▉| 380/383 [00:36<00:00, 18.26it/s] 98%|█████████▊| 375/383 [00:36<00:00, 14.87it/s]
 98%|█████████▊| 374/383 [00:36<00:00, 13.79it/s][A100%|██████████| 383/383 [00:36<00:00, 19.29it/s]100%|██████████| 383/383 [00:36<00:00, 10.47it/s]
 98%|█████████▊| 376/383 [00:36<00:00, 15.80it/s]/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
 99%|█████████▊| 378/383 [00:36<00:00, 16.91it/s]/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)

 98%|█████████▊| 377/383 [00:36<00:00, 15.74it/s][A/home/bagus/pytorch/torch/utils/checkpoint.py:426: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 99%|█████████▉| 379/383 [00:36<00:00, 18.39it/s] 99%|█████████▉| 381/383 [00:36<00:00, 19.49it/s]
 99%|█████████▉| 380/383 [00:36<00:00, 17.69it/s][A100%|█████████▉| 382/383 [00:36<00:00, 20.61it/s]100%|██████████| 383/383 [00:36<00:00, 10.40it/s]
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
100%|██████████| 383/383 [00:36<00:00, 10.38it/s]
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/pytorch/torch/utils/checkpoint.py:426: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

100%|██████████| 383/383 [00:36<00:00, 18.55it/s][A100%|██████████| 383/383 [00:36<00:00, 10.36it/s]
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
                                                     {'mmlu_loss': 3.2459204193817417, 'mmlu_eval_accuracy_conceptual_physics': 0.3076923076923077, 'mmlu_eval_accuracy_logical_fallacies': nan, 'mmlu_eval_accuracy_professional_psychology': nan, 'mmlu_eval_accuracy_public_relations': nan, 'mmlu_eval_accuracy_security_studies': nan, 'mmlu_eval_accuracy_high_school_physics': nan, 'mmlu_eval_accuracy_formal_logic': 0.2857142857142857, 'mmlu_eval_accuracy_anatomy': 0.2857142857142857, 'mmlu_eval_accuracy_moral_scenarios': nan, 'mmlu_eval_accuracy_moral_disputes': nan, 'mmlu_eval_accuracy_management': nan, 'mmlu_eval_accuracy_high_school_european_history': 0.2777777777777778, 'mmlu_eval_accuracy_professional_law': nan, 'mmlu_eval_accuracy_elementary_mathematics': 0.24390243902439024, 'mmlu_eval_accuracy_high_school_computer_science': 0.3333333333333333, 'mmlu_eval_accuracy_high_school_geography': 0.3333333333333333, 'mmlu_eval_accuracy_global_facts': 0.5, 'mmlu_eval_accuracy_high_school_biology': 0.21875, 'mmlu_eval_accuracy_philosophy': nan, 'mmlu_eval_accuracy_astronomy': 0.25, 'mmlu_eval_accuracy_nutrition': nan, 'mmlu_eval_accuracy_high_school_government_and_politics': nan, 'mmlu_eval_accuracy_medical_genetics': nan, 'mmlu_eval_accuracy_human_aging': nan, 'mmlu_eval_accuracy_human_sexuality': nan, 'mmlu_eval_accuracy_college_medicine': 0.13636363636363635, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_college_physics': 0.2727272727272727, 'mmlu_eval_accuracy_us_foreign_policy': nan, 'mmlu_eval_accuracy_college_computer_science': 0.09090909090909091, 'mmlu_eval_accuracy_machine_learning': nan, 'mmlu_eval_accuracy_professional_accounting': nan, 'mmlu_eval_accuracy_world_religions': nan, 'mmlu_eval_accuracy_miscellaneous': nan, 'mmlu_eval_accuracy_jurisprudence': nan, 'mmlu_eval_accuracy_high_school_chemistry': 0.2727272727272727, 'mmlu_eval_accuracy_virology': nan, 'mmlu_eval_accuracy_high_school_mathematics': nan, 'mmlu_eval_accuracy_electrical_engineering': 0.3125, 'mmlu_eval_accuracy_high_school_world_history': nan, 'mmlu_eval_accuracy_prehistory': nan, 'mmlu_eval_accuracy_high_school_statistics': nan, 'mmlu_eval_accuracy_high_school_psychology': nan, 'mmlu_eval_accuracy_high_school_microeconomics': nan, 'mmlu_eval_accuracy_high_school_us_history': nan, 'mmlu_eval_accuracy_college_biology': 0.3125, 'mmlu_eval_accuracy_high_school_macroeconomics': nan, 'mmlu_eval_accuracy_professional_medicine': nan, 'mmlu_eval_accuracy_clinical_knowledge': 0.3103448275862069, 'mmlu_eval_accuracy_business_ethics': 0.0, 'mmlu_eval_accuracy_college_chemistry': 0.0, 'mmlu_eval_accuracy_econometrics': 0.3333333333333333, 'mmlu_eval_accuracy_marketing': nan, 'mmlu_eval_accuracy_international_law': nan, 'mmlu_eval_accuracy_sociology': nan, 'mmlu_eval_accuracy_computer_security': 0.18181818181818182, 'mmlu_eval_accuracy_college_mathematics': 0.09090909090909091, 'mmlu_eval_accuracy': nan, 'epoch': 11.14}
 75%|███████▌  | 1536/2048 [2:08:21<39:48,  4.66s/it]/home/bagus/pytorch/torch/utils/checkpoint.py:426: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/pytorch/torch/utils/checkpoint.py:426: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 75%|███████▌  | 1537/2048 [2:08:26<2:50:56, 20.07s/it] 75%|███████▌  | 1538/2048 [2:08:30<2:09:36, 15.25s/it] 75%|███████▌  | 1539/2048 [2:08:34<1:41:39, 11.98s/it] 75%|███████▌  | 1540/2048 [2:08:38<1:21:15,  9.60s/it] 75%|███████▌  | 1541/2048 [2:08:42<1:05:56,  7.80s/it] 75%|███████▌  | 1542/2048 [2:08:45<55:46,  6.61s/it]   75%|███████▌  | 1543/2048 [2:08:51<52:00,  6.18s/it] 75%|███████▌  | 1544/2048 [2:08:56<49:22,  5.88s/it] 75%|███████▌  | 1545/2048 [2:09:01<47:25,  5.66s/it] 75%|███████▌  | 1546/2048 [2:09:06<45:38,  5.46s/it] 76%|███████▌  | 1547/2048 [2:09:11<44:03,  5.28s/it] 76%|███████▌  | 1548/2048 [2:09:15<42:25,  5.09s/it] 76%|███████▌  | 1549/2048 [2:09:20<40:26,  4.86s/it] 76%|███████▌  | 1550/2048 [2:09:24<38:30,  4.64s/it] 76%|███████▌  | 1551/2048 [2:09:28<36:55,  4.46s/it] 76%|███████▌  | 1552/2048 [2:09:32<35:41,  4.32s/it]                                                     {'loss': 0.0904, 'learning_rate': 2.9233416475254284e-05, 'epoch': 11.26}
 76%|███████▌  | 1552/2048 [2:09:32<35:41,  4.32s/it] 76%|███████▌  | 1553/2048 [2:09:36<35:05,  4.25s/it] 76%|███████▌  | 1554/2048 [2:09:39<32:09,  3.91s/it] 76%|███████▌  | 1555/2048 [2:09:44<34:30,  4.20s/it] 76%|███████▌  | 1556/2048 [2:09:49<36:49,  4.49s/it] 76%|███████▌  | 1557/2048 [2:09:54<38:21,  4.69s/it] 76%|███████▌  | 1558/2048 [2:09:59<39:19,  4.82s/it] 76%|███████▌  | 1559/2048 [2:10:04<39:34,  4.85s/it] 76%|███████▌  | 1560/2048 [2:10:09<38:59,  4.79s/it] 76%|███████▌  | 1561/2048 [2:10:13<37:39,  4.64s/it] 76%|███████▋  | 1562/2048 [2:10:17<36:31,  4.51s/it] 76%|███████▋  | 1563/2048 [2:10:22<35:34,  4.40s/it] 76%|███████▋  | 1564/2048 [2:10:25<33:22,  4.14s/it] 76%|███████▋  | 1565/2048 [2:10:29<32:35,  4.05s/it] 76%|███████▋  | 1566/2048 [2:10:32<31:06,  3.87s/it] 77%|███████▋  | 1567/2048 [2:10:36<31:01,  3.87s/it] 77%|███████▋  | 1568/2048 [2:10:41<34:04,  4.26s/it]                                                     {'loss': 0.0933, 'learning_rate': 2.7468007560896437e-05, 'epoch': 11.37}
 77%|███████▋  | 1568/2048 [2:10:41<34:04,  4.26s/it] 77%|███████▋  | 1569/2048 [2:10:47<36:10,  4.53s/it] 77%|███████▋  | 1570/2048 [2:10:52<37:34,  4.72s/it] 77%|███████▋  | 1571/2048 [2:10:57<38:13,  4.81s/it] 77%|███████▋  | 1572/2048 [2:11:02<38:10,  4.81s/it] 77%|███████▋  | 1573/2048 [2:11:06<37:36,  4.75s/it] 77%|███████▋  | 1574/2048 [2:11:11<36:47,  4.66s/it] 77%|███████▋  | 1575/2048 [2:11:15<35:45,  4.54s/it] 77%|███████▋  | 1576/2048 [2:11:19<34:48,  4.42s/it] 77%|███████▋  | 1577/2048 [2:11:23<33:23,  4.25s/it] 77%|███████▋  | 1578/2048 [2:11:26<31:43,  4.05s/it] 77%|███████▋  | 1579/2048 [2:11:30<29:32,  3.78s/it] 77%|███████▋  | 1580/2048 [2:11:35<32:03,  4.11s/it] 77%|███████▋  | 1581/2048 [2:11:40<34:27,  4.43s/it] 77%|███████▋  | 1582/2048 [2:11:45<36:05,  4.65s/it] 77%|███████▋  | 1583/2048 [2:11:50<37:08,  4.79s/it] 77%|███████▋  | 1584/2048 [2:11:55<37:04,  4.79s/it]                                                     {'loss': 0.0845, 'learning_rate': 2.574905954950867e-05, 'epoch': 11.49}
 77%|███████▋  | 1584/2048 [2:11:55<37:04,  4.79s/it] 77%|███████▋  | 1585/2048 [2:12:00<37:05,  4.81s/it] 77%|███████▋  | 1586/2048 [2:12:04<36:40,  4.76s/it] 77%|███████▋  | 1587/2048 [2:12:08<35:06,  4.57s/it] 78%|███████▊  | 1588/2048 [2:12:12<33:31,  4.37s/it] 78%|███████▊  | 1589/2048 [2:12:16<32:42,  4.28s/it] 78%|███████▊  | 1590/2048 [2:12:20<31:35,  4.14s/it] 78%|███████▊  | 1591/2048 [2:12:24<30:18,  3.98s/it] 78%|███████▊  | 1592/2048 [2:12:28<30:01,  3.95s/it] 78%|███████▊  | 1593/2048 [2:12:33<32:43,  4.32s/it] 78%|███████▊  | 1594/2048 [2:12:38<34:34,  4.57s/it] 78%|███████▊  | 1595/2048 [2:12:43<35:44,  4.73s/it] 78%|███████▊  | 1596/2048 [2:12:48<36:15,  4.81s/it] 78%|███████▊  | 1597/2048 [2:12:53<35:58,  4.79s/it] 78%|███████▊  | 1598/2048 [2:12:57<34:59,  4.67s/it] 78%|███████▊  | 1599/2048 [2:13:01<33:34,  4.49s/it] 78%|███████▊  | 1600/2048 [2:13:05<32:12,  4.31s/it]                                                     {'loss': 0.0924, 'learning_rate': 2.4077673525923805e-05, 'epoch': 11.6}
 78%|███████▊  | 1600/2048 [2:13:05<32:12,  4.31s/it] 78%|███████▊  | 1601/2048 [2:13:09<30:38,  4.11s/it] 78%|███████▊  | 1602/2048 [2:13:13<30:11,  4.06s/it] 78%|███████▊  | 1603/2048 [2:13:17<29:21,  3.96s/it] 78%|███████▊  | 1604/2048 [2:13:20<27:34,  3.73s/it] 78%|███████▊  | 1605/2048 [2:13:25<30:04,  4.07s/it] 78%|███████▊  | 1606/2048 [2:13:30<32:25,  4.40s/it] 78%|███████▊  | 1607/2048 [2:13:35<34:01,  4.63s/it] 79%|███████▊  | 1608/2048 [2:13:40<35:03,  4.78s/it] 79%|███████▊  | 1609/2048 [2:13:45<35:22,  4.83s/it] 79%|███████▊  | 1610/2048 [2:13:50<34:52,  4.78s/it] 79%|███████▊  | 1611/2048 [2:13:54<33:58,  4.66s/it] 79%|███████▊  | 1612/2048 [2:13:58<32:25,  4.46s/it] 79%|███████▉  | 1613/2048 [2:14:02<31:02,  4.28s/it] 79%|███████▉  | 1614/2048 [2:14:06<29:35,  4.09s/it] 79%|███████▉  | 1615/2048 [2:14:10<29:39,  4.11s/it] 79%|███████▉  | 1616/2048 [2:14:13<28:46,  4.00s/it]                                                     {'loss': 0.0862, 'learning_rate': 2.2454920108792355e-05, 'epoch': 11.72}
 79%|███████▉  | 1616/2048 [2:14:13<28:46,  4.00s/it] 79%|███████▉  | 1617/2048 [2:14:17<28:25,  3.96s/it] 79%|███████▉  | 1618/2048 [2:14:22<30:57,  4.32s/it] 79%|███████▉  | 1619/2048 [2:14:28<32:41,  4.57s/it] 79%|███████▉  | 1620/2048 [2:14:33<33:50,  4.74s/it] 79%|███████▉  | 1621/2048 [2:14:38<34:16,  4.82s/it] 79%|███████▉  | 1622/2048 [2:14:42<34:03,  4.80s/it] 79%|███████▉  | 1623/2048 [2:14:47<33:20,  4.71s/it] 79%|███████▉  | 1624/2048 [2:14:51<32:17,  4.57s/it] 79%|███████▉  | 1625/2048 [2:14:55<30:42,  4.35s/it] 79%|███████▉  | 1626/2048 [2:14:59<30:04,  4.28s/it] 79%|███████▉  | 1627/2048 [2:15:03<28:19,  4.04s/it] 79%|███████▉  | 1628/2048 [2:15:06<27:24,  3.92s/it] 80%|███████▉  | 1629/2048 [2:15:10<25:59,  3.72s/it] 80%|███████▉  | 1630/2048 [2:15:14<28:20,  4.07s/it] 80%|███████▉  | 1631/2048 [2:15:20<30:33,  4.40s/it] 80%|███████▉  | 1632/2048 [2:15:25<32:03,  4.62s/it]                                                     {'loss': 0.0962, 'learning_rate': 2.0881838764790007e-05, 'epoch': 11.84}
 80%|███████▉  | 1632/2048 [2:15:25<32:03,  4.62s/it] 80%|███████▉  | 1633/2048 [2:15:30<33:03,  4.78s/it] 80%|███████▉  | 1634/2048 [2:15:35<33:24,  4.84s/it] 80%|███████▉  | 1635/2048 [2:15:40<33:13,  4.83s/it] 80%|███████▉  | 1636/2048 [2:15:44<32:40,  4.76s/it] 80%|███████▉  | 1637/2048 [2:15:49<31:44,  4.63s/it] 80%|███████▉  | 1638/2048 [2:15:53<30:11,  4.42s/it] 80%|████████  | 1639/2048 [2:15:56<29:08,  4.28s/it] 80%|████████  | 1640/2048 [2:16:00<27:59,  4.12s/it] 80%|████████  | 1641/2048 [2:16:03<26:09,  3.86s/it] 80%|████████  | 1642/2048 [2:16:07<26:01,  3.85s/it] 80%|████████  | 1643/2048 [2:16:12<28:38,  4.24s/it] 80%|████████  | 1644/2048 [2:16:18<30:24,  4.52s/it] 80%|████████  | 1645/2048 [2:16:23<31:36,  4.71s/it] 80%|████████  | 1646/2048 [2:16:28<32:05,  4.79s/it] 80%|████████  | 1647/2048 [2:16:33<32:02,  4.79s/it] 80%|████████  | 1648/2048 [2:16:37<31:22,  4.71s/it]                                                     {'loss': 0.1012, 'learning_rate': 1.9359437142779412e-05, 'epoch': 11.95}
 80%|████████  | 1648/2048 [2:16:37<31:22,  4.71s/it] 81%|████████  | 1649/2048 [2:16:41<30:20,  4.56s/it] 81%|████████  | 1650/2048 [2:16:45<29:04,  4.38s/it] 81%|████████  | 1651/2048 [2:16:49<28:36,  4.32s/it] 81%|████████  | 1652/2048 [2:16:54<28:25,  4.31s/it] 81%|████████  | 1653/2048 [2:16:58<27:23,  4.16s/it] 81%|████████  | 1654/2048 [2:17:01<25:09,  3.83s/it] 81%|████████  | 1655/2048 [2:17:05<27:09,  4.15s/it] 81%|████████  | 1656/2048 [2:17:11<29:05,  4.45s/it] 81%|████████  | 1657/2048 [2:17:16<30:24,  4.67s/it] 81%|████████  | 1658/2048 [2:17:21<31:15,  4.81s/it] 81%|████████  | 1659/2048 [2:17:26<31:37,  4.88s/it] 81%|████████  | 1660/2048 [2:17:31<31:15,  4.83s/it] 81%|████████  | 1661/2048 [2:17:35<30:07,  4.67s/it] 81%|████████  | 1662/2048 [2:17:39<29:05,  4.52s/it] 81%|████████  | 1663/2048 [2:17:43<27:53,  4.35s/it] 81%|████████▏ | 1664/2048 [2:17:47<27:00,  4.22s/it]                                                     {'loss': 0.0902, 'learning_rate': 1.788869042835335e-05, 'epoch': 12.07}
 81%|████████▏ | 1664/2048 [2:17:47<27:00,  4.22s/it] 81%|████████▏ | 1665/2048 [2:17:51<26:19,  4.12s/it] 81%|████████▏ | 1666/2048 [2:17:55<25:29,  4.00s/it] 81%|████████▏ | 1667/2048 [2:17:58<23:45,  3.74s/it] 81%|████████▏ | 1668/2048 [2:18:03<26:24,  4.17s/it] 81%|████████▏ | 1669/2048 [2:18:08<28:13,  4.47s/it] 82%|████████▏ | 1670/2048 [2:18:13<29:26,  4.67s/it] 82%|████████▏ | 1671/2048 [2:18:18<30:09,  4.80s/it] 82%|████████▏ | 1672/2048 [2:18:23<30:09,  4.81s/it] 82%|████████▏ | 1673/2048 [2:18:28<29:55,  4.79s/it] 82%|████████▏ | 1674/2048 [2:18:32<28:52,  4.63s/it] 82%|████████▏ | 1675/2048 [2:18:36<27:33,  4.43s/it] 82%|████████▏ | 1676/2048 [2:18:40<26:17,  4.24s/it] 82%|████████▏ | 1677/2048 [2:18:44<25:31,  4.13s/it] 82%|████████▏ | 1678/2048 [2:18:48<25:20,  4.11s/it] 82%|████████▏ | 1679/2048 [2:18:51<24:04,  3.91s/it] 82%|████████▏ | 1680/2048 [2:18:55<24:23,  3.98s/it]                                                     {'loss': 0.0893, 'learning_rate': 1.647054071917231e-05, 'epoch': 12.18}
 82%|████████▏ | 1680/2048 [2:18:55<24:23,  3.98s/it] 82%|████████▏ | 1681/2048 [2:19:01<26:30,  4.33s/it] 82%|████████▏ | 1682/2048 [2:19:06<27:56,  4.58s/it] 82%|████████▏ | 1683/2048 [2:19:11<28:51,  4.74s/it] 82%|████████▏ | 1684/2048 [2:19:16<29:07,  4.80s/it] 82%|████████▏ | 1685/2048 [2:19:21<29:07,  4.81s/it] 82%|████████▏ | 1686/2048 [2:19:25<28:28,  4.72s/it] 82%|████████▏ | 1687/2048 [2:19:29<27:13,  4.52s/it] 82%|████████▏ | 1688/2048 [2:19:33<26:20,  4.39s/it] 82%|████████▏ | 1689/2048 [2:19:38<25:52,  4.32s/it] 83%|████████▎ | 1690/2048 [2:19:42<25:22,  4.25s/it] 83%|████████▎ | 1691/2048 [2:19:45<24:24,  4.10s/it] 83%|████████▎ | 1692/2048 [2:19:48<22:32,  3.80s/it] 83%|████████▎ | 1693/2048 [2:19:54<24:54,  4.21s/it] 83%|████████▎ | 1694/2048 [2:19:59<26:32,  4.50s/it] 83%|████████▎ | 1695/2048 [2:20:04<27:37,  4.70s/it] 83%|████████▎ | 1696/2048 [2:20:09<28:19,  4.83s/it]                                                     {'loss': 0.0859, 'learning_rate': 1.5105896421497113e-05, 'epoch': 12.3}
 83%|████████▎ | 1696/2048 [2:20:09<28:19,  4.83s/it] 83%|████████▎ | 1697/2048 [2:20:14<28:27,  4.86s/it] 83%|████████▎ | 1698/2048 [2:20:19<27:53,  4.78s/it] 83%|████████▎ | 1699/2048 [2:20:23<27:28,  4.72s/it] 83%|████████▎ | 1700/2048 [2:20:27<26:26,  4.56s/it] 83%|████████▎ | 1701/2048 [2:20:31<25:15,  4.37s/it] 83%|████████▎ | 1702/2048 [2:20:35<23:53,  4.14s/it] 83%|████████▎ | 1703/2048 [2:20:39<23:17,  4.05s/it] 83%|████████▎ | 1704/2048 [2:20:42<22:27,  3.92s/it] 83%|████████▎ | 1705/2048 [2:20:47<22:44,  3.98s/it] 83%|████████▎ | 1706/2048 [2:20:52<24:42,  4.33s/it] 83%|████████▎ | 1707/2048 [2:20:57<26:03,  4.58s/it] 83%|████████▎ | 1708/2048 [2:21:02<26:55,  4.75s/it] 83%|████████▎ | 1709/2048 [2:21:07<27:18,  4.83s/it] 83%|████████▎ | 1710/2048 [2:21:12<27:10,  4.82s/it] 84%|████████▎ | 1711/2048 [2:21:17<26:54,  4.79s/it] 84%|████████▎ | 1712/2048 [2:21:21<25:37,  4.58s/it]                                                     {'loss': 0.0941, 'learning_rate': 1.37956316683023e-05, 'epoch': 12.42}
 84%|████████▎ | 1712/2048 [2:21:21<25:37,  4.58s/it] 84%|████████▎ | 1713/2048 [2:21:25<24:48,  4.44s/it] 84%|████████▎ | 1714/2048 [2:21:28<23:22,  4.20s/it] 84%|████████▎ | 1715/2048 [2:21:32<22:29,  4.05s/it] 84%|████████▍ | 1716/2048 [2:21:36<21:32,  3.89s/it] 84%|████████▍ | 1717/2048 [2:21:39<20:07,  3.65s/it] 84%|████████▍ | 1718/2048 [2:21:44<22:34,  4.10s/it] 84%|████████▍ | 1719/2048 [2:21:49<24:15,  4.42s/it] 84%|████████▍ | 1720/2048 [2:21:54<25:22,  4.64s/it] 84%|████████▍ | 1721/2048 [2:21:59<26:05,  4.79s/it] 84%|████████▍ | 1722/2048 [2:22:04<26:20,  4.85s/it] 84%|████████▍ | 1723/2048 [2:22:09<25:59,  4.80s/it] 84%|████████▍ | 1724/2048 [2:22:13<25:14,  4.67s/it] 84%|████████▍ | 1725/2048 [2:22:18<24:24,  4.54s/it] 84%|████████▍ | 1726/2048 [2:22:21<23:23,  4.36s/it] 84%|████████▍ | 1727/2048 [2:22:26<22:56,  4.29s/it] 84%|████████▍ | 1728/2048 [2:22:29<22:07,  4.15s/it]                                                     {'loss': 0.0845, 'learning_rate': 1.2540585759343958e-05, 'epoch': 12.53}
 84%|████████▍ | 1728/2048 [2:22:29<22:07,  4.15s/it] 84%|████████▍ | 1729/2048 [2:22:33<20:49,  3.92s/it] 84%|████████▍ | 1730/2048 [2:22:37<20:59,  3.96s/it] 85%|████████▍ | 1731/2048 [2:22:42<22:50,  4.32s/it] 85%|████████▍ | 1732/2048 [2:22:47<24:05,  4.57s/it] 85%|████████▍ | 1733/2048 [2:22:52<24:48,  4.73s/it] 85%|████████▍ | 1734/2048 [2:22:57<25:01,  4.78s/it] 85%|████████▍ | 1735/2048 [2:23:02<24:50,  4.76s/it] 85%|████████▍ | 1736/2048 [2:23:06<24:24,  4.69s/it] 85%|████████▍ | 1737/2048 [2:23:11<24:02,  4.64s/it] 85%|████████▍ | 1738/2048 [2:23:15<23:01,  4.46s/it] 85%|████████▍ | 1739/2048 [2:23:19<22:12,  4.31s/it] 85%|████████▍ | 1740/2048 [2:23:23<21:50,  4.25s/it] 85%|████████▌ | 1741/2048 [2:23:27<20:55,  4.09s/it] 85%|████████▌ | 1742/2048 [2:23:30<19:22,  3.80s/it] 85%|████████▌ | 1743/2048 [2:23:35<21:23,  4.21s/it] 85%|████████▌ | 1744/2048 [2:23:40<22:47,  4.50s/it]                                                     {'loss': 0.0818, 'learning_rate': 1.1341562623540081e-05, 'epoch': 12.65}
 85%|████████▌ | 1744/2048 [2:23:40<22:47,  4.50s/it] 85%|████████▌ | 1745/2048 [2:23:45<23:41,  4.69s/it] 85%|████████▌ | 1746/2048 [2:23:50<24:13,  4.81s/it] 85%|████████▌ | 1747/2048 [2:23:55<24:06,  4.81s/it] 85%|████████▌ | 1748/2048 [2:24:00<23:52,  4.78s/it] 85%|████████▌ | 1749/2048 [2:24:04<23:06,  4.64s/it] 85%|████████▌ | 1750/2048 [2:24:09<22:31,  4.53s/it] 85%|████████▌ | 1751/2048 [2:24:13<21:33,  4.35s/it] 86%|████████▌ | 1752/2048 [2:24:16<20:49,  4.22s/it] 86%|████████▌ | 1753/2048 [2:24:21<20:36,  4.19s/it] 86%|████████▌ | 1754/2048 [2:24:24<19:34,  3.99s/it] 86%|████████▌ | 1755/2048 [2:24:28<19:39,  4.02s/it] 86%|████████▌ | 1756/2048 [2:24:33<21:15,  4.37s/it] 86%|████████▌ | 1757/2048 [2:24:39<22:19,  4.60s/it] 86%|████████▌ | 1758/2048 [2:24:44<22:58,  4.75s/it] 86%|████████▌ | 1759/2048 [2:24:48<22:59,  4.77s/it] 86%|████████▌ | 1760/2048 [2:24:53<22:44,  4.74s/it]                                                     {'loss': 0.0885, 'learning_rate': 1.019933030400786e-05, 'epoch': 12.77}
 86%|████████▌ | 1760/2048 [2:24:53<22:44,  4.74s/it] 86%|████████▌ | 1761/2048 [2:24:58<22:32,  4.71s/it] 86%|████████▌ | 1762/2048 [2:25:02<21:37,  4.54s/it] 86%|████████▌ | 1763/2048 [2:25:06<20:24,  4.30s/it] 86%|████████▌ | 1764/2048 [2:25:10<20:18,  4.29s/it] 86%|████████▌ | 1765/2048 [2:25:14<19:30,  4.14s/it] 86%|████████▌ | 1766/2048 [2:25:17<18:43,  3.98s/it] 86%|████████▋ | 1767/2048 [2:25:20<17:22,  3.71s/it] 86%|████████▋ | 1768/2048 [2:25:26<19:21,  4.15s/it] 86%|████████▋ | 1769/2048 [2:25:31<20:42,  4.45s/it] 86%|████████▋ | 1770/2048 [2:25:36<21:35,  4.66s/it] 86%|████████▋ | 1771/2048 [2:25:41<22:09,  4.80s/it] 87%|████████▋ | 1772/2048 [2:25:46<22:10,  4.82s/it] 87%|████████▋ | 1773/2048 [2:25:50<21:51,  4.77s/it] 87%|████████▋ | 1774/2048 [2:25:55<21:14,  4.65s/it] 87%|████████▋ | 1775/2048 [2:25:59<20:23,  4.48s/it] 87%|████████▋ | 1776/2048 [2:26:03<19:37,  4.33s/it]                                                     {'loss': 0.0976, 'learning_rate': 9.114620466088108e-06, 'epoch': 12.88}
 87%|████████▋ | 1776/2048 [2:26:03<19:37,  4.33s/it] 87%|████████▋ | 1777/2048 [2:26:07<18:49,  4.17s/it] 87%|████████▋ | 1778/2048 [2:26:11<18:32,  4.12s/it] 87%|████████▋ | 1779/2048 [2:26:14<17:41,  3.95s/it] 87%|████████▋ | 1780/2048 [2:26:18<17:50,  4.00s/it] 87%|████████▋ | 1781/2048 [2:26:24<19:21,  4.35s/it] 87%|████████▋ | 1782/2048 [2:26:29<20:21,  4.59s/it] 87%|████████▋ | 1783/2048 [2:26:34<21:00,  4.76s/it] 87%|████████▋ | 1784/2048 [2:26:39<21:21,  4.85s/it] 87%|████████▋ | 1785/2048 [2:26:44<21:27,  4.90s/it] 87%|████████▋ | 1786/2048 [2:26:48<20:56,  4.80s/it] 87%|████████▋ | 1787/2048 [2:26:53<20:19,  4.67s/it] 87%|████████▋ | 1788/2048 [2:26:57<19:18,  4.46s/it] 87%|████████▋ | 1789/2048 [2:27:01<18:23,  4.26s/it] 87%|████████▋ | 1790/2048 [2:27:04<17:38,  4.10s/it] 87%|████████▋ | 1791/2048 [2:27:08<16:41,  3.90s/it] 88%|████████▊ | 1792/2048 [2:27:11<15:38,  3.66s/it]                                                     {'loss': 0.0896, 'learning_rate': 8.088127928671586e-06, 'epoch': 13.0}
 88%|████████▊ | 1792/2048 [2:27:11<15:38,  3.66s/it]
  0%|          | 0/256 [00:00<?, ?it/s][A
  2%|▏         | 4/256 [00:00<00:09, 27.42it/s][A
  3%|▎         | 7/256 [00:00<00:11, 21.13it/s][A
  4%|▍         | 10/256 [00:00<00:12, 19.36it/s][A
  5%|▍         | 12/256 [00:00<00:13, 18.41it/s][A
  5%|▌         | 14/256 [00:00<00:13, 17.87it/s][A
  6%|▋         | 16/256 [00:00<00:13, 18.21it/s][A
  7%|▋         | 18/256 [00:00<00:13, 17.81it/s][A
  8%|▊         | 21/256 [00:01<00:12, 18.55it/s][A
  9%|▉         | 23/256 [00:01<00:12, 18.05it/s][A
 10%|▉         | 25/256 [00:01<00:13, 17.69it/s][A
 11%|█         | 27/256 [00:01<00:12, 17.74it/s][A
 11%|█▏        | 29/256 [00:01<00:12, 17.47it/s][A
 12%|█▏        | 31/256 [00:01<00:13, 17.28it/s][A
 13%|█▎        | 34/256 [00:01<00:11, 18.72it/s][A
 14%|█▍        | 36/256 [00:01<00:12, 18.18it/s][A
 15%|█▍        | 38/256 [00:02<00:12, 17.65it/s][A
 16%|█▌        | 40/256 [00:02<00:12, 17.38it/s][A
 16%|█▋        | 42/256 [00:02<00:12, 17.75it/s][A
 17%|█▋        | 44/256 [00:02<00:12, 17.49it/s][A
 18%|█▊        | 46/256 [00:02<00:12, 17.28it/s][A
 19%|█▉        | 48/256 [00:02<00:11, 17.39it/s][A
 20%|█▉        | 50/256 [00:02<00:12, 17.10it/s][A
 20%|██        | 52/256 [00:02<00:11, 17.01it/s][A
 21%|██        | 54/256 [00:02<00:11, 17.79it/s][A
 22%|██▏       | 56/256 [00:03<00:11, 17.61it/s][A
 23%|██▎       | 58/256 [00:03<00:11, 17.52it/s][A
 23%|██▎       | 60/256 [00:03<00:11, 17.28it/s][A
 24%|██▍       | 62/256 [00:03<00:11, 16.95it/s][A
 25%|██▌       | 64/256 [00:03<00:11, 17.25it/s][A
 26%|██▌       | 66/256 [00:03<00:11, 17.10it/s][A
 27%|██▋       | 68/256 [00:03<00:11, 17.01it/s][A
 27%|██▋       | 70/256 [00:03<00:10, 17.15it/s][A
 28%|██▊       | 72/256 [00:04<00:10, 17.04it/s][A
 29%|██▉       | 74/256 [00:04<00:10, 16.99it/s][A
 30%|██▉       | 76/256 [00:04<00:10, 17.00it/s][A
 30%|███       | 78/256 [00:04<00:10, 16.98it/s][A
 31%|███▏      | 80/256 [00:04<00:10, 16.93it/s][A
 32%|███▏      | 82/256 [00:04<00:10, 17.00it/s][A
 33%|███▎      | 84/256 [00:04<00:10, 17.16it/s][A
 34%|███▎      | 86/256 [00:04<00:09, 17.06it/s][A
 34%|███▍      | 88/256 [00:04<00:09, 17.29it/s][A
 35%|███▌      | 90/256 [00:05<00:09, 17.03it/s][A
 36%|███▌      | 92/256 [00:05<00:09, 16.93it/s][A
 37%|███▋      | 94/256 [00:05<00:09, 17.01it/s][A
 38%|███▊      | 96/256 [00:05<00:09, 16.92it/s][A
 39%|███▊      | 99/256 [00:05<00:08, 17.74it/s][A
 39%|███▉      | 101/256 [00:05<00:08, 17.38it/s][A
 40%|████      | 103/256 [00:05<00:08, 17.48it/s][A
 41%|████▏     | 106/256 [00:06<00:08, 18.36it/s][A
 42%|████▏     | 108/256 [00:06<00:08, 17.98it/s][A
 43%|████▎     | 110/256 [00:06<00:08, 17.75it/s][A
 44%|████▍     | 112/256 [00:06<00:08, 17.45it/s][A
 45%|████▍     | 114/256 [00:06<00:08, 17.59it/s][A
 46%|████▌     | 117/256 [00:06<00:07, 18.04it/s][A
 46%|████▋     | 119/256 [00:06<00:07, 17.84it/s][A
 47%|████▋     | 121/256 [00:06<00:07, 17.46it/s][A
 48%|████▊     | 123/256 [00:06<00:07, 17.33it/s][A
 49%|████▉     | 125/256 [00:07<00:07, 17.15it/s][A
 50%|█████     | 128/256 [00:07<00:06, 18.67it/s][A
 51%|█████     | 130/256 [00:07<00:06, 18.24it/s][A
 52%|█████▏    | 132/256 [00:07<00:07, 17.69it/s][A
 52%|█████▏    | 134/256 [00:07<00:06, 17.55it/s][A
 53%|█████▎    | 136/256 [00:07<00:06, 17.40it/s][A
 54%|█████▍    | 138/256 [00:07<00:06, 17.20it/s][A
 55%|█████▍    | 140/256 [00:07<00:06, 17.07it/s][A
 56%|█████▌    | 143/256 [00:08<00:06, 18.21it/s][A
 57%|█████▋    | 145/256 [00:08<00:06, 17.65it/s][A
 57%|█████▋    | 147/256 [00:08<00:06, 17.44it/s][A
 58%|█████▊    | 149/256 [00:08<00:06, 17.25it/s][A
 59%|█████▉    | 151/256 [00:08<00:06, 17.12it/s][A
 60%|██████    | 154/256 [00:08<00:05, 17.74it/s][A
 61%|██████    | 156/256 [00:08<00:05, 17.60it/s][A
 62%|██████▏   | 158/256 [00:08<00:05, 17.25it/s][A
 63%|██████▎   | 161/256 [00:09<00:05, 17.98it/s][A
 64%|██████▎   | 163/256 [00:09<00:05, 17.55it/s][A
 64%|██████▍   | 165/256 [00:09<00:05, 17.45it/s][A
 65%|██████▌   | 167/256 [00:09<00:05, 17.08it/s][A
 66%|██████▌   | 169/256 [00:09<00:05, 17.33it/s][A
 67%|██████▋   | 171/256 [00:09<00:04, 17.08it/s][A
 68%|██████▊   | 173/256 [00:09<00:04, 16.94it/s][A
 68%|██████▊   | 175/256 [00:09<00:04, 17.00it/s][A
 69%|██████▉   | 177/256 [00:10<00:04, 16.83it/s][A
 70%|██████▉   | 179/256 [00:10<00:04, 16.92it/s][A
 71%|███████   | 182/256 [00:10<00:03, 19.22it/s][A
 72%|███████▏  | 184/256 [00:10<00:03, 18.60it/s][A
 73%|███████▎  | 186/256 [00:10<00:03, 17.93it/s][A
 74%|███████▍  | 189/256 [00:10<00:03, 18.43it/s][A
 75%|███████▍  | 191/256 [00:10<00:03, 17.84it/s][A
 75%|███████▌  | 193/256 [00:10<00:03, 17.67it/s][A
 76%|███████▌  | 195/256 [00:11<00:03, 17.28it/s][A
 77%|███████▋  | 197/256 [00:11<00:03, 17.58it/s][A
 78%|███████▊  | 199/256 [00:11<00:03, 18.18it/s][A
 79%|███████▊  | 201/256 [00:11<00:03, 17.87it/s][A
 79%|███████▉  | 203/256 [00:11<00:03, 17.41it/s][A
 80%|████████  | 205/256 [00:11<00:02, 17.36it/s][A
 81%|████████  | 207/256 [00:11<00:02, 17.05it/s][A
 82%|████████▏ | 209/256 [00:11<00:02, 16.95it/s][A
 82%|████████▏ | 211/256 [00:11<00:02, 16.92it/s][A
 83%|████████▎ | 213/256 [00:12<00:02, 16.88it/s][A
 84%|████████▍ | 215/256 [00:12<00:02, 16.84it/s][A
 85%|████████▍ | 217/256 [00:12<00:02, 16.83it/s][A
 86%|████████▌ | 219/256 [00:12<00:02, 16.93it/s][A
 86%|████████▋ | 221/256 [00:12<00:02, 16.76it/s][A
 87%|████████▋ | 223/256 [00:12<00:01, 16.77it/s][A
 88%|████████▊ | 225/256 [00:12<00:01, 17.20it/s][A
 89%|████████▊ | 227/256 [00:12<00:01, 16.93it/s][A
 89%|████████▉ | 229/256 [00:13<00:01, 16.85it/s][A
 90%|█████████ | 231/256 [00:13<00:01, 17.08it/s][A
 91%|█████████ | 233/256 [00:13<00:01, 17.05it/s][A
 92%|█████████▏| 235/256 [00:13<00:01, 17.02it/s][A
 93%|█████████▎| 238/256 [00:13<00:01, 17.94it/s][A
 94%|█████████▍| 240/256 [00:13<00:00, 17.73it/s][A
 95%|█████████▍| 242/256 [00:13<00:00, 17.45it/s][A
 95%|█████████▌| 244/256 [00:13<00:00, 17.77it/s][A
 96%|█████████▋| 247/256 [00:14<00:00, 18.26it/s][A
 97%|█████████▋| 249/256 [00:14<00:00, 17.95it/s][A
 98%|█████████▊| 252/256 [00:14<00:00, 19.02it/s][A
 99%|█████████▉| 254/256 [00:14<00:00, 18.55it/s][A
100%|██████████| 256/256 [00:14<00:00, 16.94it/s][A                                                     
                                                 [A{'eval_loss': 2.228281021118164, 'eval_runtime': 14.7459, 'eval_samples_per_second': 69.443, 'eval_steps_per_second': 17.361, 'epoch': 13.0}
 88%|████████▊ | 1792/2048 [2:27:26<15:38,  3.66s/it]
100%|██████████| 256/256 [00:14<00:00, 16.94it/s][A
                                                 [A  0%|          | 0/383 [00:00<?, ?it/s]  0%|          | 0/383 [00:00<?, ?it/s]  0%|          | 0/383 [00:00<?, ?it/s]
  0%|          | 0/383 [00:00<?, ?it/s][A  0%|          | 1/383 [00:00<00:40,  9.47it/s]
  0%|          | 1/383 [00:00<00:51,  7.48it/s][A  1%|          | 2/383 [00:00<00:25, 14.66it/s]  1%|          | 2/383 [00:00<00:26, 14.13it/s]  1%|          | 3/383 [00:00<00:24, 15.39it/s]
  1%|          | 4/383 [00:00<00:23, 16.41it/s][A  1%|▏         | 5/383 [00:00<00:19, 19.41it/s]  1%|▏         | 5/383 [00:00<00:20, 18.43it/s]  1%|▏         | 5/383 [00:00<00:21, 17.38it/s]  2%|▏         | 7/383 [00:00<00:20, 17.93it/s]
  2%|▏         | 7/383 [00:00<00:20, 18.72it/s][A  2%|▏         | 7/383 [00:00<00:21, 17.20it/s]  2%|▏         | 7/383 [00:00<00:22, 16.78it/s]  2%|▏         | 9/383 [00:00<00:23, 16.10it/s]  2%|▏         | 9/383 [00:00<00:24, 15.47it/s]
  2%|▏         | 9/383 [00:00<00:23, 16.24it/s][A  2%|▏         | 9/383 [00:00<00:23, 16.02it/s]  3%|▎         | 11/383 [00:00<00:23, 15.54it/s]
  3%|▎         | 11/383 [00:00<00:24, 15.50it/s][A  3%|▎         | 11/383 [00:00<00:25, 14.65it/s]  3%|▎         | 11/383 [00:00<00:24, 15.04it/s]  3%|▎         | 13/383 [00:00<00:24, 15.08it/s]  3%|▎         | 13/383 [00:00<00:24, 15.34it/s]
  3%|▎         | 13/383 [00:00<00:24, 14.90it/s][A  3%|▎         | 13/383 [00:00<00:25, 14.44it/s]  4%|▍         | 15/383 [00:00<00:22, 16.13it/s]  4%|▍         | 15/383 [00:00<00:22, 16.26it/s]
  4%|▍         | 15/383 [00:00<00:23, 15.93it/s][A  4%|▍         | 15/383 [00:00<00:23, 15.46it/s]  4%|▍         | 17/383 [00:01<00:21, 16.78it/s]  4%|▍         | 17/383 [00:01<00:21, 16.99it/s]
  4%|▍         | 17/383 [00:01<00:21, 16.81it/s][A  4%|▍         | 17/383 [00:01<00:22, 16.25it/s]  5%|▍         | 19/383 [00:01<00:20, 17.35it/s]  5%|▍         | 19/383 [00:01<00:20, 17.54it/s]
  5%|▍         | 19/383 [00:01<00:20, 17.42it/s][A  5%|▍         | 19/383 [00:01<00:22, 16.52it/s]  5%|▌         | 21/383 [00:01<00:20, 17.36it/s]  5%|▌         | 21/383 [00:01<00:20, 17.69it/s]
  5%|▌         | 21/383 [00:01<00:20, 17.71it/s][A  5%|▌         | 21/383 [00:01<00:21, 16.82it/s]  6%|▌         | 23/383 [00:01<00:20, 17.23it/s]
  6%|▌         | 23/383 [00:01<00:20, 17.61it/s][A  6%|▌         | 23/383 [00:01<00:20, 17.36it/s]  6%|▌         | 23/383 [00:01<00:21, 17.01it/s]  7%|▋         | 25/383 [00:01<00:21, 16.74it/s]
  7%|▋         | 25/383 [00:01<00:20, 17.39it/s][A  7%|▋         | 25/383 [00:01<00:21, 17.02it/s]  7%|▋         | 25/383 [00:01<00:21, 16.73it/s]
  7%|▋         | 27/383 [00:01<00:20, 17.11it/s][A  7%|▋         | 27/383 [00:01<00:24, 14.62it/s]  7%|▋         | 27/383 [00:01<00:24, 14.69it/s]  7%|▋         | 27/383 [00:01<00:24, 14.42it/s]
  8%|▊         | 29/383 [00:01<00:25, 13.69it/s][A  8%|▊         | 29/383 [00:01<00:27, 12.84it/s]  8%|▊         | 29/383 [00:01<00:27, 12.83it/s]  8%|▊         | 29/383 [00:01<00:28, 12.45it/s]
  8%|▊         | 31/383 [00:01<00:25, 13.58it/s][A  8%|▊         | 31/383 [00:02<00:27, 13.00it/s]  8%|▊         | 31/383 [00:02<00:26, 13.09it/s]  8%|▊         | 31/383 [00:02<00:26, 13.18it/s]
  9%|▊         | 33/383 [00:02<00:25, 13.78it/s][A  9%|▊         | 33/383 [00:02<00:25, 13.69it/s]  9%|▊         | 33/383 [00:02<00:25, 13.80it/s]  9%|▊         | 33/383 [00:02<00:24, 14.05it/s]
  9%|▉         | 35/383 [00:02<00:23, 14.51it/s][A  9%|▉         | 35/383 [00:02<00:24, 14.12it/s]  9%|▉         | 35/383 [00:02<00:24, 14.37it/s]  9%|▉         | 35/383 [00:02<00:23, 14.64it/s]
 10%|▉         | 37/383 [00:02<00:23, 14.83it/s][A 10%|▉         | 37/383 [00:02<00:23, 14.92it/s] 10%|▉         | 37/383 [00:02<00:23, 14.97it/s] 10%|▉         | 37/383 [00:02<00:22, 15.23it/s]
 10%|█         | 39/383 [00:02<00:22, 15.36it/s][A 10%|█         | 39/383 [00:02<00:22, 15.48it/s] 10%|█         | 39/383 [00:02<00:22, 15.56it/s] 10%|█         | 39/383 [00:02<00:21, 15.72it/s]
 11%|█         | 41/383 [00:02<00:21, 15.96it/s][A 11%|█         | 41/383 [00:02<00:21, 15.92it/s] 11%|█         | 41/383 [00:02<00:21, 16.04it/s] 11%|█         | 41/383 [00:02<00:21, 15.87it/s] 11%|█         | 43/383 [00:02<00:20, 16.91it/s]
 11%|█▏        | 44/383 [00:02<00:18, 17.85it/s][A 11%|█▏        | 44/383 [00:02<00:18, 18.35it/s] 11%|█         | 43/383 [00:02<00:20, 16.51it/s] 12%|█▏        | 46/383 [00:02<00:17, 19.54it/s]
 12%|█▏        | 47/383 [00:02<00:16, 20.04it/s][A 12%|█▏        | 47/383 [00:02<00:16, 20.62it/s] 12%|█▏        | 46/383 [00:02<00:17, 19.16it/s] 13%|█▎        | 49/383 [00:02<00:15, 21.45it/s]
 13%|█▎        | 50/383 [00:02<00:15, 21.59it/s][A 13%|█▎        | 49/383 [00:03<00:15, 21.20it/s] 13%|█▎        | 50/383 [00:03<00:16, 20.80it/s]
 14%|█▍        | 53/383 [00:03<00:17, 18.41it/s][A 14%|█▎        | 52/383 [00:03<00:18, 17.43it/s] 14%|█▍        | 53/383 [00:03<00:17, 18.46it/s] 14%|█▎        | 52/383 [00:03<00:18, 18.13it/s]
 14%|█▍        | 55/383 [00:03<00:17, 18.32it/s][A 14%|█▍        | 54/383 [00:03<00:18, 17.76it/s] 14%|█▍        | 55/383 [00:03<00:17, 18.38it/s] 14%|█▍        | 54/383 [00:03<00:18, 17.88it/s]
 15%|█▍        | 57/383 [00:03<00:18, 18.03it/s][A 15%|█▍        | 56/383 [00:03<00:18, 17.99it/s] 15%|█▍        | 57/383 [00:03<00:17, 18.23it/s] 15%|█▍        | 56/383 [00:03<00:18, 17.97it/s] 15%|█▌        | 58/383 [00:03<00:18, 17.39it/s]
 15%|█▌        | 59/383 [00:03<00:18, 17.16it/s][A 15%|█▌        | 59/383 [00:03<00:18, 17.18it/s] 15%|█▌        | 58/383 [00:03<00:18, 17.21it/s] 16%|█▌        | 60/383 [00:03<00:19, 16.83it/s]
 16%|█▌        | 61/383 [00:03<00:19, 16.20it/s][A 16%|█▌        | 61/383 [00:03<00:19, 16.77it/s] 16%|█▌        | 60/383 [00:03<00:19, 16.84it/s] 16%|█▌        | 62/383 [00:03<00:19, 16.09it/s]
 16%|█▋        | 63/383 [00:03<00:19, 16.16it/s][A 16%|█▋        | 63/383 [00:03<00:19, 16.40it/s] 16%|█▌        | 62/383 [00:03<00:19, 16.23it/s]
 17%|█▋        | 65/383 [00:03<00:19, 16.14it/s][A 17%|█▋        | 64/383 [00:03<00:19, 15.98it/s] 17%|█▋        | 64/383 [00:03<00:19, 16.25it/s] 17%|█▋        | 65/383 [00:03<00:19, 15.91it/s] 17%|█▋        | 66/383 [00:04<00:20, 15.73it/s]
 17%|█▋        | 67/383 [00:04<00:20, 15.74it/s][A 17%|█▋        | 66/383 [00:04<00:19, 16.25it/s] 17%|█▋        | 67/383 [00:04<00:20, 15.68it/s] 18%|█▊        | 68/383 [00:04<00:20, 15.65it/s] 18%|█▊        | 68/383 [00:04<00:20, 15.04it/s]
 18%|█▊        | 69/383 [00:04<00:21, 14.94it/s][A 18%|█▊        | 69/383 [00:04<00:20, 14.95it/s]
 19%|█▊        | 71/383 [00:04<00:20, 15.10it/s][A 18%|█▊        | 70/383 [00:04<00:20, 15.05it/s] 18%|█▊        | 70/383 [00:04<00:21, 14.63it/s] 19%|█▊        | 71/383 [00:04<00:20, 15.11it/s]
 19%|█▉        | 73/383 [00:04<00:19, 16.06it/s][A 19%|█▉        | 72/383 [00:04<00:20, 15.53it/s] 19%|█▉        | 72/383 [00:04<00:19, 15.71it/s] 19%|█▉        | 73/383 [00:04<00:19, 15.71it/s]
 20%|█▉        | 75/383 [00:04<00:19, 15.93it/s][A 19%|█▉        | 74/383 [00:04<00:19, 16.21it/s] 20%|█▉        | 75/383 [00:04<00:19, 15.70it/s] 19%|█▉        | 74/383 [00:04<00:20, 15.29it/s] 20%|█▉        | 76/383 [00:04<00:18, 16.27it/s]
 20%|██        | 77/383 [00:04<00:19, 15.95it/s][A 20%|█▉        | 76/383 [00:04<00:19, 15.65it/s] 20%|██        | 77/383 [00:04<00:19, 15.82it/s] 20%|██        | 78/383 [00:04<00:18, 16.17it/s]
 21%|██        | 79/383 [00:04<00:19, 15.94it/s][A 20%|██        | 78/383 [00:04<00:19, 15.82it/s] 21%|██        | 79/383 [00:04<00:19, 15.84it/s] 21%|██        | 80/383 [00:04<00:18, 16.10it/s]
 21%|██        | 81/383 [00:04<00:19, 15.75it/s][A 21%|██        | 80/383 [00:04<00:19, 15.76it/s] 21%|██        | 81/383 [00:05<00:19, 15.60it/s]
 21%|██▏       | 82/383 [00:05<00:18, 15.95it/s] 22%|██▏       | 83/383 [00:05<00:18, 15.93it/s][A 21%|██▏       | 82/383 [00:05<00:18, 15.96it/s] 22%|██▏       | 83/383 [00:05<00:19, 15.75it/s] 22%|██▏       | 84/383 [00:05<00:18, 15.99it/s]
 22%|██▏       | 85/383 [00:05<00:18, 15.72it/s][A 22%|██▏       | 84/383 [00:05<00:18, 16.12it/s] 22%|██▏       | 85/383 [00:05<00:18, 16.06it/s] 22%|██▏       | 86/383 [00:05<00:18, 16.24it/s] 22%|██▏       | 86/383 [00:05<00:18, 16.08it/s]
 23%|██▎       | 87/383 [00:05<00:20, 14.24it/s][A 23%|██▎       | 87/383 [00:05<00:20, 14.18it/s] 23%|██▎       | 88/383 [00:05<00:22, 13.00it/s] 23%|██▎       | 88/383 [00:05<00:23, 12.53it/s]
 23%|██▎       | 89/383 [00:05<00:24, 11.83it/s][A 23%|██▎       | 89/383 [00:05<00:32,  9.03it/s] 23%|██▎       | 90/383 [00:05<00:34,  8.60it/s]
 24%|██▍       | 91/383 [00:06<00:39,  7.39it/s][A 23%|██▎       | 90/383 [00:06<00:42,  6.90it/s] 24%|██▍       | 91/383 [00:06<00:48,  6.02it/s] 24%|██▍       | 92/383 [00:06<00:48,  5.98it/s] 24%|██▍       | 92/383 [00:06<00:47,  6.09it/s] 24%|██▍       | 92/383 [00:06<00:52,  5.56it/s]
 24%|██▍       | 93/383 [00:06<00:50,  5.71it/s][A 25%|██▍       | 94/383 [00:06<00:39,  7.30it/s]
 25%|██▌       | 96/383 [00:06<00:36,  7.92it/s][A 24%|██▍       | 93/383 [00:06<00:55,  5.26it/s] 25%|██▌       | 96/383 [00:06<00:31,  9.18it/s] 24%|██▍       | 93/383 [00:06<00:53,  5.44it/s]
 26%|██▌       | 98/383 [00:06<00:30,  9.45it/s][A 25%|██▍       | 95/383 [00:06<00:41,  6.93it/s] 26%|██▌       | 98/383 [00:06<00:25, 11.06it/s] 25%|██▍       | 95/383 [00:06<00:40,  7.17it/s]
 26%|██▌       | 100/383 [00:07<00:25, 10.89it/s][A 25%|██▌       | 97/383 [00:07<00:32,  8.78it/s] 26%|██▌       | 100/383 [00:07<00:22, 12.39it/s] 25%|██▌       | 97/383 [00:07<00:31,  9.01it/s]
 27%|██▋       | 102/383 [00:07<00:22, 12.24it/s][A 26%|██▌       | 99/383 [00:07<00:27, 10.48it/s] 27%|██▋       | 102/383 [00:07<00:20, 13.76it/s] 26%|██▌       | 99/383 [00:07<00:26, 10.70it/s]
 27%|██▋       | 104/383 [00:07<00:20, 13.36it/s][A 26%|██▋       | 101/383 [00:07<00:23, 11.80it/s] 27%|██▋       | 104/383 [00:07<00:18, 15.03it/s] 26%|██▋       | 101/383 [00:07<00:23, 12.11it/s]
 28%|██▊       | 106/383 [00:07<00:18, 14.74it/s][A 28%|██▊       | 106/383 [00:07<00:17, 16.14it/s] 27%|██▋       | 103/383 [00:07<00:21, 13.08it/s] 27%|██▋       | 103/383 [00:07<00:20, 13.49it/s]
 28%|██▊       | 108/383 [00:07<00:17, 15.91it/s][A 28%|██▊       | 108/383 [00:07<00:16, 16.87it/s] 27%|██▋       | 105/383 [00:07<00:19, 14.33it/s] 27%|██▋       | 105/383 [00:07<00:18, 14.82it/s]
 29%|██▊       | 110/383 [00:07<00:16, 16.79it/s][A 29%|██▊       | 110/383 [00:07<00:15, 17.55it/s] 28%|██▊       | 107/383 [00:07<00:17, 15.45it/s] 28%|██▊       | 107/383 [00:07<00:17, 15.99it/s]
 29%|██▉       | 112/383 [00:07<00:15, 17.41it/s][A 29%|██▉       | 112/383 [00:07<00:15, 18.05it/s] 28%|██▊       | 109/383 [00:07<00:16, 16.40it/s] 28%|██▊       | 109/383 [00:07<00:16, 16.88it/s]
 30%|██▉       | 114/383 [00:07<00:15, 17.92it/s][A 30%|██▉       | 114/383 [00:07<00:14, 18.44it/s] 29%|██▉       | 111/383 [00:07<00:15, 17.14it/s] 29%|██▉       | 111/383 [00:07<00:15, 17.51it/s]
 30%|███       | 116/383 [00:07<00:14, 17.93it/s][A 30%|██▉       | 113/383 [00:07<00:15, 17.84it/s] 30%|███       | 116/383 [00:07<00:15, 17.75it/s] 30%|██▉       | 113/383 [00:07<00:15, 17.90it/s]
 31%|███       | 118/383 [00:08<00:15, 17.51it/s][A 30%|███       | 115/383 [00:08<00:15, 17.79it/s] 30%|███       | 115/383 [00:08<00:15, 17.85it/s] 31%|███       | 118/383 [00:08<00:15, 17.21it/s]
 31%|███▏      | 120/383 [00:08<00:15, 17.06it/s][A 31%|███       | 117/383 [00:08<00:15, 16.99it/s] 31%|███       | 117/383 [00:08<00:15, 17.39it/s] 31%|███▏      | 120/383 [00:08<00:15, 17.17it/s]
 32%|███▏      | 122/383 [00:08<00:15, 16.88it/s][A 32%|███▏      | 122/383 [00:08<00:15, 17.39it/s] 31%|███       | 119/383 [00:08<00:15, 16.85it/s] 31%|███       | 119/383 [00:08<00:15, 16.93it/s]
 32%|███▏      | 124/383 [00:08<00:14, 17.46it/s][A 32%|███▏      | 124/383 [00:08<00:14, 18.08it/s] 32%|███▏      | 121/383 [00:08<00:15, 16.61it/s] 32%|███▏      | 121/383 [00:08<00:15, 16.58it/s]
 33%|███▎      | 126/383 [00:08<00:14, 18.04it/s][A 33%|███▎      | 126/383 [00:08<00:14, 18.35it/s] 32%|███▏      | 123/383 [00:08<00:15, 16.92it/s] 32%|███▏      | 123/383 [00:08<00:14, 17.39it/s]
 33%|███▎      | 128/383 [00:08<00:13, 18.56it/s][A 33%|███▎      | 128/383 [00:08<00:13, 18.70it/s] 33%|███▎      | 125/383 [00:08<00:14, 17.43it/s] 33%|███▎      | 125/383 [00:08<00:14, 17.87it/s]
 34%|███▍      | 130/383 [00:08<00:14, 17.62it/s][A 34%|███▍      | 130/383 [00:08<00:14, 17.09it/s] 33%|███▎      | 127/383 [00:08<00:13, 18.30it/s] 33%|███▎      | 127/383 [00:08<00:14, 17.85it/s]
 34%|███▍      | 132/383 [00:08<00:14, 16.76it/s][A 34%|███▎      | 129/383 [00:08<00:13, 18.21it/s] 34%|███▎      | 129/383 [00:08<00:14, 17.33it/s] 34%|███▍      | 132/383 [00:08<00:15, 16.54it/s]
 35%|███▍      | 134/383 [00:08<00:15, 16.43it/s][A 34%|███▍      | 131/383 [00:08<00:14, 17.61it/s] 34%|███▍      | 131/383 [00:08<00:14, 16.89it/s] 35%|███▍      | 134/383 [00:09<00:15, 16.48it/s]
 36%|███▌      | 136/383 [00:09<00:15, 16.42it/s][A 35%|███▍      | 133/383 [00:09<00:14, 16.89it/s] 35%|███▍      | 133/383 [00:09<00:15, 16.56it/s] 36%|███▌      | 136/383 [00:09<00:15, 16.32it/s]
 36%|███▌      | 138/383 [00:09<00:14, 16.44it/s][A 35%|███▌      | 135/383 [00:09<00:14, 16.67it/s] 36%|███▌      | 138/383 [00:09<00:14, 16.43it/s] 35%|███▌      | 135/383 [00:09<00:15, 16.31it/s]
 37%|███▋      | 140/383 [00:09<00:14, 16.33it/s][A 36%|███▌      | 137/383 [00:09<00:14, 16.48it/s] 37%|███▋      | 140/383 [00:09<00:14, 16.42it/s] 36%|███▌      | 137/383 [00:09<00:15, 16.03it/s]
 37%|███▋      | 142/383 [00:09<00:14, 16.26it/s][A 36%|███▋      | 139/383 [00:09<00:14, 16.57it/s] 37%|███▋      | 142/383 [00:09<00:14, 16.38it/s] 36%|███▋      | 139/383 [00:09<00:15, 15.98it/s]
 38%|███▊      | 144/383 [00:09<00:14, 16.21it/s][A 37%|███▋      | 141/383 [00:09<00:14, 16.17it/s] 38%|███▊      | 144/383 [00:09<00:14, 16.44it/s] 37%|███▋      | 141/383 [00:09<00:14, 16.16it/s]
 38%|███▊      | 146/383 [00:09<00:14, 16.13it/s][A 37%|███▋      | 143/383 [00:09<00:14, 16.22it/s] 38%|███▊      | 146/383 [00:09<00:14, 16.40it/s] 37%|███▋      | 143/383 [00:09<00:14, 16.23it/s]
 39%|███▊      | 148/383 [00:09<00:14, 16.07it/s][A 38%|███▊      | 145/383 [00:09<00:14, 16.18it/s] 38%|███▊      | 145/383 [00:09<00:14, 16.07it/s] 39%|███▊      | 148/383 [00:09<00:15, 14.92it/s] 38%|███▊      | 147/383 [00:09<00:14, 16.19it/s] 38%|███▊      | 147/383 [00:09<00:14, 16.18it/s]
 39%|███▉      | 150/383 [00:10<00:17, 13.13it/s][A 39%|███▉      | 150/383 [00:10<00:18, 12.59it/s] 39%|███▉      | 149/383 [00:10<00:17, 13.41it/s] 39%|███▉      | 149/383 [00:10<00:17, 13.35it/s]
 40%|███▉      | 152/383 [00:10<00:19, 12.01it/s][A 40%|███▉      | 152/383 [00:10<00:19, 11.75it/s] 39%|███▉      | 151/383 [00:10<00:19, 12.02it/s] 39%|███▉      | 151/383 [00:10<00:19, 11.83it/s]
 40%|████      | 154/383 [00:10<00:25,  9.16it/s][A 40%|███▉      | 153/383 [00:10<00:19, 11.51it/s] 40%|███▉      | 153/383 [00:10<00:21, 10.88it/s] 40%|████      | 154/383 [00:10<00:26,  8.60it/s] 40%|████      | 155/383 [00:11<00:30,  7.41it/s]
 41%|████      | 156/383 [00:11<00:35,  6.34it/s][A 40%|████      | 155/383 [00:11<00:32,  7.11it/s] 41%|████      | 156/383 [00:11<00:36,  6.24it/s] 41%|████      | 156/383 [00:11<00:36,  6.30it/s] 41%|████      | 156/383 [00:11<00:36,  6.18it/s]
 41%|████      | 157/383 [00:11<00:41,  5.45it/s][A 41%|████      | 157/383 [00:11<00:39,  5.71it/s] 41%|████      | 157/383 [00:11<00:39,  5.73it/s] 41%|████      | 157/383 [00:11<00:41,  5.44it/s] 41%|████▏     | 158/383 [00:11<00:43,  5.19it/s]
 41%|████▏     | 158/383 [00:11<00:46,  4.80it/s][A 41%|████▏     | 158/383 [00:11<00:43,  5.11it/s] 42%|████▏     | 159/383 [00:11<00:42,  5.28it/s] 41%|████▏     | 158/383 [00:11<00:45,  4.98it/s]
 42%|████▏     | 159/383 [00:12<00:49,  4.54it/s][A 42%|████▏     | 159/383 [00:12<00:42,  5.24it/s] 42%|████▏     | 160/383 [00:12<00:45,  4.89it/s] 42%|████▏     | 159/383 [00:12<00:48,  4.61it/s] 42%|████▏     | 160/383 [00:12<00:45,  4.95it/s]
 42%|████▏     | 160/383 [00:12<00:51,  4.32it/s][A 42%|████▏     | 161/383 [00:12<00:44,  5.00it/s] 42%|████▏     | 160/383 [00:12<00:47,  4.71it/s]
 42%|████▏     | 161/383 [00:12<00:48,  4.60it/s][A 42%|████▏     | 161/383 [00:12<00:46,  4.79it/s] 42%|████▏     | 162/383 [00:12<00:42,  5.21it/s] 42%|████▏     | 161/383 [00:12<00:49,  4.51it/s]
 42%|████▏     | 162/383 [00:12<00:49,  4.49it/s][A 42%|████▏     | 162/383 [00:12<00:47,  4.62it/s] 43%|████▎     | 163/383 [00:12<00:46,  4.73it/s] 42%|████▏     | 162/383 [00:12<00:46,  4.71it/s]
 43%|████▎     | 163/383 [00:12<00:46,  4.70it/s][A 43%|████▎     | 163/383 [00:12<00:47,  4.67it/s] 43%|████▎     | 164/383 [00:13<00:50,  4.38it/s] 43%|████▎     | 163/383 [00:13<00:46,  4.78it/s]
 43%|████▎     | 164/383 [00:13<00:50,  4.38it/s][A 43%|████▎     | 164/383 [00:13<00:49,  4.41it/s] 43%|████▎     | 165/383 [00:13<00:46,  4.68it/s] 43%|████▎     | 164/383 [00:13<00:48,  4.50it/s] 44%|████▍     | 168/383 [00:13<00:25,  8.53it/s]
 43%|████▎     | 165/383 [00:13<00:48,  4.52it/s][A 43%|████▎     | 165/383 [00:13<00:47,  4.57it/s]
 44%|████▍     | 168/383 [00:13<00:25,  8.30it/s][A 45%|████▍     | 171/383 [00:13<00:17, 11.83it/s] 44%|████▍     | 168/383 [00:13<00:25,  8.39it/s] 45%|████▌     | 173/383 [00:13<00:15, 13.42it/s] 43%|████▎     | 165/383 [00:13<00:51,  4.22it/s]
 45%|████▍     | 171/383 [00:13<00:17, 11.82it/s][A 45%|████▍     | 171/383 [00:13<00:17, 11.94it/s] 44%|████▍     | 168/383 [00:13<00:27,  7.88it/s]
 45%|████▌     | 174/383 [00:13<00:14, 14.52it/s][A 46%|████▌     | 175/383 [00:13<00:15, 13.40it/s] 45%|████▌     | 174/383 [00:13<00:14, 14.69it/s] 45%|████▍     | 171/383 [00:13<00:18, 11.31it/s]
 46%|████▌     | 176/383 [00:13<00:14, 14.23it/s][A 46%|████▌     | 177/383 [00:13<00:15, 13.19it/s] 46%|████▌     | 176/383 [00:13<00:14, 14.34it/s] 45%|████▌     | 174/383 [00:13<00:15, 13.62it/s] 47%|████▋     | 179/383 [00:13<00:14, 14.54it/s]
 46%|████▋     | 178/383 [00:13<00:14, 14.55it/s][A 46%|████▋     | 178/383 [00:14<00:13, 14.68it/s]
 47%|████▋     | 180/383 [00:14<00:13, 15.57it/s][A 47%|████▋     | 181/383 [00:14<00:13, 15.35it/s] 46%|████▌     | 176/383 [00:14<00:15, 13.53it/s] 47%|████▋     | 180/383 [00:14<00:12, 15.71it/s] 48%|████▊     | 183/383 [00:14<00:12, 16.21it/s]
 48%|████▊     | 182/383 [00:14<00:12, 16.06it/s][A 46%|████▋     | 178/383 [00:14<00:14, 13.99it/s] 48%|████▊     | 182/383 [00:14<00:12, 16.33it/s]
 48%|████▊     | 184/383 [00:14<00:11, 16.70it/s][A 48%|████▊     | 185/383 [00:14<00:12, 15.98it/s] 47%|████▋     | 180/383 [00:14<00:13, 15.09it/s] 48%|████▊     | 184/383 [00:14<00:11, 16.86it/s]
 49%|████▊     | 186/383 [00:14<00:12, 16.05it/s][A 48%|████▊     | 182/383 [00:14<00:12, 15.76it/s] 49%|████▉     | 187/383 [00:14<00:12, 15.15it/s] 49%|████▊     | 186/383 [00:14<00:12, 15.58it/s]
 49%|████▉     | 188/383 [00:14<00:11, 16.58it/s][A 48%|████▊     | 184/383 [00:14<00:12, 16.56it/s] 50%|████▉     | 190/383 [00:14<00:10, 17.59it/s] 49%|████▉     | 188/383 [00:14<00:12, 16.12it/s]
 50%|████▉     | 191/383 [00:14<00:10, 18.58it/s][A 50%|█████     | 192/383 [00:14<00:10, 17.92it/s] 49%|████▊     | 186/383 [00:14<00:12, 15.42it/s] 50%|████▉     | 191/383 [00:14<00:10, 18.33it/s]
 50%|█████     | 193/383 [00:14<00:10, 18.49it/s][A 51%|█████     | 194/383 [00:14<00:10, 18.14it/s] 49%|████▉     | 188/383 [00:14<00:12, 15.38it/s] 50%|█████     | 193/383 [00:14<00:10, 18.47it/s]
 51%|█████     | 195/383 [00:14<00:10, 18.41it/s][A 51%|█████     | 196/383 [00:14<00:10, 18.44it/s] 51%|█████     | 195/383 [00:14<00:10, 18.54it/s] 50%|████▉     | 191/383 [00:14<00:10, 17.79it/s]
 51%|█████▏    | 197/383 [00:15<00:09, 18.68it/s][A 52%|█████▏    | 199/383 [00:15<00:09, 20.04it/s] 51%|█████▏    | 197/383 [00:15<00:09, 18.85it/s] 50%|█████     | 193/383 [00:15<00:10, 18.10it/s]
 52%|█████▏    | 199/383 [00:15<00:09, 19.03it/s][A 53%|█████▎    | 202/383 [00:15<00:08, 21.83it/s] 52%|█████▏    | 199/383 [00:15<00:09, 19.11it/s] 51%|█████     | 195/383 [00:15<00:10, 18.32it/s]
 53%|█████▎    | 202/383 [00:15<00:08, 21.12it/s][A 54%|█████▎    | 205/383 [00:15<00:07, 23.05it/s] 53%|█████▎    | 202/383 [00:15<00:08, 21.22it/s] 51%|█████▏    | 197/383 [00:15<00:10, 18.48it/s]
 54%|█████▎    | 205/383 [00:15<00:07, 22.77it/s][A 54%|█████▍    | 208/383 [00:15<00:07, 23.89it/s] 54%|█████▎    | 205/383 [00:15<00:07, 22.51it/s] 52%|█████▏    | 200/383 [00:15<00:09, 19.61it/s]
 54%|█████▍    | 208/383 [00:15<00:07, 23.61it/s][A 55%|█████▌    | 211/383 [00:15<00:07, 24.54it/s] 54%|█████▍    | 208/383 [00:15<00:07, 23.48it/s] 53%|█████▎    | 203/383 [00:15<00:08, 21.41it/s]
 55%|█████▌    | 211/383 [00:15<00:07, 24.30it/s][A 56%|█████▌    | 214/383 [00:15<00:06, 24.32it/s] 55%|█████▌    | 211/383 [00:15<00:07, 24.23it/s] 54%|█████▍    | 206/383 [00:15<00:07, 22.76it/s]
 56%|█████▌    | 214/383 [00:15<00:06, 24.72it/s][A 57%|█████▋    | 217/383 [00:15<00:06, 24.80it/s] 56%|█████▌    | 214/383 [00:15<00:06, 24.67it/s] 55%|█████▍    | 209/383 [00:15<00:07, 23.56it/s]
 57%|█████▋    | 217/383 [00:15<00:06, 24.32it/s][A 57%|█████▋    | 220/383 [00:15<00:06, 25.00it/s] 57%|█████▋    | 217/383 [00:15<00:06, 24.86it/s] 55%|█████▌    | 212/383 [00:15<00:07, 24.07it/s]
 57%|█████▋    | 220/383 [00:15<00:06, 24.82it/s][A 57%|█████▋    | 220/383 [00:15<00:06, 25.15it/s] 56%|█████▌    | 215/383 [00:16<00:06, 24.56it/s] 58%|█████▊    | 223/383 [00:16<00:07, 22.18it/s]
 58%|█████▊    | 223/383 [00:16<00:07, 22.79it/s][A 57%|█████▋    | 218/383 [00:16<00:06, 24.91it/s] 58%|█████▊    | 223/383 [00:16<00:07, 22.16it/s] 59%|█████▉    | 226/383 [00:16<00:07, 20.43it/s] 58%|█████▊    | 221/383 [00:16<00:06, 23.76it/s]
 59%|█████▉    | 226/383 [00:16<00:07, 20.64it/s][A 59%|█████▉    | 226/383 [00:16<00:07, 20.15it/s] 60%|█████▉    | 229/383 [00:16<00:07, 19.26it/s] 58%|█████▊    | 224/383 [00:16<00:07, 20.88it/s]
 60%|█████▉    | 229/383 [00:16<00:08, 19.10it/s][A 60%|█████▉    | 229/383 [00:16<00:08, 18.93it/s] 60%|██████    | 231/383 [00:16<00:08, 17.31it/s]
 60%|██████    | 231/383 [00:16<00:08, 17.84it/s][A 59%|█████▉    | 227/383 [00:16<00:08, 19.22it/s] 60%|██████    | 231/383 [00:16<00:08, 18.03it/s] 61%|██████    | 233/383 [00:16<00:09, 15.98it/s]
 61%|██████    | 233/383 [00:16<00:09, 16.35it/s][A 61%|██████    | 233/383 [00:16<00:09, 16.27it/s] 60%|██████    | 230/383 [00:16<00:08, 18.42it/s] 61%|██████▏   | 235/383 [00:16<00:09, 15.02it/s]
 61%|██████▏   | 235/383 [00:16<00:09, 15.32it/s][A 61%|██████    | 232/383 [00:16<00:08, 16.78it/s] 61%|██████▏   | 235/383 [00:16<00:09, 14.83it/s] 62%|██████▏   | 237/383 [00:17<00:10, 14.22it/s]
 62%|██████▏   | 237/383 [00:17<00:10, 14.47it/s][A 61%|██████    | 234/383 [00:17<00:09, 15.76it/s] 62%|██████▏   | 237/383 [00:17<00:10, 14.22it/s] 62%|██████▏   | 239/383 [00:17<00:10, 13.69it/s]
 62%|██████▏   | 239/383 [00:17<00:10, 13.98it/s][A 62%|██████▏   | 236/383 [00:17<00:09, 14.98it/s] 62%|██████▏   | 239/383 [00:17<00:10, 13.82it/s] 63%|██████▎   | 241/383 [00:17<00:10, 13.32it/s]
 63%|██████▎   | 241/383 [00:17<00:10, 13.64it/s][A 62%|██████▏   | 238/383 [00:17<00:10, 14.37it/s] 63%|██████▎   | 241/383 [00:17<00:10, 13.34it/s] 63%|██████▎   | 243/383 [00:17<00:10, 13.18it/s]
 63%|██████▎   | 243/383 [00:17<00:10, 13.29it/s][A 63%|██████▎   | 240/383 [00:17<00:10, 14.05it/s] 63%|██████▎   | 243/383 [00:17<00:10, 13.20it/s] 64%|██████▍   | 245/383 [00:17<00:10, 12.96it/s]
 64%|██████▍   | 245/383 [00:17<00:10, 13.27it/s][A 63%|██████▎   | 242/383 [00:17<00:10, 13.69it/s] 64%|██████▍   | 245/383 [00:17<00:10, 13.06it/s] 64%|██████▍   | 247/383 [00:17<00:10, 13.08it/s]
 64%|██████▍   | 247/383 [00:17<00:10, 13.28it/s][A 64%|██████▎   | 244/383 [00:17<00:10, 13.60it/s] 64%|██████▍   | 247/383 [00:17<00:10, 12.86it/s] 65%|██████▌   | 249/383 [00:17<00:10, 13.01it/s]
 65%|██████▌   | 249/383 [00:18<00:10, 12.84it/s][A 64%|██████▍   | 246/383 [00:18<00:10, 13.28it/s] 65%|██████▌   | 249/383 [00:18<00:10, 12.86it/s] 66%|██████▌   | 251/383 [00:18<00:10, 12.64it/s]
 66%|██████▌   | 251/383 [00:18<00:10, 12.98it/s][A 65%|██████▍   | 248/383 [00:18<00:10, 13.14it/s] 66%|██████▌   | 251/383 [00:18<00:10, 12.95it/s] 66%|██████▌   | 253/383 [00:18<00:10, 12.69it/s]
 66%|██████▌   | 253/383 [00:18<00:10, 12.96it/s][A 65%|██████▌   | 250/383 [00:18<00:10, 13.03it/s] 66%|██████▌   | 253/383 [00:18<00:10, 12.66it/s] 67%|██████▋   | 255/383 [00:18<00:09, 12.91it/s]
 67%|██████▋   | 255/383 [00:18<00:09, 13.06it/s][A 66%|██████▌   | 252/383 [00:18<00:10, 12.87it/s] 67%|██████▋   | 255/383 [00:18<00:10, 12.64it/s] 67%|██████▋   | 257/383 [00:18<00:09, 13.10it/s]
 67%|██████▋   | 257/383 [00:18<00:09, 13.17it/s][A 66%|██████▋   | 254/383 [00:18<00:10, 12.76it/s] 67%|██████▋   | 257/383 [00:18<00:09, 12.94it/s] 68%|██████▊   | 259/383 [00:18<00:09, 13.36it/s]
 68%|██████▊   | 259/383 [00:18<00:09, 13.29it/s][A 67%|██████▋   | 256/383 [00:18<00:09, 12.87it/s] 68%|██████▊   | 259/383 [00:18<00:09, 13.06it/s] 68%|██████▊   | 261/383 [00:18<00:09, 13.23it/s]
 68%|██████▊   | 261/383 [00:18<00:09, 13.49it/s][A 67%|██████▋   | 258/383 [00:18<00:09, 12.98it/s] 68%|██████▊   | 261/383 [00:19<00:09, 13.09it/s] 69%|██████▊   | 263/383 [00:19<00:08, 13.43it/s]
 69%|██████▊   | 263/383 [00:19<00:08, 13.48it/s][A 68%|██████▊   | 260/383 [00:19<00:09, 13.14it/s] 69%|██████▉   | 266/383 [00:19<00:07, 16.41it/s] 69%|██████▊   | 263/383 [00:19<00:09, 13.17it/s]
 69%|██████▉   | 266/383 [00:19<00:07, 16.05it/s][A 70%|███████   | 269/383 [00:19<00:06, 18.77it/s] 68%|██████▊   | 262/383 [00:19<00:09, 13.28it/s]
 70%|███████   | 269/383 [00:19<00:06, 18.55it/s][A 69%|██████▉   | 266/383 [00:19<00:07, 15.69it/s] 69%|██████▉   | 264/383 [00:19<00:08, 14.39it/s] 71%|███████   | 272/383 [00:19<00:05, 19.01it/s] 70%|███████   | 269/383 [00:19<00:06, 18.04it/s]
 71%|███████   | 272/383 [00:19<00:05, 19.77it/s][A 70%|██████▉   | 267/383 [00:19<00:07, 16.45it/s] 72%|███████▏  | 274/383 [00:19<00:06, 17.69it/s] 71%|███████   | 272/383 [00:19<00:05, 19.09it/s]
 72%|███████▏  | 275/383 [00:19<00:05, 18.50it/s][A 70%|███████   | 270/383 [00:19<00:05, 18.84it/s] 72%|███████▏  | 274/383 [00:19<00:06, 17.83it/s] 72%|███████▏  | 276/383 [00:19<00:06, 16.51it/s]
 72%|███████▏  | 277/383 [00:19<00:05, 17.72it/s][A 71%|███████▏  | 273/383 [00:19<00:05, 19.42it/s] 72%|███████▏  | 276/383 [00:19<00:06, 17.41it/s] 73%|███████▎  | 278/383 [00:19<00:06, 16.33it/s]
 73%|███████▎  | 279/383 [00:19<00:06, 16.86it/s][A 72%|███████▏  | 275/383 [00:19<00:05, 18.47it/s] 73%|███████▎  | 280/383 [00:19<00:06, 16.17it/s] 73%|███████▎  | 278/383 [00:19<00:06, 16.56it/s]
 73%|███████▎  | 281/383 [00:20<00:06, 16.18it/s][A 72%|███████▏  | 277/383 [00:20<00:05, 17.75it/s] 73%|███████▎  | 280/383 [00:20<00:06, 16.27it/s] 74%|███████▎  | 282/383 [00:20<00:07, 13.86it/s] 73%|███████▎  | 279/383 [00:20<00:06, 16.43it/s]
 74%|███████▍  | 283/383 [00:20<00:07, 14.27it/s][A 74%|███████▎  | 282/383 [00:20<00:07, 14.12it/s] 74%|███████▍  | 284/383 [00:20<00:07, 12.93it/s] 73%|███████▎  | 281/383 [00:20<00:06, 14.68it/s]
 74%|███████▍  | 285/383 [00:20<00:07, 12.90it/s][A 74%|███████▍  | 284/383 [00:20<00:07, 12.77it/s] 75%|███████▍  | 286/383 [00:20<00:07, 12.21it/s] 74%|███████▍  | 283/383 [00:20<00:07, 13.05it/s]
 75%|███████▍  | 287/383 [00:20<00:07, 12.08it/s][A 75%|███████▍  | 286/383 [00:20<00:08, 12.12it/s] 75%|███████▌  | 288/383 [00:20<00:07, 11.90it/s] 74%|███████▍  | 285/383 [00:20<00:07, 12.67it/s] 75%|███████▌  | 288/383 [00:20<00:08, 11.38it/s] 75%|███████▍  | 287/383 [00:20<00:07, 12.33it/s]
 75%|███████▌  | 289/383 [00:20<00:10,  8.76it/s][A 76%|███████▌  | 290/383 [00:21<00:12,  7.74it/s] 75%|███████▌  | 289/383 [00:21<00:10,  9.27it/s] 76%|███████▌  | 290/383 [00:21<00:12,  7.31it/s] 76%|███████▌  | 291/383 [00:21<00:13,  6.62it/s]
 76%|███████▌  | 291/383 [00:21<00:14,  6.29it/s][A 76%|███████▌  | 291/383 [00:21<00:14,  6.22it/s] 76%|███████▌  | 292/383 [00:21<00:15,  5.83it/s]
 76%|███████▌  | 292/383 [00:21<00:16,  5.59it/s][A 76%|███████▌  | 291/383 [00:21<00:14,  6.39it/s] 76%|███████▌  | 292/383 [00:21<00:17,  5.33it/s] 77%|███████▋  | 293/383 [00:21<00:17,  5.12it/s] 76%|███████▌  | 292/383 [00:22<00:16,  5.62it/s]
 77%|███████▋  | 293/383 [00:22<00:18,  4.92it/s][A 77%|███████▋  | 293/383 [00:22<00:18,  4.91it/s] 77%|███████▋  | 294/383 [00:22<00:18,  4.79it/s]
 77%|███████▋  | 294/383 [00:22<00:19,  4.60it/s][A 77%|███████▋  | 293/383 [00:22<00:17,  5.10it/s] 77%|███████▋  | 295/383 [00:22<00:18,  4.71it/s] 77%|███████▋  | 294/383 [00:22<00:19,  4.50it/s]
 77%|███████▋  | 295/383 [00:22<00:19,  4.42it/s][A 77%|███████▋  | 294/383 [00:22<00:18,  4.73it/s] 77%|███████▋  | 296/383 [00:22<00:20,  4.27it/s] 77%|███████▋  | 295/383 [00:22<00:20,  4.20it/s] 77%|███████▋  | 295/383 [00:22<00:19,  4.54it/s]
 77%|███████▋  | 296/383 [00:22<00:20,  4.18it/s][A 78%|███████▊  | 297/383 [00:22<00:19,  4.31it/s] 77%|███████▋  | 296/383 [00:23<00:21,  4.08it/s] 77%|███████▋  | 296/383 [00:23<00:20,  4.26it/s]
 78%|███████▊  | 297/383 [00:23<00:21,  3.93it/s][A 78%|███████▊  | 298/383 [00:23<00:20,  4.10it/s] 78%|███████▊  | 297/383 [00:23<00:21,  4.08it/s]
 78%|███████▊  | 298/383 [00:23<00:21,  4.03it/s][A 78%|███████▊  | 297/383 [00:23<00:21,  4.06it/s] 78%|███████▊  | 298/383 [00:23<00:19,  4.28it/s] 78%|███████▊  | 299/383 [00:23<00:21,  3.95it/s]
 78%|███████▊  | 299/383 [00:23<00:20,  4.01it/s][A 78%|███████▊  | 298/383 [00:23<00:22,  3.80it/s] 78%|███████▊  | 300/383 [00:23<00:20,  4.06it/s] 78%|███████▊  | 299/383 [00:23<00:20,  4.06it/s]
 78%|███████▊  | 300/383 [00:23<00:21,  3.85it/s][A 78%|███████▊  | 299/383 [00:23<00:22,  3.78it/s] 79%|███████▊  | 301/383 [00:24<00:20,  3.92it/s] 78%|███████▊  | 300/383 [00:24<00:21,  3.89it/s]
 79%|███████▊  | 301/383 [00:24<00:21,  3.78it/s][A 79%|███████▊  | 301/383 [00:24<00:20,  4.03it/s] 78%|███████▊  | 300/383 [00:24<00:22,  3.71it/s] 79%|███████▉  | 302/383 [00:24<00:21,  3.85it/s]
 79%|███████▉  | 302/383 [00:24<00:21,  3.74it/s][A 79%|███████▉  | 303/383 [00:24<00:21,  3.80it/s] 79%|███████▉  | 302/383 [00:24<00:21,  3.82it/s] 79%|███████▊  | 301/383 [00:24<00:22,  3.65it/s]
 79%|███████▉  | 303/383 [00:24<00:20,  3.93it/s][A 79%|███████▉  | 303/383 [00:24<00:20,  3.97it/s] 79%|███████▉  | 302/383 [00:24<00:22,  3.62it/s] 79%|███████▉  | 304/383 [00:24<00:21,  3.68it/s]
 79%|███████▉  | 304/383 [00:25<00:21,  3.75it/s][A 79%|███████▉  | 304/383 [00:25<00:19,  4.00it/s] 80%|███████▉  | 305/383 [00:25<00:20,  3.83it/s] 79%|███████▉  | 303/383 [00:25<00:22,  3.63it/s] 80%|███████▉  | 306/383 [00:25<00:18,  4.09it/s]
 80%|███████▉  | 305/383 [00:25<00:21,  3.63it/s][A 80%|███████▉  | 305/383 [00:25<00:20,  3.85it/s] 79%|███████▉  | 304/383 [00:25<00:21,  3.62it/s] 80%|████████  | 307/383 [00:25<00:19,  3.95it/s]
 80%|███████▉  | 306/383 [00:25<00:20,  3.67it/s][A 80%|███████▉  | 306/383 [00:25<00:20,  3.81it/s] 80%|███████▉  | 305/383 [00:25<00:21,  3.63it/s] 80%|████████  | 308/383 [00:25<00:19,  3.81it/s]
 80%|████████  | 307/383 [00:25<00:20,  3.65it/s][A 80%|████████  | 307/383 [00:25<00:20,  3.73it/s] 80%|███████▉  | 306/383 [00:25<00:21,  3.62it/s] 80%|████████  | 308/383 [00:26<00:19,  3.87it/s] 81%|████████  | 309/383 [00:26<00:19,  3.85it/s]
 80%|████████  | 308/383 [00:26<00:20,  3.66it/s][A 80%|████████  | 307/383 [00:26<00:21,  3.59it/s]
 81%|████████  | 309/383 [00:26<00:19,  3.76it/s][A 81%|████████  | 309/383 [00:26<00:19,  3.79it/s] 81%|████████  | 310/383 [00:26<00:19,  3.70it/s] 80%|████████  | 308/383 [00:26<00:20,  3.57it/s]
 81%|████████  | 310/383 [00:26<00:18,  3.86it/s][A 81%|████████  | 310/383 [00:26<00:19,  3.71it/s] 81%|████████  | 311/383 [00:26<00:19,  3.64it/s] 81%|████████  | 309/383 [00:26<00:19,  3.80it/s]
 81%|████████  | 311/383 [00:26<00:18,  3.82it/s][A 81%|████████  | 311/383 [00:26<00:19,  3.69it/s] 81%|████████▏ | 312/383 [00:26<00:19,  3.63it/s] 81%|████████  | 310/383 [00:26<00:18,  3.90it/s]
 81%|████████▏ | 312/383 [00:27<00:18,  3.85it/s][A 81%|████████▏ | 312/383 [00:27<00:19,  3.64it/s] 82%|████████▏ | 313/383 [00:27<00:19,  3.64it/s] 81%|████████  | 311/383 [00:27<00:18,  3.81it/s]
 82%|████████▏ | 313/383 [00:27<00:19,  3.65it/s][A 82%|████████▏ | 313/383 [00:27<00:18,  3.69it/s] 81%|████████▏ | 312/383 [00:27<00:18,  3.88it/s] 82%|████████▏ | 314/383 [00:27<00:19,  3.61it/s]
 82%|████████▏ | 314/383 [00:27<00:18,  3.67it/s][A 82%|████████▏ | 314/383 [00:27<00:18,  3.79it/s] 82%|████████▏ | 313/383 [00:27<00:17,  3.91it/s] 82%|████████▏ | 315/383 [00:27<00:19,  3.51it/s] 82%|████████▏ | 315/383 [00:27<00:17,  3.84it/s]
 82%|████████▏ | 315/383 [00:27<00:18,  3.67it/s][A 82%|████████▏ | 314/383 [00:27<00:17,  3.96it/s] 83%|████████▎ | 316/383 [00:28<00:18,  3.64it/s] 82%|████████▏ | 315/383 [00:28<00:16,  4.09it/s] 83%|████████▎ | 316/383 [00:28<00:17,  3.81it/s]
 83%|████████▎ | 316/383 [00:28<00:18,  3.71it/s][A 83%|████████▎ | 317/383 [00:28<00:18,  3.64it/s]
 83%|████████▎ | 317/383 [00:28<00:17,  3.70it/s][A 83%|████████▎ | 316/383 [00:28<00:17,  3.80it/s] 83%|████████▎ | 317/383 [00:28<00:18,  3.64it/s] 83%|████████▎ | 318/383 [00:28<00:18,  3.51it/s] 83%|████████▎ | 317/383 [00:28<00:17,  3.75it/s]
 83%|████████▎ | 318/383 [00:28<00:18,  3.55it/s][A 83%|████████▎ | 318/383 [00:28<00:17,  3.61it/s] 83%|████████▎ | 319/383 [00:28<00:17,  3.70it/s]
 83%|████████▎ | 318/383 [00:29<00:17,  3.71it/s] 83%|████████▎ | 319/383 [00:29<00:17,  3.68it/s][A 83%|████████▎ | 319/383 [00:29<00:17,  3.67it/s] 84%|████████▎ | 320/383 [00:29<00:17,  3.69it/s]
 84%|████████▎ | 320/383 [00:29<00:16,  3.76it/s][A 83%|████████▎ | 319/383 [00:29<00:17,  3.71it/s] 84%|████████▎ | 320/383 [00:29<00:17,  3.69it/s] 84%|████████▍ | 321/383 [00:29<00:17,  3.60it/s]
 84%|████████▍ | 321/383 [00:29<00:16,  3.77it/s][A 84%|████████▍ | 321/383 [00:29<00:16,  3.73it/s] 84%|████████▎ | 320/383 [00:29<00:17,  3.68it/s] 84%|████████▍ | 322/383 [00:29<00:16,  3.63it/s]
 84%|████████▍ | 322/383 [00:29<00:15,  3.84it/s][A 84%|████████▍ | 321/383 [00:29<00:17,  3.59it/s] 84%|████████▍ | 322/383 [00:29<00:16,  3.59it/s] 84%|████████▍ | 323/383 [00:29<00:16,  3.75it/s]
 84%|████████▍ | 323/383 [00:30<00:15,  3.98it/s][A 84%|████████▍ | 322/383 [00:30<00:16,  3.78it/s] 84%|████████▍ | 323/383 [00:30<00:16,  3.73it/s] 85%|████████▍ | 324/383 [00:30<00:15,  3.71it/s]
 85%|████████▍ | 324/383 [00:30<00:15,  3.77it/s][A 84%|████████▍ | 323/383 [00:30<00:16,  3.72it/s] 85%|████████▍ | 324/383 [00:30<00:15,  3.72it/s] 85%|████████▍ | 325/383 [00:30<00:15,  3.80it/s]
 85%|████████▍ | 325/383 [00:30<00:15,  3.86it/s][A 85%|████████▍ | 325/383 [00:30<00:15,  3.82it/s] 85%|████████▍ | 324/383 [00:30<00:16,  3.68it/s] 85%|████████▌ | 326/383 [00:30<00:14,  3.96it/s]
 85%|████████▌ | 326/383 [00:30<00:15,  3.75it/s][A 85%|████████▌ | 326/383 [00:30<00:14,  3.85it/s] 85%|████████▍ | 325/383 [00:30<00:15,  3.67it/s] 85%|████████▌ | 327/383 [00:30<00:14,  3.89it/s]
 85%|████████▌ | 327/383 [00:31<00:14,  3.74it/s][A 85%|████████▌ | 326/383 [00:31<00:14,  3.82it/s] 85%|████████▌ | 327/383 [00:31<00:15,  3.67it/s] 86%|████████▌ | 328/383 [00:31<00:14,  3.79it/s]
 86%|████████▌ | 328/383 [00:31<00:14,  3.67it/s][A 85%|████████▌ | 327/383 [00:31<00:14,  3.89it/s] 86%|████████▌ | 328/383 [00:31<00:15,  3.64it/s] 86%|████████▌ | 329/383 [00:31<00:14,  3.80it/s]
 86%|████████▌ | 329/383 [00:31<00:14,  3.74it/s][A 86%|████████▌ | 328/383 [00:31<00:14,  3.88it/s] 86%|████████▌ | 329/383 [00:31<00:14,  3.76it/s] 86%|████████▌ | 330/383 [00:31<00:14,  3.74it/s] 86%|████████▋ | 331/383 [00:31<00:12,  4.25it/s]
 86%|████████▌ | 330/383 [00:31<00:14,  3.75it/s][A 86%|████████▌ | 329/383 [00:31<00:13,  3.89it/s] 86%|████████▌ | 330/383 [00:32<00:14,  3.64it/s] 87%|████████▋ | 332/383 [00:32<00:10,  4.84it/s] 86%|████████▌ | 330/383 [00:32<00:12,  4.13it/s] 86%|████████▋ | 331/383 [00:32<00:12,  4.27it/s]
 86%|████████▋ | 331/383 [00:32<00:13,  3.73it/s][A 87%|████████▋ | 333/383 [00:32<00:09,  5.33it/s] 87%|████████▋ | 332/383 [00:32<00:11,  4.60it/s] 87%|████████▋ | 334/383 [00:32<00:08,  5.62it/s]
 87%|████████▋ | 332/383 [00:32<00:12,  4.22it/s][A 86%|████████▋ | 331/383 [00:32<00:13,  3.99it/s] 87%|████████▋ | 333/383 [00:32<00:10,  4.85it/s] 87%|████████▋ | 335/383 [00:32<00:08,  5.81it/s]
 87%|████████▋ | 333/383 [00:32<00:10,  4.70it/s][A 87%|████████▋ | 332/383 [00:32<00:11,  4.37it/s] 88%|████████▊ | 336/383 [00:32<00:07,  5.94it/s]
 87%|████████▋ | 334/383 [00:32<00:09,  5.06it/s][A 87%|████████▋ | 334/383 [00:32<00:09,  5.09it/s] 87%|████████▋ | 333/383 [00:32<00:10,  4.96it/s] 87%|████████▋ | 335/383 [00:32<00:08,  5.63it/s] 88%|████████▊ | 337/383 [00:32<00:07,  6.11it/s]
 87%|████████▋ | 335/383 [00:32<00:09,  5.24it/s][A 87%|████████▋ | 334/383 [00:32<00:09,  5.43it/s] 88%|████████▊ | 336/383 [00:33<00:08,  5.73it/s] 88%|████████▊ | 338/383 [00:33<00:07,  5.92it/s]
 88%|████████▊ | 336/383 [00:33<00:08,  5.25it/s][A 87%|████████▋ | 335/383 [00:33<00:08,  5.49it/s] 88%|████████▊ | 337/383 [00:33<00:07,  5.89it/s] 89%|████████▉ | 340/383 [00:33<00:05,  7.69it/s] 88%|████████▊ | 336/383 [00:33<00:07,  5.89it/s]
 88%|████████▊ | 337/383 [00:33<00:08,  5.39it/s][A 88%|████████▊ | 338/383 [00:33<00:07,  6.01it/s] 89%|████████▉ | 342/383 [00:33<00:04,  9.47it/s] 88%|████████▊ | 337/383 [00:33<00:07,  6.11it/s]
 88%|████████▊ | 338/383 [00:33<00:08,  5.49it/s][A 89%|████████▉ | 340/383 [00:33<00:05,  8.40it/s] 90%|████████▉ | 344/383 [00:33<00:03, 10.79it/s] 88%|████████▊ | 338/383 [00:33<00:07,  6.17it/s]
 89%|████████▊ | 339/383 [00:33<00:07,  5.73it/s][A 89%|████████▉ | 342/383 [00:33<00:04,  9.83it/s] 90%|█████████ | 346/383 [00:33<00:03, 11.54it/s] 89%|████████▉ | 340/383 [00:33<00:05,  8.26it/s]
 89%|████████▉ | 341/383 [00:33<00:05,  7.90it/s][A 90%|████████▉ | 344/383 [00:33<00:03, 11.11it/s] 91%|█████████ | 348/383 [00:33<00:02, 12.47it/s] 89%|████████▉ | 342/383 [00:33<00:04,  9.96it/s]
 90%|████████▉ | 343/383 [00:33<00:04,  9.62it/s][A 90%|█████████ | 346/383 [00:33<00:03, 12.20it/s] 91%|█████████▏| 350/383 [00:33<00:02, 13.35it/s] 90%|████████▉ | 344/383 [00:33<00:03, 10.54it/s]
 90%|█████████ | 345/383 [00:34<00:03, 11.04it/s][A 91%|█████████ | 348/383 [00:34<00:02, 12.99it/s] 92%|█████████▏| 352/383 [00:34<00:02, 13.38it/s] 90%|█████████ | 346/383 [00:34<00:03, 11.91it/s]
 91%|█████████ | 347/383 [00:34<00:02, 12.19it/s][A 91%|█████████▏| 350/383 [00:34<00:02, 13.06it/s] 92%|█████████▏| 354/383 [00:34<00:02, 13.70it/s] 91%|█████████ | 348/383 [00:34<00:02, 12.86it/s]
 91%|█████████ | 349/383 [00:34<00:02, 12.90it/s][A 92%|█████████▏| 352/383 [00:34<00:02, 13.78it/s] 93%|█████████▎| 356/383 [00:34<00:01, 14.47it/s] 91%|█████████▏| 350/383 [00:34<00:02, 13.39it/s]
 92%|█████████▏| 351/383 [00:34<00:02, 13.30it/s][A 93%|█████████▎| 358/383 [00:34<00:01, 15.28it/s] 92%|█████████▏| 354/383 [00:34<00:02, 14.07it/s] 92%|█████████▏| 352/383 [00:34<00:02, 13.67it/s]
 92%|█████████▏| 353/383 [00:34<00:02, 13.27it/s][A 93%|█████████▎| 356/383 [00:34<00:01, 14.37it/s] 92%|█████████▏| 354/383 [00:34<00:02, 13.76it/s] 93%|█████████▎| 358/383 [00:34<00:01, 14.75it/s]
 93%|█████████▎| 355/383 [00:34<00:02, 13.59it/s][A 94%|█████████▍| 360/383 [00:34<00:02,  9.73it/s] 93%|█████████▎| 356/383 [00:34<00:01, 13.63it/s]
 93%|█████████▎| 357/383 [00:34<00:01, 14.40it/s][A 93%|█████████▎| 358/383 [00:34<00:01, 14.87it/s]
 94%|█████████▎| 359/383 [00:34<00:01, 15.45it/s][A 94%|█████████▍| 360/383 [00:35<00:02,  9.97it/s] 95%|█████████▍| 362/383 [00:35<00:02,  8.33it/s] 94%|█████████▍| 360/383 [00:35<00:01, 11.85it/s]
 94%|█████████▍| 361/383 [00:35<00:02, 10.16it/s][A 95%|█████████▍| 362/383 [00:35<00:02,  8.17it/s] 95%|█████████▌| 364/383 [00:35<00:02,  7.20it/s] 95%|█████████▍| 362/383 [00:35<00:02,  9.10it/s]
 95%|█████████▍| 363/383 [00:35<00:02,  8.47it/s][A 95%|█████████▌| 365/383 [00:35<00:02,  6.86it/s] 95%|█████████▌| 364/383 [00:35<00:02,  7.14it/s] 96%|█████████▌| 367/383 [00:35<00:01,  8.69it/s] 95%|█████████▌| 364/383 [00:35<00:02,  7.81it/s] 96%|█████████▋| 369/383 [00:35<00:01, 10.47it/s] 95%|█████████▌| 365/383 [00:35<00:02,  6.85it/s]
 95%|█████████▌| 365/383 [00:35<00:02,  7.44it/s][A 97%|█████████▋| 371/383 [00:35<00:00, 12.16it/s] 95%|█████████▌| 365/383 [00:36<00:02,  7.36it/s] 96%|█████████▌| 367/383 [00:36<00:01,  8.61it/s]
 96%|█████████▌| 366/383 [00:36<00:02,  7.38it/s][A 97%|█████████▋| 373/383 [00:36<00:00, 13.67it/s] 96%|█████████▌| 367/383 [00:36<00:01,  9.24it/s] 96%|█████████▋| 369/383 [00:36<00:01, 10.40it/s]
 96%|█████████▌| 368/383 [00:36<00:01,  9.20it/s][A 98%|█████████▊| 375/383 [00:36<00:00, 14.64it/s] 96%|█████████▋| 369/383 [00:36<00:01, 10.96it/s] 97%|█████████▋| 371/383 [00:36<00:00, 12.04it/s] 98%|█████████▊| 377/383 [00:36<00:00, 15.82it/s]
 97%|█████████▋| 370/383 [00:36<00:01, 10.88it/s][A 97%|█████████▋| 371/383 [00:36<00:00, 12.44it/s] 97%|█████████▋| 373/383 [00:36<00:00, 13.42it/s]
 97%|█████████▋| 372/383 [00:36<00:00, 12.34it/s][A 99%|█████████▉| 380/383 [00:36<00:00, 18.26it/s] 97%|█████████▋| 373/383 [00:36<00:00, 13.83it/s] 98%|█████████▊| 375/383 [00:36<00:00, 14.87it/s]
 98%|█████████▊| 374/383 [00:36<00:00, 13.79it/s][A100%|██████████| 383/383 [00:36<00:00, 19.34it/s]100%|██████████| 383/383 [00:36<00:00, 10.47it/s]
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
 98%|█████████▊| 376/383 [00:36<00:00, 15.79it/s]/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
 99%|█████████▊| 378/383 [00:36<00:00, 16.91it/s]/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/pytorch/torch/utils/checkpoint.py:426: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 98%|█████████▊| 377/383 [00:36<00:00, 15.74it/s][A 99%|█████████▉| 379/383 [00:36<00:00, 18.38it/s] 99%|█████████▉| 381/383 [00:36<00:00, 19.49it/s]
 99%|█████████▉| 380/383 [00:36<00:00, 17.69it/s][A100%|██████████| 383/383 [00:36<00:00, 10.40it/s]
100%|█████████▉| 382/383 [00:36<00:00, 20.59it/s]/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
100%|██████████| 383/383 [00:36<00:00, 10.38it/s]
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/pytorch/torch/utils/checkpoint.py:426: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)

100%|██████████| 383/383 [00:36<00:00, 18.73it/s][A100%|██████████| 383/383 [00:36<00:00, 10.36it/s]
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/pytorch/torch/utils/checkpoint.py:426: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
                                                     {'mmlu_loss': 3.489238771539444, 'mmlu_eval_accuracy_conceptual_physics': 0.23076923076923078, 'mmlu_eval_accuracy_logical_fallacies': nan, 'mmlu_eval_accuracy_professional_psychology': nan, 'mmlu_eval_accuracy_public_relations': nan, 'mmlu_eval_accuracy_security_studies': nan, 'mmlu_eval_accuracy_high_school_physics': nan, 'mmlu_eval_accuracy_formal_logic': 0.21428571428571427, 'mmlu_eval_accuracy_anatomy': 0.2857142857142857, 'mmlu_eval_accuracy_moral_scenarios': nan, 'mmlu_eval_accuracy_moral_disputes': nan, 'mmlu_eval_accuracy_management': nan, 'mmlu_eval_accuracy_high_school_european_history': 0.3888888888888889, 'mmlu_eval_accuracy_professional_law': nan, 'mmlu_eval_accuracy_elementary_mathematics': 0.2682926829268293, 'mmlu_eval_accuracy_high_school_computer_science': 0.3333333333333333, 'mmlu_eval_accuracy_high_school_geography': 0.4166666666666667, 'mmlu_eval_accuracy_global_facts': 0.5, 'mmlu_eval_accuracy_high_school_biology': 0.25, 'mmlu_eval_accuracy_philosophy': nan, 'mmlu_eval_accuracy_astronomy': 0.25, 'mmlu_eval_accuracy_nutrition': nan, 'mmlu_eval_accuracy_high_school_government_and_politics': nan, 'mmlu_eval_accuracy_medical_genetics': nan, 'mmlu_eval_accuracy_human_aging': nan, 'mmlu_eval_accuracy_human_sexuality': nan, 'mmlu_eval_accuracy_college_medicine': 0.13636363636363635, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_us_foreign_policy': nan, 'mmlu_eval_accuracy_college_computer_science': 0.09090909090909091, 'mmlu_eval_accuracy_machine_learning': nan, 'mmlu_eval_accuracy_professional_accounting': nan, 'mmlu_eval_accuracy_world_religions': nan, 'mmlu_eval_accuracy_miscellaneous': nan, 'mmlu_eval_accuracy_jurisprudence': nan, 'mmlu_eval_accuracy_high_school_chemistry': 0.22727272727272727, 'mmlu_eval_accuracy_virology': nan, 'mmlu_eval_accuracy_high_school_mathematics': nan, 'mmlu_eval_accuracy_electrical_engineering': 0.3125, 'mmlu_eval_accuracy_high_school_world_history': nan, 'mmlu_eval_accuracy_prehistory': nan, 'mmlu_eval_accuracy_high_school_statistics': nan, 'mmlu_eval_accuracy_high_school_psychology': nan, 'mmlu_eval_accuracy_high_school_microeconomics': nan, 'mmlu_eval_accuracy_high_school_us_history': nan, 'mmlu_eval_accuracy_college_biology': 0.375, 'mmlu_eval_accuracy_high_school_macroeconomics': nan, 'mmlu_eval_accuracy_professional_medicine': nan, 'mmlu_eval_accuracy_clinical_knowledge': 0.3448275862068966, 'mmlu_eval_accuracy_business_ethics': 0.09090909090909091, 'mmlu_eval_accuracy_college_chemistry': 0.0, 'mmlu_eval_accuracy_econometrics': 0.3333333333333333, 'mmlu_eval_accuracy_marketing': nan, 'mmlu_eval_accuracy_international_law': nan, 'mmlu_eval_accuracy_sociology': nan, 'mmlu_eval_accuracy_computer_security': 0.18181818181818182, 'mmlu_eval_accuracy_college_mathematics': 0.09090909090909091, 'mmlu_eval_accuracy': nan, 'epoch': 13.0}
 88%|████████▊ | 1792/2048 [2:28:03<15:38,  3.66s/it]/home/bagus/pytorch/torch/utils/checkpoint.py:426: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 88%|████████▊ | 1793/2048 [2:28:08<1:23:36, 19.67s/it] 88%|████████▊ | 1794/2048 [2:28:13<1:04:51, 15.32s/it] 88%|████████▊ | 1795/2048 [2:28:18<51:45, 12.27s/it]   88%|████████▊ | 1796/2048 [2:28:23<42:34, 10.14s/it] 88%|████████▊ | 1797/2048 [2:28:28<35:55,  8.59s/it] 88%|████████▊ | 1798/2048 [2:28:33<31:04,  7.46s/it] 88%|████████▊ | 1799/2048 [2:28:38<27:17,  6.58s/it] 88%|████████▊ | 1800/2048 [2:28:42<24:18,  5.88s/it] 88%|████████▊ | 1801/2048 [2:28:46<22:08,  5.38s/it] 88%|████████▊ | 1802/2048 [2:28:50<20:25,  4.98s/it] 88%|████████▊ | 1803/2048 [2:28:54<18:34,  4.55s/it] 88%|████████▊ | 1804/2048 [2:28:57<16:49,  4.14s/it] 88%|████████▊ | 1805/2048 [2:29:00<15:47,  3.90s/it] 88%|████████▊ | 1806/2048 [2:29:05<17:16,  4.28s/it] 88%|████████▊ | 1807/2048 [2:29:11<18:15,  4.55s/it] 88%|████████▊ | 1808/2048 [2:29:16<18:55,  4.73s/it]                                                     {'loss': 0.0673, 'learning_rate': 7.120510219127619e-06, 'epoch': 13.11}
 88%|████████▊ | 1808/2048 [2:29:16<18:55,  4.73s/it] 88%|████████▊ | 1809/2048 [2:29:21<19:13,  4.83s/it] 88%|████████▊ | 1810/2048 [2:29:26<19:17,  4.86s/it] 88%|████████▊ | 1811/2048 [2:29:30<18:51,  4.77s/it] 88%|████████▊ | 1812/2048 [2:29:35<18:08,  4.61s/it] 89%|████████▊ | 1813/2048 [2:29:39<17:16,  4.41s/it] 89%|████████▊ | 1814/2048 [2:29:43<16:50,  4.32s/it] 89%|████████▊ | 1815/2048 [2:29:47<16:35,  4.27s/it] 89%|████████▊ | 1816/2048 [2:29:51<16:12,  4.19s/it] 89%|████████▊ | 1817/2048 [2:29:54<15:01,  3.90s/it] 89%|████████▉ | 1818/2048 [2:29:58<15:29,  4.04s/it] 89%|████████▉ | 1819/2048 [2:30:04<16:43,  4.38s/it] 89%|████████▉ | 1820/2048 [2:30:09<17:32,  4.61s/it] 89%|████████▉ | 1821/2048 [2:30:14<18:02,  4.77s/it] 89%|████████▉ | 1822/2048 [2:30:19<18:11,  4.83s/it] 89%|████████▉ | 1823/2048 [2:30:24<18:02,  4.81s/it] 89%|████████▉ | 1824/2048 [2:30:28<17:33,  4.70s/it]                                                     {'loss': 0.0972, 'learning_rate': 6.2123871521200915e-06, 'epoch': 13.23}
 89%|████████▉ | 1824/2048 [2:30:28<17:33,  4.70s/it] 89%|████████▉ | 1825/2048 [2:30:32<16:51,  4.54s/it] 89%|████████▉ | 1826/2048 [2:30:36<16:10,  4.37s/it] 89%|████████▉ | 1827/2048 [2:30:40<15:50,  4.30s/it] 89%|████████▉ | 1828/2048 [2:30:44<15:09,  4.13s/it] 89%|████████▉ | 1829/2048 [2:30:48<14:22,  3.94s/it] 89%|████████▉ | 1830/2048 [2:30:51<13:40,  3.76s/it] 89%|████████▉ | 1831/2048 [2:30:56<15:07,  4.18s/it] 89%|████████▉ | 1832/2048 [2:31:01<16:07,  4.48s/it] 90%|████████▉ | 1833/2048 [2:31:06<16:46,  4.68s/it] 90%|████████▉ | 1834/2048 [2:31:11<17:09,  4.81s/it] 90%|████████▉ | 1835/2048 [2:31:16<17:03,  4.81s/it] 90%|████████▉ | 1836/2048 [2:31:21<16:46,  4.75s/it] 90%|████████▉ | 1837/2048 [2:31:25<16:23,  4.66s/it] 90%|████████▉ | 1838/2048 [2:31:29<15:43,  4.49s/it] 90%|████████▉ | 1839/2048 [2:31:33<14:53,  4.28s/it] 90%|████████▉ | 1840/2048 [2:31:37<14:34,  4.21s/it]                                                     {'loss': 0.0855, 'learning_rate': 5.364340432580606e-06, 'epoch': 13.35}
 90%|████████▉ | 1840/2048 [2:31:37<14:34,  4.21s/it] 90%|████████▉ | 1841/2048 [2:31:41<14:17,  4.14s/it] 90%|████████▉ | 1842/2048 [2:31:45<13:38,  3.97s/it] 90%|████████▉ | 1843/2048 [2:31:49<13:58,  4.09s/it] 90%|█████████ | 1844/2048 [2:31:54<15:00,  4.41s/it] 90%|█████████ | 1845/2048 [2:32:00<15:40,  4.63s/it] 90%|█████████ | 1846/2048 [2:32:05<16:02,  4.77s/it] 90%|█████████ | 1847/2048 [2:32:10<16:10,  4.83s/it] 90%|█████████ | 1848/2048 [2:32:14<15:59,  4.80s/it] 90%|█████████ | 1849/2048 [2:32:19<15:36,  4.70s/it] 90%|█████████ | 1850/2048 [2:32:23<15:11,  4.60s/it] 90%|█████████ | 1851/2048 [2:32:27<14:25,  4.39s/it] 90%|█████████ | 1852/2048 [2:32:31<14:01,  4.30s/it] 90%|█████████ | 1853/2048 [2:32:35<13:16,  4.08s/it] 91%|█████████ | 1854/2048 [2:32:38<12:44,  3.94s/it] 91%|█████████ | 1855/2048 [2:32:42<12:09,  3.78s/it] 91%|█████████ | 1856/2048 [2:32:47<13:26,  4.20s/it]                                                     {'loss': 0.0856, 'learning_rate': 4.576913283093098e-06, 'epoch': 13.46}
 91%|█████████ | 1856/2048 [2:32:47<13:26,  4.20s/it] 91%|█████████ | 1857/2048 [2:32:52<14:17,  4.49s/it] 91%|█████████ | 1858/2048 [2:32:57<14:50,  4.68s/it] 91%|█████████ | 1859/2048 [2:33:02<15:04,  4.79s/it] 91%|█████████ | 1860/2048 [2:33:07<15:03,  4.80s/it] 91%|█████████ | 1861/2048 [2:33:12<14:54,  4.78s/it] 91%|█████████ | 1862/2048 [2:33:16<14:33,  4.69s/it] 91%|█████████ | 1863/2048 [2:33:20<13:58,  4.53s/it] 91%|█████████ | 1864/2048 [2:33:24<13:15,  4.32s/it] 91%|█████████ | 1865/2048 [2:33:28<12:44,  4.18s/it] 91%|█████████ | 1866/2048 [2:33:32<12:06,  3.99s/it] 91%|█████████ | 1867/2048 [2:33:35<11:27,  3.80s/it] 91%|█████████ | 1868/2048 [2:33:39<11:54,  3.97s/it] 91%|█████████▏| 1869/2048 [2:33:45<12:54,  4.33s/it] 91%|█████████▏| 1870/2048 [2:33:50<13:35,  4.58s/it] 91%|█████████▏| 1871/2048 [2:33:55<14:00,  4.75s/it] 91%|█████████▏| 1872/2048 [2:34:00<14:02,  4.79s/it]                                                     {'loss': 0.0748, 'learning_rate': 3.850610095928608e-06, 'epoch': 13.58}
 91%|█████████▏| 1872/2048 [2:34:00<14:02,  4.79s/it] 91%|█████████▏| 1873/2048 [2:34:05<13:56,  4.78s/it] 92%|█████████▏| 1874/2048 [2:34:09<13:32,  4.67s/it] 92%|█████████▏| 1875/2048 [2:34:13<12:55,  4.48s/it] 92%|█████████▏| 1876/2048 [2:34:17<12:23,  4.32s/it] 92%|█████████▏| 1877/2048 [2:34:21<11:54,  4.18s/it] 92%|█████████▏| 1878/2048 [2:34:25<11:44,  4.14s/it] 92%|█████████▏| 1879/2048 [2:34:28<11:10,  3.97s/it] 92%|█████████▏| 1880/2048 [2:34:32<10:33,  3.77s/it] 92%|█████████▏| 1881/2048 [2:34:37<11:39,  4.19s/it] 92%|█████████▏| 1882/2048 [2:34:42<12:23,  4.48s/it] 92%|█████████▏| 1883/2048 [2:34:47<12:52,  4.68s/it] 92%|█████████▏| 1884/2048 [2:34:52<13:11,  4.83s/it] 92%|█████████▏| 1885/2048 [2:34:57<13:06,  4.82s/it] 92%|█████████▏| 1886/2048 [2:35:02<12:56,  4.79s/it] 92%|█████████▏| 1887/2048 [2:35:06<12:34,  4.69s/it] 92%|█████████▏| 1888/2048 [2:35:11<12:07,  4.55s/it]                                                     {'loss': 0.0997, 'learning_rate': 3.185896109953057e-06, 'epoch': 13.69}
 92%|█████████▏| 1888/2048 [2:35:11<12:07,  4.55s/it] 92%|█████████▏| 1889/2048 [2:35:14<11:30,  4.34s/it] 92%|█████████▏| 1890/2048 [2:35:18<11:03,  4.20s/it] 92%|█████████▏| 1891/2048 [2:35:22<10:47,  4.13s/it] 92%|█████████▏| 1892/2048 [2:35:25<09:57,  3.83s/it] 92%|█████████▏| 1893/2048 [2:35:30<10:17,  3.98s/it] 92%|█████████▏| 1894/2048 [2:35:35<11:07,  4.34s/it] 93%|█████████▎| 1895/2048 [2:35:40<11:41,  4.58s/it] 93%|█████████▎| 1896/2048 [2:35:45<12:02,  4.75s/it] 93%|█████████▎| 1897/2048 [2:35:50<12:06,  4.81s/it] 93%|█████████▎| 1898/2048 [2:35:55<12:00,  4.80s/it] 93%|█████████▎| 1899/2048 [2:35:59<11:34,  4.66s/it] 93%|█████████▎| 1900/2048 [2:36:03<11:12,  4.54s/it] 93%|█████████▎| 1901/2048 [2:36:07<10:41,  4.36s/it] 93%|█████████▎| 1902/2048 [2:36:11<10:15,  4.22s/it] 93%|█████████▎| 1903/2048 [2:36:16<10:14,  4.24s/it] 93%|█████████▎| 1904/2048 [2:36:19<09:45,  4.06s/it]                                                     {'loss': 0.0767, 'learning_rate': 2.5831971126150766e-06, 'epoch': 13.81}
 93%|█████████▎| 1904/2048 [2:36:19<09:45,  4.06s/it] 93%|█████████▎| 1905/2048 [2:36:23<09:14,  3.88s/it] 93%|█████████▎| 1906/2048 [2:36:28<10:05,  4.27s/it] 93%|█████████▎| 1907/2048 [2:36:33<10:39,  4.54s/it] 93%|█████████▎| 1908/2048 [2:36:38<11:01,  4.72s/it] 93%|█████████▎| 1909/2048 [2:36:43<11:09,  4.81s/it] 93%|█████████▎| 1910/2048 [2:36:48<11:08,  4.84s/it] 93%|█████████▎| 1911/2048 [2:36:53<10:51,  4.75s/it] 93%|█████████▎| 1912/2048 [2:36:57<10:29,  4.63s/it] 93%|█████████▎| 1913/2048 [2:37:01<09:58,  4.43s/it] 93%|█████████▎| 1914/2048 [2:37:05<09:36,  4.31s/it] 94%|█████████▎| 1915/2048 [2:37:09<09:11,  4.15s/it] 94%|█████████▎| 1916/2048 [2:37:13<09:05,  4.13s/it] 94%|█████████▎| 1917/2048 [2:37:16<08:24,  3.85s/it] 94%|█████████▎| 1918/2048 [2:37:20<08:40,  4.00s/it] 94%|█████████▎| 1919/2048 [2:37:26<09:21,  4.35s/it] 94%|█████████▍| 1920/2048 [2:37:31<09:48,  4.60s/it]                                                     {'loss': 0.0762, 'learning_rate': 2.0428991672047258e-06, 'epoch': 13.93}
 94%|█████████▍| 1920/2048 [2:37:31<09:48,  4.60s/it] 94%|█████████▍| 1921/2048 [2:37:36<10:03,  4.75s/it] 94%|█████████▍| 1922/2048 [2:37:41<10:07,  4.82s/it] 94%|█████████▍| 1923/2048 [2:37:46<09:56,  4.77s/it] 94%|█████████▍| 1924/2048 [2:37:50<09:38,  4.67s/it] 94%|█████████▍| 1925/2048 [2:37:54<09:13,  4.50s/it] 94%|█████████▍| 1926/2048 [2:37:58<08:54,  4.38s/it] 94%|█████████▍| 1927/2048 [2:38:02<08:36,  4.27s/it] 94%|█████████▍| 1928/2048 [2:38:06<08:21,  4.18s/it] 94%|█████████▍| 1929/2048 [2:38:10<08:00,  4.03s/it] 94%|█████████▍| 1930/2048 [2:38:13<07:31,  3.82s/it] 94%|█████████▍| 1931/2048 [2:38:18<08:11,  4.20s/it] 94%|█████████▍| 1932/2048 [2:38:23<08:40,  4.49s/it] 94%|█████████▍| 1933/2048 [2:38:29<08:59,  4.69s/it] 94%|█████████▍| 1934/2048 [2:38:34<09:07,  4.80s/it] 94%|█████████▍| 1935/2048 [2:38:39<09:09,  4.87s/it] 95%|█████████▍| 1936/2048 [2:38:43<09:02,  4.84s/it]                                                     {'loss': 0.0946, 'learning_rate': 1.565348365557795e-06, 'epoch': 14.04}
 95%|█████████▍| 1936/2048 [2:38:43<09:02,  4.84s/it] 95%|█████████▍| 1937/2048 [2:38:48<08:49,  4.77s/it] 95%|█████████▍| 1938/2048 [2:38:52<08:31,  4.65s/it] 95%|█████████▍| 1939/2048 [2:38:56<08:05,  4.45s/it] 95%|█████████▍| 1940/2048 [2:39:00<07:41,  4.27s/it] 95%|█████████▍| 1941/2048 [2:39:04<07:17,  4.08s/it] 95%|█████████▍| 1942/2048 [2:39:07<06:53,  3.90s/it] 95%|█████████▍| 1943/2048 [2:39:11<06:40,  3.81s/it] 95%|█████████▍| 1944/2048 [2:39:16<07:18,  4.22s/it] 95%|█████████▍| 1945/2048 [2:39:21<07:43,  4.50s/it] 95%|█████████▌| 1946/2048 [2:39:26<07:59,  4.70s/it] 95%|█████████▌| 1947/2048 [2:39:32<08:04,  4.80s/it] 95%|█████████▌| 1948/2048 [2:39:36<07:59,  4.79s/it] 95%|█████████▌| 1949/2048 [2:39:41<07:46,  4.71s/it] 95%|█████████▌| 1950/2048 [2:39:45<07:37,  4.67s/it] 95%|█████████▌| 1951/2048 [2:39:49<07:14,  4.48s/it] 95%|█████████▌| 1952/2048 [2:39:54<07:03,  4.41s/it]                                                     {'loss': 0.0746, 'learning_rate': 1.1508506063641177e-06, 'epoch': 14.16}
 95%|█████████▌| 1952/2048 [2:39:54<07:03,  4.41s/it] 95%|█████████▌| 1953/2048 [2:39:57<06:41,  4.23s/it] 95%|█████████▌| 1954/2048 [2:40:01<06:20,  4.05s/it] 95%|█████████▌| 1955/2048 [2:40:04<05:54,  3.81s/it] 96%|█████████▌| 1956/2048 [2:40:09<06:12,  4.05s/it] 96%|█████████▌| 1957/2048 [2:40:14<06:39,  4.39s/it] 96%|█████████▌| 1958/2048 [2:40:19<06:55,  4.62s/it] 96%|█████████▌| 1959/2048 [2:40:24<07:04,  4.77s/it] 96%|█████████▌| 1960/2048 [2:40:29<07:07,  4.86s/it] 96%|█████████▌| 1961/2048 [2:40:34<07:01,  4.84s/it] 96%|█████████▌| 1962/2048 [2:40:39<06:44,  4.71s/it] 96%|█████████▌| 1963/2048 [2:40:43<06:28,  4.57s/it] 96%|█████████▌| 1964/2048 [2:40:47<06:11,  4.42s/it] 96%|█████████▌| 1965/2048 [2:40:51<05:50,  4.22s/it] 96%|█████████▌| 1966/2048 [2:40:55<05:39,  4.14s/it] 96%|█████████▌| 1967/2048 [2:40:58<05:21,  3.97s/it] 96%|█████████▌| 1968/2048 [2:41:02<05:08,  3.85s/it]                                                     {'loss': 0.0802, 'learning_rate': 7.996713992219351e-07, 'epoch': 14.27}
 96%|█████████▌| 1968/2048 [2:41:02<05:08,  3.85s/it] 96%|█████████▌| 1969/2048 [2:41:07<05:35,  4.25s/it] 96%|█████████▌| 1970/2048 [2:41:12<05:52,  4.52s/it] 96%|█████████▌| 1971/2048 [2:41:17<06:02,  4.70s/it] 96%|█████████▋| 1972/2048 [2:41:22<06:03,  4.78s/it] 96%|█████████▋| 1973/2048 [2:41:27<06:01,  4.82s/it] 96%|█████████▋| 1974/2048 [2:41:32<05:52,  4.76s/it] 96%|█████████▋| 1975/2048 [2:41:36<05:37,  4.63s/it] 96%|█████████▋| 1976/2048 [2:41:40<05:23,  4.50s/it] 97%|█████████▋| 1977/2048 [2:41:44<05:11,  4.38s/it] 97%|█████████▋| 1978/2048 [2:41:48<04:50,  4.15s/it] 97%|█████████▋| 1979/2048 [2:41:52<04:37,  4.03s/it] 97%|█████████▋| 1980/2048 [2:41:55<04:18,  3.80s/it] 97%|█████████▋| 1981/2048 [2:42:00<04:30,  4.04s/it] 97%|█████████▋| 1982/2048 [2:42:05<04:49,  4.38s/it] 97%|█████████▋| 1983/2048 [2:42:10<04:59,  4.61s/it] 97%|█████████▋| 1984/2048 [2:42:15<05:05,  4.77s/it]                                                     {'loss': 0.0816, 'learning_rate': 5.120356945637173e-07, 'epoch': 14.39}
 97%|█████████▋| 1984/2048 [2:42:15<05:05,  4.77s/it] 97%|█████████▋| 1985/2048 [2:42:20<05:06,  4.86s/it] 97%|█████████▋| 1986/2048 [2:42:25<05:00,  4.84s/it] 97%|█████████▋| 1987/2048 [2:42:29<04:47,  4.71s/it] 97%|█████████▋| 1988/2048 [2:42:33<04:31,  4.52s/it] 97%|█████████▋| 1989/2048 [2:42:38<04:19,  4.41s/it] 97%|█████████▋| 1990/2048 [2:42:41<04:02,  4.19s/it] 97%|█████████▋| 1991/2048 [2:42:45<03:50,  4.04s/it] 97%|█████████▋| 1992/2048 [2:42:49<03:38,  3.91s/it] 97%|█████████▋| 1993/2048 [2:42:52<03:28,  3.80s/it] 97%|█████████▋| 1994/2048 [2:42:57<03:47,  4.21s/it] 97%|█████████▋| 1995/2048 [2:43:02<03:58,  4.50s/it] 97%|█████████▋| 1996/2048 [2:43:08<04:03,  4.68s/it] 98%|█████████▊| 1997/2048 [2:43:12<04:02,  4.75s/it] 98%|█████████▊| 1998/2048 [2:43:17<03:56,  4.74s/it] 98%|█████████▊| 1999/2048 [2:43:22<03:47,  4.65s/it] 98%|█████████▊| 2000/2048 [2:43:26<03:34,  4.47s/it]                                                     {'loss': 0.1046, 'learning_rate': 2.8812773956256035e-07, 'epoch': 14.51}
 98%|█████████▊| 2000/2048 [2:43:26<03:34,  4.47s/it] 98%|█████████▊| 2001/2048 [2:43:30<03:24,  4.35s/it] 98%|█████████▊| 2002/2048 [2:43:34<03:13,  4.22s/it] 98%|█████████▊| 2003/2048 [2:43:38<03:06,  4.13s/it] 98%|█████████▊| 2004/2048 [2:43:41<02:57,  4.03s/it] 98%|█████████▊| 2005/2048 [2:43:45<02:41,  3.76s/it] 98%|█████████▊| 2006/2048 [2:43:49<02:48,  4.02s/it] 98%|█████████▊| 2007/2048 [2:43:54<02:58,  4.36s/it] 98%|█████████▊| 2008/2048 [2:43:59<03:04,  4.60s/it] 98%|█████████▊| 2009/2048 [2:44:05<03:05,  4.75s/it] 98%|█████████▊| 2010/2048 [2:44:09<03:01,  4.79s/it] 98%|█████████▊| 2011/2048 [2:44:14<02:55,  4.75s/it] 98%|█████████▊| 2012/2048 [2:44:18<02:47,  4.65s/it] 98%|█████████▊| 2013/2048 [2:44:23<02:38,  4.53s/it] 98%|█████████▊| 2014/2048 [2:44:27<02:27,  4.33s/it] 98%|█████████▊| 2015/2048 [2:44:31<02:18,  4.21s/it] 98%|█████████▊| 2016/2048 [2:44:35<02:18,  4.32s/it]                                                     {'loss': 0.0774, 'learning_rate': 1.280909601112379e-07, 'epoch': 14.62}
 98%|█████████▊| 2016/2048 [2:44:35<02:18,  4.32s/it] 98%|█████████▊| 2017/2048 [2:44:39<02:09,  4.19s/it] 99%|█████████▊| 2018/2048 [2:44:43<02:00,  4.02s/it] 99%|█████████▊| 2019/2048 [2:44:48<02:06,  4.37s/it] 99%|█████████▊| 2020/2048 [2:44:53<02:08,  4.61s/it] 99%|█████████▊| 2021/2048 [2:44:58<02:08,  4.77s/it] 99%|█████████▊| 2022/2048 [2:45:03<02:05,  4.84s/it] 99%|█████████▉| 2023/2048 [2:45:08<02:01,  4.86s/it] 99%|█████████▉| 2024/2048 [2:45:13<01:55,  4.81s/it] 99%|█████████▉| 2025/2048 [2:45:17<01:46,  4.62s/it] 99%|█████████▉| 2026/2048 [2:45:21<01:37,  4.42s/it] 99%|█████████▉| 2027/2048 [2:45:25<01:28,  4.23s/it] 99%|█████████▉| 2028/2048 [2:45:29<01:24,  4.23s/it] 99%|█████████▉| 2029/2048 [2:45:33<01:18,  4.14s/it] 99%|█████████▉| 2030/2048 [2:45:36<01:10,  3.90s/it] 99%|█████████▉| 2031/2048 [2:45:41<01:09,  4.11s/it] 99%|█████████▉| 2032/2048 [2:45:46<01:10,  4.43s/it]                                                     {'loss': 0.0779, 'learning_rate': 3.202786894975773e-08, 'epoch': 14.74}
 99%|█████████▉| 2032/2048 [2:45:46<01:10,  4.43s/it] 99%|█████████▉| 2033/2048 [2:45:51<01:09,  4.64s/it] 99%|█████████▉| 2034/2048 [2:45:56<01:06,  4.79s/it] 99%|█████████▉| 2035/2048 [2:46:01<01:02,  4.84s/it] 99%|█████████▉| 2036/2048 [2:46:06<00:57,  4.79s/it] 99%|█████████▉| 2037/2048 [2:46:10<00:51,  4.64s/it]100%|█████████▉| 2038/2048 [2:46:14<00:44,  4.48s/it]100%|█████████▉| 2039/2048 [2:46:18<00:38,  4.23s/it]100%|█████████▉| 2040/2048 [2:46:22<00:33,  4.16s/it]100%|█████████▉| 2041/2048 [2:46:26<00:28,  4.08s/it]100%|█████████▉| 2042/2048 [2:46:29<00:23,  3.90s/it]100%|█████████▉| 2043/2048 [2:46:33<00:19,  3.81s/it]100%|█████████▉| 2044/2048 [2:46:38<00:16,  4.22s/it]100%|█████████▉| 2045/2048 [2:46:43<00:13,  4.50s/it]100%|█████████▉| 2046/2048 [2:46:48<00:09,  4.70s/it]100%|█████████▉| 2047/2048 [2:46:53<00:04,  4.80s/it]100%|██████████| 2048/2048 [2:46:58<00:00,  4.80s/it]                                                     {'loss': 0.0927, 'learning_rate': 0.0, 'epoch': 14.85}
100%|██████████| 2048/2048 [2:46:58<00:00,  4.80s/it]
  0%|          | 0/256 [00:00<?, ?it/s][A
  2%|▏         | 4/256 [00:00<00:09, 27.42it/s][A
  3%|▎         | 7/256 [00:00<00:11, 21.15it/s][A
  4%|▍         | 10/256 [00:00<00:12, 19.38it/s][A
  5%|▍         | 12/256 [00:00<00:13, 18.42it/s][A
  5%|▌         | 14/256 [00:00<00:13, 17.87it/s][A
  6%|▋         | 16/256 [00:00<00:13, 18.21it/s][A
  7%|▋         | 18/256 [00:00<00:13, 17.82it/s][A
  8%|▊         | 21/256 [00:01<00:12, 18.55it/s][A
  9%|▉         | 23/256 [00:01<00:12, 18.05it/s][A
 10%|▉         | 25/256 [00:01<00:13, 17.69it/s][A
 11%|█         | 27/256 [00:01<00:12, 17.77it/s][A
 11%|█▏        | 29/256 [00:01<00:12, 17.47it/s][A
 12%|█▏        | 31/256 [00:01<00:13, 17.27it/s][A
 13%|█▎        | 34/256 [00:01<00:11, 18.73it/s][A
 14%|█▍        | 36/256 [00:01<00:12, 18.16it/s][A
 15%|█▍        | 38/256 [00:02<00:12, 17.64it/s][A
 16%|█▌        | 40/256 [00:02<00:12, 17.38it/s][A
 16%|█▋        | 42/256 [00:02<00:12, 17.75it/s][A
 17%|█▋        | 44/256 [00:02<00:12, 17.49it/s][A
 18%|█▊        | 46/256 [00:02<00:12, 17.29it/s][A
 19%|█▉        | 48/256 [00:02<00:11, 17.40it/s][A
 20%|█▉        | 50/256 [00:02<00:12, 17.10it/s][A
 20%|██        | 52/256 [00:02<00:11, 17.00it/s][A
 21%|██        | 54/256 [00:02<00:11, 17.79it/s][A
 22%|██▏       | 56/256 [00:03<00:11, 17.60it/s][A
 23%|██▎       | 58/256 [00:03<00:11, 17.52it/s][A
 23%|██▎       | 60/256 [00:03<00:11, 17.28it/s][A
 24%|██▍       | 62/256 [00:03<00:11, 16.95it/s][A
 25%|██▌       | 64/256 [00:03<00:11, 17.25it/s][A
 26%|██▌       | 66/256 [00:03<00:11, 17.09it/s][A
 27%|██▋       | 68/256 [00:03<00:11, 17.01it/s][A
 27%|██▋       | 70/256 [00:03<00:10, 17.15it/s][A
 28%|██▊       | 72/256 [00:04<00:10, 17.03it/s][A
 29%|██▉       | 74/256 [00:04<00:10, 16.99it/s][A
 30%|██▉       | 76/256 [00:04<00:10, 17.00it/s][A
 30%|███       | 78/256 [00:04<00:10, 16.98it/s][A
 31%|███▏      | 80/256 [00:04<00:10, 16.94it/s][A
 32%|███▏      | 82/256 [00:04<00:10, 16.98it/s][A
 33%|███▎      | 84/256 [00:04<00:10, 17.15it/s][A
 34%|███▎      | 86/256 [00:04<00:09, 17.06it/s][A
 34%|███▍      | 88/256 [00:04<00:09, 17.28it/s][A
 35%|███▌      | 90/256 [00:05<00:09, 17.03it/s][A
 36%|███▌      | 92/256 [00:05<00:09, 16.93it/s][A
 37%|███▋      | 94/256 [00:05<00:09, 17.00it/s][A
 38%|███▊      | 96/256 [00:05<00:09, 16.91it/s][A
 39%|███▊      | 99/256 [00:05<00:08, 17.73it/s][A
 39%|███▉      | 101/256 [00:05<00:08, 17.38it/s][A
 40%|████      | 103/256 [00:05<00:08, 17.49it/s][A
 41%|████▏     | 106/256 [00:06<00:08, 18.35it/s][A
 42%|████▏     | 108/256 [00:06<00:08, 17.97it/s][A
 43%|████▎     | 110/256 [00:06<00:08, 17.74it/s][A
 44%|████▍     | 112/256 [00:06<00:08, 17.45it/s][A
 45%|████▍     | 114/256 [00:06<00:08, 17.58it/s][A
 46%|████▌     | 117/256 [00:06<00:07, 18.03it/s][A
 46%|████▋     | 119/256 [00:06<00:07, 17.83it/s][A
 47%|████▋     | 121/256 [00:06<00:07, 17.46it/s][A
 48%|████▊     | 123/256 [00:06<00:07, 17.32it/s][A
 49%|████▉     | 125/256 [00:07<00:07, 17.15it/s][A
 50%|█████     | 128/256 [00:07<00:06, 18.67it/s][A
 51%|█████     | 130/256 [00:07<00:06, 18.23it/s][A
 52%|█████▏    | 132/256 [00:07<00:07, 17.69it/s][A
 52%|█████▏    | 134/256 [00:07<00:06, 17.54it/s][A
 53%|█████▎    | 136/256 [00:07<00:06, 17.39it/s][A
 54%|█████▍    | 138/256 [00:07<00:06, 17.20it/s][A
 55%|█████▍    | 140/256 [00:07<00:06, 17.07it/s][A
 56%|█████▌    | 143/256 [00:08<00:06, 18.20it/s][A
 57%|█████▋    | 145/256 [00:08<00:06, 17.64it/s][A
 57%|█████▋    | 147/256 [00:08<00:06, 17.44it/s][A
 58%|█████▊    | 149/256 [00:08<00:06, 17.25it/s][A
 59%|█████▉    | 151/256 [00:08<00:06, 17.11it/s][A
 60%|██████    | 154/256 [00:08<00:05, 17.73it/s][A
 61%|██████    | 156/256 [00:08<00:05, 17.59it/s][A
 62%|██████▏   | 158/256 [00:08<00:05, 17.24it/s][A
 63%|██████▎   | 161/256 [00:09<00:05, 17.98it/s][A
 64%|██████▎   | 163/256 [00:09<00:05, 17.54it/s][A
 64%|██████▍   | 165/256 [00:09<00:05, 17.45it/s][A
 65%|██████▌   | 167/256 [00:09<00:05, 17.07it/s][A
 66%|██████▌   | 169/256 [00:09<00:05, 17.32it/s][A
 67%|██████▋   | 171/256 [00:09<00:04, 17.08it/s][A
 68%|██████▊   | 173/256 [00:09<00:04, 16.94it/s][A
 68%|██████▊   | 175/256 [00:09<00:04, 17.00it/s][A
 69%|██████▉   | 177/256 [00:10<00:04, 16.84it/s][A
 70%|██████▉   | 179/256 [00:10<00:04, 16.92it/s][A
 71%|███████   | 182/256 [00:10<00:03, 19.23it/s][A
 72%|███████▏  | 184/256 [00:10<00:03, 18.60it/s][A
 73%|███████▎  | 186/256 [00:10<00:03, 17.94it/s][A
 74%|███████▍  | 189/256 [00:10<00:03, 18.43it/s][A
 75%|███████▍  | 191/256 [00:10<00:03, 17.85it/s][A
 75%|███████▌  | 193/256 [00:10<00:03, 17.68it/s][A
 76%|███████▌  | 195/256 [00:11<00:03, 17.28it/s][A
 77%|███████▋  | 197/256 [00:11<00:03, 17.59it/s][A
 78%|███████▊  | 199/256 [00:11<00:03, 18.19it/s][A
 79%|███████▊  | 201/256 [00:11<00:03, 17.87it/s][A
 79%|███████▉  | 203/256 [00:11<00:03, 17.41it/s][A
 80%|████████  | 205/256 [00:11<00:02, 17.36it/s][A
 81%|████████  | 207/256 [00:11<00:02, 17.05it/s][A
 82%|████████▏ | 209/256 [00:11<00:02, 16.95it/s][A
 82%|████████▏ | 211/256 [00:11<00:02, 16.93it/s][A
 83%|████████▎ | 213/256 [00:12<00:02, 16.89it/s][A
 84%|████████▍ | 215/256 [00:12<00:02, 16.85it/s][A
 85%|████████▍ | 217/256 [00:12<00:02, 16.84it/s][A
 86%|████████▌ | 219/256 [00:12<00:02, 16.94it/s][A
 86%|████████▋ | 221/256 [00:12<00:02, 16.78it/s][A
 87%|████████▋ | 223/256 [00:12<00:01, 16.79it/s][A
 88%|████████▊ | 225/256 [00:12<00:01, 17.22it/s][A
 89%|████████▊ | 227/256 [00:12<00:01, 16.95it/s][A
 89%|████████▉ | 229/256 [00:13<00:01, 16.86it/s][A
 90%|█████████ | 231/256 [00:13<00:01, 17.09it/s][A
 91%|█████████ | 233/256 [00:13<00:01, 17.06it/s][A
 92%|█████████▏| 235/256 [00:13<00:01, 17.03it/s][A
 93%|█████████▎| 238/256 [00:13<00:01, 17.96it/s][A
 94%|█████████▍| 240/256 [00:13<00:00, 17.74it/s][A
 95%|█████████▍| 242/256 [00:13<00:00, 17.46it/s][A
 95%|█████████▌| 244/256 [00:13<00:00, 17.78it/s][A
 96%|█████████▋| 247/256 [00:14<00:00, 18.28it/s][A
 97%|█████████▋| 249/256 [00:14<00:00, 17.96it/s][A
 98%|█████████▊| 252/256 [00:14<00:00, 19.03it/s][A
 99%|█████████▉| 254/256 [00:14<00:00, 18.55it/s][A
100%|██████████| 256/256 [00:14<00:00, 17.08it/s][A                                                     
                                                 [A{'eval_loss': 2.2517879009246826, 'eval_runtime': 14.7508, 'eval_samples_per_second': 69.42, 'eval_steps_per_second': 17.355, 'epoch': 14.85}
100%|██████████| 2048/2048 [2:47:13<00:00,  4.80s/it]
100%|██████████| 256/256 [00:14<00:00, 17.08it/s][A
                                                 [A  0%|          | 0/383 [00:00<?, ?it/s]  0%|          | 0/383 [00:00<?, ?it/s]  0%|          | 0/383 [00:00<?, ?it/s]
  0%|          | 0/383 [00:00<?, ?it/s][A  0%|          | 1/383 [00:00<00:40,  9.41it/s]
  0%|          | 1/383 [00:00<00:51,  7.47it/s][A  1%|          | 2/383 [00:00<00:26, 14.52it/s]  1%|          | 2/383 [00:00<00:27, 14.04it/s]  1%|          | 3/383 [00:00<00:24, 15.33it/s]
  1%|          | 4/383 [00:00<00:23, 16.40it/s][A  1%|▏         | 5/383 [00:00<00:19, 19.36it/s]  1%|▏         | 5/383 [00:00<00:20, 18.34it/s]  1%|▏         | 5/383 [00:00<00:21, 17.34it/s]  2%|▏         | 7/383 [00:00<00:20, 17.91it/s]
  2%|▏         | 7/383 [00:00<00:20, 18.72it/s][A  2%|▏         | 7/383 [00:00<00:21, 17.15it/s]  2%|▏         | 7/383 [00:00<00:22, 16.76it/s]  2%|▏         | 9/383 [00:00<00:23, 16.08it/s]
  2%|▏         | 9/383 [00:00<00:23, 16.24it/s][A  2%|▏         | 9/383 [00:00<00:24, 15.44it/s]  2%|▏         | 9/383 [00:00<00:23, 16.01it/s]  3%|▎         | 11/383 [00:00<00:23, 15.53it/s]
  3%|▎         | 11/383 [00:00<00:24, 15.50it/s][A  3%|▎         | 11/383 [00:00<00:25, 14.64it/s]  3%|▎         | 11/383 [00:00<00:24, 15.03it/s]  3%|▎         | 13/383 [00:00<00:24, 15.07it/s]  3%|▎         | 13/383 [00:00<00:24, 15.34it/s]
  3%|▎         | 13/383 [00:00<00:24, 14.90it/s][A  3%|▎         | 13/383 [00:00<00:25, 14.43it/s]  4%|▍         | 15/383 [00:00<00:22, 16.13it/s]  4%|▍         | 15/383 [00:00<00:22, 16.26it/s]
  4%|▍         | 15/383 [00:00<00:23, 15.91it/s][A  4%|▍         | 15/383 [00:00<00:23, 15.46it/s]  4%|▍         | 17/383 [00:01<00:21, 16.78it/s]  4%|▍         | 17/383 [00:01<00:21, 16.99it/s]
  4%|▍         | 17/383 [00:01<00:21, 16.81it/s][A  4%|▍         | 17/383 [00:01<00:22, 16.26it/s]  5%|▍         | 19/383 [00:01<00:20, 17.35it/s]  5%|▍         | 19/383 [00:01<00:20, 17.54it/s]
  5%|▍         | 19/383 [00:01<00:20, 17.43it/s][A  5%|▍         | 19/383 [00:01<00:22, 16.53it/s]  5%|▌         | 21/383 [00:01<00:20, 17.36it/s]  5%|▌         | 21/383 [00:01<00:20, 17.69it/s]
  5%|▌         | 21/383 [00:01<00:20, 17.72it/s][A  5%|▌         | 21/383 [00:01<00:21, 16.82it/s]  6%|▌         | 23/383 [00:01<00:20, 17.23it/s]
  6%|▌         | 23/383 [00:01<00:20, 17.62it/s][A  6%|▌         | 23/383 [00:01<00:20, 17.36it/s]  6%|▌         | 23/383 [00:01<00:21, 17.02it/s]  7%|▋         | 25/383 [00:01<00:21, 16.74it/s]
  7%|▋         | 25/383 [00:01<00:20, 17.39it/s][A  7%|▋         | 25/383 [00:01<00:21, 17.02it/s]  7%|▋         | 25/383 [00:01<00:21, 16.74it/s]
  7%|▋         | 27/383 [00:01<00:20, 17.12it/s][A  7%|▋         | 27/383 [00:01<00:24, 14.62it/s]  7%|▋         | 27/383 [00:01<00:24, 14.69it/s]  7%|▋         | 27/383 [00:01<00:24, 14.43it/s]
  8%|▊         | 29/383 [00:01<00:25, 13.69it/s][A  8%|▊         | 29/383 [00:01<00:27, 12.84it/s]  8%|▊         | 29/383 [00:01<00:27, 12.83it/s]  8%|▊         | 29/383 [00:01<00:28, 12.46it/s]
  8%|▊         | 31/383 [00:01<00:25, 13.60it/s][A  8%|▊         | 31/383 [00:02<00:27, 13.00it/s]  8%|▊         | 31/383 [00:02<00:26, 13.09it/s]  8%|▊         | 31/383 [00:02<00:26, 13.19it/s]
  9%|▊         | 33/383 [00:02<00:25, 13.79it/s][A  9%|▊         | 33/383 [00:02<00:25, 13.69it/s]  9%|▊         | 33/383 [00:02<00:25, 13.80it/s]  9%|▊         | 33/383 [00:02<00:24, 14.06it/s]
  9%|▉         | 35/383 [00:02<00:23, 14.52it/s][A  9%|▉         | 35/383 [00:02<00:24, 14.12it/s]  9%|▉         | 35/383 [00:02<00:24, 14.37it/s]  9%|▉         | 35/383 [00:02<00:23, 14.65it/s]
 10%|▉         | 37/383 [00:02<00:23, 14.83it/s][A 10%|▉         | 37/383 [00:02<00:23, 14.93it/s] 10%|▉         | 37/383 [00:02<00:23, 14.97it/s] 10%|▉         | 37/383 [00:02<00:22, 15.24it/s]
 10%|█         | 39/383 [00:02<00:22, 15.36it/s][A 10%|█         | 39/383 [00:02<00:22, 15.48it/s] 10%|█         | 39/383 [00:02<00:22, 15.56it/s] 10%|█         | 39/383 [00:02<00:21, 15.73it/s]
 11%|█         | 41/383 [00:02<00:21, 15.99it/s][A 11%|█         | 41/383 [00:02<00:21, 15.91it/s] 11%|█         | 41/383 [00:02<00:21, 16.04it/s] 11%|█         | 41/383 [00:02<00:21, 15.88it/s] 11%|█         | 43/383 [00:02<00:20, 16.91it/s]
 11%|█▏        | 44/383 [00:02<00:18, 17.87it/s][A 11%|█▏        | 44/383 [00:02<00:18, 18.35it/s] 11%|█         | 43/383 [00:02<00:20, 16.53it/s] 12%|█▏        | 46/383 [00:02<00:17, 19.54it/s]
 12%|█▏        | 47/383 [00:02<00:16, 20.05it/s][A 12%|█▏        | 47/383 [00:02<00:16, 20.62it/s] 12%|█▏        | 46/383 [00:02<00:17, 19.18it/s] 13%|█▎        | 49/383 [00:02<00:15, 21.45it/s]
 13%|█▎        | 50/383 [00:02<00:15, 21.61it/s][A 13%|█▎        | 49/383 [00:03<00:15, 21.22it/s] 13%|█▎        | 50/383 [00:03<00:16, 20.81it/s]
 14%|█▍        | 53/383 [00:03<00:17, 18.42it/s][A 14%|█▎        | 52/383 [00:03<00:18, 17.43it/s] 14%|█▍        | 53/383 [00:03<00:17, 18.46it/s] 14%|█▎        | 52/383 [00:03<00:18, 18.15it/s]
 14%|█▍        | 55/383 [00:03<00:17, 18.33it/s][A 14%|█▍        | 54/383 [00:03<00:18, 17.76it/s] 14%|█▍        | 55/383 [00:03<00:17, 18.38it/s] 14%|█▍        | 54/383 [00:03<00:18, 17.90it/s]
 15%|█▍        | 57/383 [00:03<00:18, 18.03it/s][A 15%|█▍        | 56/383 [00:03<00:18, 17.99it/s] 15%|█▍        | 57/383 [00:03<00:17, 18.23it/s] 15%|█▍        | 56/383 [00:03<00:18, 17.98it/s] 15%|█▌        | 58/383 [00:03<00:18, 17.39it/s]
 15%|█▌        | 59/383 [00:03<00:18, 17.16it/s][A 15%|█▌        | 59/383 [00:03<00:18, 17.18it/s] 15%|█▌        | 58/383 [00:03<00:18, 17.23it/s] 16%|█▌        | 60/383 [00:03<00:19, 16.83it/s]
 16%|█▌        | 61/383 [00:03<00:19, 16.20it/s][A 16%|█▌        | 61/383 [00:03<00:19, 16.77it/s] 16%|█▌        | 60/383 [00:03<00:19, 16.85it/s] 16%|█▌        | 62/383 [00:03<00:19, 16.09it/s]
 16%|█▋        | 63/383 [00:03<00:19, 16.16it/s][A 16%|█▋        | 63/383 [00:03<00:19, 16.40it/s] 16%|█▌        | 62/383 [00:03<00:19, 16.24it/s]
 17%|█▋        | 65/383 [00:03<00:19, 16.14it/s][A 17%|█▋        | 64/383 [00:03<00:19, 15.98it/s] 17%|█▋        | 64/383 [00:03<00:19, 16.26it/s] 17%|█▋        | 65/383 [00:03<00:19, 15.91it/s] 17%|█▋        | 66/383 [00:04<00:20, 15.73it/s]
 17%|█▋        | 67/383 [00:04<00:20, 15.75it/s][A 17%|█▋        | 66/383 [00:04<00:19, 16.26it/s] 17%|█▋        | 67/383 [00:04<00:20, 15.68it/s] 18%|█▊        | 68/383 [00:04<00:20, 15.66it/s] 18%|█▊        | 68/383 [00:04<00:20, 15.04it/s]
 18%|█▊        | 69/383 [00:04<00:21, 14.95it/s][A 18%|█▊        | 69/383 [00:04<00:20, 14.95it/s]
 19%|█▊        | 71/383 [00:04<00:20, 15.11it/s][A 18%|█▊        | 70/383 [00:04<00:20, 15.06it/s] 18%|█▊        | 70/383 [00:04<00:21, 14.63it/s] 19%|█▊        | 71/383 [00:04<00:20, 15.11it/s]
 19%|█▉        | 73/383 [00:04<00:19, 16.07it/s][A 19%|█▉        | 72/383 [00:04<00:19, 15.73it/s] 19%|█▉        | 72/383 [00:04<00:20, 15.53it/s] 19%|█▉        | 73/383 [00:04<00:19, 15.71it/s]
 20%|█▉        | 75/383 [00:04<00:19, 15.94it/s][A 19%|█▉        | 74/383 [00:04<00:19, 16.23it/s] 20%|█▉        | 75/383 [00:04<00:19, 15.70it/s] 19%|█▉        | 74/383 [00:04<00:20, 15.29it/s]
 20%|██        | 77/383 [00:04<00:19, 15.96it/s][A 20%|█▉        | 76/383 [00:04<00:18, 16.28it/s] 20%|█▉        | 76/383 [00:04<00:19, 15.65it/s] 20%|██        | 77/383 [00:04<00:19, 15.82it/s]
 21%|██        | 79/383 [00:04<00:19, 15.95it/s][A 20%|██        | 78/383 [00:04<00:18, 16.18it/s] 20%|██        | 78/383 [00:04<00:19, 15.82it/s] 21%|██        | 79/383 [00:04<00:19, 15.85it/s] 21%|██        | 80/383 [00:04<00:18, 16.11it/s]
 21%|██        | 81/383 [00:04<00:19, 15.75it/s][A 21%|██        | 80/383 [00:04<00:19, 15.76it/s] 21%|██        | 81/383 [00:05<00:19, 15.60it/s]
 22%|██▏       | 83/383 [00:05<00:18, 15.93it/s][A 21%|██▏       | 82/383 [00:05<00:18, 15.97it/s] 21%|██▏       | 82/383 [00:05<00:18, 15.96it/s] 22%|██▏       | 83/383 [00:05<00:19, 15.75it/s] 22%|██▏       | 84/383 [00:05<00:18, 16.00it/s]
 22%|██▏       | 85/383 [00:05<00:18, 15.73it/s][A 22%|██▏       | 84/383 [00:05<00:18, 16.11it/s] 22%|██▏       | 85/383 [00:05<00:18, 16.06it/s] 22%|██▏       | 86/383 [00:05<00:18, 16.24it/s] 22%|██▏       | 86/383 [00:05<00:18, 16.08it/s]
 23%|██▎       | 87/383 [00:05<00:20, 14.26it/s][A 23%|██▎       | 87/383 [00:05<00:20, 14.18it/s] 23%|██▎       | 88/383 [00:05<00:22, 13.01it/s] 23%|██▎       | 88/383 [00:05<00:23, 12.53it/s]
 23%|██▎       | 89/383 [00:05<00:24, 11.84it/s][A 23%|██▎       | 89/383 [00:05<00:32,  9.02it/s] 23%|██▎       | 90/383 [00:05<00:34,  8.61it/s]
 24%|██▍       | 91/383 [00:06<00:39,  7.40it/s][A 23%|██▎       | 90/383 [00:06<00:42,  6.90it/s] 24%|██▍       | 91/383 [00:06<00:48,  6.02it/s] 24%|██▍       | 92/383 [00:06<00:48,  5.99it/s] 24%|██▍       | 92/383 [00:06<00:47,  6.09it/s] 24%|██▍       | 92/383 [00:06<00:52,  5.56it/s]
 24%|██▍       | 93/383 [00:06<00:50,  5.71it/s][A 25%|██▍       | 94/383 [00:06<00:39,  7.30it/s]
 25%|██▌       | 96/383 [00:06<00:36,  7.92it/s][A 24%|██▍       | 93/383 [00:06<00:55,  5.26it/s] 25%|██▌       | 96/383 [00:06<00:31,  9.18it/s] 24%|██▍       | 93/383 [00:06<00:53,  5.44it/s]
 26%|██▌       | 98/383 [00:06<00:30,  9.46it/s][A 25%|██▍       | 95/383 [00:06<00:41,  6.93it/s] 26%|██▌       | 98/383 [00:06<00:25, 11.06it/s] 25%|██▍       | 95/383 [00:06<00:40,  7.17it/s]
 26%|██▌       | 100/383 [00:07<00:25, 10.89it/s][A 25%|██▌       | 97/383 [00:07<00:32,  8.78it/s] 26%|██▌       | 100/383 [00:07<00:22, 12.40it/s] 25%|██▌       | 97/383 [00:07<00:31,  9.01it/s]
 27%|██▋       | 102/383 [00:07<00:22, 12.24it/s][A 26%|██▌       | 99/383 [00:07<00:27, 10.48it/s] 27%|██▋       | 102/383 [00:07<00:20, 13.77it/s] 26%|██▌       | 99/383 [00:07<00:26, 10.69it/s]
 27%|██▋       | 104/383 [00:07<00:20, 13.36it/s][A 26%|██▋       | 101/383 [00:07<00:23, 11.80it/s] 27%|██▋       | 104/383 [00:07<00:18, 15.03it/s] 26%|██▋       | 101/383 [00:07<00:23, 12.11it/s]
 28%|██▊       | 106/383 [00:07<00:18, 14.75it/s][A 27%|██▋       | 103/383 [00:07<00:21, 13.08it/s] 28%|██▊       | 106/383 [00:07<00:17, 16.15it/s] 27%|██▋       | 103/383 [00:07<00:20, 13.49it/s]
 28%|██▊       | 108/383 [00:07<00:17, 15.91it/s][A 28%|██▊       | 108/383 [00:07<00:16, 16.87it/s] 27%|██▋       | 105/383 [00:07<00:19, 14.34it/s] 27%|██▋       | 105/383 [00:07<00:18, 14.82it/s]
 29%|██▊       | 110/383 [00:07<00:16, 16.79it/s][A 29%|██▊       | 110/383 [00:07<00:15, 17.55it/s] 28%|██▊       | 107/383 [00:07<00:17, 15.46it/s] 28%|██▊       | 107/383 [00:07<00:17, 15.99it/s]
 29%|██▉       | 112/383 [00:07<00:15, 17.41it/s][A 29%|██▉       | 112/383 [00:07<00:15, 18.05it/s] 28%|██▊       | 109/383 [00:07<00:16, 16.41it/s] 28%|██▊       | 109/383 [00:07<00:16, 16.88it/s]
 30%|██▉       | 114/383 [00:07<00:15, 17.92it/s][A 30%|██▉       | 114/383 [00:07<00:14, 18.44it/s] 29%|██▉       | 111/383 [00:07<00:15, 17.15it/s] 29%|██▉       | 111/383 [00:07<00:15, 17.51it/s]
 30%|███       | 116/383 [00:07<00:14, 17.94it/s][A 30%|██▉       | 113/383 [00:07<00:15, 17.85it/s] 30%|███       | 116/383 [00:07<00:15, 17.75it/s] 30%|██▉       | 113/383 [00:07<00:15, 17.90it/s]
 31%|███       | 118/383 [00:08<00:15, 17.52it/s][A 30%|███       | 115/383 [00:08<00:15, 17.80it/s] 30%|███       | 115/383 [00:08<00:15, 17.85it/s] 31%|███       | 118/383 [00:08<00:15, 17.21it/s]
 31%|███▏      | 120/383 [00:08<00:15, 17.07it/s][A 31%|███       | 117/383 [00:08<00:15, 17.00it/s] 31%|███       | 117/383 [00:08<00:15, 17.39it/s] 31%|███▏      | 120/383 [00:08<00:15, 17.17it/s]
 32%|███▏      | 122/383 [00:08<00:15, 16.89it/s][A 31%|███       | 119/383 [00:08<00:15, 16.86it/s] 32%|███▏      | 122/383 [00:08<00:15, 17.39it/s] 31%|███       | 119/383 [00:08<00:15, 16.93it/s]
 32%|███▏      | 124/383 [00:08<00:14, 17.47it/s][A 32%|███▏      | 124/383 [00:08<00:14, 18.08it/s] 32%|███▏      | 121/383 [00:08<00:15, 16.62it/s] 32%|███▏      | 121/383 [00:08<00:15, 16.58it/s]
 33%|███▎      | 126/383 [00:08<00:14, 18.05it/s][A 33%|███▎      | 126/383 [00:08<00:14, 18.35it/s] 32%|███▏      | 123/383 [00:08<00:15, 16.93it/s] 32%|███▏      | 123/383 [00:08<00:14, 17.39it/s]
 33%|███▎      | 128/383 [00:08<00:13, 18.57it/s][A 33%|███▎      | 128/383 [00:08<00:13, 18.70it/s] 33%|███▎      | 125/383 [00:08<00:14, 17.44it/s] 33%|███▎      | 125/383 [00:08<00:14, 17.87it/s]
 34%|███▍      | 130/383 [00:08<00:14, 17.63it/s][A 33%|███▎      | 127/383 [00:08<00:14, 17.87it/s] 34%|███▍      | 130/383 [00:08<00:14, 17.08it/s] 33%|███▎      | 127/383 [00:08<00:13, 18.30it/s]
 34%|███▍      | 132/383 [00:08<00:14, 16.75it/s][A 34%|███▎      | 129/383 [00:08<00:13, 18.21it/s] 34%|███▎      | 129/383 [00:08<00:14, 17.35it/s] 34%|███▍      | 132/383 [00:08<00:15, 16.54it/s]
 35%|███▍      | 134/383 [00:08<00:15, 16.41it/s][A 34%|███▍      | 131/383 [00:08<00:14, 17.61it/s] 34%|███▍      | 131/383 [00:08<00:14, 16.90it/s] 35%|███▍      | 134/383 [00:09<00:15, 16.48it/s]
 36%|███▌      | 136/383 [00:09<00:15, 16.40it/s][A 35%|███▍      | 133/383 [00:09<00:14, 16.89it/s] 35%|███▍      | 133/383 [00:09<00:15, 16.58it/s] 36%|███▌      | 136/383 [00:09<00:15, 16.32it/s]
 36%|███▌      | 138/383 [00:09<00:14, 16.43it/s][A 35%|███▌      | 135/383 [00:09<00:14, 16.67it/s] 36%|███▌      | 138/383 [00:09<00:14, 16.43it/s] 35%|███▌      | 135/383 [00:09<00:15, 16.32it/s]
 37%|███▋      | 140/383 [00:09<00:14, 16.33it/s][A 36%|███▌      | 137/383 [00:09<00:14, 16.48it/s] 37%|███▋      | 140/383 [00:09<00:14, 16.43it/s] 36%|███▌      | 137/383 [00:09<00:15, 16.04it/s]
 37%|███▋      | 142/383 [00:09<00:14, 16.26it/s][A 36%|███▋      | 139/383 [00:09<00:14, 16.57it/s] 37%|███▋      | 142/383 [00:09<00:14, 16.38it/s] 36%|███▋      | 139/383 [00:09<00:15, 15.99it/s]
 38%|███▊      | 144/383 [00:09<00:14, 16.21it/s][A 37%|███▋      | 141/383 [00:09<00:14, 16.18it/s] 38%|███▊      | 144/383 [00:09<00:14, 16.44it/s] 37%|███▋      | 141/383 [00:09<00:14, 16.17it/s]
 38%|███▊      | 146/383 [00:09<00:14, 16.13it/s][A 37%|███▋      | 143/383 [00:09<00:14, 16.22it/s] 38%|███▊      | 146/383 [00:09<00:14, 16.40it/s] 37%|███▋      | 143/383 [00:09<00:14, 16.24it/s]
 39%|███▊      | 148/383 [00:09<00:14, 16.07it/s][A 38%|███▊      | 145/383 [00:09<00:14, 16.18it/s] 38%|███▊      | 145/383 [00:09<00:14, 16.08it/s] 39%|███▊      | 148/383 [00:09<00:15, 14.92it/s] 38%|███▊      | 147/383 [00:09<00:14, 16.19it/s] 38%|███▊      | 147/383 [00:09<00:14, 16.19it/s]
 39%|███▉      | 150/383 [00:10<00:17, 13.14it/s][A 39%|███▉      | 150/383 [00:10<00:18, 12.59it/s] 39%|███▉      | 149/383 [00:10<00:17, 13.41it/s] 39%|███▉      | 149/383 [00:10<00:17, 13.36it/s]
 40%|███▉      | 152/383 [00:10<00:19, 12.01it/s][A 40%|███▉      | 152/383 [00:10<00:19, 11.75it/s] 39%|███▉      | 151/383 [00:10<00:19, 12.02it/s] 39%|███▉      | 151/383 [00:10<00:19, 11.84it/s]
 40%|████      | 154/383 [00:10<00:24,  9.16it/s][A 40%|███▉      | 153/383 [00:10<00:19, 11.51it/s] 40%|███▉      | 153/383 [00:10<00:21, 10.89it/s] 40%|████      | 154/383 [00:10<00:26,  8.60it/s] 40%|████      | 155/383 [00:11<00:30,  7.41it/s]
 41%|████      | 156/383 [00:11<00:35,  6.34it/s][A 40%|████      | 155/383 [00:11<00:32,  7.11it/s] 41%|████      | 156/383 [00:11<00:36,  6.24it/s] 41%|████      | 156/383 [00:11<00:36,  6.30it/s] 41%|████      | 156/383 [00:11<00:36,  6.18it/s]
 41%|████      | 157/383 [00:11<00:41,  5.45it/s][A 41%|████      | 157/383 [00:11<00:39,  5.71it/s] 41%|████      | 157/383 [00:11<00:39,  5.73it/s] 41%|████      | 157/383 [00:11<00:41,  5.44it/s] 41%|████▏     | 158/383 [00:11<00:43,  5.19it/s]
 41%|████▏     | 158/383 [00:11<00:46,  4.80it/s][A 41%|████▏     | 158/383 [00:11<00:43,  5.11it/s] 42%|████▏     | 159/383 [00:11<00:42,  5.28it/s] 41%|████▏     | 158/383 [00:11<00:45,  4.98it/s]
 42%|████▏     | 159/383 [00:11<00:49,  4.54it/s][A 42%|████▏     | 159/383 [00:12<00:42,  5.24it/s] 42%|████▏     | 160/383 [00:12<00:45,  4.89it/s] 42%|████▏     | 159/383 [00:12<00:48,  4.61it/s]
 42%|████▏     | 160/383 [00:12<00:51,  4.31it/s][A 42%|████▏     | 160/383 [00:12<00:45,  4.95it/s] 42%|████▏     | 161/383 [00:12<00:44,  5.00it/s] 42%|████▏     | 160/383 [00:12<00:47,  4.72it/s]
 42%|████▏     | 161/383 [00:12<00:48,  4.60it/s][A 42%|████▏     | 161/383 [00:12<00:46,  4.79it/s] 42%|████▏     | 162/383 [00:12<00:42,  5.21it/s] 42%|████▏     | 161/383 [00:12<00:49,  4.51it/s]
 42%|████▏     | 162/383 [00:12<00:49,  4.49it/s][A 42%|████▏     | 162/383 [00:12<00:47,  4.62it/s] 43%|████▎     | 163/383 [00:12<00:46,  4.73it/s] 42%|████▏     | 162/383 [00:12<00:46,  4.71it/s]
 43%|████▎     | 163/383 [00:12<00:46,  4.70it/s][A 43%|████▎     | 163/383 [00:12<00:47,  4.67it/s] 43%|████▎     | 163/383 [00:13<00:46,  4.78it/s] 43%|████▎     | 164/383 [00:13<00:50,  4.38it/s]
 43%|████▎     | 164/383 [00:13<00:50,  4.38it/s][A 43%|████▎     | 164/383 [00:13<00:49,  4.41it/s] 43%|████▎     | 165/383 [00:13<00:46,  4.68it/s] 43%|████▎     | 164/383 [00:13<00:48,  4.50it/s] 44%|████▍     | 168/383 [00:13<00:25,  8.53it/s]
 43%|████▎     | 165/383 [00:13<00:48,  4.52it/s][A 43%|████▎     | 165/383 [00:13<00:47,  4.57it/s]
 44%|████▍     | 168/383 [00:13<00:25,  8.29it/s][A 45%|████▍     | 171/383 [00:13<00:17, 11.83it/s] 44%|████▍     | 168/383 [00:13<00:25,  8.40it/s] 45%|████▌     | 173/383 [00:13<00:15, 13.42it/s] 43%|████▎     | 165/383 [00:13<00:51,  4.22it/s]
 45%|████▍     | 171/383 [00:13<00:17, 11.83it/s][A 45%|████▍     | 171/383 [00:13<00:17, 11.94it/s] 44%|████▍     | 168/383 [00:13<00:27,  7.88it/s]
 45%|████▌     | 174/383 [00:13<00:14, 14.53it/s][A 46%|████▌     | 175/383 [00:13<00:15, 13.40it/s] 45%|████▌     | 174/383 [00:13<00:14, 14.69it/s] 45%|████▍     | 171/383 [00:13<00:18, 11.32it/s]
 46%|████▌     | 176/383 [00:13<00:14, 14.23it/s][A 46%|████▌     | 177/383 [00:13<00:15, 13.19it/s] 46%|████▌     | 176/383 [00:13<00:14, 14.34it/s] 45%|████▌     | 174/383 [00:13<00:15, 13.63it/s] 47%|████▋     | 179/383 [00:13<00:14, 14.54it/s]
 46%|████▋     | 178/383 [00:13<00:14, 14.57it/s][A 46%|████▋     | 178/383 [00:14<00:13, 14.68it/s]
 47%|████▋     | 180/383 [00:14<00:13, 15.58it/s][A 47%|████▋     | 181/383 [00:14<00:13, 15.35it/s] 46%|████▌     | 176/383 [00:14<00:15, 13.53it/s] 47%|████▋     | 180/383 [00:14<00:12, 15.72it/s] 48%|████▊     | 183/383 [00:14<00:12, 16.20it/s]
 48%|████▊     | 182/383 [00:14<00:12, 16.07it/s][A 46%|████▋     | 178/383 [00:14<00:14, 13.99it/s] 48%|████▊     | 182/383 [00:14<00:12, 16.33it/s]
 48%|████▊     | 184/383 [00:14<00:11, 16.70it/s][A 48%|████▊     | 185/383 [00:14<00:12, 15.98it/s] 47%|████▋     | 180/383 [00:14<00:13, 15.09it/s] 48%|████▊     | 184/383 [00:14<00:11, 16.86it/s]
 49%|████▊     | 186/383 [00:14<00:12, 16.05it/s][A 48%|████▊     | 182/383 [00:14<00:12, 15.77it/s] 49%|████▉     | 187/383 [00:14<00:12, 15.15it/s] 49%|████▊     | 186/383 [00:14<00:12, 15.58it/s]
 49%|████▉     | 188/383 [00:14<00:11, 16.59it/s][A 48%|████▊     | 184/383 [00:14<00:12, 16.57it/s] 50%|████▉     | 190/383 [00:14<00:10, 17.59it/s] 49%|████▉     | 188/383 [00:14<00:12, 16.12it/s]
 50%|████▉     | 191/383 [00:14<00:10, 18.59it/s][A 49%|████▊     | 186/383 [00:14<00:12, 15.42it/s] 50%|█████     | 192/383 [00:14<00:10, 17.92it/s] 50%|████▉     | 191/383 [00:14<00:10, 18.33it/s]
 50%|█████     | 193/383 [00:14<00:10, 18.50it/s][A 51%|█████     | 194/383 [00:14<00:10, 18.13it/s] 49%|████▉     | 188/383 [00:14<00:12, 15.39it/s] 50%|█████     | 193/383 [00:14<00:10, 18.48it/s]
 51%|█████     | 195/383 [00:14<00:10, 18.43it/s][A 51%|█████     | 196/383 [00:14<00:10, 18.44it/s] 50%|████▉     | 191/383 [00:14<00:10, 17.80it/s] 51%|█████     | 195/383 [00:14<00:10, 18.54it/s]
 51%|█████▏    | 197/383 [00:15<00:09, 18.68it/s][A 52%|█████▏    | 199/383 [00:15<00:09, 20.04it/s] 51%|█████▏    | 197/383 [00:15<00:09, 18.85it/s] 50%|█████     | 193/383 [00:15<00:10, 18.11it/s]
 52%|█████▏    | 199/383 [00:15<00:09, 19.03it/s][A 53%|█████▎    | 202/383 [00:15<00:08, 21.83it/s] 52%|█████▏    | 199/383 [00:15<00:09, 19.11it/s] 51%|█████     | 195/383 [00:15<00:10, 18.33it/s]
 53%|█████▎    | 202/383 [00:15<00:08, 21.12it/s][A 54%|█████▎    | 205/383 [00:15<00:07, 23.05it/s] 51%|█████▏    | 197/383 [00:15<00:10, 18.49it/s] 53%|█████▎    | 202/383 [00:15<00:08, 21.21it/s]
 54%|█████▎    | 205/383 [00:15<00:07, 22.76it/s][A 54%|█████▍    | 208/383 [00:15<00:07, 23.89it/s] 54%|█████▎    | 205/383 [00:15<00:07, 22.51it/s] 52%|█████▏    | 200/383 [00:15<00:09, 19.63it/s]
 54%|█████▍    | 208/383 [00:15<00:07, 23.59it/s][A 55%|█████▌    | 211/383 [00:15<00:07, 24.54it/s] 54%|█████▍    | 208/383 [00:15<00:07, 23.48it/s] 53%|█████▎    | 203/383 [00:15<00:08, 21.43it/s]
 55%|█████▌    | 211/383 [00:15<00:07, 24.27it/s][A 56%|█████▌    | 214/383 [00:15<00:06, 24.32it/s] 55%|█████▌    | 211/383 [00:15<00:07, 24.23it/s] 54%|█████▍    | 206/383 [00:15<00:07, 22.78it/s]
 56%|█████▌    | 214/383 [00:15<00:06, 24.71it/s][A 57%|█████▋    | 217/383 [00:15<00:06, 24.81it/s] 56%|█████▌    | 214/383 [00:15<00:06, 24.66it/s] 55%|█████▍    | 209/383 [00:15<00:07, 23.58it/s]
 57%|█████▋    | 217/383 [00:15<00:06, 24.29it/s][A 57%|█████▋    | 220/383 [00:15<00:06, 25.00it/s] 57%|█████▋    | 217/383 [00:15<00:06, 24.86it/s] 55%|█████▌    | 212/383 [00:15<00:07, 24.09it/s]
 57%|█████▋    | 220/383 [00:15<00:06, 24.80it/s][A 57%|█████▋    | 220/383 [00:15<00:06, 25.15it/s] 56%|█████▌    | 215/383 [00:16<00:06, 24.58it/s] 58%|█████▊    | 223/383 [00:16<00:07, 22.17it/s]
 58%|█████▊    | 223/383 [00:16<00:07, 22.76it/s][A 57%|█████▋    | 218/383 [00:16<00:06, 24.94it/s] 58%|█████▊    | 223/383 [00:16<00:07, 22.16it/s] 59%|█████▉    | 226/383 [00:16<00:07, 20.43it/s] 58%|█████▊    | 221/383 [00:16<00:06, 23.78it/s]
 59%|█████▉    | 226/383 [00:16<00:07, 20.63it/s][A 59%|█████▉    | 226/383 [00:16<00:07, 20.15it/s] 60%|█████▉    | 229/383 [00:16<00:07, 19.25it/s] 58%|█████▊    | 224/383 [00:16<00:07, 20.90it/s]
 60%|█████▉    | 229/383 [00:16<00:08, 19.07it/s][A 60%|█████▉    | 229/383 [00:16<00:08, 18.93it/s] 60%|██████    | 231/383 [00:16<00:08, 17.31it/s]
 60%|██████    | 231/383 [00:16<00:08, 17.82it/s][A 59%|█████▉    | 227/383 [00:16<00:08, 19.23it/s] 60%|██████    | 231/383 [00:16<00:08, 18.03it/s] 61%|██████    | 233/383 [00:16<00:09, 15.97it/s]
 61%|██████    | 233/383 [00:16<00:09, 16.33it/s][A 60%|██████    | 230/383 [00:16<00:08, 18.43it/s] 61%|██████    | 233/383 [00:16<00:09, 16.27it/s] 61%|██████▏   | 235/383 [00:16<00:09, 15.02it/s]
 61%|██████▏   | 235/383 [00:16<00:09, 15.29it/s][A 61%|██████    | 232/383 [00:16<00:08, 16.79it/s] 61%|██████▏   | 235/383 [00:16<00:09, 14.82it/s] 62%|██████▏   | 237/383 [00:17<00:10, 14.22it/s]
 62%|██████▏   | 237/383 [00:17<00:10, 14.44it/s][A 61%|██████    | 234/383 [00:17<00:09, 15.77it/s] 62%|██████▏   | 237/383 [00:17<00:10, 14.22it/s] 62%|██████▏   | 239/383 [00:17<00:10, 13.69it/s]
 62%|██████▏   | 239/383 [00:17<00:10, 13.95it/s][A 62%|██████▏   | 236/383 [00:17<00:09, 14.99it/s] 62%|██████▏   | 239/383 [00:17<00:10, 13.81it/s] 63%|██████▎   | 241/383 [00:17<00:10, 13.31it/s]
 63%|██████▎   | 241/383 [00:17<00:10, 13.61it/s][A 62%|██████▏   | 238/383 [00:17<00:10, 14.38it/s] 63%|██████▎   | 241/383 [00:17<00:10, 13.34it/s] 63%|██████▎   | 243/383 [00:17<00:10, 13.18it/s]
 63%|██████▎   | 243/383 [00:17<00:10, 13.26it/s][A 63%|██████▎   | 240/383 [00:17<00:10, 14.06it/s] 63%|██████▎   | 243/383 [00:17<00:10, 13.20it/s] 64%|██████▍   | 245/383 [00:17<00:10, 12.96it/s]
 64%|██████▍   | 245/383 [00:17<00:10, 13.25it/s][A 63%|██████▎   | 242/383 [00:17<00:10, 13.70it/s] 64%|██████▍   | 245/383 [00:17<00:10, 13.06it/s] 64%|██████▍   | 247/383 [00:17<00:10, 13.08it/s]
 64%|██████▍   | 247/383 [00:17<00:10, 13.26it/s][A 64%|██████▎   | 244/383 [00:17<00:10, 13.61it/s] 64%|██████▍   | 247/383 [00:17<00:10, 12.86it/s] 65%|██████▌   | 249/383 [00:17<00:10, 13.01it/s]
 65%|██████▌   | 249/383 [00:18<00:10, 12.82it/s][A 64%|██████▍   | 246/383 [00:18<00:10, 13.29it/s] 65%|██████▌   | 249/383 [00:18<00:10, 12.87it/s] 66%|██████▌   | 251/383 [00:18<00:10, 12.64it/s]
 66%|██████▌   | 251/383 [00:18<00:10, 12.96it/s][A 65%|██████▍   | 248/383 [00:18<00:10, 13.15it/s] 66%|██████▌   | 251/383 [00:18<00:10, 12.96it/s] 66%|██████▌   | 253/383 [00:18<00:10, 12.69it/s]
 66%|██████▌   | 253/383 [00:18<00:10, 12.94it/s][A 65%|██████▌   | 250/383 [00:18<00:10, 13.04it/s] 66%|██████▌   | 253/383 [00:18<00:10, 12.67it/s] 67%|██████▋   | 255/383 [00:18<00:09, 12.91it/s]
 67%|██████▋   | 255/383 [00:18<00:09, 13.04it/s][A 66%|██████▌   | 252/383 [00:18<00:10, 12.88it/s] 67%|██████▋   | 255/383 [00:18<00:10, 12.64it/s] 67%|██████▋   | 257/383 [00:18<00:09, 13.10it/s]
 67%|██████▋   | 257/383 [00:18<00:09, 13.14it/s][A 66%|██████▋   | 254/383 [00:18<00:10, 12.77it/s] 67%|██████▋   | 257/383 [00:18<00:09, 12.94it/s] 68%|██████▊   | 259/383 [00:18<00:09, 13.36it/s]
 68%|██████▊   | 259/383 [00:18<00:09, 13.26it/s][A 67%|██████▋   | 256/383 [00:18<00:09, 12.88it/s] 68%|██████▊   | 259/383 [00:18<00:09, 13.06it/s] 68%|██████▊   | 261/383 [00:18<00:09, 13.23it/s]
 68%|██████▊   | 261/383 [00:18<00:09, 13.46it/s][A 67%|██████▋   | 258/383 [00:18<00:09, 12.98it/s] 68%|██████▊   | 261/383 [00:19<00:09, 13.09it/s] 69%|██████▊   | 263/383 [00:19<00:08, 13.43it/s]
 69%|██████▊   | 263/383 [00:19<00:08, 13.46it/s][A 68%|██████▊   | 260/383 [00:19<00:09, 13.15it/s] 69%|██████▉   | 266/383 [00:19<00:07, 16.41it/s] 69%|██████▊   | 263/383 [00:19<00:09, 13.17it/s]
 69%|██████▉   | 266/383 [00:19<00:07, 16.05it/s][A 70%|███████   | 269/383 [00:19<00:06, 18.77it/s] 68%|██████▊   | 262/383 [00:19<00:09, 13.29it/s]
 70%|███████   | 269/383 [00:19<00:06, 18.55it/s][A 69%|██████▉   | 266/383 [00:19<00:07, 15.69it/s] 69%|██████▉   | 264/383 [00:19<00:08, 14.40it/s] 71%|███████   | 272/383 [00:19<00:05, 19.01it/s] 70%|███████   | 269/383 [00:19<00:06, 18.04it/s]
 71%|███████   | 272/383 [00:19<00:05, 19.77it/s][A 70%|██████▉   | 267/383 [00:19<00:07, 16.47it/s] 72%|███████▏  | 274/383 [00:19<00:06, 17.69it/s] 71%|███████   | 272/383 [00:19<00:05, 19.09it/s]
 72%|███████▏  | 275/383 [00:19<00:05, 18.51it/s][A 70%|███████   | 270/383 [00:19<00:05, 18.85it/s] 72%|███████▏  | 274/383 [00:19<00:06, 17.84it/s] 72%|███████▏  | 276/383 [00:19<00:06, 16.51it/s]
 72%|███████▏  | 277/383 [00:19<00:05, 17.72it/s][A 71%|███████▏  | 273/383 [00:19<00:05, 19.43it/s] 72%|███████▏  | 276/383 [00:19<00:06, 17.41it/s] 73%|███████▎  | 278/383 [00:19<00:06, 16.33it/s]
 73%|███████▎  | 279/383 [00:19<00:06, 16.86it/s][A 72%|███████▏  | 275/383 [00:19<00:05, 18.48it/s] 73%|███████▎  | 280/383 [00:19<00:06, 16.17it/s] 73%|███████▎  | 278/383 [00:19<00:06, 16.55it/s]
 73%|███████▎  | 281/383 [00:20<00:06, 16.18it/s][A 72%|███████▏  | 277/383 [00:20<00:05, 17.76it/s] 73%|███████▎  | 280/383 [00:20<00:06, 16.27it/s] 74%|███████▎  | 282/383 [00:20<00:07, 13.86it/s] 73%|███████▎  | 279/383 [00:20<00:06, 16.44it/s]
 74%|███████▍  | 283/383 [00:20<00:07, 14.27it/s][A 74%|███████▎  | 282/383 [00:20<00:07, 14.12it/s] 74%|███████▍  | 284/383 [00:20<00:07, 12.93it/s] 73%|███████▎  | 281/383 [00:20<00:06, 14.69it/s]
 74%|███████▍  | 285/383 [00:20<00:07, 12.90it/s][A 74%|███████▍  | 284/383 [00:20<00:07, 12.77it/s] 75%|███████▍  | 286/383 [00:20<00:07, 12.21it/s] 74%|███████▍  | 283/383 [00:20<00:07, 13.06it/s]
 75%|███████▍  | 287/383 [00:20<00:07, 12.08it/s][A 75%|███████▍  | 286/383 [00:20<00:08, 12.12it/s] 75%|███████▌  | 288/383 [00:20<00:07, 11.90it/s] 74%|███████▍  | 285/383 [00:20<00:07, 12.68it/s] 75%|███████▌  | 288/383 [00:20<00:08, 11.38it/s] 75%|███████▍  | 287/383 [00:20<00:07, 12.34it/s]
 75%|███████▌  | 289/383 [00:20<00:10,  8.77it/s][A 76%|███████▌  | 290/383 [00:21<00:12,  7.74it/s] 75%|███████▌  | 289/383 [00:21<00:10,  9.27it/s] 76%|███████▌  | 290/383 [00:21<00:12,  7.31it/s] 76%|███████▌  | 291/383 [00:21<00:13,  6.62it/s]
 76%|███████▌  | 291/383 [00:21<00:14,  6.29it/s][A 76%|███████▌  | 291/383 [00:21<00:14,  6.22it/s] 76%|███████▌  | 292/383 [00:21<00:15,  5.84it/s]
 76%|███████▌  | 292/383 [00:21<00:16,  5.59it/s][A 76%|███████▌  | 291/383 [00:21<00:14,  6.39it/s] 76%|███████▌  | 292/383 [00:21<00:17,  5.33it/s] 77%|███████▋  | 293/383 [00:21<00:17,  5.12it/s] 76%|███████▌  | 292/383 [00:22<00:16,  5.62it/s]
 77%|███████▋  | 293/383 [00:22<00:18,  4.92it/s][A 77%|███████▋  | 293/383 [00:22<00:18,  4.91it/s] 77%|███████▋  | 294/383 [00:22<00:18,  4.79it/s] 77%|███████▋  | 293/383 [00:22<00:17,  5.10it/s]
 77%|███████▋  | 294/383 [00:22<00:19,  4.60it/s][A 77%|███████▋  | 295/383 [00:22<00:18,  4.71it/s] 77%|███████▋  | 294/383 [00:22<00:19,  4.49it/s]
 77%|███████▋  | 295/383 [00:22<00:19,  4.42it/s][A 77%|███████▋  | 294/383 [00:22<00:18,  4.73it/s] 77%|███████▋  | 296/383 [00:22<00:20,  4.27it/s] 77%|███████▋  | 295/383 [00:22<00:20,  4.20it/s] 77%|███████▋  | 295/383 [00:22<00:19,  4.54it/s]
 77%|███████▋  | 296/383 [00:22<00:20,  4.18it/s][A 78%|███████▊  | 297/383 [00:22<00:19,  4.31it/s] 77%|███████▋  | 296/383 [00:23<00:21,  4.08it/s] 77%|███████▋  | 296/383 [00:23<00:20,  4.26it/s]
 78%|███████▊  | 297/383 [00:23<00:21,  3.93it/s][A 78%|███████▊  | 298/383 [00:23<00:20,  4.10it/s] 78%|███████▊  | 297/383 [00:23<00:21,  4.08it/s] 78%|███████▊  | 297/383 [00:23<00:21,  4.06it/s]
 78%|███████▊  | 298/383 [00:23<00:21,  4.03it/s][A 78%|███████▊  | 298/383 [00:23<00:19,  4.28it/s] 78%|███████▊  | 299/383 [00:23<00:21,  3.95it/s]
 78%|███████▊  | 299/383 [00:23<00:20,  4.00it/s][A 78%|███████▊  | 298/383 [00:23<00:22,  3.80it/s] 78%|███████▊  | 300/383 [00:23<00:20,  4.06it/s] 78%|███████▊  | 299/383 [00:23<00:20,  4.06it/s]
 78%|███████▊  | 300/383 [00:23<00:21,  3.85it/s][A 78%|███████▊  | 299/383 [00:23<00:22,  3.78it/s] 79%|███████▊  | 301/383 [00:24<00:20,  3.92it/s] 78%|███████▊  | 300/383 [00:24<00:21,  3.89it/s]
 79%|███████▊  | 301/383 [00:24<00:21,  3.78it/s][A 78%|███████▊  | 300/383 [00:24<00:22,  3.71it/s] 79%|███████▊  | 301/383 [00:24<00:20,  4.03it/s] 79%|███████▉  | 302/383 [00:24<00:21,  3.85it/s]
 79%|███████▉  | 302/383 [00:24<00:21,  3.74it/s][A 79%|███████▊  | 301/383 [00:24<00:22,  3.65it/s] 79%|███████▉  | 303/383 [00:24<00:21,  3.80it/s] 79%|███████▉  | 302/383 [00:24<00:21,  3.81it/s]
 79%|███████▉  | 303/383 [00:24<00:20,  3.93it/s][A 79%|███████▉  | 303/383 [00:24<00:20,  3.97it/s] 79%|███████▉  | 302/383 [00:24<00:22,  3.62it/s] 79%|███████▉  | 304/383 [00:24<00:21,  3.68it/s]
 79%|███████▉  | 304/383 [00:25<00:21,  3.76it/s][A 79%|███████▉  | 304/383 [00:25<00:19,  4.00it/s] 80%|███████▉  | 305/383 [00:25<00:20,  3.83it/s] 79%|███████▉  | 303/383 [00:25<00:22,  3.63it/s] 80%|███████▉  | 306/383 [00:25<00:18,  4.09it/s]
 80%|███████▉  | 305/383 [00:25<00:21,  3.63it/s][A 80%|███████▉  | 305/383 [00:25<00:20,  3.85it/s] 79%|███████▉  | 304/383 [00:25<00:21,  3.62it/s] 80%|████████  | 307/383 [00:25<00:19,  3.96it/s]
 80%|███████▉  | 306/383 [00:25<00:20,  3.67it/s][A 80%|███████▉  | 306/383 [00:25<00:20,  3.81it/s] 80%|███████▉  | 305/383 [00:25<00:21,  3.63it/s] 80%|████████  | 308/383 [00:25<00:19,  3.81it/s]
 80%|████████  | 307/383 [00:25<00:20,  3.65it/s][A 80%|████████  | 307/383 [00:25<00:20,  3.73it/s] 80%|███████▉  | 306/383 [00:25<00:21,  3.62it/s] 81%|████████  | 309/383 [00:26<00:19,  3.85it/s] 80%|████████  | 308/383 [00:26<00:19,  3.87it/s]
 80%|████████  | 308/383 [00:26<00:20,  3.67it/s][A 80%|████████  | 307/383 [00:26<00:21,  3.59it/s]
 81%|████████  | 309/383 [00:26<00:19,  3.76it/s][A 81%|████████  | 309/383 [00:26<00:19,  3.79it/s] 81%|████████  | 310/383 [00:26<00:19,  3.70it/s] 80%|████████  | 308/383 [00:26<00:20,  3.57it/s]
 81%|████████  | 310/383 [00:26<00:18,  3.86it/s][A 81%|████████  | 310/383 [00:26<00:19,  3.71it/s] 81%|████████  | 311/383 [00:26<00:19,  3.64it/s] 81%|████████  | 309/383 [00:26<00:19,  3.80it/s]
 81%|████████  | 311/383 [00:26<00:18,  3.83it/s][A 81%|████████  | 311/383 [00:26<00:19,  3.69it/s] 81%|████████▏ | 312/383 [00:26<00:19,  3.63it/s] 81%|████████  | 310/383 [00:26<00:18,  3.90it/s]
 81%|████████▏ | 312/383 [00:27<00:18,  3.86it/s][A 81%|████████▏ | 312/383 [00:27<00:19,  3.64it/s] 82%|████████▏ | 313/383 [00:27<00:19,  3.64it/s] 81%|████████  | 311/383 [00:27<00:18,  3.81it/s]
 82%|████████▏ | 313/383 [00:27<00:19,  3.66it/s][A 82%|████████▏ | 313/383 [00:27<00:18,  3.69it/s] 81%|████████▏ | 312/383 [00:27<00:18,  3.88it/s] 82%|████████▏ | 314/383 [00:27<00:19,  3.61it/s]
 82%|████████▏ | 314/383 [00:27<00:18,  3.68it/s][A 82%|████████▏ | 314/383 [00:27<00:18,  3.79it/s] 82%|████████▏ | 313/383 [00:27<00:17,  3.91it/s] 82%|████████▏ | 315/383 [00:27<00:19,  3.51it/s] 82%|████████▏ | 315/383 [00:27<00:17,  3.84it/s] 82%|████████▏ | 314/383 [00:27<00:17,  3.96it/s]
 82%|████████▏ | 315/383 [00:27<00:18,  3.67it/s][A 83%|████████▎ | 316/383 [00:28<00:18,  3.64it/s] 82%|████████▏ | 315/383 [00:28<00:16,  4.09it/s]
 83%|████████▎ | 316/383 [00:28<00:18,  3.72it/s][A 83%|████████▎ | 316/383 [00:28<00:17,  3.81it/s] 83%|████████▎ | 317/383 [00:28<00:18,  3.64it/s]
 83%|████████▎ | 317/383 [00:28<00:17,  3.71it/s][A 83%|████████▎ | 316/383 [00:28<00:17,  3.80it/s] 83%|████████▎ | 317/383 [00:28<00:18,  3.63it/s] 83%|████████▎ | 318/383 [00:28<00:18,  3.51it/s] 83%|████████▎ | 317/383 [00:28<00:17,  3.75it/s]
 83%|████████▎ | 318/383 [00:28<00:18,  3.56it/s][A 83%|████████▎ | 318/383 [00:28<00:17,  3.61it/s] 83%|████████▎ | 319/383 [00:28<00:17,  3.70it/s] 83%|████████▎ | 318/383 [00:29<00:17,  3.71it/s]
 83%|████████▎ | 319/383 [00:29<00:17,  3.68it/s][A 83%|████████▎ | 319/383 [00:29<00:17,  3.67it/s] 84%|████████▎ | 320/383 [00:29<00:17,  3.69it/s]
 84%|████████▎ | 320/383 [00:29<00:16,  3.76it/s][A 83%|████████▎ | 319/383 [00:29<00:17,  3.71it/s] 84%|████████▎ | 320/383 [00:29<00:17,  3.69it/s] 84%|████████▍ | 321/383 [00:29<00:17,  3.60it/s]
 84%|████████▍ | 321/383 [00:29<00:16,  3.78it/s][A 84%|████████▎ | 320/383 [00:29<00:17,  3.68it/s] 84%|████████▍ | 321/383 [00:29<00:16,  3.73it/s] 84%|████████▍ | 322/383 [00:29<00:16,  3.63it/s]
 84%|████████▍ | 322/383 [00:29<00:15,  3.85it/s][A 84%|████████▍ | 321/383 [00:29<00:17,  3.59it/s] 84%|████████▍ | 322/383 [00:29<00:16,  3.59it/s] 84%|████████▍ | 323/383 [00:29<00:16,  3.75it/s]
 84%|████████▍ | 323/383 [00:30<00:15,  3.98it/s][A 84%|████████▍ | 322/383 [00:30<00:16,  3.78it/s] 84%|████████▍ | 323/383 [00:30<00:16,  3.73it/s] 85%|████████▍ | 324/383 [00:30<00:15,  3.71it/s]
 85%|████████▍ | 324/383 [00:30<00:15,  3.77it/s][A 84%|████████▍ | 323/383 [00:30<00:16,  3.72it/s] 85%|████████▍ | 324/383 [00:30<00:15,  3.72it/s] 85%|████████▍ | 325/383 [00:30<00:15,  3.80it/s]
 85%|████████▍ | 325/383 [00:30<00:15,  3.85it/s][A 85%|████████▍ | 325/383 [00:30<00:15,  3.82it/s] 85%|████████▍ | 324/383 [00:30<00:16,  3.68it/s] 85%|████████▌ | 326/383 [00:30<00:14,  3.96it/s]
 85%|████████▌ | 326/383 [00:30<00:15,  3.75it/s][A 85%|████████▌ | 326/383 [00:30<00:14,  3.85it/s] 85%|████████▍ | 325/383 [00:30<00:15,  3.67it/s] 85%|████████▌ | 327/383 [00:30<00:14,  3.89it/s]
 85%|████████▌ | 327/383 [00:31<00:14,  3.74it/s][A 85%|████████▌ | 326/383 [00:31<00:14,  3.82it/s] 85%|████████▌ | 327/383 [00:31<00:15,  3.67it/s] 86%|████████▌ | 328/383 [00:31<00:14,  3.79it/s]
 86%|████████▌ | 328/383 [00:31<00:14,  3.68it/s][A 85%|████████▌ | 327/383 [00:31<00:14,  3.89it/s] 86%|████████▌ | 328/383 [00:31<00:15,  3.64it/s] 86%|████████▌ | 329/383 [00:31<00:14,  3.80it/s]
 86%|████████▌ | 329/383 [00:31<00:14,  3.74it/s][A 86%|████████▌ | 328/383 [00:31<00:14,  3.88it/s] 86%|████████▌ | 329/383 [00:31<00:14,  3.76it/s] 86%|████████▌ | 330/383 [00:31<00:14,  3.74it/s] 86%|████████▋ | 331/383 [00:31<00:12,  4.25it/s] 86%|████████▌ | 329/383 [00:31<00:13,  3.89it/s]
 86%|████████▌ | 330/383 [00:31<00:14,  3.75it/s][A 86%|████████▌ | 330/383 [00:32<00:14,  3.64it/s] 87%|████████▋ | 332/383 [00:32<00:10,  4.84it/s] 86%|████████▌ | 330/383 [00:32<00:12,  4.13it/s] 86%|████████▋ | 331/383 [00:32<00:12,  4.27it/s]
 86%|████████▋ | 331/383 [00:32<00:13,  3.73it/s][A 87%|████████▋ | 333/383 [00:32<00:09,  5.34it/s] 87%|████████▋ | 332/383 [00:32<00:11,  4.60it/s] 87%|████████▋ | 334/383 [00:32<00:08,  5.62it/s]
 87%|████████▋ | 332/383 [00:32<00:12,  4.22it/s][A 86%|████████▋ | 331/383 [00:32<00:13,  3.99it/s] 87%|████████▋ | 333/383 [00:32<00:10,  4.85it/s]
 87%|████████▋ | 333/383 [00:32<00:10,  4.70it/s][A 87%|████████▋ | 335/383 [00:32<00:08,  5.81it/s] 87%|████████▋ | 332/383 [00:32<00:11,  4.37it/s] 88%|████████▊ | 336/383 [00:32<00:07,  5.94it/s]
 87%|████████▋ | 334/383 [00:32<00:09,  5.06it/s][A 87%|████████▋ | 334/383 [00:32<00:09,  5.09it/s] 87%|████████▋ | 333/383 [00:32<00:10,  4.96it/s] 87%|████████▋ | 335/383 [00:32<00:08,  5.63it/s] 88%|████████▊ | 337/383 [00:32<00:07,  6.11it/s]
 87%|████████▋ | 335/383 [00:32<00:09,  5.24it/s][A 87%|████████▋ | 334/383 [00:32<00:09,  5.43it/s] 88%|████████▊ | 336/383 [00:33<00:08,  5.73it/s] 88%|████████▊ | 338/383 [00:33<00:07,  5.92it/s] 87%|████████▋ | 335/383 [00:33<00:08,  5.49it/s]
 88%|████████▊ | 336/383 [00:33<00:08,  5.25it/s][A 88%|████████▊ | 337/383 [00:33<00:07,  5.89it/s] 88%|████████▊ | 336/383 [00:33<00:07,  5.89it/s] 89%|████████▉ | 340/383 [00:33<00:05,  7.69it/s]
 88%|████████▊ | 337/383 [00:33<00:08,  5.39it/s][A 88%|████████▊ | 338/383 [00:33<00:07,  6.01it/s] 89%|████████▉ | 342/383 [00:33<00:04,  9.47it/s] 88%|████████▊ | 337/383 [00:33<00:07,  6.11it/s]
 88%|████████▊ | 338/383 [00:33<00:08,  5.49it/s][A 89%|████████▉ | 340/383 [00:33<00:05,  8.40it/s] 90%|████████▉ | 344/383 [00:33<00:03, 10.79it/s] 88%|████████▊ | 338/383 [00:33<00:07,  6.18it/s]
 89%|████████▊ | 339/383 [00:33<00:07,  5.73it/s][A 89%|████████▉ | 342/383 [00:33<00:04,  9.83it/s] 90%|█████████ | 346/383 [00:33<00:03, 11.54it/s] 89%|████████▉ | 340/383 [00:33<00:05,  8.27it/s]
 89%|████████▉ | 341/383 [00:33<00:05,  7.90it/s][A 90%|████████▉ | 344/383 [00:33<00:03, 11.11it/s] 91%|█████████ | 348/383 [00:33<00:02, 12.47it/s] 89%|████████▉ | 342/383 [00:33<00:04,  9.96it/s]
 90%|████████▉ | 343/383 [00:33<00:04,  9.62it/s][A 90%|█████████ | 346/383 [00:33<00:03, 12.21it/s] 91%|█████████▏| 350/383 [00:33<00:02, 13.35it/s] 90%|████████▉ | 344/383 [00:33<00:03, 10.54it/s]
 90%|█████████ | 345/383 [00:33<00:03, 11.04it/s][A 91%|█████████ | 348/383 [00:34<00:02, 12.99it/s] 92%|█████████▏| 352/383 [00:34<00:02, 13.38it/s] 90%|█████████ | 346/383 [00:34<00:03, 11.92it/s]
 91%|█████████ | 347/383 [00:34<00:02, 12.18it/s][A 91%|█████████▏| 350/383 [00:34<00:02, 13.07it/s] 92%|█████████▏| 354/383 [00:34<00:02, 13.70it/s] 91%|█████████ | 348/383 [00:34<00:02, 12.87it/s]
 91%|█████████ | 349/383 [00:34<00:02, 12.89it/s][A 92%|█████████▏| 352/383 [00:34<00:02, 13.79it/s] 93%|█████████▎| 356/383 [00:34<00:01, 14.48it/s] 91%|█████████▏| 350/383 [00:34<00:02, 13.40it/s]
 92%|█████████▏| 351/383 [00:34<00:02, 13.29it/s][A 93%|█████████▎| 358/383 [00:34<00:01, 15.29it/s] 92%|█████████▏| 354/383 [00:34<00:02, 14.08it/s] 92%|█████████▏| 352/383 [00:34<00:02, 13.68it/s]
 92%|█████████▏| 353/383 [00:34<00:02, 13.27it/s][A 93%|█████████▎| 356/383 [00:34<00:01, 14.37it/s] 92%|█████████▏| 354/383 [00:34<00:02, 13.77it/s]
 93%|█████████▎| 355/383 [00:34<00:02, 13.59it/s][A 93%|█████████▎| 358/383 [00:34<00:01, 14.76it/s] 93%|█████████▎| 356/383 [00:34<00:01, 13.64it/s] 94%|█████████▍| 360/383 [00:34<00:02,  9.74it/s]
 93%|█████████▎| 357/383 [00:34<00:01, 14.40it/s][A 93%|█████████▎| 358/383 [00:34<00:01, 14.88it/s]
 94%|█████████▎| 359/383 [00:34<00:01, 15.44it/s][A 94%|█████████▍| 360/383 [00:35<00:02,  9.97it/s] 95%|█████████▍| 362/383 [00:35<00:02,  8.33it/s] 94%|█████████▍| 360/383 [00:35<00:01, 11.86it/s]
 94%|█████████▍| 361/383 [00:35<00:02, 10.16it/s][A 95%|█████████▍| 362/383 [00:35<00:02,  8.17it/s] 95%|█████████▌| 364/383 [00:35<00:02,  7.21it/s] 95%|█████████▍| 362/383 [00:35<00:02,  9.10it/s]
 95%|█████████▍| 363/383 [00:35<00:02,  8.47it/s][A 95%|█████████▌| 365/383 [00:35<00:02,  6.86it/s] 95%|█████████▌| 364/383 [00:35<00:02,  7.13it/s] 96%|█████████▌| 367/383 [00:35<00:01,  8.69it/s] 95%|█████████▌| 364/383 [00:35<00:02,  7.82it/s] 96%|█████████▋| 369/383 [00:35<00:01, 10.48it/s] 95%|█████████▌| 365/383 [00:35<00:02,  6.85it/s]
 95%|█████████▌| 365/383 [00:35<00:02,  7.44it/s][A 97%|█████████▋| 371/383 [00:35<00:00, 12.16it/s] 95%|█████████▌| 365/383 [00:36<00:02,  7.36it/s] 96%|█████████▌| 367/383 [00:36<00:01,  8.61it/s]
 96%|█████████▌| 366/383 [00:36<00:02,  7.38it/s][A 97%|█████████▋| 373/383 [00:36<00:00, 13.67it/s] 96%|█████████▌| 367/383 [00:36<00:01,  9.24it/s] 96%|█████████▋| 369/383 [00:36<00:01, 10.40it/s]
 96%|█████████▌| 368/383 [00:36<00:01,  9.20it/s][A 98%|█████████▊| 375/383 [00:36<00:00, 14.64it/s] 96%|█████████▋| 369/383 [00:36<00:01, 10.96it/s] 97%|█████████▋| 371/383 [00:36<00:00, 12.04it/s] 98%|█████████▊| 377/383 [00:36<00:00, 15.82it/s]
 97%|█████████▋| 370/383 [00:36<00:01, 10.89it/s][A 97%|█████████▋| 371/383 [00:36<00:00, 12.44it/s] 97%|█████████▋| 373/383 [00:36<00:00, 13.42it/s]
 97%|█████████▋| 372/383 [00:36<00:00, 12.34it/s][A 99%|█████████▉| 380/383 [00:36<00:00, 18.26it/s] 97%|█████████▋| 373/383 [00:36<00:00, 13.83it/s] 98%|█████████▊| 375/383 [00:36<00:00, 14.87it/s]
 98%|█████████▊| 374/383 [00:36<00:00, 13.79it/s][A100%|██████████| 383/383 [00:36<00:00, 19.32it/s]100%|██████████| 383/383 [00:36<00:00, 10.47it/s]
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
 98%|█████████▊| 376/383 [00:36<00:00, 15.79it/s]/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
 99%|█████████▊| 378/383 [00:36<00:00, 16.91it/s]/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)

 98%|█████████▊| 377/383 [00:36<00:00, 15.74it/s][A 99%|█████████▉| 379/383 [00:36<00:00, 18.39it/s] 99%|█████████▉| 381/383 [00:36<00:00, 19.49it/s]
 99%|█████████▉| 380/383 [00:36<00:00, 17.68it/s][A100%|█████████▉| 382/383 [00:36<00:00, 20.61it/s]100%|██████████| 383/383 [00:36<00:00, 10.40it/s]
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
100%|██████████| 383/383 [00:36<00:00, 10.38it/s]
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)

100%|██████████| 383/383 [00:36<00:00, 18.62it/s][A100%|██████████| 383/383 [00:36<00:00, 10.36it/s]
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
                                                     {'mmlu_loss': 3.621104790087468, 'mmlu_eval_accuracy_conceptual_physics': 0.23076923076923078, 'mmlu_eval_accuracy_logical_fallacies': nan, 'mmlu_eval_accuracy_professional_psychology': nan, 'mmlu_eval_accuracy_public_relations': nan, 'mmlu_eval_accuracy_security_studies': nan, 'mmlu_eval_accuracy_high_school_physics': nan, 'mmlu_eval_accuracy_formal_logic': 0.2857142857142857, 'mmlu_eval_accuracy_anatomy': 0.2857142857142857, 'mmlu_eval_accuracy_moral_scenarios': nan, 'mmlu_eval_accuracy_moral_disputes': nan, 'mmlu_eval_accuracy_management': nan, 'mmlu_eval_accuracy_high_school_european_history': 0.3333333333333333, 'mmlu_eval_accuracy_professional_law': nan, 'mmlu_eval_accuracy_elementary_mathematics': 0.24390243902439024, 'mmlu_eval_accuracy_high_school_computer_science': 0.3333333333333333, 'mmlu_eval_accuracy_high_school_geography': 0.4166666666666667, 'mmlu_eval_accuracy_global_facts': 0.5, 'mmlu_eval_accuracy_high_school_biology': 0.25, 'mmlu_eval_accuracy_philosophy': nan, 'mmlu_eval_accuracy_astronomy': 0.25, 'mmlu_eval_accuracy_nutrition': nan, 'mmlu_eval_accuracy_high_school_government_and_politics': nan, 'mmlu_eval_accuracy_medical_genetics': nan, 'mmlu_eval_accuracy_human_aging': nan, 'mmlu_eval_accuracy_human_sexuality': nan, 'mmlu_eval_accuracy_college_medicine': 0.09090909090909091, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_college_physics': 0.2727272727272727, 'mmlu_eval_accuracy_us_foreign_policy': nan, 'mmlu_eval_accuracy_college_computer_science': 0.09090909090909091, 'mmlu_eval_accuracy_machine_learning': nan, 'mmlu_eval_accuracy_professional_accounting': nan, 'mmlu_eval_accuracy_world_religions': nan, 'mmlu_eval_accuracy_miscellaneous': nan, 'mmlu_eval_accuracy_jurisprudence': nan, 'mmlu_eval_accuracy_high_school_chemistry': 0.22727272727272727, 'mmlu_eval_accuracy_virology': nan, 'mmlu_eval_accuracy_high_school_mathematics': nan, 'mmlu_eval_accuracy_electrical_engineering': 0.3125, 'mmlu_eval_accuracy_high_school_world_history': nan, 'mmlu_eval_accuracy_prehistory': nan, 'mmlu_eval_accuracy_high_school_statistics': nan, 'mmlu_eval_accuracy_high_school_psychology': nan, 'mmlu_eval_accuracy_high_school_microeconomics': nan, 'mmlu_eval_accuracy_high_school_us_history': nan, 'mmlu_eval_accuracy_college_biology': 0.3125, 'mmlu_eval_accuracy_high_school_macroeconomics': nan, 'mmlu_eval_accuracy_professional_medicine': nan, 'mmlu_eval_accuracy_clinical_knowledge': 0.3103448275862069, 'mmlu_eval_accuracy_business_ethics': 0.09090909090909091, 'mmlu_eval_accuracy_college_chemistry': 0.125, 'mmlu_eval_accuracy_econometrics': 0.3333333333333333, 'mmlu_eval_accuracy_marketing': nan, 'mmlu_eval_accuracy_international_law': nan, 'mmlu_eval_accuracy_sociology': nan, 'mmlu_eval_accuracy_computer_security': 0.18181818181818182, 'mmlu_eval_accuracy_college_mathematics': 0.09090909090909091, 'mmlu_eval_accuracy': nan, 'epoch': 14.85}
100%|██████████| 2048/2048 [2:47:50<00:00,  4.80s/it]Saving PEFT checkpoint...Saving PEFT checkpoint...

                                                     {'train_runtime': 10070.6347, 'train_samples_per_second': 13.015, 'train_steps_per_second': 0.203, 'train_loss': 0.4251143292058259, 'epoch': 14.85}
100%|██████████| 2048/2048 [2:47:50<00:00,  4.80s/it]Saving PEFT checkpoint...
100%|██████████| 2048/2048 [2:47:50<00:00,  4.92s/it]
Saving PEFT checkpoint...
***** train metrics *****
  epoch                    =      14.85
  train_loss               =     0.4251
  train_runtime            = 2:47:50.63
  train_samples_per_second =     13.015
  train_steps_per_second   =      0.203
  0%|          | 0/256 [00:00<?, ?it/s]  1%|          | 3/256 [00:00<00:09, 26.14it/s]  2%|▏         | 6/256 [00:00<00:12, 19.46it/s]  4%|▎         | 9/256 [00:00<00:13, 18.86it/s]  4%|▍         | 11/256 [00:00<00:13, 18.13it/s]  5%|▌         | 13/256 [00:00<00:13, 17.71it/s]  6%|▌         | 15/256 [00:00<00:13, 17.75it/s]  7%|▋         | 17/256 [00:00<00:13, 17.85it/s]  7%|▋         | 19/256 [00:01<00:13, 17.88it/s]  8%|▊         | 21/256 [00:01<00:12, 18.41it/s]  9%|▉         | 23/256 [00:01<00:13, 17.90it/s] 10%|▉         | 25/256 [00:01<00:13, 17.56it/s] 11%|█         | 27/256 [00:01<00:12, 17.66it/s] 11%|█▏        | 29/256 [00:01<00:13, 17.38it/s] 12%|█▏        | 31/256 [00:01<00:13, 17.21it/s] 13%|█▎        | 34/256 [00:01<00:11, 18.69it/s] 14%|█▍        | 36/256 [00:01<00:12, 18.12it/s] 15%|█▍        | 38/256 [00:02<00:12, 17.63it/s] 16%|█▌        | 40/256 [00:02<00:12, 17.36it/s] 16%|█▋        | 42/256 [00:02<00:12, 17.74it/s] 17%|█▋        | 44/256 [00:02<00:12, 17.48it/s] 18%|█▊        | 46/256 [00:02<00:12, 17.27it/s] 19%|█▉        | 48/256 [00:02<00:11, 17.34it/s] 20%|█▉        | 50/256 [00:02<00:12, 17.10it/s] 20%|██        | 52/256 [00:02<00:12, 16.99it/s] 21%|██        | 54/256 [00:03<00:11, 17.78it/s] 22%|██▏       | 56/256 [00:03<00:11, 17.58it/s] 23%|██▎       | 58/256 [00:03<00:11, 17.46it/s] 23%|██▎       | 60/256 [00:03<00:11, 17.27it/s] 24%|██▍       | 62/256 [00:03<00:11, 16.95it/s] 25%|██▌       | 64/256 [00:03<00:11, 17.21it/s] 26%|██▌       | 66/256 [00:03<00:11, 17.06it/s] 27%|██▋       | 68/256 [00:03<00:11, 16.98it/s] 27%|██▋       | 70/256 [00:03<00:10, 17.14it/s] 28%|██▊       | 72/256 [00:04<00:10, 17.01it/s] 29%|██▉       | 74/256 [00:04<00:10, 16.97it/s] 30%|██▉       | 76/256 [00:04<00:10, 16.94it/s] 30%|███       | 78/256 [00:04<00:10, 16.98it/s] 31%|███▏      | 80/256 [00:04<00:10, 16.91it/s] 32%|███▏      | 82/256 [00:04<00:10, 16.93it/s] 33%|███▎      | 84/256 [00:04<00:10, 17.14it/s] 34%|███▎      | 86/256 [00:04<00:09, 17.04it/s] 34%|███▍      | 88/256 [00:05<00:09, 17.25it/s] 35%|███▌      | 90/256 [00:05<00:09, 17.02it/s] 36%|███▌      | 92/256 [00:05<00:09, 16.92it/s] 37%|███▋      | 94/256 [00:05<00:09, 16.99it/s] 38%|███▊      | 96/256 [00:05<00:09, 16.87it/s] 39%|███▊      | 99/256 [00:05<00:08, 17.72it/s] 39%|███▉      | 101/256 [00:05<00:08, 17.39it/s] 40%|████      | 103/256 [00:05<00:08, 17.47it/s] 41%|████▏     | 106/256 [00:06<00:08, 18.34it/s] 42%|████▏     | 108/256 [00:06<00:08, 17.97it/s] 43%|████▎     | 110/256 [00:06<00:08, 17.73it/s] 44%|████▍     | 112/256 [00:06<00:08, 17.41it/s] 45%|████▍     | 114/256 [00:06<00:08, 17.60it/s] 46%|████▌     | 117/256 [00:06<00:07, 18.04it/s] 46%|████▋     | 119/256 [00:06<00:07, 17.83it/s] 47%|████▋     | 121/256 [00:06<00:07, 17.46it/s] 48%|████▊     | 123/256 [00:07<00:07, 17.28it/s] 49%|████▉     | 125/256 [00:07<00:07, 17.14it/s] 50%|█████     | 128/256 [00:07<00:06, 18.66it/s] 51%|█████     | 130/256 [00:07<00:06, 18.21it/s] 52%|█████▏    | 132/256 [00:07<00:07, 17.67it/s] 52%|█████▏    | 134/256 [00:07<00:06, 17.51it/s] 53%|█████▎    | 136/256 [00:07<00:06, 17.39it/s] 54%|█████▍    | 138/256 [00:07<00:06, 17.20it/s] 55%|█████▍    | 140/256 [00:07<00:06, 17.06it/s] 56%|█████▌    | 143/256 [00:08<00:06, 18.20it/s] 57%|█████▋    | 145/256 [00:08<00:06, 17.64it/s] 57%|█████▋    | 147/256 [00:08<00:06, 17.43it/s] 58%|█████▊    | 149/256 [00:08<00:06, 17.24it/s] 59%|█████▉    | 151/256 [00:08<00:06, 17.10it/s] 60%|██████    | 154/256 [00:08<00:05, 17.70it/s] 61%|██████    | 156/256 [00:08<00:05, 17.57it/s] 62%|██████▏   | 158/256 [00:09<00:05, 17.22it/s] 63%|██████▎   | 161/256 [00:09<00:05, 17.95it/s] 64%|██████▎   | 163/256 [00:09<00:05, 17.52it/s] 64%|██████▍   | 165/256 [00:09<00:05, 17.43it/s] 65%|██████▌   | 167/256 [00:09<00:05, 17.07it/s] 66%|██████▌   | 169/256 [00:09<00:05, 17.32it/s] 67%|██████▋   | 171/256 [00:09<00:04, 17.08it/s] 68%|██████▊   | 173/256 [00:09<00:04, 16.94it/s] 68%|██████▊   | 175/256 [00:09<00:04, 17.00it/s] 69%|██████▉   | 177/256 [00:10<00:04, 16.84it/s] 70%|██████▉   | 179/256 [00:10<00:04, 16.92it/s] 71%|███████   | 182/256 [00:10<00:03, 19.22it/s] 72%|███████▏  | 184/256 [00:10<00:03, 18.60it/s] 73%|███████▎  | 186/256 [00:10<00:03, 17.93it/s] 74%|███████▍  | 189/256 [00:10<00:03, 18.40it/s] 75%|███████▍  | 191/256 [00:10<00:03, 17.84it/s] 75%|███████▌  | 193/256 [00:10<00:03, 17.67it/s] 76%|███████▌  | 195/256 [00:11<00:03, 17.28it/s] 77%|███████▋  | 197/256 [00:11<00:03, 17.54it/s] 78%|███████▊  | 200/256 [00:11<00:03, 18.12it/s] 79%|███████▉  | 202/256 [00:11<00:03, 17.66it/s] 80%|███████▉  | 204/256 [00:11<00:02, 17.48it/s] 80%|████████  | 206/256 [00:11<00:02, 17.16it/s] 81%|████████▏ | 208/256 [00:11<00:02, 17.06it/s] 82%|████████▏ | 210/256 [00:11<00:02, 16.97it/s] 83%|████████▎ | 212/256 [00:12<00:02, 16.92it/s] 84%|████████▎ | 214/256 [00:12<00:02, 16.99it/s] 84%|████████▍ | 216/256 [00:12<00:02, 16.81it/s] 85%|████████▌ | 218/256 [00:12<00:02, 16.85it/s] 86%|████████▌ | 220/256 [00:12<00:02, 16.78it/s] 87%|████████▋ | 222/256 [00:12<00:02, 16.77it/s] 88%|████████▊ | 224/256 [00:12<00:01, 16.90it/s] 88%|████████▊ | 226/256 [00:12<00:01, 17.14it/s] 89%|████████▉ | 228/256 [00:13<00:01, 17.01it/s] 90%|████████▉ | 230/256 [00:13<00:01, 17.19it/s] 91%|█████████ | 232/256 [00:13<00:01, 17.06it/s] 91%|█████████▏| 234/256 [00:13<00:01, 16.96it/s] 93%|█████████▎| 237/256 [00:13<00:01, 17.91it/s] 93%|█████████▎| 239/256 [00:13<00:00, 17.71it/s] 94%|█████████▍| 241/256 [00:13<00:00, 17.29it/s] 95%|█████████▍| 243/256 [00:13<00:00, 17.68it/s] 96%|█████████▌| 245/256 [00:14<00:00, 17.64it/s] 97%|█████████▋| 248/256 [00:14<00:00, 18.07it/s] 98%|█████████▊| 251/256 [00:14<00:00, 19.19it/s] 99%|█████████▉| 253/256 [00:14<00:00, 18.55it/s]100%|█████████▉| 255/256 [00:14<00:00, 18.46it/s]100%|██████████| 256/256 [00:14<00:00, 17.45it/s]
  0%|          | 0/383 [00:00<?, ?it/s]  0%|          | 0/383 [00:00<?, ?it/s]  0%|          | 0/383 [00:00<?, ?it/s]  0%|          | 0/383 [00:00<?, ?it/s]  0%|          | 1/383 [00:00<00:41,  9.23it/s]  1%|          | 2/383 [00:00<00:25, 14.77it/s]  1%|          | 2/383 [00:00<00:26, 14.46it/s]  1%|          | 2/383 [00:00<00:27, 14.01it/s]  1%|          | 3/383 [00:00<00:24, 15.24it/s]  1%|▏         | 5/383 [00:00<00:19, 19.33it/s]  1%|▏         | 5/383 [00:00<00:20, 18.55it/s]  1%|▏         | 5/383 [00:00<00:20, 18.31it/s]  1%|▏         | 5/383 [00:00<00:21, 17.29it/s]  2%|▏         | 7/383 [00:00<00:21, 17.89it/s]  2%|▏         | 7/383 [00:00<00:21, 17.13it/s]  2%|▏         | 7/383 [00:00<00:22, 16.73it/s]  2%|▏         | 8/383 [00:00<00:20, 18.39it/s]  2%|▏         | 9/383 [00:00<00:23, 16.07it/s]  2%|▏         | 9/383 [00:00<00:24, 15.42it/s]  2%|▏         | 9/383 [00:00<00:23, 15.99it/s]  3%|▎         | 10/383 [00:00<00:23, 16.16it/s]  3%|▎         | 11/383 [00:00<00:23, 15.52it/s]  3%|▎         | 11/383 [00:00<00:25, 14.62it/s]  3%|▎         | 11/383 [00:00<00:24, 15.02it/s]  3%|▎         | 12/383 [00:00<00:23, 15.52it/s]  3%|▎         | 13/383 [00:00<00:24, 15.06it/s]  3%|▎         | 13/383 [00:00<00:24, 15.33it/s]  3%|▎         | 13/383 [00:00<00:25, 14.42it/s]  4%|▎         | 14/383 [00:00<00:23, 15.69it/s]  4%|▍         | 15/383 [00:00<00:22, 16.12it/s]  4%|▍         | 15/383 [00:00<00:22, 16.25it/s]  4%|▍         | 15/383 [00:00<00:23, 15.45it/s]  4%|▍         | 16/383 [00:00<00:22, 16.58it/s]  4%|▍         | 17/383 [00:01<00:21, 16.76it/s]  4%|▍         | 17/383 [00:01<00:21, 16.99it/s]  4%|▍         | 17/383 [00:01<00:22, 16.25it/s]  5%|▍         | 18/383 [00:01<00:21, 17.22it/s]  5%|▍         | 19/383 [00:01<00:20, 17.34it/s]  5%|▍         | 19/383 [00:01<00:20, 17.53it/s]  5%|▌         | 20/383 [00:01<00:20, 17.72it/s]  5%|▍         | 19/383 [00:01<00:22, 16.52it/s]  5%|▌         | 21/383 [00:01<00:20, 17.35it/s]  5%|▌         | 21/383 [00:01<00:20, 17.69it/s]  6%|▌         | 22/383 [00:01<00:20, 17.55it/s]  5%|▌         | 21/383 [00:01<00:21, 16.82it/s]  6%|▌         | 23/383 [00:01<00:20, 17.21it/s]  6%|▌         | 23/383 [00:01<00:20, 17.36it/s]  6%|▋         | 24/383 [00:01<00:20, 17.57it/s]  6%|▌         | 23/383 [00:01<00:21, 17.02it/s]  7%|▋         | 25/383 [00:01<00:21, 16.73it/s]  7%|▋         | 25/383 [00:01<00:21, 17.02it/s]  7%|▋         | 26/383 [00:01<00:20, 17.35it/s]  7%|▋         | 25/383 [00:01<00:21, 16.73it/s]  7%|▋         | 27/383 [00:01<00:24, 14.61it/s]  7%|▋         | 28/383 [00:01<00:23, 15.43it/s]  7%|▋         | 27/383 [00:01<00:24, 14.69it/s]  7%|▋         | 27/383 [00:01<00:24, 14.43it/s]  8%|▊         | 29/383 [00:01<00:27, 12.83it/s]  8%|▊         | 30/383 [00:01<00:25, 13.59it/s]  8%|▊         | 29/383 [00:01<00:27, 12.83it/s]  8%|▊         | 29/383 [00:01<00:28, 12.46it/s]  8%|▊         | 32/383 [00:02<00:25, 13.80it/s]  8%|▊         | 31/383 [00:02<00:27, 12.99it/s]  8%|▊         | 31/383 [00:02<00:26, 13.09it/s]  8%|▊         | 31/383 [00:02<00:26, 13.18it/s]  9%|▉         | 34/383 [00:02<00:24, 14.06it/s]  9%|▊         | 33/383 [00:02<00:25, 13.68it/s]  9%|▊         | 33/383 [00:02<00:25, 13.80it/s]  9%|▊         | 33/383 [00:02<00:24, 14.06it/s]  9%|▉         | 36/383 [00:02<00:23, 14.70it/s]  9%|▉         | 35/383 [00:02<00:24, 14.11it/s]  9%|▉         | 35/383 [00:02<00:24, 14.38it/s]  9%|▉         | 35/383 [00:02<00:23, 14.65it/s] 10%|▉         | 38/383 [00:02<00:22, 15.01it/s] 10%|▉         | 37/383 [00:02<00:23, 14.92it/s] 10%|▉         | 37/383 [00:02<00:23, 14.97it/s] 10%|▉         | 37/383 [00:02<00:22, 15.24it/s] 10%|█         | 39/383 [00:02<00:22, 15.48it/s] 10%|█         | 40/383 [00:02<00:22, 15.45it/s] 10%|█         | 39/383 [00:02<00:22, 15.56it/s] 10%|█         | 39/383 [00:02<00:21, 15.73it/s] 11%|█         | 42/383 [00:02<00:20, 16.54it/s] 11%|█         | 41/383 [00:02<00:21, 15.91it/s] 11%|█         | 41/383 [00:02<00:21, 16.04it/s] 11%|█         | 41/383 [00:02<00:21, 15.88it/s] 11%|█         | 43/383 [00:02<00:20, 16.92it/s] 12%|█▏        | 45/383 [00:02<00:18, 18.71it/s] 11%|█▏        | 44/383 [00:02<00:18, 18.35it/s] 11%|█         | 43/383 [00:02<00:20, 16.52it/s] 12%|█▏        | 46/383 [00:02<00:17, 19.55it/s] 13%|█▎        | 48/383 [00:02<00:16, 20.77it/s] 12%|█▏        | 47/383 [00:02<00:16, 20.62it/s] 12%|█▏        | 46/383 [00:02<00:17, 19.17it/s] 13%|█▎        | 49/383 [00:02<00:15, 21.46it/s] 13%|█▎        | 49/383 [00:03<00:15, 21.22it/s] 13%|█▎        | 51/383 [00:03<00:16, 20.54it/s] 13%|█▎        | 50/383 [00:03<00:16, 20.80it/s] 14%|█▍        | 54/383 [00:03<00:17, 18.44it/s] 14%|█▎        | 52/383 [00:03<00:18, 17.44it/s] 14%|█▍        | 53/383 [00:03<00:17, 18.45it/s] 14%|█▎        | 52/383 [00:03<00:18, 18.14it/s] 14%|█▍        | 54/383 [00:03<00:18, 17.77it/s] 15%|█▍        | 56/383 [00:03<00:17, 18.21it/s] 14%|█▍        | 55/383 [00:03<00:17, 18.37it/s] 14%|█▍        | 54/383 [00:03<00:18, 17.90it/s] 15%|█▍        | 56/383 [00:03<00:18, 17.99it/s] 15%|█▍        | 57/383 [00:03<00:17, 18.23it/s] 15%|█▍        | 56/383 [00:03<00:18, 17.98it/s] 15%|█▌        | 58/383 [00:03<00:18, 17.65it/s] 15%|█▌        | 58/383 [00:03<00:18, 17.40it/s] 15%|█▌        | 59/383 [00:03<00:18, 17.18it/s] 15%|█▌        | 58/383 [00:03<00:18, 17.22it/s] 16%|█▌        | 60/383 [00:03<00:19, 16.72it/s] 16%|█▌        | 60/383 [00:03<00:19, 16.83it/s] 16%|█▌        | 61/383 [00:03<00:19, 16.76it/s] 16%|█▌        | 60/383 [00:03<00:19, 16.85it/s] 16%|█▌        | 62/383 [00:03<00:19, 16.32it/s] 16%|█▌        | 62/383 [00:03<00:19, 16.09it/s] 16%|█▋        | 63/383 [00:03<00:19, 16.39it/s] 16%|█▌        | 62/383 [00:03<00:19, 16.24it/s] 17%|█▋        | 64/383 [00:03<00:19, 16.08it/s] 17%|█▋        | 64/383 [00:03<00:19, 15.98it/s] 17%|█▋        | 64/383 [00:03<00:19, 16.26it/s] 17%|█▋        | 65/383 [00:03<00:19, 15.90it/s] 17%|█▋        | 66/383 [00:03<00:20, 15.81it/s] 17%|█▋        | 66/383 [00:04<00:20, 15.73it/s] 17%|█▋        | 66/383 [00:04<00:19, 16.25it/s] 17%|█▋        | 67/383 [00:04<00:20, 15.67it/s] 18%|█▊        | 68/383 [00:04<00:20, 15.47it/s] 18%|█▊        | 68/383 [00:04<00:20, 15.65it/s] 18%|█▊        | 68/383 [00:04<00:20, 15.04it/s] 18%|█▊        | 69/383 [00:04<00:21, 14.94it/s] 18%|█▊        | 70/383 [00:04<00:21, 14.65it/s] 18%|█▊        | 70/383 [00:04<00:20, 15.06it/s] 18%|█▊        | 70/383 [00:04<00:21, 14.63it/s] 19%|█▊        | 71/383 [00:04<00:20, 15.10it/s] 19%|█▉        | 72/383 [00:04<00:19, 15.66it/s] 19%|█▉        | 72/383 [00:04<00:19, 15.73it/s] 19%|█▉        | 72/383 [00:04<00:20, 15.53it/s] 19%|█▉        | 73/383 [00:04<00:19, 15.71it/s] 19%|█▉        | 74/383 [00:04<00:19, 16.02it/s] 19%|█▉        | 74/383 [00:04<00:19, 16.23it/s] 20%|█▉        | 76/383 [00:04<00:19, 16.04it/s] 19%|█▉        | 74/383 [00:04<00:20, 15.29it/s] 20%|█▉        | 75/383 [00:04<00:19, 15.69it/s] 20%|█▉        | 76/383 [00:04<00:18, 16.27it/s] 20%|█▉        | 76/383 [00:04<00:19, 15.66it/s] 20%|██        | 78/383 [00:04<00:18, 16.08it/s] 20%|██        | 77/383 [00:04<00:19, 15.81it/s] 20%|██        | 78/383 [00:04<00:18, 16.18it/s] 20%|██        | 78/383 [00:04<00:19, 15.83it/s] 21%|██        | 79/383 [00:04<00:19, 15.84it/s] 21%|██        | 80/383 [00:04<00:19, 15.65it/s] 21%|██        | 80/383 [00:04<00:18, 16.10it/s] 21%|██        | 80/383 [00:04<00:19, 15.76it/s] 21%|██▏       | 82/383 [00:04<00:18, 15.97it/s] 21%|██        | 81/383 [00:05<00:19, 15.59it/s] 21%|██▏       | 82/383 [00:05<00:18, 15.96it/s] 21%|██▏       | 82/383 [00:05<00:18, 15.97it/s] 22%|██▏       | 84/383 [00:05<00:18, 16.10it/s] 22%|██▏       | 83/383 [00:05<00:19, 15.75it/s] 22%|██▏       | 84/383 [00:05<00:18, 16.00it/s] 22%|██▏       | 84/383 [00:05<00:18, 16.12it/s] 22%|██▏       | 85/383 [00:05<00:18, 16.06it/s] 22%|██▏       | 86/383 [00:05<00:18, 15.65it/s] 22%|██▏       | 86/383 [00:05<00:18, 16.24it/s] 22%|██▏       | 86/383 [00:05<00:18, 16.08it/s] 23%|██▎       | 87/383 [00:05<00:20, 14.17it/s] 23%|██▎       | 88/383 [00:05<00:23, 12.74it/s] 23%|██▎       | 88/383 [00:05<00:22, 13.01it/s] 23%|██▎       | 88/383 [00:05<00:23, 12.53it/s] 23%|██▎       | 90/383 [00:05<00:31,  9.41it/s] 23%|██▎       | 89/383 [00:05<00:32,  9.02it/s] 23%|██▎       | 90/383 [00:05<00:34,  8.61it/s] 23%|██▎       | 90/383 [00:06<00:42,  6.90it/s] 24%|██▍       | 92/383 [00:06<00:47,  6.14it/s] 24%|██▍       | 91/383 [00:06<00:48,  6.02it/s] 24%|██▍       | 92/383 [00:06<00:48,  5.99it/s] 24%|██▍       | 92/383 [00:06<00:47,  6.09it/s] 24%|██▍       | 93/383 [00:06<00:50,  5.73it/s] 24%|██▍       | 92/383 [00:06<00:52,  5.56it/s] 25%|██▍       | 95/383 [00:06<00:38,  7.53it/s] 25%|██▍       | 94/383 [00:06<00:39,  7.30it/s] 25%|██▌       | 97/383 [00:06<00:30,  9.40it/s] 24%|██▍       | 93/383 [00:06<00:55,  5.26it/s] 25%|██▌       | 96/383 [00:06<00:31,  9.18it/s] 24%|██▍       | 93/383 [00:06<00:53,  5.44it/s] 26%|██▌       | 99/383 [00:06<00:25, 11.28it/s] 25%|██▍       | 95/383 [00:06<00:41,  6.93it/s] 26%|██▌       | 98/383 [00:06<00:25, 11.06it/s] 25%|██▍       | 95/383 [00:06<00:40,  7.17it/s] 25%|██▌       | 97/383 [00:07<00:32,  8.78it/s] 26%|██▋       | 101/383 [00:07<00:22, 12.50it/s] 26%|██▌       | 100/383 [00:07<00:22, 12.40it/s] 25%|██▌       | 97/383 [00:07<00:31,  9.01it/s] 26%|██▌       | 99/383 [00:07<00:27, 10.49it/s] 27%|██▋       | 103/383 [00:07<00:20, 13.73it/s] 27%|██▋       | 102/383 [00:07<00:20, 13.76it/s] 26%|██▌       | 99/383 [00:07<00:26, 10.70it/s] 27%|██▋       | 105/383 [00:07<00:18, 14.96it/s] 26%|██▋       | 101/383 [00:07<00:23, 11.80it/s] 27%|██▋       | 104/383 [00:07<00:18, 15.03it/s] 26%|██▋       | 101/383 [00:07<00:23, 12.11it/s] 28%|██▊       | 107/383 [00:07<00:17, 16.15it/s] 27%|██▋       | 103/383 [00:07<00:21, 13.08it/s] 28%|██▊       | 106/383 [00:07<00:17, 16.15it/s] 27%|██▋       | 103/383 [00:07<00:20, 13.49it/s] 28%|██▊       | 109/383 [00:07<00:16, 17.12it/s] 27%|██▋       | 105/383 [00:07<00:19, 14.33it/s] 28%|██▊       | 108/383 [00:07<00:16, 16.87it/s] 27%|██▋       | 105/383 [00:07<00:18, 14.82it/s] 29%|██▉       | 111/383 [00:07<00:15, 17.62it/s] 29%|██▊       | 110/383 [00:07<00:15, 17.55it/s] 28%|██▊       | 107/383 [00:07<00:17, 15.46it/s] 28%|██▊       | 107/383 [00:07<00:17, 16.00it/s] 30%|██▉       | 113/383 [00:07<00:14, 18.04it/s] 29%|██▉       | 112/383 [00:07<00:15, 18.05it/s] 28%|██▊       | 109/383 [00:07<00:16, 16.41it/s] 28%|██▊       | 109/383 [00:07<00:16, 16.88it/s] 30%|███       | 115/383 [00:07<00:14, 18.45it/s] 30%|██▉       | 114/383 [00:07<00:14, 18.44it/s] 29%|██▉       | 111/383 [00:07<00:15, 17.15it/s] 29%|██▉       | 111/383 [00:07<00:15, 17.51it/s] 31%|███       | 117/383 [00:07<00:14, 17.77it/s] 30%|██▉       | 113/383 [00:07<00:15, 17.85it/s] 30%|███       | 116/383 [00:07<00:15, 17.75it/s] 30%|██▉       | 113/383 [00:07<00:15, 17.90it/s] 30%|███       | 115/383 [00:08<00:15, 17.80it/s] 31%|███       | 119/383 [00:08<00:15, 17.29it/s] 30%|███       | 115/383 [00:08<00:15, 17.85it/s] 31%|███       | 118/383 [00:08<00:15, 17.21it/s] 32%|███▏      | 121/383 [00:08<00:15, 17.09it/s] 31%|███       | 117/383 [00:08<00:15, 17.00it/s] 31%|███       | 117/383 [00:08<00:15, 17.38it/s] 31%|███▏      | 120/383 [00:08<00:15, 17.16it/s] 32%|███▏      | 123/383 [00:08<00:15, 17.14it/s] 31%|███       | 119/383 [00:08<00:15, 16.86it/s] 32%|███▏      | 122/383 [00:08<00:15, 17.39it/s] 31%|███       | 119/383 [00:08<00:15, 16.92it/s] 33%|███▎      | 125/383 [00:08<00:14, 17.80it/s] 32%|███▏      | 124/383 [00:08<00:14, 18.07it/s] 32%|███▏      | 121/383 [00:08<00:15, 16.62it/s] 32%|███▏      | 121/383 [00:08<00:15, 16.57it/s] 33%|███▎      | 127/383 [00:08<00:13, 18.40it/s] 33%|███▎      | 126/383 [00:08<00:14, 18.34it/s] 32%|███▏      | 123/383 [00:08<00:15, 16.93it/s] 32%|███▏      | 123/383 [00:08<00:14, 17.38it/s] 34%|███▎      | 129/383 [00:08<00:13, 18.15it/s] 33%|███▎      | 128/383 [00:08<00:13, 18.69it/s] 33%|███▎      | 125/383 [00:08<00:14, 17.44it/s] 33%|███▎      | 125/383 [00:08<00:14, 17.86it/s] 34%|███▍      | 131/383 [00:08<00:14, 16.99it/s] 33%|███▎      | 127/383 [00:08<00:14, 17.86it/s] 33%|███▎      | 127/383 [00:08<00:13, 18.30it/s] 34%|███▍      | 130/383 [00:08<00:14, 17.07it/s] 34%|███▎      | 129/383 [00:08<00:13, 18.21it/s] 35%|███▍      | 133/383 [00:08<00:15, 16.57it/s] 34%|███▎      | 129/383 [00:08<00:14, 17.34it/s] 34%|███▍      | 132/383 [00:08<00:15, 16.52it/s] 34%|███▍      | 131/383 [00:08<00:14, 17.61it/s] 35%|███▌      | 135/383 [00:08<00:15, 16.46it/s] 34%|███▍      | 131/383 [00:08<00:14, 16.90it/s] 35%|███▍      | 134/383 [00:09<00:15, 16.47it/s] 36%|███▌      | 137/383 [00:09<00:14, 16.45it/s] 35%|███▍      | 133/383 [00:09<00:14, 16.89it/s] 35%|███▍      | 133/383 [00:09<00:15, 16.58it/s] 36%|███▌      | 136/383 [00:09<00:15, 16.31it/s] 36%|███▋      | 139/383 [00:09<00:14, 16.36it/s] 35%|███▌      | 135/383 [00:09<00:14, 16.67it/s] 35%|███▌      | 135/383 [00:09<00:15, 16.32it/s] 36%|███▌      | 138/383 [00:09<00:14, 16.42it/s] 37%|███▋      | 141/383 [00:09<00:14, 16.37it/s] 36%|███▌      | 137/383 [00:09<00:14, 16.48it/s] 37%|███▋      | 140/383 [00:09<00:14, 16.42it/s] 36%|███▌      | 137/383 [00:09<00:15, 16.04it/s] 37%|███▋      | 143/383 [00:09<00:14, 16.22it/s] 36%|███▋      | 139/383 [00:09<00:14, 16.57it/s] 37%|███▋      | 142/383 [00:09<00:14, 16.37it/s] 36%|███▋      | 139/383 [00:09<00:15, 15.99it/s] 38%|███▊      | 145/383 [00:09<00:14, 16.15it/s] 37%|███▋      | 141/383 [00:09<00:14, 16.18it/s] 38%|███▊      | 144/383 [00:09<00:14, 16.44it/s] 37%|███▋      | 141/383 [00:09<00:14, 16.18it/s] 38%|███▊      | 147/383 [00:09<00:14, 16.26it/s] 37%|███▋      | 143/383 [00:09<00:14, 16.22it/s] 38%|███▊      | 146/383 [00:09<00:14, 16.40it/s] 37%|███▋      | 143/383 [00:09<00:14, 16.24it/s] 38%|███▊      | 145/383 [00:09<00:14, 16.18it/s] 38%|███▊      | 145/383 [00:09<00:14, 16.08it/s] 39%|███▉      | 149/383 [00:09<00:15, 14.63it/s] 39%|███▊      | 148/383 [00:09<00:15, 14.92it/s] 38%|███▊      | 147/383 [00:09<00:14, 16.19it/s] 38%|███▊      | 147/383 [00:09<00:14, 16.19it/s] 39%|███▉      | 151/383 [00:10<00:18, 12.40it/s] 39%|███▉      | 150/383 [00:10<00:18, 12.58it/s] 39%|███▉      | 149/383 [00:10<00:17, 13.41it/s] 39%|███▉      | 149/383 [00:10<00:17, 13.36it/s] 40%|███▉      | 152/383 [00:10<00:19, 11.75it/s] 40%|███▉      | 153/383 [00:10<00:20, 11.42it/s] 39%|███▉      | 151/383 [00:10<00:19, 12.02it/s] 39%|███▉      | 151/383 [00:10<00:19, 11.84it/s] 40%|███▉      | 153/383 [00:10<00:19, 11.51it/s] 40%|███▉      | 153/383 [00:10<00:21, 10.89it/s] 40%|████      | 154/383 [00:10<00:26,  8.59it/s] 40%|████      | 155/383 [00:10<00:29,  7.65it/s] 40%|████      | 155/383 [00:11<00:30,  7.41it/s] 41%|████      | 156/383 [00:11<00:36,  6.18it/s] 40%|████      | 155/383 [00:11<00:32,  7.11it/s] 41%|████      | 156/383 [00:11<00:36,  6.24it/s] 41%|████      | 156/383 [00:11<00:36,  6.30it/s] 41%|████      | 157/383 [00:11<00:42,  5.26it/s] 41%|████      | 156/383 [00:11<00:36,  6.18it/s] 41%|████      | 157/383 [00:11<00:39,  5.71it/s] 41%|████      | 157/383 [00:11<00:39,  5.73it/s] 41%|████      | 157/383 [00:11<00:41,  5.44it/s] 41%|████▏     | 158/383 [00:11<00:48,  4.62it/s] 41%|████▏     | 158/383 [00:11<00:43,  5.19it/s] 41%|████▏     | 158/383 [00:11<00:44,  5.11it/s] 42%|████▏     | 159/383 [00:11<00:42,  5.28it/s] 41%|████▏     | 158/383 [00:11<00:45,  4.98it/s] 42%|████▏     | 159/383 [00:11<00:50,  4.40it/s] 42%|████▏     | 159/383 [00:12<00:42,  5.24it/s] 42%|████▏     | 160/383 [00:12<00:45,  4.89it/s] 42%|████▏     | 159/383 [00:12<00:48,  4.61it/s] 42%|████▏     | 160/383 [00:12<00:52,  4.21it/s] 42%|████▏     | 160/383 [00:12<00:45,  4.95it/s] 42%|████▏     | 161/383 [00:12<00:44,  5.00it/s] 42%|████▏     | 160/383 [00:12<00:47,  4.72it/s] 42%|████▏     | 161/383 [00:12<00:49,  4.52it/s] 42%|████▏     | 161/383 [00:12<00:46,  4.79it/s] 42%|████▏     | 162/383 [00:12<00:42,  5.21it/s] 42%|████▏     | 162/383 [00:12<00:49,  4.44it/s] 42%|████▏     | 161/383 [00:12<00:49,  4.51it/s] 42%|████▏     | 162/383 [00:12<00:47,  4.62it/s] 43%|████▎     | 163/383 [00:12<00:46,  4.73it/s] 43%|████▎     | 163/383 [00:12<00:47,  4.66it/s] 42%|████▏     | 162/383 [00:12<00:46,  4.71it/s] 43%|████▎     | 163/383 [00:12<00:47,  4.67it/s] 43%|████▎     | 163/383 [00:13<00:46,  4.78it/s] 43%|████▎     | 164/383 [00:13<00:50,  4.38it/s] 43%|████▎     | 164/383 [00:13<00:50,  4.35it/s] 43%|████▎     | 164/383 [00:13<00:49,  4.41it/s] 43%|████▎     | 165/383 [00:13<00:46,  4.68it/s] 43%|████▎     | 164/383 [00:13<00:48,  4.50it/s] 43%|████▎     | 165/383 [00:13<00:48,  4.50it/s] 44%|████▍     | 168/383 [00:13<00:25,  8.53it/s] 43%|████▎     | 165/383 [00:13<00:47,  4.57it/s] 44%|████▍     | 168/383 [00:13<00:25,  8.30it/s] 45%|████▍     | 171/383 [00:13<00:17, 11.82it/s] 44%|████▍     | 168/383 [00:13<00:25,  8.39it/s] 45%|████▍     | 171/383 [00:13<00:17, 11.85it/s] 43%|████▎     | 165/383 [00:13<00:51,  4.22it/s] 45%|████▌     | 173/383 [00:13<00:15, 13.41it/s] 45%|████▍     | 171/383 [00:13<00:17, 11.94it/s] 45%|████▌     | 174/383 [00:13<00:14, 14.56it/s] 44%|████▍     | 168/383 [00:13<00:27,  7.88it/s] 46%|████▌     | 175/383 [00:13<00:15, 13.39it/s] 45%|████▌     | 174/383 [00:13<00:14, 14.69it/s] 45%|████▍     | 171/383 [00:13<00:18, 11.32it/s] 46%|████▌     | 176/383 [00:13<00:14, 14.24it/s] 46%|████▌     | 177/383 [00:13<00:15, 13.18it/s] 46%|████▌     | 176/383 [00:13<00:14, 14.34it/s] 46%|████▋     | 178/383 [00:13<00:14, 14.58it/s] 45%|████▌     | 174/383 [00:13<00:15, 13.63it/s] 47%|████▋     | 179/383 [00:13<00:14, 14.53it/s] 46%|████▋     | 178/383 [00:14<00:13, 14.68it/s] 47%|████▋     | 180/383 [00:14<00:13, 15.59it/s] 47%|████▋     | 181/383 [00:14<00:13, 15.34it/s] 46%|████▌     | 176/383 [00:14<00:15, 13.53it/s] 47%|████▋     | 180/383 [00:14<00:12, 15.72it/s] 48%|████▊     | 182/383 [00:14<00:12, 16.08it/s] 48%|████▊     | 183/383 [00:14<00:12, 16.20it/s] 46%|████▋     | 178/383 [00:14<00:14, 13.99it/s] 48%|████▊     | 182/383 [00:14<00:12, 16.33it/s] 48%|████▊     | 184/383 [00:14<00:11, 16.71it/s] 48%|████▊     | 185/383 [00:14<00:12, 15.97it/s] 47%|████▋     | 180/383 [00:14<00:13, 15.09it/s] 48%|████▊     | 184/383 [00:14<00:11, 16.86it/s] 49%|████▊     | 186/383 [00:14<00:12, 16.06it/s] 48%|████▊     | 182/383 [00:14<00:12, 15.77it/s] 49%|████▉     | 187/383 [00:14<00:12, 15.14it/s] 49%|████▊     | 186/383 [00:14<00:12, 15.58it/s] 49%|████▉     | 188/383 [00:14<00:11, 16.60it/s] 48%|████▊     | 184/383 [00:14<00:12, 16.57it/s] 50%|████▉     | 190/383 [00:14<00:10, 17.59it/s] 49%|████▉     | 188/383 [00:14<00:12, 16.12it/s] 50%|████▉     | 191/383 [00:14<00:10, 18.61it/s] 49%|████▊     | 186/383 [00:14<00:12, 15.43it/s] 50%|█████     | 192/383 [00:14<00:10, 17.92it/s] 50%|████▉     | 191/383 [00:14<00:10, 18.33it/s] 50%|█████     | 193/383 [00:14<00:10, 18.52it/s] 51%|█████     | 194/383 [00:14<00:10, 18.13it/s] 49%|████▉     | 188/383 [00:14<00:12, 15.40it/s] 50%|█████     | 193/383 [00:14<00:10, 18.48it/s] 51%|█████     | 195/383 [00:14<00:10, 18.42it/s] 51%|█████     | 196/383 [00:14<00:10, 18.44it/s] 50%|████▉     | 191/383 [00:14<00:10, 17.81it/s] 51%|█████     | 195/383 [00:14<00:10, 18.54it/s] 51%|█████▏    | 197/383 [00:14<00:09, 18.69it/s] 52%|█████▏    | 199/383 [00:15<00:09, 20.03it/s] 51%|█████▏    | 197/383 [00:15<00:09, 18.85it/s] 50%|█████     | 193/383 [00:15<00:10, 18.11it/s] 52%|█████▏    | 199/383 [00:15<00:09, 19.04it/s] 53%|█████▎    | 202/383 [00:15<00:08, 21.82it/s] 52%|█████▏    | 199/383 [00:15<00:09, 19.12it/s] 51%|█████     | 195/383 [00:15<00:10, 18.34it/s] 53%|█████▎    | 202/383 [00:15<00:08, 21.14it/s] 54%|█████▎    | 205/383 [00:15<00:07, 23.04it/s] 51%|█████▏    | 197/383 [00:15<00:10, 18.49it/s] 53%|█████▎    | 202/383 [00:15<00:08, 21.22it/s] 54%|█████▎    | 205/383 [00:15<00:07, 22.78it/s] 54%|█████▍    | 208/383 [00:15<00:07, 23.88it/s] 54%|█████▎    | 205/383 [00:15<00:07, 22.51it/s] 54%|█████▍    | 208/383 [00:15<00:07, 23.63it/s] 52%|█████▏    | 200/383 [00:15<00:09, 19.63it/s] 55%|█████▌    | 211/383 [00:15<00:07, 24.53it/s] 54%|█████▍    | 208/383 [00:15<00:07, 23.48it/s] 55%|█████▌    | 211/383 [00:15<00:07, 24.32it/s] 53%|█████▎    | 203/383 [00:15<00:08, 21.43it/s] 56%|█████▌    | 214/383 [00:15<00:06, 24.30it/s] 55%|█████▌    | 211/383 [00:15<00:07, 24.23it/s] 56%|█████▌    | 214/383 [00:15<00:06, 24.74it/s] 54%|█████▍    | 206/383 [00:15<00:07, 22.78it/s] 57%|█████▋    | 217/383 [00:15<00:06, 24.79it/s] 56%|█████▌    | 214/383 [00:15<00:06, 24.67it/s] 55%|█████▍    | 209/383 [00:15<00:07, 23.58it/s] 57%|█████▋    | 217/383 [00:15<00:06, 24.33it/s] 57%|█████▋    | 220/383 [00:15<00:06, 24.99it/s] 57%|█████▋    | 217/383 [00:15<00:06, 24.86it/s] 55%|█████▌    | 212/383 [00:15<00:07, 24.10it/s] 57%|█████▋    | 220/383 [00:15<00:06, 24.83it/s] 57%|█████▋    | 220/383 [00:15<00:06, 25.15it/s] 56%|█████▌    | 215/383 [00:16<00:06, 24.59it/s] 58%|█████▊    | 223/383 [00:16<00:07, 22.17it/s] 58%|█████▊    | 223/383 [00:16<00:07, 22.79it/s] 57%|█████▋    | 218/383 [00:16<00:06, 24.94it/s] 58%|█████▊    | 223/383 [00:16<00:07, 22.15it/s] 59%|█████▉    | 226/383 [00:16<00:07, 20.43it/s] 59%|█████▉    | 226/383 [00:16<00:07, 20.63it/s] 58%|█████▊    | 221/383 [00:16<00:06, 23.78it/s] 59%|█████▉    | 226/383 [00:16<00:07, 20.14it/s] 60%|█████▉    | 229/383 [00:16<00:07, 19.25it/s] 60%|█████▉    | 229/383 [00:16<00:08, 19.09it/s] 58%|█████▊    | 224/383 [00:16<00:07, 20.89it/s] 60%|█████▉    | 229/383 [00:16<00:08, 18.92it/s] 60%|██████    | 231/383 [00:16<00:08, 17.31it/s] 60%|██████    | 231/383 [00:16<00:08, 17.83it/s] 59%|█████▉    | 227/383 [00:16<00:08, 19.23it/s] 60%|██████    | 231/383 [00:16<00:08, 18.02it/s] 61%|██████    | 233/383 [00:16<00:09, 16.35it/s] 61%|██████    | 233/383 [00:16<00:09, 15.97it/s] 60%|██████    | 230/383 [00:16<00:08, 18.43it/s] 61%|██████    | 233/383 [00:16<00:09, 16.27it/s] 61%|██████▏   | 235/383 [00:16<00:09, 15.32it/s] 61%|██████▏   | 235/383 [00:16<00:09, 15.02it/s] 61%|██████    | 232/383 [00:16<00:08, 16.79it/s] 61%|██████▏   | 235/383 [00:16<00:09, 14.83it/s] 62%|██████▏   | 237/383 [00:17<00:10, 14.47it/s] 62%|██████▏   | 237/383 [00:17<00:10, 14.22it/s] 61%|██████    | 234/383 [00:17<00:09, 15.76it/s] 62%|██████▏   | 237/383 [00:17<00:10, 14.22it/s] 62%|██████▏   | 239/383 [00:17<00:10, 13.98it/s] 62%|██████▏   | 239/383 [00:17<00:10, 13.69it/s] 62%|██████▏   | 236/383 [00:17<00:09, 14.99it/s] 62%|██████▏   | 239/383 [00:17<00:10, 13.82it/s] 63%|██████▎   | 241/383 [00:17<00:10, 13.64it/s] 63%|██████▎   | 241/383 [00:17<00:10, 13.31it/s] 62%|██████▏   | 238/383 [00:17<00:10, 14.37it/s] 63%|██████▎   | 241/383 [00:17<00:10, 13.34it/s] 63%|██████▎   | 243/383 [00:17<00:10, 13.29it/s] 63%|██████▎   | 243/383 [00:17<00:10, 13.18it/s] 63%|██████▎   | 240/383 [00:17<00:10, 14.06it/s] 63%|██████▎   | 243/383 [00:17<00:10, 13.20it/s] 64%|██████▍   | 245/383 [00:17<00:10, 13.27it/s] 64%|██████▍   | 245/383 [00:17<00:10, 12.96it/s] 63%|██████▎   | 242/383 [00:17<00:10, 13.70it/s] 64%|██████▍   | 245/383 [00:17<00:10, 13.06it/s] 64%|██████▍   | 247/383 [00:17<00:10, 13.27it/s] 64%|██████▍   | 247/383 [00:17<00:10, 13.08it/s] 64%|██████▎   | 244/383 [00:17<00:10, 13.61it/s] 64%|██████▍   | 247/383 [00:17<00:10, 12.86it/s] 65%|██████▌   | 249/383 [00:17<00:10, 12.82it/s] 65%|██████▌   | 249/383 [00:17<00:10, 13.01it/s] 64%|██████▍   | 246/383 [00:18<00:10, 13.29it/s] 65%|██████▌   | 249/383 [00:18<00:10, 12.86it/s] 66%|██████▌   | 251/383 [00:18<00:10, 12.97it/s] 66%|██████▌   | 251/383 [00:18<00:10, 12.63it/s] 65%|██████▍   | 248/383 [00:18<00:10, 13.15it/s] 66%|██████▌   | 251/383 [00:18<00:10, 12.96it/s] 66%|██████▌   | 253/383 [00:18<00:10, 12.95it/s] 66%|██████▌   | 253/383 [00:18<00:10, 12.69it/s] 65%|██████▌   | 250/383 [00:18<00:10, 13.04it/s] 66%|██████▌   | 253/383 [00:18<00:10, 12.67it/s] 67%|██████▋   | 255/383 [00:18<00:09, 13.05it/s] 67%|██████▋   | 255/383 [00:18<00:09, 12.91it/s] 66%|██████▌   | 252/383 [00:18<00:10, 12.88it/s] 67%|██████▋   | 255/383 [00:18<00:10, 12.64it/s] 67%|██████▋   | 257/383 [00:18<00:09, 13.15it/s] 67%|██████▋   | 257/383 [00:18<00:09, 13.10it/s] 66%|██████▋   | 254/383 [00:18<00:10, 12.77it/s] 67%|██████▋   | 257/383 [00:18<00:09, 12.94it/s] 68%|██████▊   | 259/383 [00:18<00:09, 13.27it/s] 68%|██████▊   | 259/383 [00:18<00:09, 13.36it/s] 67%|██████▋   | 256/383 [00:18<00:09, 12.88it/s] 68%|██████▊   | 261/383 [00:18<00:09, 13.47it/s] 68%|██████▊   | 259/383 [00:18<00:09, 13.06it/s] 68%|██████▊   | 261/383 [00:18<00:09, 13.23it/s] 67%|██████▋   | 258/383 [00:18<00:09, 12.98it/s] 69%|██████▊   | 263/383 [00:19<00:08, 13.45it/s] 68%|██████▊   | 261/383 [00:19<00:09, 13.09it/s] 69%|██████▊   | 263/383 [00:19<00:08, 13.43it/s] 68%|██████▊   | 260/383 [00:19<00:09, 13.15it/s] 69%|██████▉   | 266/383 [00:19<00:07, 16.03it/s] 69%|██████▉   | 266/383 [00:19<00:07, 16.40it/s] 69%|██████▊   | 263/383 [00:19<00:09, 13.17it/s] 70%|███████   | 269/383 [00:19<00:06, 18.54it/s] 70%|███████   | 269/383 [00:19<00:06, 18.77it/s] 68%|██████▊   | 262/383 [00:19<00:09, 13.29it/s] 69%|██████▉   | 266/383 [00:19<00:07, 15.70it/s] 69%|██████▉   | 264/383 [00:19<00:08, 14.40it/s] 71%|███████   | 272/383 [00:19<00:05, 19.77it/s] 70%|███████   | 269/383 [00:19<00:06, 18.04it/s] 71%|███████   | 272/383 [00:19<00:05, 19.01it/s] 70%|██████▉   | 267/383 [00:19<00:07, 16.47it/s] 71%|███████   | 272/383 [00:19<00:05, 19.10it/s] 72%|███████▏  | 274/383 [00:19<00:06, 17.68it/s] 72%|███████▏  | 275/383 [00:19<00:05, 18.50it/s] 70%|███████   | 270/383 [00:19<00:05, 18.85it/s] 72%|███████▏  | 274/383 [00:19<00:06, 17.84it/s] 72%|███████▏  | 277/383 [00:19<00:05, 17.72it/s] 72%|███████▏  | 276/383 [00:19<00:06, 16.51it/s] 71%|███████▏  | 273/383 [00:19<00:05, 19.44it/s] 72%|███████▏  | 276/383 [00:19<00:06, 17.41it/s] 73%|███████▎  | 278/383 [00:19<00:06, 16.33it/s] 73%|███████▎  | 279/383 [00:19<00:06, 16.87it/s] 72%|███████▏  | 275/383 [00:19<00:05, 18.48it/s] 73%|███████▎  | 278/383 [00:19<00:06, 16.56it/s] 73%|███████▎  | 280/383 [00:19<00:06, 16.17it/s] 73%|███████▎  | 281/383 [00:19<00:06, 16.18it/s] 72%|███████▏  | 277/383 [00:20<00:05, 17.77it/s] 73%|███████▎  | 280/383 [00:20<00:06, 16.27it/s] 74%|███████▎  | 282/383 [00:20<00:07, 13.86it/s] 74%|███████▍  | 283/383 [00:20<00:07, 14.26it/s] 73%|███████▎  | 279/383 [00:20<00:06, 16.44it/s] 74%|███████▎  | 282/383 [00:20<00:07, 14.12it/s] 74%|███████▍  | 284/383 [00:20<00:07, 12.93it/s] 74%|███████▍  | 285/383 [00:20<00:07, 12.89it/s] 73%|███████▎  | 281/383 [00:20<00:06, 14.69it/s] 74%|███████▍  | 284/383 [00:20<00:07, 12.77it/s] 75%|███████▍  | 286/383 [00:20<00:07, 12.21it/s] 75%|███████▍  | 287/383 [00:20<00:07, 12.08it/s] 74%|███████▍  | 283/383 [00:20<00:07, 13.06it/s] 75%|███████▍  | 286/383 [00:20<00:08, 12.12it/s] 75%|███████▌  | 288/383 [00:20<00:07, 11.90it/s] 74%|███████▍  | 285/383 [00:20<00:07, 12.68it/s] 75%|███████▌  | 288/383 [00:20<00:08, 11.38it/s] 75%|███████▍  | 287/383 [00:20<00:07, 12.34it/s] 75%|███████▌  | 289/383 [00:20<00:10,  8.76it/s] 76%|███████▌  | 290/383 [00:21<00:12,  7.74it/s] 75%|███████▌  | 289/383 [00:21<00:10,  9.27it/s] 76%|███████▌  | 290/383 [00:21<00:12,  7.31it/s] 76%|███████▌  | 291/383 [00:21<00:13,  6.62it/s] 76%|███████▌  | 291/383 [00:21<00:14,  6.29it/s] 76%|███████▌  | 291/383 [00:21<00:14,  6.22it/s] 76%|███████▌  | 292/383 [00:21<00:15,  5.83it/s] 76%|███████▌  | 292/383 [00:21<00:16,  5.59it/s] 76%|███████▌  | 291/383 [00:21<00:14,  6.39it/s] 76%|███████▌  | 292/383 [00:21<00:17,  5.33it/s] 77%|███████▋  | 293/383 [00:21<00:17,  5.12it/s] 77%|███████▋  | 293/383 [00:22<00:18,  4.91it/s] 76%|███████▌  | 292/383 [00:22<00:16,  5.62it/s] 77%|███████▋  | 293/383 [00:22<00:18,  4.91it/s] 77%|███████▋  | 294/383 [00:22<00:18,  4.79it/s] 77%|███████▋  | 294/383 [00:22<00:19,  4.59it/s] 77%|███████▋  | 293/383 [00:22<00:17,  5.10it/s] 77%|███████▋  | 295/383 [00:22<00:18,  4.71it/s] 77%|███████▋  | 294/383 [00:22<00:19,  4.49it/s] 77%|███████▋  | 295/383 [00:22<00:19,  4.42it/s] 77%|███████▋  | 294/383 [00:22<00:18,  4.73it/s] 77%|███████▋  | 296/383 [00:22<00:20,  4.27it/s] 77%|███████▋  | 295/383 [00:22<00:20,  4.20it/s] 77%|███████▋  | 296/383 [00:22<00:20,  4.18it/s] 77%|███████▋  | 295/383 [00:22<00:19,  4.54it/s] 78%|███████▊  | 297/383 [00:22<00:19,  4.31it/s] 77%|███████▋  | 296/383 [00:23<00:21,  4.08it/s] 77%|███████▋  | 296/383 [00:23<00:20,  4.26it/s] 78%|███████▊  | 297/383 [00:23<00:21,  3.93it/s] 78%|███████▊  | 298/383 [00:23<00:20,  4.10it/s] 78%|███████▊  | 297/383 [00:23<00:21,  4.08it/s] 78%|███████▊  | 298/383 [00:23<00:21,  4.03it/s] 78%|███████▊  | 297/383 [00:23<00:21,  4.06it/s] 78%|███████▊  | 298/383 [00:23<00:19,  4.28it/s] 78%|███████▊  | 299/383 [00:23<00:21,  3.95it/s] 78%|███████▊  | 299/383 [00:23<00:20,  4.00it/s] 78%|███████▊  | 298/383 [00:23<00:22,  3.80it/s] 78%|███████▊  | 300/383 [00:23<00:20,  4.06it/s] 78%|███████▊  | 299/383 [00:23<00:20,  4.06it/s] 78%|███████▊  | 300/383 [00:23<00:21,  3.85it/s] 78%|███████▊  | 299/383 [00:23<00:22,  3.78it/s] 79%|███████▊  | 301/383 [00:24<00:20,  3.92it/s] 78%|███████▊  | 300/383 [00:24<00:21,  3.89it/s] 79%|███████▊  | 301/383 [00:24<00:21,  3.78it/s] 78%|███████▊  | 300/383 [00:24<00:22,  3.71it/s] 79%|███████▊  | 301/383 [00:24<00:20,  4.03it/s] 79%|███████▉  | 302/383 [00:24<00:21,  3.85it/s] 79%|███████▉  | 302/383 [00:24<00:21,  3.74it/s] 79%|███████▊  | 301/383 [00:24<00:22,  3.65it/s] 79%|███████▉  | 303/383 [00:24<00:21,  3.80it/s] 79%|███████▉  | 302/383 [00:24<00:21,  3.82it/s] 79%|███████▉  | 303/383 [00:24<00:20,  3.92it/s] 79%|███████▉  | 303/383 [00:24<00:20,  3.97it/s] 79%|███████▉  | 302/383 [00:24<00:22,  3.62it/s] 79%|███████▉  | 304/383 [00:24<00:21,  3.68it/s] 79%|███████▉  | 304/383 [00:24<00:21,  3.75it/s] 79%|███████▉  | 304/383 [00:25<00:19,  4.00it/s] 80%|███████▉  | 305/383 [00:25<00:20,  3.83it/s] 79%|███████▉  | 303/383 [00:25<00:22,  3.63it/s] 80%|███████▉  | 305/383 [00:25<00:21,  3.62it/s] 80%|███████▉  | 306/383 [00:25<00:18,  4.09it/s] 80%|███████▉  | 305/383 [00:25<00:20,  3.85it/s] 79%|███████▉  | 304/383 [00:25<00:21,  3.62it/s] 80%|███████▉  | 306/383 [00:25<00:21,  3.67it/s] 80%|████████  | 307/383 [00:25<00:19,  3.95it/s] 80%|███████▉  | 306/383 [00:25<00:20,  3.81it/s] 80%|███████▉  | 305/383 [00:25<00:21,  3.63it/s] 80%|████████  | 307/383 [00:25<00:20,  3.64it/s] 80%|████████  | 308/383 [00:25<00:19,  3.81it/s] 80%|████████  | 307/383 [00:25<00:20,  3.73it/s] 80%|███████▉  | 306/383 [00:25<00:21,  3.62it/s] 80%|████████  | 308/383 [00:26<00:20,  3.66it/s] 80%|████████  | 308/383 [00:26<00:19,  3.87it/s] 81%|████████  | 309/383 [00:26<00:19,  3.85it/s] 80%|████████  | 307/383 [00:26<00:21,  3.59it/s] 81%|████████  | 309/383 [00:26<00:19,  3.75it/s] 81%|████████  | 309/383 [00:26<00:19,  3.79it/s] 81%|████████  | 310/383 [00:26<00:19,  3.70it/s] 80%|████████  | 308/383 [00:26<00:20,  3.57it/s] 81%|████████  | 310/383 [00:26<00:18,  3.85it/s] 81%|████████  | 310/383 [00:26<00:19,  3.71it/s] 81%|████████  | 311/383 [00:26<00:19,  3.64it/s] 81%|████████  | 309/383 [00:26<00:19,  3.80it/s] 81%|████████  | 311/383 [00:26<00:18,  3.82it/s] 81%|████████  | 311/383 [00:26<00:19,  3.69it/s] 81%|████████▏ | 312/383 [00:26<00:19,  3.63it/s] 81%|████████  | 310/383 [00:26<00:18,  3.90it/s] 81%|████████▏ | 312/383 [00:27<00:18,  3.85it/s] 81%|████████▏ | 312/383 [00:27<00:19,  3.64it/s] 82%|████████▏ | 313/383 [00:27<00:19,  3.64it/s] 81%|████████  | 311/383 [00:27<00:18,  3.81it/s] 82%|████████▏ | 313/383 [00:27<00:19,  3.66it/s] 82%|████████▏ | 313/383 [00:27<00:18,  3.69it/s] 81%|████████▏ | 312/383 [00:27<00:18,  3.88it/s] 82%|████████▏ | 314/383 [00:27<00:19,  3.61it/s] 82%|████████▏ | 314/383 [00:27<00:18,  3.68it/s] 82%|████████▏ | 314/383 [00:27<00:18,  3.79it/s] 82%|████████▏ | 313/383 [00:27<00:17,  3.91it/s] 82%|████████▏ | 315/383 [00:27<00:19,  3.51it/s] 82%|████████▏ | 315/383 [00:27<00:18,  3.67it/s] 82%|████████▏ | 315/383 [00:27<00:17,  3.84it/s] 82%|████████▏ | 314/383 [00:27<00:17,  3.97it/s] 83%|████████▎ | 316/383 [00:28<00:18,  3.63it/s] 82%|████████▏ | 315/383 [00:28<00:16,  4.09it/s] 83%|████████▎ | 316/383 [00:28<00:18,  3.71it/s] 83%|████████▎ | 316/383 [00:28<00:17,  3.81it/s] 83%|████████▎ | 317/383 [00:28<00:18,  3.64it/s] 83%|████████▎ | 317/383 [00:28<00:17,  3.70it/s] 83%|████████▎ | 316/383 [00:28<00:17,  3.80it/s] 83%|████████▎ | 317/383 [00:28<00:18,  3.63it/s] 83%|████████▎ | 318/383 [00:28<00:18,  3.51it/s] 83%|████████▎ | 317/383 [00:28<00:17,  3.75it/s] 83%|████████▎ | 318/383 [00:28<00:18,  3.55it/s] 83%|████████▎ | 318/383 [00:28<00:17,  3.61it/s] 83%|████████▎ | 319/383 [00:28<00:17,  3.70it/s] 83%|████████▎ | 319/383 [00:29<00:17,  3.68it/s] 83%|████████▎ | 318/383 [00:29<00:17,  3.71it/s] 83%|████████▎ | 319/383 [00:29<00:17,  3.67it/s] 84%|████████▎ | 320/383 [00:29<00:17,  3.69it/s] 84%|████████▎ | 320/383 [00:29<00:16,  3.76it/s] 83%|████████▎ | 319/383 [00:29<00:17,  3.71it/s] 84%|████████▎ | 320/383 [00:29<00:17,  3.69it/s] 84%|████████▍ | 321/383 [00:29<00:17,  3.60it/s] 84%|████████▍ | 321/383 [00:29<00:16,  3.77it/s] 84%|████████▎ | 320/383 [00:29<00:17,  3.68it/s] 84%|████████▍ | 321/383 [00:29<00:16,  3.73it/s] 84%|████████▍ | 322/383 [00:29<00:16,  3.63it/s] 84%|████████▍ | 322/383 [00:29<00:15,  3.85it/s] 84%|████████▍ | 321/383 [00:29<00:17,  3.59it/s] 84%|████████▍ | 322/383 [00:29<00:16,  3.59it/s] 84%|████████▍ | 323/383 [00:29<00:15,  3.75it/s] 84%|████████▍ | 323/383 [00:30<00:15,  3.98it/s] 84%|████████▍ | 322/383 [00:30<00:16,  3.78it/s] 84%|████████▍ | 323/383 [00:30<00:16,  3.73it/s] 85%|████████▍ | 324/383 [00:30<00:15,  3.71it/s] 85%|████████▍ | 324/383 [00:30<00:15,  3.77it/s] 84%|████████▍ | 323/383 [00:30<00:16,  3.72it/s] 85%|████████▍ | 324/383 [00:30<00:15,  3.72it/s] 85%|████████▍ | 325/383 [00:30<00:15,  3.80it/s] 85%|████████▍ | 325/383 [00:30<00:15,  3.85it/s] 85%|████████▍ | 325/383 [00:30<00:15,  3.82it/s] 85%|████████▍ | 324/383 [00:30<00:16,  3.68it/s] 85%|████████▌ | 326/383 [00:30<00:14,  3.96it/s] 85%|████████▌ | 326/383 [00:30<00:15,  3.75it/s] 85%|████████▌ | 326/383 [00:30<00:14,  3.85it/s] 85%|████████▍ | 325/383 [00:30<00:15,  3.67it/s] 85%|████████▌ | 327/383 [00:30<00:14,  3.89it/s] 85%|████████▌ | 327/383 [00:31<00:14,  3.74it/s] 85%|████████▌ | 326/383 [00:31<00:14,  3.82it/s] 85%|████████▌ | 327/383 [00:31<00:15,  3.67it/s] 86%|████████▌ | 328/383 [00:31<00:14,  3.79it/s] 86%|████████▌ | 328/383 [00:31<00:14,  3.68it/s] 85%|████████▌ | 327/383 [00:31<00:14,  3.89it/s] 86%|████████▌ | 328/383 [00:31<00:15,  3.64it/s] 86%|████████▌ | 329/383 [00:31<00:14,  3.80it/s] 86%|████████▌ | 329/383 [00:31<00:14,  3.74it/s] 86%|████████▌ | 328/383 [00:31<00:14,  3.88it/s] 86%|████████▌ | 329/383 [00:31<00:14,  3.76it/s] 86%|████████▌ | 330/383 [00:31<00:14,  3.74it/s] 86%|████████▌ | 330/383 [00:31<00:14,  3.75it/s] 86%|████████▌ | 329/383 [00:31<00:13,  3.89it/s] 86%|████████▋ | 331/383 [00:31<00:12,  4.25it/s] 86%|████████▌ | 330/383 [00:32<00:14,  3.64it/s] 87%|████████▋ | 332/383 [00:32<00:10,  4.84it/s] 86%|████████▌ | 330/383 [00:32<00:12,  4.13it/s] 86%|████████▋ | 331/383 [00:32<00:12,  4.27it/s] 86%|████████▋ | 331/383 [00:32<00:13,  3.72it/s] 87%|████████▋ | 333/383 [00:32<00:09,  5.33it/s] 87%|████████▋ | 332/383 [00:32<00:12,  4.22it/s] 87%|████████▋ | 332/383 [00:32<00:11,  4.60it/s] 87%|████████▋ | 334/383 [00:32<00:08,  5.62it/s] 86%|████████▋ | 331/383 [00:32<00:13,  3.99it/s] 87%|████████▋ | 333/383 [00:32<00:10,  4.70it/s] 87%|████████▋ | 333/383 [00:32<00:10,  4.85it/s] 87%|████████▋ | 335/383 [00:32<00:08,  5.81it/s] 87%|████████▋ | 332/383 [00:32<00:11,  4.37it/s] 87%|████████▋ | 334/383 [00:32<00:09,  5.06it/s] 88%|████████▊ | 336/383 [00:32<00:07,  5.94it/s] 87%|████████▋ | 334/383 [00:32<00:09,  5.09it/s] 87%|████████▋ | 333/383 [00:32<00:10,  4.96it/s] 87%|████████▋ | 335/383 [00:32<00:09,  5.24it/s] 87%|████████▋ | 335/383 [00:32<00:08,  5.63it/s] 88%|████████▊ | 337/383 [00:32<00:07,  6.11it/s] 87%|████████▋ | 334/383 [00:32<00:09,  5.43it/s] 88%|████████▊ | 336/383 [00:33<00:08,  5.73it/s] 88%|████████▊ | 336/383 [00:33<00:08,  5.25it/s] 88%|████████▊ | 338/383 [00:33<00:07,  5.92it/s] 87%|████████▋ | 335/383 [00:33<00:08,  5.49it/s] 88%|████████▊ | 337/383 [00:33<00:07,  5.89it/s] 88%|████████▊ | 336/383 [00:33<00:07,  5.89it/s] 89%|████████▉ | 340/383 [00:33<00:05,  7.69it/s] 88%|████████▊ | 337/383 [00:33<00:08,  5.39it/s] 88%|████████▊ | 338/383 [00:33<00:07,  6.01it/s] 89%|████████▉ | 342/383 [00:33<00:04,  9.47it/s] 88%|████████▊ | 337/383 [00:33<00:07,  6.11it/s] 88%|████████▊ | 338/383 [00:33<00:08,  5.49it/s] 89%|████████▉ | 340/383 [00:33<00:05,  8.40it/s] 90%|████████▉ | 344/383 [00:33<00:03, 10.79it/s] 88%|████████▊ | 338/383 [00:33<00:07,  6.17it/s] 89%|████████▊ | 339/383 [00:33<00:07,  5.72it/s] 89%|████████▉ | 342/383 [00:33<00:04,  9.83it/s] 90%|█████████ | 346/383 [00:33<00:03, 11.54it/s] 89%|████████▉ | 340/383 [00:33<00:05,  8.27it/s] 89%|████████▉ | 341/383 [00:33<00:05,  7.89it/s] 90%|████████▉ | 344/383 [00:33<00:03, 11.11it/s] 91%|█████████ | 348/383 [00:33<00:02, 12.47it/s] 89%|████████▉ | 342/383 [00:33<00:04,  9.96it/s] 90%|████████▉ | 343/383 [00:33<00:04,  9.60it/s] 90%|█████████ | 346/383 [00:33<00:03, 12.21it/s] 91%|█████████▏| 350/383 [00:33<00:02, 13.35it/s] 90%|█████████ | 345/383 [00:33<00:03, 11.03it/s] 90%|████████▉ | 344/383 [00:33<00:03, 10.54it/s] 91%|█████████ | 348/383 [00:34<00:02, 12.99it/s] 92%|█████████▏| 352/383 [00:34<00:02, 13.38it/s] 91%|█████████ | 347/383 [00:34<00:02, 12.16it/s] 90%|█████████ | 346/383 [00:34<00:03, 11.92it/s] 91%|█████████▏| 350/383 [00:34<00:02, 13.07it/s] 92%|█████████▏| 354/383 [00:34<00:02, 13.70it/s] 91%|█████████ | 348/383 [00:34<00:02, 12.87it/s] 91%|█████████ | 349/383 [00:34<00:02, 12.88it/s] 92%|█████████▏| 352/383 [00:34<00:02, 13.79it/s] 93%|█████████▎| 356/383 [00:34<00:01, 14.47it/s] 91%|█████████▏| 350/383 [00:34<00:02, 13.40it/s] 92%|█████████▏| 351/383 [00:34<00:02, 13.28it/s] 93%|█████████▎| 358/383 [00:34<00:01, 15.28it/s] 92%|█████████▏| 354/383 [00:34<00:02, 14.08it/s] 92%|█████████▏| 352/383 [00:34<00:02, 13.68it/s] 92%|█████████▏| 353/383 [00:34<00:02, 13.26it/s] 93%|█████████▎| 356/383 [00:34<00:01, 14.37it/s] 92%|█████████▏| 354/383 [00:34<00:02, 13.77it/s] 93%|█████████▎| 355/383 [00:34<00:02, 13.58it/s] 93%|█████████▎| 358/383 [00:34<00:01, 14.76it/s] 93%|█████████▎| 357/383 [00:34<00:01, 14.40it/s] 93%|█████████▎| 356/383 [00:34<00:01, 13.64it/s] 94%|█████████▍| 360/383 [00:34<00:02,  9.74it/s] 94%|█████████▎| 359/383 [00:34<00:01, 15.45it/s] 93%|█████████▎| 358/383 [00:34<00:01, 14.88it/s] 94%|█████████▍| 360/383 [00:35<00:02,  9.97it/s] 95%|█████████▍| 362/383 [00:35<00:02,  8.33it/s] 94%|█████████▍| 360/383 [00:35<00:01, 11.86it/s] 94%|█████████▍| 361/383 [00:35<00:02, 10.16it/s] 95%|█████████▍| 362/383 [00:35<00:02,  8.17it/s] 95%|█████████▍| 362/383 [00:35<00:02,  9.10it/s] 95%|█████████▌| 364/383 [00:35<00:02,  7.21it/s] 95%|█████████▍| 363/383 [00:35<00:02,  8.47it/s] 95%|█████████▌| 365/383 [00:35<00:02,  6.86it/s] 95%|█████████▌| 364/383 [00:35<00:02,  7.14it/s] 96%|█████████▌| 367/383 [00:35<00:01,  8.69it/s] 95%|█████████▌| 364/383 [00:35<00:02,  7.82it/s] 96%|█████████▋| 369/383 [00:35<00:01, 10.48it/s] 95%|█████████▌| 365/383 [00:35<00:02,  7.44it/s] 95%|█████████▌| 365/383 [00:35<00:02,  6.85it/s] 97%|█████████▋| 371/383 [00:35<00:00, 12.16it/s] 95%|█████████▌| 365/383 [00:36<00:02,  7.36it/s] 96%|█████████▌| 367/383 [00:36<00:01,  8.61it/s] 96%|█████████▌| 366/383 [00:36<00:02,  7.38it/s] 97%|█████████▋| 373/383 [00:36<00:00, 13.67it/s] 96%|█████████▌| 367/383 [00:36<00:01,  9.24it/s] 96%|█████████▋| 369/383 [00:36<00:01, 10.40it/s] 96%|█████████▌| 368/383 [00:36<00:01,  9.19it/s] 98%|█████████▊| 375/383 [00:36<00:00, 14.64it/s] 96%|█████████▋| 369/383 [00:36<00:01, 10.96it/s] 97%|█████████▋| 371/383 [00:36<00:00, 12.04it/s] 97%|█████████▋| 370/383 [00:36<00:01, 10.89it/s] 98%|█████████▊| 377/383 [00:36<00:00, 15.82it/s] 97%|█████████▋| 371/383 [00:36<00:00, 12.45it/s] 97%|█████████▋| 373/383 [00:36<00:00, 13.42it/s] 97%|█████████▋| 372/383 [00:36<00:00, 12.34it/s] 99%|█████████▉| 380/383 [00:36<00:00, 18.26it/s] 97%|█████████▋| 373/383 [00:36<00:00, 13.84it/s] 98%|█████████▊| 375/383 [00:36<00:00, 14.88it/s] 98%|█████████▊| 374/383 [00:36<00:00, 13.79it/s]100%|██████████| 383/383 [00:36<00:00, 19.32it/s]100%|██████████| 383/383 [00:36<00:00, 10.47it/s]
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
 98%|█████████▊| 376/383 [00:36<00:00, 15.79it/s]/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
 99%|█████████▊| 378/383 [00:36<00:00, 16.92it/s]/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
 98%|█████████▊| 377/383 [00:36<00:00, 15.74it/s]/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
 99%|█████████▉| 379/383 [00:36<00:00, 18.39it/s] 99%|█████████▉| 381/383 [00:36<00:00, 19.49it/s] 99%|█████████▉| 380/383 [00:36<00:00, 17.69it/s]100%|█████████▉| 382/383 [00:36<00:00, 20.60it/s]100%|██████████| 383/383 [00:36<00:00, 10.40it/s]
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
100%|██████████| 383/383 [00:36<00:00, 10.38it/s]
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
100%|██████████| 383/383 [00:36<00:00, 18.62it/s]100%|██████████| 383/383 [00:36<00:00, 10.37it/s]
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
***** eval metrics *****
  epoch                   =      14.85
  eval_loss               =     2.2518
  eval_runtime            = 0:00:14.75
  eval_samples_per_second =     69.384
  eval_steps_per_second   =     17.346
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/bagus/anaconda3/envs/qlora_1/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
[2023-08-08 00:07:44,101] [INFO] [launch.py:347:main] Process 786772 exits successfully.
[2023-08-08 00:07:44,101] [INFO] [launch.py:347:main] Process 786773 exits successfully.
[2023-08-08 00:07:45,101] [INFO] [launch.py:347:main] Process 786770 exits successfully.
[2023-08-08 00:07:45,101] [INFO] [launch.py:347:main] Process 786771 exits successfully.
